[
  {
    "query": "Stanford University generative AI policy",
    "title": "Generative AI Policy Guidance | Office of Community Standards",
    "url": "https://communitystandards.stanford.edu/generative-ai-policy-guidance",
    "text": "Generative AI Policy Guidance | Office of Community Standards\n[Skip to main content] [Skip to secondary navigation] \n[Stanford University(link is external)] \n[\nStanford\nOffice of Community Standards\n] \nSearch this siteSubmit Search\n![Chatbot Chat with AI. Credit: serba011d64_201 / iStock] \n# Generative AI Policy Guidance\nMain content start\nGuidance adopted on February 16, 2023\n![Decorative Cardinal Red accent line.] \n## Honor Code Implications of Generative AI Tools\nThe Board on Conduct Affairs (BCA) has been asked to address the Honor Code implications of generative AI tools such as ChatGPT, Bard, DALL-E, and Stable Diffusion. These are novel tools, and both students and instructors have been experimenting with their use in academic settings.\nWhile these tools have applications that foster student learning and understanding, these tools can also be used in ways that bypass key learning objectives.\nTo give sufficient space for instructors to explore uses of generative AI tools in their courses, and to set clear guidelines to students about what uses are and are not consistent with the Stanford Honor Code, the BCA has set forth the following policy guidance regarding generative AI in the context of coursework:\n*Absent a clear statement from a course instructor, use of or consultation with generative AI shall be treated analogously to assistance from another person. In particular, using generative AI tools to substantially complete an assignment or exam (e.g. by entering exam or assignment questions) is not permitted. Students should acknowledge the use of generative AI (other than incidental use) and default to disclosing such assistance when in doubt.*\n*Individual course instructors are free to set their own policies regulating the use of generative AI tools in their courses, including allowing or disallowing some or all uses of such tools. Course instructors should set such policies in their course syllabi and clearly communicate such policies to students. Students who are unsure of policies regarding generative AI tools are encouraged to ask their instructors for clarification.*\nThe BCA will continue to monitor developments in these tools and their use in academic settings and may update this guidance. Members of the community are encouraged to contact the BCA to provide input, suggestions, and comments on this policy.\nNOTE: As part of the BCA’s guidance on clear communication of a course’s generative AI policy, OCS recommends course instructors provide clear advance notice that they may use detection software to review work submitted for use of generative AI.Other helpful information for faculty and course assistants can be found[HERE].\n![Decorative Cardinal Red accent line.] \nIf you are in doubt about whether a generative AI source (or any source) is permitted aid in the context of a particular assignment, talk with the instructor.\n![] \nBack to Top"
  },
  {
    "query": "Stanford University generative AI policy",
    "title": "Course Policies on Generative AI Use - Teaching and Learning Hub",
    "url": "https://tlhub.stanford.edu/docs/course-policies-on-generative-ai-use/",
    "text": "Course Policies on Generative AI Use - Teaching and Learning Hub\n[Skip to content] \nStanford University\n**\nFor immediate technical assistance during class time, call the GSB classroom support hotline: 1(650)736-3342\nOr submit a[support ticket] to Stanford Service-Now\n[![Teaching and Learning Hub, Stanford GSB]] \n[**MenuClose] \n# Course Policies on Generative AI Use\n![] \n# Engaging Students\n9\n* **[Pre-recording Videos to Share with Your Students] \n* **[Handout: Zoom Instructions for Remote Guest Speakers] \n* **[Faculty Tips: Teaching with Presence and Connection] \n* **[Tools and Technology] \n* **[Asynchronous Course Discussions] \n* **[Conducting Role-Plays] \n* **[Using Polls in the GSB Classroom] \n* **[Building Classroom Community] \n* **[Leading Effective Discussions] \n![] \n# Feedback\n1\n* **[Collecting Mid-quarter Student Feedback] \n![] \n# Resources for Remote Teaching\n1\n* **[Teaching in the Virtual Classroom] \n![] \n# Open Discourse\n6\n* **[Teaching During an Election Season] \n* **[Handling Sensitive Topics] \n* **[Writing Content Notices for Sensitive Content] \n* **[GSB Students on Open Discourse: May 24, 2023 Panel Highlights] \n* **[Engaging with Difference in Course Content] \n* **[GSB Faculty on Open Discourse and Participation: April 13, 2022 Panel Highlights] \n![betterdocs-category-icon] \n# Teaching and AI\n6\n* **[GSB Student Voices Panel on Exploring AI: Event Highlights] \n* **[Faculty Showcase on AI and Teaching] \n* **[Starting Small with AI in the Classroom] \n* **[Teaching in the AI Era] \n* **[Course Policies on Generative AI Use] \n* **[Quick Start Guide to AI and Teaching] \n[] \n* [Teaching and AI] \n# Course Policies on Generative AI Use\n* updated onDecember 11, 2025\nTable of contents\n* [GSB Guidelines on Classroom AI Use] \n* [GSB Policy] \n* [Tips for Setting Your Approach to AI Use] \n* [Citing Generative AI Use] \n* [Template Syllabus Statements for Generative AI Use] \n* [Additional Resources] \n## GSB Guidelines on Classroom AI Use\n#### GSB Policy\n**MBA/MSx courses: Instructors may not ban student use of AI tools for take-home coursework**, including assignments and exams.**Instructors may choose whether to allow student use of AI tools for in-class work**, including exams. Except for in-class exams, students should be allowed to use AI tools in cases where they are able to use the internet and electronic devices. In-class assignments for which electronic devices are prohibited inherently also prohibit use of the internet, AI tools, tablets, laptops, calculators, etc.\n**PhD/Undergraduate courses:**Follow the[Generative AI Policy Guidance] from Stanford&#8217;s Office of Community Standards.\n#### Tips for Setting Your Approach to AI Use\n* **Consider your course goals**to inform how you guide students on using AI in your course.\n* **Explain your choices about AI use to students.**Connect your guidance to the learning goals of the course, industry standards (e.g., publication standards), or common industry cases (e.g., needing to give thoughtful/persuasive/clear input on the spot during board meetings).\n* **State your policy in your course syllabus and the[Course Policies and Norms form] **, and then follow up in class with your students.\n## Citing Generative AI Use\nGSB students are expected to**cite all resources, including generative AI tools**, used to prepare their academic work.See the MBA and MSx[assignment preparation policy].\nNevertheless, many students are hesitant to report AI use. It can help to ask all students to report*how*they used AI (instead of*whether*they used AI). Asking students to regularly report their AI use can help normalize transparent AI use.\nHere are some ways you can**ask students to acknowledge AI use**:\n* Submit chat logs associated with major assignments or exams\n* Disclose AI use according to your industry&#8217;s emerging standards, if available\n* Include statements with coursework to disclose how AI was used\n> Example statement: *> Gemini was used to gather sources for this project and create an initial written draft based on the author’s outline. Substantial revisions were made by the author, with additional fact-checking with ChatGPT. Claude was used to generate data visualizations. The ideas discussed in this report are those of the author alone.\n*\n* Reflect on how they used AI for the assignment\n> Sample prompt: *> How did you use AI in this assignment? What was or wasn’t effective? What will you do differently next time?\n*\n* Follow Stanford Library’s[guide to citing AI] in academic research, including[guidelines] and[examples] \n* Follow examples from Stanford UIT on[citing AI-generated content] (see the FAQ “Can you provide some examples of how to cite when content is generated by AI?”).\n## Template Syllabus Statements for Generative AI Use\nAdapt the template syllabus statements below to fit your course.\n**\n### AI Tools Permitted for All Coursework\nIn this course, you may use generative AI tools for all coursework, including in-class work, according to the following guidelines for use.\nGuidelines for use:\n* The following**Stanford-approved generative AI tools**should be used whenever possible when using AI tools for your coursework. These secure platforms may be used for most prompts, materials, and data (except[high risk data]).\n* [Stanford AI Playground] \n* [Google Gemini] and[NotebookLM] (only when authenticated using your SUNet ID)\n* Nectir AI (accessed through Canvas)[*Include if you have enabled this tool for your course.*]\n* [*Optional: Share links to additional tools that are recommended for your course.*]\n* Review and**follow the guidelines provided in Stanford IT’s resource on[Responsible AI at Stanford] **.\n* **When using a third-party, non-Stanford-approved AI tool**such as a personal ChatGPT account, make sure to check the fine print terms before signing up.**Avoid inputting information that should not be made public**. This includes personal or confidential information of your own or that others share with you, as well as proprietary or copyrighted materials ([*include relevant material types for your course:*e.g., case studies, data sets, assignment prompts]) that may be included in your coursework.\n[*Optional: Select one or more additional guidelines for use from the list below, according to what applies for your course. Highlighted text should be edited to fit your course policies.*]\n* In this course,**you will work with[sensitive / copyrighted / personal]data**, including[*examples from your course: e.g.,*data sets, slide decks, case studies, interview transcripts]. When working with this data, you**may only use the Stanford-approved tools**[listed above]****. You may not input this data into third-party AI tools, according to the guidelines provided in[Responsible AI at Stanford]. Note that[high risk data] is never appropriate for use with any AI tool.\n* **Cite all AI-generated material and/or explain how you have drawn on AI-generated material in your work.**Please cite generative AI use that contributes to"
  },
  {
    "query": "Stanford University generative AI policy",
    "title": "Creating your course policy on AI - Stanford Teaching Commons",
    "url": "https://teachingcommons.stanford.edu/teaching-guides/artificial-intelligence-teaching-guide/creating-your-course-policy-ai",
    "text": "# Creating your course policy on AI\n\nMain content start\n\nHere we offer example syllabus statements, suggestions for what to include, and sample sentences that you might use as you think through your own course policy on AI and begin writing a statement to put in your syllabus.\n\n## Analyzing the implications of AI chatbots\n\nKey points from the previous module\n\n- Academic integrity, student success, and workload balance represent three important areas of focus regarding the impact of AI chatbots.\n- The Office of Community Standards has stated that absent a clear statement from the instructors, the use of generative AI will be treated the same as assistance from another person.\n- Evaluate your own courses carefully when thinking about if and how you would use AI chatbots and how you would direct students to use them.\n\n[Go to the previous module] \n\n## Outcomes for this module\n\nHere we aim to guide you through developing a draft course policy on AI use that you can include in your syllabi.\n\nAfter completing this module, you should be able to:\n\n- Reflect on the purpose(s) of an AI policy in your course syllabi.\n- Describe some options for AI policy syllabus statements.\n- Create an AI policy for your course that takes into consideration academic integrity, student success, and workload balance.\n\n## Thinking about your syllabus\n\nA syllabus serves many purposes and can have multiple audiences depending on how and when it is used. Let's focus on the syllabus as a way to communicate to students the most important aspects and learning goals of your course. Before you begin to write your course policy on AI, consider the tone and voice of your syllabus, course policies already in place, and so on.\n\nWe offer the prompt \"What is one complexity about your course that could be clarified or improved for your students?\" that you can respond to in the poll below.\n\n## Explore example syllabi statements\n\nConsider these example syllabi statements on AI use:\n\n- [Classroom Policies for AI Generative Tools], created by Lance Eaton\n- [Sample text for syllabi], from the Sentient Syllabus Project\n- Sample language from the [AI Guidance & FAQs] web page from the Harvard Office of Undergraduate Education (at the end of the page)\n\n## Reflection prompt\n\nWhat important conversations do you want to have with your students about AI and learning in your course?\n\n## What to include in your AI course policy statement\n\nAn effective syllabus is designed to motivate learning, define learning goals, explain the structure of the course, offer support, and so on—which also applies to any statements about AI. Many of the strategies for writing effective syllabi in general, such as using student-centered language, aiming for transparency and precision, providing examples, and using the syllabus as a starting point for conversations with students, would apply here as well (Gannon, 2023). Remember that some students may not be aware of campus policies or have varying degrees of preparation for navigating such issues, so they would likely benefit from a clear and comprehensive statement about AI use in your course.\n\nConsider the following when developing your AI course policy:\n\n- What is the policy and what tools does it apply to specifically?\n- When does it apply? What conditions or contexts allow or preclude the use of AI?\n- What processes and consequences result from non-compliance?\n- What rationale and reasoning guide this policy?\n- How do you provide support to students in relation to this policy?\n- How does the policy show support for student well-being?\n\n## Ideas to get started\n\nBelow we provide options for ideas and wordings that you might use to begin crafting your own course policy. The following worksheet also contains more sample language and a chatbot prompt to help you refine your policy statement.\n\n[Worksheet for creating your AI course policy] (129.75 KB)\n\n### Type of policy\n\n- Yes, always allowed\n- Yes, but...\n- No, but...\n- No, never allowed\n\n### Specific tools it applies to\n\n- AI chatbots (such as ChatGPT, Google Bard, Claude, Bing Chat)\n- AI image generators (such as DALL-E, Midjourney, Stable Diffusion, Adobe Firefly)\n- AI code generators (such as CoPilot, Tabnine, Cody)\n- AI audio or music generators (such as Amper, AIVA, Soundful)\n\n### Conditions and contexts\n\n- Only for specific assignments\n- Only specified AI tools\n- Only when students openly explain how and why they used AI tools\n- Only when students and the teaching team consent to having their data entered into AI tools\n- Only when used for purposes that are not private, sensitive, or high-risk\n- Only with supervision during class, section, or office hours\n- Only after students have gained skills for using chatbots appropriately\n- Only by request and in consultation with the instructor or teaching team\n- Only with your own data. Do not enter private, sensitive, or copyrighted data from others into AI tools without their consent.\n- Only prohibited for graded assignments; for non-graded assignments, students may use chatbots\n- Must cite the AI tools and prompts they use\n\n### Rationales\n\n- Yes, always because...\n - \"The students in this course have strong learning skills and have shown themselves to be responsible, effective, self-directed learners.\"\n - \"I’ve designed robust assessments and learning activities in this course that have value regardless of the use of chatbots.\"\n - \"The use of chatbots aligns with the goals of the course in a way that enhances learning.\"\n - \"I consider learning to use chatbots an important skill in the discipline.\"\n - \"Students are informed about AI, its risks and benefits, and can decide for themselves if and how they would use AI tools.\"\n- Yes, but... because...\n - \"Students need to first develop their skills to demonstrate they can use chatbots effectively and responsibly.\"\n - \"Chatbots would enhance learning in certain specific situations but could be a detriment to learning outside of those situations.\"\n - \"The teaching team only has enough resources to support students working with chatbots in limited ways.\"\n - \"Students first need to understand issues around privacy and data security and consent to using a chatbot.\"\n- No, but... because...\n - \"It could enhance learning for certain students in unique circumstances but otherwise would likely inhibit learning.\"\n - \"It depends on the individual student's goals, situation, skills, and needs that need to be evaluated on a case-by-case basis.\"\n - \"The teaching team only has enough resources to support a limited number of students with the greatest need to use chatbots for learning.\"\n - \"As chatbots rapidly evolve, the teaching team needs more time to adapt the course to properly support students.\"\n - \"Content in this course is private, sensitive, or copyrighted, and should only be entered into a chatbot under certain circumstances.\"\n- No, never because...\n - \"The teaching team considers chatbots incompatible with the course goals and likely a detriment to studen"
  },
  {
    "query": "Stanford University generative AI policy",
    "title": "3.32: Generative Artificial Intelligence (AI) Policy | MD Program",
    "url": "https://med.stanford.edu/md/mdhandbook/section-3-md-requirements-procedures/3-32--generative-artificial-intelligence--ai--policy.html",
    "text": "3.32: Generative Artificial Intelligence (AI) Policy | MD Program | Stanford Medicine\n[Skip to Content] \n[Stanford Medicine] [MD Program] Site Nav\nMenu\nMD Program\nStudent Handbook\nClose menu\n## Generative Artificial Intelligence (AI) Policy\nArtificial Intelligence (AI) refers to a broad range of technologies that enable computers and machines to simulate human learning through advanced techniques and processes that perform complex tasks, such as large language models (LLMs), machine learning (ML), and natural language processing (NLP). AI encompasses a range of technologies that enable machines to interpret data, recognize patterns, make decisions, and solve problems. In the context of medicine, this may include a range of technologies and applications that enhance clinical decision-making, improve patient care, and streamline healthcare operations.\nThis policy aims to create a framework for the responsible integration of generative AI in the MD and MSPA Programs, ensuring that students develop the necessary skills and knowledge to utilize these tools effectively while maintaining the highest standards of academic integrity and patient care.\nThe MD and MSPA Programs emphasize the importance of developing clinical reasoning and skills. While artificial intelligence (AI) can serve as a valuable resource for learning and exploration, it is crucial that students engage actively with the material, applying their knowledge and critical thinking to clinical scenarios. AI can be a collaborative tool that enhances understanding and skills, but not a substitute for the rigorous training and cognitive skills necessary for effective medical practice.\n**The intent is to support the judicious use of AI as educational tools while safeguarding academic integrity, patient confidentiality, and the development of critical reasoning and clinical skills.****AI should enhance, not replace, the foundational knowledge and clinical competencies essential for medical practice.**\nThe guidance and policy will be reviewed and revised as necessary to reflect changes in technology, educational practices, and institutional standards.\nPermitted Uses and Privacy\n* Learning and Clarification: Students may utilize AI to enhance their understanding of medical concepts, definitions, and for grammar/style editing, provided it does not conflict with specific assignment instructions or be inconsistent with faculty-authorized tasks as detailed below.\n* Faculty-Authorized Tasks: Any use of AI must align with explicit faculty guidance as outlined in course syllabi. Faculty may designate appropriate contexts for AI use in assignments and projects.\n* Platforms: Stanford has several tools that have been specifically designed for use within the Stanford Medicine community that are compliant with institutional policies and suitable for handling sensitive information (e.g., patient health information). When permitted for use, please use these tools to maintain confidentiality and compliance with Stanford Medicine policies.\n* Stanford Healthcare Secure GPT:[https://securegpt.stanfordhealthcare.org/] \n* Stanford AI Playground:[https://] [aiplayground] [.stanford.edu] \n* Responsible AI at Stanford:[https://uit.stanford.edu/security/responsibleai] \n* Stanford Security Risk Classifications:[https://uit.stanford.edu/guide/riskclassifications] \n* To ensure that patient confidentiality is maintained at all times, no confidential research, patient data, or other protected health information (PHI) is to be entered into public AI platforms (e.g., ChatGPT, Gemini, OpenEvidence, Doximity, etc.).\n* Note that anonymized, general inputs are still appropriate (e.g., “Middle-aged woman with asthma exacerbation and aspirin allergy complaining of worsening symptoms…”) but directly copying in a patient progress note into any such public system would be a clear privacy violation.\nResponsible Use of AI Tools\nWhile we recognize that restricting the use of AI tools cannot be strictly enforced, we expect students to optimize their learning progress, which is likely best achieved through a balanced approach that includes both AI use and traditional methods, and students will be held accountable for demonstrating competency in clinical skills and reasoning without the aid of AI tools.\n* Assessments: The use of AI tools is discouraged for any activities in which students are evaluated on their own knowledge or skills, unless explicitly granted permission by the faculty. Unless explicitly advised by individual instructors, the use of AI tools is prohibited for any “closed book” exams or assignments where internet use is restricted.\n* Clinical Documentation: Use of AI for clinical documentation must adhere to the policies and guidelines for the clinical site and comply with HIPAA regulations. Unless supported by the Electronic Health Record, students should refrain from using AI for clinical documentation, including writing the History and Physical (H&amp;Ps) unless permitted by the clerkship.\n* Patient Confidential Data: The use of patient-identifying information or protected health information (PHI) in public AI tools is strictly forbidden.\n* Note that individual policies around AI use may vary by course and clerkships and this information will be outlined in the course syllabus. Students are responsible for verifying with faculty whether the use of generative AI tools is permissible for specific assignments or tasks.\nAcknowledgement and Citation\n* Any substantial contributions from AI tools on assignments, presentations, and scholarly abstracts or proposals must be disclosed and properly cited. This includes the tool name, model/version, date, query, and output excerpt. For example: ChatGPT (GPT-4; June 10, 2025), prompt: “Explain ARDS pathophysiology,” output used in Section 2.\n* AI tools may not be listed as authors on scholarly works.\nAccuracy &amp; Accountability\n* Students are responsible for any AI-generated content they submit, even if flawed or biased. Verification of information and judgment of quality is expected.\nEthics &amp; Education\n* Students should develop AI literacy, understanding the limitations, potential for inaccuracies (hallucinations), biases, and privacy risks associated with AI.\n* AI education will be integrated into the curriculum to provide students with additional guidance and training on the proper and effective use of AI.\nAll MD and MSPA students are expected to comply with this policy. Violations may result in disciplinary action in accordance with the School of Medicine’s policies on academic integrity and professional conduct.\nFor more information refer to Stanford University's[Generative AI Policy Guidance].\n*updated August 2025*\n[**]"
  },
  {
    "query": "Stanford University AI policy academic integrity",
    "title": "Academic Integrity Working Group addresses generative AI and ...",
    "url": "https://studentaffairs.stanford.edu/news/academic-integrity-working-group-addresses-generative-ai-and-exam-policies",
    "text": "Academic Integrity Working Group addresses generative AI and exam policies | Student Affairs\n[Skip to main content] [Skip to secondary navigation] \n[Stanford University(link is external)] \n[\nStanford\nStudent Affairs\n] \nSearch this siteSubmit Search\nMain content start\n[Academic Integrity Working Group] \n# Academic Integrity Working Group addresses generative AI and exam policies\nAs it enters its third year of a proctoring pilot, the university’s Academic Integrity Working Group offers guidance and resources for faculty.\nDecember 11, 2025\n|\nDeanna Grasser\n![The landmark 1941 Hoover Tower dominates the campus skyline of Stanford University.] \nThe campus skyline of Stanford University.\n**As it enters its third year of a proctoring pilot, the university’s Academic Integrity Working Group offers guidance and resources**for faculty.\n![Decorative Cardinal Red accent line. ] \nAs the university’s[Academic Integrity Working Group] proctoring pilot enters its third year, the group has partnered with faculty and departments across campus to update guidance for instructors on generative AI use and test administration.\n“Our work with campus partners is helping instructors preserve the importance of independent student work while also making sure our students are prepared for a future in which AI tools will be integral to many professions,” said AIWG Faculty Co-Chair Jennifer Schwartz Poehlmann.\nThe Academic Integrity Working Group (AIWG) was[formed in winter 2024] to study Stanford’s academic landscape and identify the scope of academic dishonesty, including its root causes and its relationship to teaching practices. A multi-year pilot study of equitable proctoring practices launched in spring 2024 with seven courses. More than 50 courses across multiple schools are participating in the pilot this quarter.\n“The AIWG’s work reinforces the university’s commitment to advancing academic integrity initiatives and leverages many campus partners,” Provost Jenny Martinez said. “As strategic campus partners in academic integrity, departments can lead these conversations, developing approaches that reflect the unique contexts of their disciplines in a changing educational landscape.”\nAs professors across the university prepare for midterms and exams this fall, the AIWG, with support from the provost’s office, has several recommendations to share with faculty. The group is also calling for departments to consider their role in initiating critical dialogue and discipline-specific policy around generative AI.\n## Proctoring and in-person assessments\nFor courses not participating in the proctoring pilot –which is currently the only place proctoring is permitted –the[AIWG website] outlines[best practices] for faculty conducting their own in-person exams, ranging from seating strategies to recommendations around cell phone use.\n“The proctoring pilot remains an important tool for creating consistent and equitable testing conditions,” Poehlmann said. “However, we want instructors to also feel empowered to create their own strategies for managing in-person exams and know that there are tools available to help.”\nFor instructors who wish to limit students’ use of AI in specific circumstances, including for high-stakes assessments, the group recommends[in-person formats] such as oral exams and in-class writing assignments. In their study of generative AI in higher education, the group identified several issues with tools designed to detect AI use in student work, including bias and occurrences of false positives and negatives. They also found that these tools often fail to reliably assess the extent of AI involvement in texts that mix AI and human writing, making them unsuitable for high-stakes situations, especially as evidence in academic misconduct cases.\n## **Departmental policies and preparing students for an AI-enabled future**\nThe AIWG emphasizes that the most effective approaches to student AI use go beyond individual course and exam management.\n“Departments and multi-section courses are encouraged to establish consistent and enforceable AI policies that can be applied fairly across sections. These policies should be reasonable, communicated clearly, and discussed with students regularly throughout the academic year,” Poehlmann said.\nThe[AIMES website], launched by the[Center for Teaching and Learning (CTL)] and VPUE, provides guidance and examples of how AI can be integrated into coursework to support student learning and career preparation. In addition, the CTL, a key partner in AIWG’s work and a resource for all faculty, encourages departments to[reach out] for individual consultations and department-level workshops.\nBringing students into the conversation about academic integrity and why policies matter can help strengthen understanding and buy-in. The[AIMES website] also includes a guide specifically for students,[AI and Your Learning: A Guide for Students], with practical advice on using AI responsibly.\n“We recognize that restrictions on AI use are essential to achieving certain learning goals, and at the same time, faculty are exploring constructive ways of weaving AI into their teaching and student learning,” said Cassandra Volpe Horri, associate vice provost for education and director of the Center for Teaching and Learning.\n## For more information\nThe AIWG continues to collect[faculty interest for the continuing proctoring pilot] in the 2025-2026 academic year. Submissions will be reviewed on a rolling basis before each quarter.\nPer the Stanford Honor Code, proctoring is currently not required and is only permitted in courses participating in the[AIWG proctoring pilot].\n[Access the original article at the Stanford Report] \n![Cover slide of the AIWG - Fall Recommendations PowerPoint presentation. ] \n## AIWG - Fall Recommendations, 10.20.25\nHere, you can review the recent Chairs &amp;&amp; Deans presentation associated with this article.\n[Access presentation] \n## Quick links\n* [Academic Integrity Working Group] \n* [AY 25-26 Stanford Proctoring Pilot: Faculty Interest Form] \n* [AIWG Best Practices for In-Person Exams] \n* [AIMES: AI Meets Education at Stanford] \n* [August 2025 policy brief on AI in Education] \n## More News Topics\n[Academic Integrity Working Group] \n## More News\n* [\n![] \n### The Flourish | Supporting You Through Every Season\n] \nJanuary 14, 2026\nAs we transition into a new year, it’s the perfect time to reflect and reset.\n* [What&#039;s Flourishing] \n* [\n![] \n### The Flourish | Find Joy and Healing This Winter Season\n] \nDecember 3, 2025\nHealing Through the Holidays\n* [What&#039;s Flourishing] \n* [\n![] \n### The Flourish, Winter 2026\n] \nDecember 3, 2025\nThe holiday season can feel different for everyone, being a mix of joy, heaviness, reflection, and change.\n* [The Flourish] \nBack to Top"
  },
  {
    "query": "Stanford University AI policy academic integrity",
    "title": "Generative AI Policies at the World's Top Universities: October 2025 ...",
    "url": "https://www.thesify.ai/blog/gen-ai-policies-update-2025",
    "text": "Generative AI Policies at the World’s Top Universities: October 2025 Update - Thesify - Ethical AI Tools That Improve Your Academic Writing\n![] \n**Special Offer! Enjoy****66% OFF****on the annual plan. Limited time only!**\n[\nClaim Discount\n] \n**Special Offer! Enjoy****66% OFF****on the annual plan. Limited time only!**\n[\nClaim Discount\n] \n**Special Offer! Enjoy****66% OFF****on the annual plan. Limited time only!**\n[\nClaim Discount\n] \n[thesify] \n/\n[Blog] \n/\n[Article] \n# Generative AI Policies at the World’s Top Universities: October 2025 Update\n![] \n[Alessandra Giugliano] \nOct 16, 2025\n![] \n![] \nAs generative artificial intelligence (AI) tools have proliferated, universities have scrambled to write or rewrite policies that balance innovation with academic integrity and privacy. In February 2025 we published an[overview of generative AI policies at the world’s top universities]. This article is our**October 2025 update**, using the[*Times Higher Education World University Rankings 2026*] as the reference list.\nThe goal remains the same: help you understand where AI is welcomed, restricted or forbidden so that you can make informed choices.[Policies evolve quickly], and some institutions have introduced new guidelines or moved in the rankings since February 2025. Always check your syllabus and official university resources before relying on AI.\n**Checked on 16 October 2025 (Europe). Policies and links verified at time of publication.**\n## Why Universities Care About Generative AI\nInstitutions have to balance innovation with responsibility.[Unacknowledged AI‑generated text can constitute**academic misconduct**]. Entering personal or institutional data into public AI tools raises[**privacy concerns**].\nOver‑reliance on AI may undermine students’ critical thinking and[writing skills]. Universities therefore develop policies that emphasise transparency, data security and skill development while allowing students and researchers to experiment with emerging technology.\n## What Changed Since February 2025\n* **Ranking shifts:**The[2026 edition of the Times Higher Education rankings] puts**MIT**in the #2 spot, ahead of**Harvard**(#3), while**Princeton**rises to #4 and**Imperial College London**enters the top 10. Columbia University and UCLA share a tie at #18, and**Cornell University**remains at #20.\n* **New or updated policies:**\n* [**Columbia University**] finalised a draft university‑wide generative AI policy that prohibits AI use without explicit permission.\n* [**Imperial College London**] released generative AI principles and departmental guidelines in March 2025\n* [**Johns Hopkins University**] issued a comprehensive responsible‑use guideline in May 2025.\n* [**UCLA**] published sample syllabus statements in April 2025 to help instructors set expectations.\n* **Regulatory context:**\n* The EU AI Act continues to influence European universities, encouraging transparency and fairness.\n* In the US, debates over copyright and academic honesty have led to[stricter disclosure requirements].\n* In Asia, policies range from strict departmental rules at[Peking University’s law schools] to general encouragement at[Tsinghua University].\n[![Visit app.thesify.ai]] ## **Updated Rankings and Methodology**\nOur analysis follows the[**Times Higher Education World University Rankings 2026**,] the most recent edition available as of October 2025. The top 20 universities in this list are:\n1. **University of Oxford**(UK)\n2. **Massachusetts Institute of Technology (MIT)**(USA)\n3. **Princeton University**(USA) (=3 tie)\n4. **University of Cambridge**(UK) (=3 tie)\n5. **Harvard University**(USA) (=5 tie)\n6. **Stanford University**(USA) (=5 tie)\n7. **California Institute of Technology (Caltech)**(USA)\n8. **Imperial College London**(UK)\n9. **University of California, Berkeley**(USA)\n10. **Yale University**(USA)\n11. **ETH Zurich**(Switzerland)\n12. **Tsinghua University**(China)\n13. **Peking University**(China)\n14. **University of Pennsylvania**(USA)\n15. **University of Chicago**(USA)\n16. **Johns Hopkins University**(USA)\n17. **National University of Singapore**(Singapore)\n18. **Cornell University**(USA) (=18 tie)\n19. **University of California, Los Angeles (UCLA)**(USA) (=18 tie)\n20. **Columbia University**(USA)\n## List of the World’s Top Universities and Their Policies (October 2025)\nEach entry below summarises the most recent generative AI policies at these universities and includes direct links to official sources. Policies often differ by department or instructor, so treat these highlights as a starting point and consult your course materials for specifics.\n### 1. University of Oxford –Responsible Use and Summative Assessment Rules\n[Oxford’s research and assessment policies] emphasise responsible use, transparency and academic integrity. Students may use generative AI to support their studies and research, but using AI in summative assessments is permitted only if explicitly allowed by the course or exam instructions. A declaration must accompany any permitted AI use, and unauthorized use is treated as academic misconduct. Avoid sharing personal data with AI tools.\n1. [Oxford research AI policy] \n2. [Oxford Guidelines on the use of generative AI] \n3. [Oxford AI use in summative assessment] \n### 2. Massachusetts Institute of Technology (MIT) –Data Privacy and Integrity\n[MIT’s Information Systems and Technology guidance advises] students and staff to consider information security, data privacy, regulatory compliance, confidentiality and intellectual‑property issues when using generative AI tools. AI may assist with non‑confidential tasks, but users must not enter personal or institutional data into public AI tools, and they should verify outputs to[avoid plagiarism] or hallucinations. Departments and instructors set course‑specific rules and may require disclosure of AI use.\n1. [MIT IS&amp;T generative AI guidance] \n2. [Generative AI use at MIT] \n3. [Teaching and Learning with ChatGPT (MIT TLL)] \n### 3 (tie). Princeton University –Permission Required and Disclosure\nPrinceton’s library guidance reminds students to confirm with their instructors whether AI use is allowed and to disclose any use. AI may be used for brainstorming and outlining if permitted, but copying AI‑generated text or using it beyond allowed scope is considered academic integrity violation. Some courses require students to[keep AI chat logs for verification].\n1. [Princeton Generative AI Guidance for Faculty] \n2. [Princeton Generative AI] \n3. [Princeton Generative AI Disclosure Policy] \n### 4 (tie). University of Cambridge –Personal Study vs. Summative Work\nCambridge allows students to use generative AI for personal study, research and formative tasks. However, unacknowledged AI‑generated content in summative assessments is academic misconduct. Departments and examiners must specify whether AI is permitted in assignments; students should verify AI outputs and recognise their limitations.\n1. [Using Generative AI Guidance for Students (Cam"
  },
  {
    "query": "Stanford University ChatGPT policy students",
    "title": "Use of Generative AI Technology - Student Affairs",
    "url": "https://law.stanford.edu/office-of-student-affairs/use-of-generative-ai-technology/",
    "text": "Icon with an X to denote closing.Play icon in a circular border.\n\n [Skip to main content] \n\nGenerative AI tools (e.g., ChatGPT) are increasingly used in legal practice and may have some beneficial uses to support learning. However, these tools come with risk: they may produce incorrect responses and otherwise inhibit learning, and some uses would constitute professional malpractice in the provision of legal services. Law school instructors should therefore exercise discretion to choose the appropriate AI policy for their learning goals.\n\nIndividual course instructors are free to set their own policies regulating the use of generative AI tools in their courses, including allowing or disallowing some or all uses of such tools, provided that the permitted uses of AI do not authorize students to contravene standard academic norms concerning plagiarism and accuracy. [Plagiarism] includes using an idea obtained from AI without attribution or submitting AI-generated text verbatim without quotation marks; accuracy norms require verifying assertions in submitted work. In coursework that involves attorney client representation, providing legal advice or services, or that would otherwise constitute the practice of law, AI use must meet all applicable rules of professional responsibility. Course instructors must state these policies in their course syllabi and clearly communicate these policies to students. Students who are unsure of policies regarding generative AI tools are encouraged to ask their instructors for clarification. This policy applies to all Stanford Law School courses, including clinics.\n\nIn the absence of a course-specific AI policy set by the instructor, students may use generative AI tools to support their learning and to aid in the development or refinement of their own ideas, provided they do not use such tools to generate content that is then presented as their own work. The use of generative AI while taking an exam or to draft or revise any portion of submitted work (e.g., drafting or revising the text of a paper or clinic work product; generating citations; fixing citation format) is not permitted unless (1) fully disclosed in advance of its use by the student to the instructor, and (2) explicitly authorized by the instructor in writing prior to the student’s use of the tool. In all cases, the student’s use of AI cannot contravene standard academic norms concerning plagiarism and accuracy, and in coursework involving the practice of law, AI must meet all applicable rules of professional responsibility. Students who are unsure whether a particular AI use is permitted should default to asking the instructor and disclosing the use.\n\nCitations to sources that do not exist will raise a presumption of generative AI use. Unauthorized use of AI tools may result in a lower grade, including a grade of F, and/or a referral to Stanford’s Office of Community Standards.\n\nThe following examples illustrate the application of this policy in the absence of a course-specific Generative AI policy:\n\n- A student uses an AI tool to organize her study outline in preparation for an exam in a doctrinal course. This use would not be prohibited.\n- A student uses AI to fix the Bluebooking of a paper and discloses in advance. This involves the use of AI to revise submitted work, so it must be authorized by the instructor in writing in advance of the student’s use of AI.\n- A student uses AI to prepare for being on panel in class. This use is not prohibited.\n- A student uses AI to write a paper or to produce clinic work product and discloses in advance prior to using such tools. This involves the use of AI to draft or revise submitted work, so it must be authorized by the instructor in writing in advance and cannot contravene standard academic norms concerning plagiarism and accuracy, or relevant professional standards for legal practice.\n- A student uses AI to write a paper or to produce clinic work product and does not disclose in advance. This is a violation of the AI policy and could result in an F and/or a referral to Stanford’s Office of Community Standards.\n- A student uses AI during an exam and does not disclose. This is a violation of the AI policy and could result in an F and/or a referral to Stanford’s Office of Community Standards.\n\nBack to the Top"
  },
  {
    "query": "Stanford University ChatGPT policy students",
    "title": "Generative AI in Higher Education: Seeing ChatGPT Through Universities' Policies, Resources, and Guidelines",
    "url": "https://scale.stanford.edu/genai/repository/generative-ai-higher-education-seeing-chatgpt-through-universities-policies",
    "text": "Generative AI in Higher Education: Seeing ChatGPT Through Universities&#039; Policies, Resources, and Guidelines | SCALE Initiative\n[Skip to main content] \n[![Stanford University]] \n## Search and Filter\nWhat is the application?\nTeaching –Instructional Materials[**] \nTeaching –Assessment and Feedback[**] \nTeaching –Professional Learning[**] \nLearning –Student Support[**] \nCommunicating / Social Tools[**] \nOrganizing[**] \nAnalyzing[**] \nWho is the user?\nStudent[**] \nParent/Caregiver[**] \nEducator[**] \nSchool Leader[**] \nOthers[**] \nWhich age?\n0-3 years\nElementary (PK5)\nMiddle School (6-8)\nHigh School (9-12)\nPost-Secondary\nAdult\nWhy use AI?\nEfficiency[**] \nOutcomes –Literacy[**] \nOutcomes –Numeracy[**] \nOutcomes –Other Academic[**] \nOutcomes –Differentiation[**] \nOutcomes –Social Emotional[**] \nOutcomes –Durable Skills[**] \nReimagined Schooling[**] \nOther[**] \nStudy design\nDescriptive –Implementation and Use[**] \nDescriptive –Product Development[**] \nImpact –Randomized Controlled Trial[**] \nImpact –Quasi-experimental[**] \nSystematic Review[**] \n## Submit a research study\nContribute to the repository:\n[Add a paper] \n# Generative AI in Higher Education: Seeing ChatGPT Through Universities&#039; Policies, Resources, and Guidelines\nAuthors\nHui Wang, Anh Dang, Zihao Wu, Son Mac\nDate\n07/2024\nPublisher\narXiv\nLink\n[https://arxiv.org/abs/2312.05235] \nThe advancements in Generative Artificial Intelligence (GenAI) provide opportunities to enrich educational experiences, but also raise concerns about academic integrity. Many educators have expressed anxiety and hesitation in integrating GenAI in their teaching practices, and are in needs of recommendations and guidance from their institutions that can support them to incorporate GenAI in their classrooms effectively. In order to respond to higher educators' needs, this study aims to explore how universities and educators respond and adapt to the development of GenAI in their academic contexts by analyzing academic policies and guidelines established by top-ranked U.S. universities regarding the use of GenAI, especially ChatGPT. Data sources include academic policies, statements, guidelines, and relevant resources provided by the top 100 universities in the U.S. Results show that the majority of these universities adopt an open but cautious approach towards GenAI. Primary concerns lie in ethical usage, accuracy, and data privacy. Most universities actively respond and provide diverse types of resources, such as syllabus templates, workshops, shared articles, and one-on-one consultations focusing on a range of topics: general technical introduction, ethical concerns, pedagogical applications, preventive strategies, data privacy, limitations, and detective tools. The findings provide four practical pedagogical implications for educators in teaching practices: accept its presence, align its use with learning objectives, evolve curriculum to prevent misuse, and adopt multifaceted evaluation strategies rather than relying on AI detectors. Two recommendations are suggested for educators in policy making: establish discipline-specific policies and guidelines, and manage sensitive information carefully.\nWhat is the application?\n[Teaching –Instructional Materials],\n[Teaching –Assessment and Feedback],\n[Teaching –Professional Learning] \nWho is the user?\n[Educator],\n[School Leader] \nWho age?\n[Post-Secondary] \nWhy use AI?\n[Efficiency],\n[Outcomes –Other Academic] \nStudy design\n[Descriptive –Implementation and Use],\n[Systematic Review] \n[![Stanford University]] \n* [Stanford Home] \n* [Maps &amp; Directions] \n* [Search Stanford] \n* [Emergency Info] \n* [Privacy] \n* [Copyright] \n* [Trademarks] \n* [Non-Discrimination] \n* [Accessibility] \n©Stanford University. Stanford, California 94305."
  },
  {
    "query": "Stanford University provost generative AI guidance",
    "title": "[PDF] Report of the AI at Stanford Advisory Committee",
    "url": "https://provost.stanford.edu/sites/g/files/sbiybj29391/files/media/file/ai-at-stanford-report.pdf",
    "text": "Report of the AI at Stanford Advisory Committee\nJanuary 9, 2025\nRuss Altman, Kenneth Fong Professor and Professor of Bioengineering, of Genetics, of\nMedicine, of Biomedical Data Science, and Senior Fellow at the Stanford Institute for HAI\n(Committee Chair)\nYi-An Chen, Senior University Counsel\nMichele Elam, William Robertson Coe Professor of Humanities and Senior Fellow at the\nStanford Institute for HAI\nZephyr Frank, Gildred Professor of Latin American Studies and Professor at the Stanford Doerr\nSchool of Sustainability\nSteve Gallagher, Chief Information Officer, University IT\nStephanie Kalfayan, Vice Provost for Academic Affairs\nSerena Rao, Senior Associate Dean for Finance and Administration, VPDOR\nMehran Sahami, Tencent Chair of the of the Computer Science Department and James and\nEllenor Chesebrough Professor\nDan Schwartz, Dean of the Graduate School of Education\nZack Al-Witri, Assistant Vice Provost for Academic Affairs (Staff to the Committee)\nIntroduction\nOn March 18, 2024, the Provost charged the AI at Stanford Advisory Committee to\nassess the role of AI at Stanford in administration, education, and research to identify\npotential policy gaps and other needs to advance the responsible use of AI at Stanford.\nThe committee met seven times between March 18 and June 19 to assess potential\npolicy gaps in the areas of administration, education, and research at Stanford. They\nwere informed by reports, policies, and resources from peer institutions that have\nassessed similar questions over the last year (e.g. Cornell, Michigan, Harvard, Yale, and\nPrinceton).\nStanford and the Rapidly Changing Landscape of AI Use\nThere are great opportunities and great challenges associated with the newest\ngeneration of AI technologies, particularly large language models (LLMs). They\nrepresent powerful transformative tools for productivity because of their ability to\ngenerate/draft, summarize, and analyze text, images and other media. At the same\ntime, they are new and have not systematically been assessed (for example, with\nrespect to biases within the data used to train them, and their performance on critical\ntasks that are fact-based). Their attractive output may lull users into a misplaced sense\nof quality, precision, and accuracy. Most importantly, these technologies are rapidly\nevolving in both their capabilities and the degree to which they can be inspected, tested,\nverified and validated. And, while such models continue to improve, their already-known\nshortcomings require that care be taken when using these systems in many contexts.\nPage 1\nStanford is a leader in the development and application of AI. The university community\nincludes experts in the creation, validation and extension of the core capabilities of AI.\nAcross all schools and units, many colleagues are already using AI to advance teaching\nand research in their disciplines. Therefore, it seems prudent to articulate a core set of\npolicies, standards, and best practices for the use of AI in the administrative,\neducational and research functions of the university. A fine balance is required. The\ncommittee wishes to encourage experimentation with the technology as it evolves, to\nmaximally benefit the university, the research enterprise, and teaching and learning,\nwhile ensuring that key principles of the university are honored. Thus, our group\napproached its charge through the lens of providing guardrails to support productive\nuses of AI and not to stifle innovation, creativity or exploration.\nPer its charge, the committee sought to identify potential policy gaps. We list these,\nalong with other considerations and recommendations for university activities in\nadministration, education and research. Given the pace of change in the AI field, the\ncommittee cautions against creating fixed, rigid policies (except where required by law\nor other compelling considerations). Rather, we recommend the adoption of approaches\nthat are flexible and do not unduly limit the creative use of AI to support the university’s\nmission. In some cases, policies are not required—guidelines and best practices will\nsuffice and provide more flexibility. At a minimum, any best practices, standards or\npolicies promulgated by the university should be evaluated and updated regularly based\non their relevance and effectiveness.\nThe committee also arrived at a set of general principles to help guide the approach to\nAI, since many situations and challenges may not be anticipated in advance. These\nprinciples should help guide the ad hoc management of new issues as they arise. Our\ngroup learned that there are hundreds of uses of AI each day conducted by faculty,\nstaff, students/trainees. General principles to help guide these “experiments” may be\nmore useful to individual users and practitioners than any policy to cover the specific\nsituations and use cases that we were able to anticipate and describe below. Of course,\nconsultation with the Office of the General Counsel (OGC) is important when potential\nlegal issues arise.\nGuiding Principles for AI at Stanford1\nWe encourage the university to view goals in this space in line with the philosophy of\nStanford’s Institute for Human-Centered AI (HAI) to make sure that the use of AI is\ncentered on improving the human condition. Thus, we prefer to focus on “augmenting\nhuman capabilities” versus “replacing humans.” Many of us at Stanford aspire towards a\n1 Examples of other related guiding principles and resources:\n● Stanford Research Policy Handbook research principles 1.1\n● Whitehouse Executive Order on Safe, Secure and Trustworthy AI\n● NIST Risk Management Framework for AI 1.0\n● National Academies of Science, Engineering and Medicine publications on AI\nPage 2\ngoal where AI positively impacts our community. We must take into consideration the\nfair distribution of benefits that come from AI. We seek to remove or repair AI systems\nthat tend to harm individuals or groups disproportionately. For critical systems affecting\nhuman health and welfare, we aspire for a high bar for validation and verification of AI\naccuracy, applicability, fairness, and equity before deployment. Moreover, we should\nresist the tendency to assume that existing laws, regulations, and university policies are\nnot applicable in the context of AI use, a tendency we call “AI exceptionalism.” In short,\nwe believe that the use of AI at Stanford should be human-centered.\nWe modify from a Cornell report a review of the elements of human-centeredness to\nformulate the following guiding principles that we can strive towards in our deployment\nof AI at Stanford:\nHuman Oversight. Humans should take responsibility for the AI systems used or\ncreated at Stanford. Each system should aim to have a clear line of authority and\nresponsibility for system procurement/creation, maintenance, monitoring and sunsetting.\nThere should be plans for re-engaging human control of systems when they do not\nwork, and there should be contingency plans for management of high-risk AI syst"
  },
  {
    "query": "Stanford University provost generative AI guidance",
    "title": "Report of the AI at Stanford Advisory Committee",
    "url": "https://provost.stanford.edu/2025/01/09/report-of-the-ai-at-stanford-advisory-committee/",
    "text": "Report of the AI at Stanford Advisory Committee | Office of the Provost\n[Skip to main content] [Skip to secondary navigation] \n[Stanford University(link is external)] \n[\nStanford\nOffice of the Provost\n] \nSearch this siteSubmit Search\nMain content start\n[From the Provost] \n# Report of the AI at Stanford Advisory Committee\nOn March 18, 2024, the Provost charged the AI at Stanford Advisory Committee to assess the role of AI at Stanford.\nJanuary 9, 2025\nOn March 18, 2024, the Provost charged the AI at Stanford Advisory Committee to assess the role of AI at Stanford in administration, education, and research to identify potential policy gaps and other needs to advance the responsible use of AI at Stanford. The committee met seven times between March 18 and June 19 to assess potential policy gaps in the areas of administration, education, and research at Stanford. They were informed by reports, policies, and resources from peer institutions that have assessed similar questions over the last year (e.g.[Cornell],[Michigan],[Harvard],[Yale], and[Princeton]).\n## Stanford and the Rapidly Changing Landscape of AI Use\nThere are great opportunities and great challenges associated with the newest generation of AI technologies, particularly large language models (LLMs). They represent powerful transformative tools for productivity because of their ability to generate/draft, summarize, and analyze text, images and other media. At the same time, they are new and have not systematically been assessed (for example, with respect to biases within the data used to train them, and their performance on critical tasks that are fact-based). Their attractive output may lull users into a misplaced sense of quality, precision, and accuracy. Most importantly, these technologies are rapidly evolving in both their capabilities and the degree to which they can be inspected, tested, verified and validated. And, while such models continue to improve, their already-known shortcomings require that care be taken when using these systems in many contexts.\nStanford is a leader in the development and application of AI. The university community includes experts in the creation, validation and extension of the core capabilities of AI. Across all schools and units, many colleagues are already using AI to advance teaching and research in their disciplines. Therefore, it seems prudent to articulate a core set of policies, standards, and best practices for the use of AI in the administrative, educational and research functions of the university. A fine balance is required. The committee wishes to encourage experimentation with the technology as it evolves, to maximally benefit the university, the research enterprise, and teaching and learning, while ensuring that key principles of the university are honored. Thus, our group approached its charge through the lens of providing guardrails to support productive uses of AI and not to stifle innovation, creativity or exploration.\nPer its charge, the committee sought to identify potential policy gaps. We list these, along with other considerations and recommendations for university activities in administration, education and research. Given the pace of change in the AI field, the committee cautions against creating fixed, rigid policies (except where required by law or other compelling considerations). Rather, we recommend the adoption of approaches that are flexible and do not unduly limit the creative use of AI to support the university’s mission. In some cases, policies are not required—guidelines and best practices will suffice and provide more flexibility. At a minimum, any best practices, standards or policies promulgated by the university should be evaluated and updated regularly based on their relevance and effectiveness.\nThe committee also arrived at a set of general principles to help guide the approach to AI, since many situations and challenges may not be anticipated in advance. These principles should help guide the*ad hoc*management of new issues as they arise. Our group learned that there are hundreds of uses of AI each day conducted by faculty, staff, students/trainees. General principles to help guide these “experiments” may be more useful to individual users and practitioners than any policy to cover the specific situations and use cases that we were able to anticipate and describe below. Of course, consultation with the Office of the General Counsel (OGC) is important when potential legal issues arise.\n## Guiding Principles for AI at Stanford\n[[1]] \nWe encourage the university to view goals in this space in line with the philosophy of Stanford’s Institute for Human-Centered AI (HAI) to make sure that the use of AI is centered on improving the human condition. Thus, we prefer to focus on “augmenting human capabilities” versus “replacing humans.” Many of us at Stanford aspire towards a goal where AI positively impacts our community. We must take into consideration the fair distribution of benefits that come from AI. We seek to remove or repair AI systems that tend to harm individuals or groups disproportionately. For critical systems affecting human health and welfare, we aspire for a high bar for validation and verification of AI accuracy, applicability, fairness, and equity before deployment. Moreover, we should resist the tendency to assume that existing laws, regulations, and university policies are not applicable in the context of AI use, a tendency we call “AI exceptionalism.” In short, we believe that the use of AI at Stanford should be human-centered.\nWe modify from a[Cornell report] a review of the elements of human-centeredness to formulate the following guiding principles that we can strive towards in our deployment of AI at Stanford:\nHuman Oversight. Humans should take responsibility for the AI systems used or created at Stanford. Each system should aim to have a clear line of authority and responsibility for system procurement/creation, maintenance, monitoring and sunsetting. There should be plans for re-engaging human control of systems when they do not work, and there should be contingency plans for management of high-risk AI systems.\nHuman Alignment. AI systems should be built/procured to support Stanford’s mission and core values. When appropriate, systems should be adopted and deployed in consultation with diverse stakeholders to identify risks and mitigation strategies–particularly for disparate negative impacts. The scope of use, system capabilities, and limitations of systems should be documented in an ongoing process and evaluated regularly for consistency with community standards.\nHuman Professionalism. All members of the Stanford community should adhere to standards and aim for high levels of rigor and quality in their work. We expect that they will exercise their best judgment and critical thinking in the use of AI tools. All community members are responsible for the content of our work and must be willing to take responsibility f"
  },
  {
    "query": "Stanford University provost generative AI guidance",
    "title": "Report outlines Stanford principles for use of AI",
    "url": "https://news.stanford.edu/stories/2025/01/report-outlines-stanford-principles-for-use-of-ai",
    "text": "[Skip to content] \n\n## Stanford Report cookie usage information\n\nWe want to provide stories, announcements, events, leadership messages and resources that are relevant to you. Your selection is stored in a browser cookie which you can remove at any time by visiting the \"Show me...\" menu at the top right of the page. For more, read our [cookie policy].\n\nAcceptDecline\n\nJanuary 9th, 2025\\| 1 min read [Leadership & Governance] \n\n# Report outlines Stanford principles for use of AI\n\nThe AI at Stanford Advisory Committee’s report includes recommendations for AI use in the university’s administration, education, and research.\n\nAs artificial intelligence (AI) continues to reshape education, research, and administration at Stanford, a [new report] from the university’s AI at Stanford Advisory Committee calls for balancing innovation with responsibility and alignment with the university’s key values.\n\nUnderscoring both the opportunities and risks of AI, the “AI at Stanford” report sets forth guiding principles to encourage experimentation and creativity while addressing challenges such as plagiarism, authorship, and ethical use.\n\n“The growth of AI technologies has huge implications for higher education, from the classroom to the research lab,” said Provost Jenny Martinez. “The possibilities are incredibly exciting, and I’m confident Stanford will continue to be a leader in this area. As we support advances in this technology, it’s crucial to assess how AI is being used at Stanford today, consider how it may be used in the future, and identify any policy gaps that may exist. I’m grateful for the advisory committee’s thoughtful and thorough work, and the guidance it has provided in advancing the responsible use of AI at Stanford.”\n\nIn March, the provost charged the AI at Stanford Advisory Committee to evaluate the role of AI in administration, education, and research, and to identify what’s needed to use AI responsibly at the university.\n\nVarious university offices and committees will consider and take on recommendations from the report, and the committee chair will provide a presentation to the Faculty Senate in the winter quarter. The committee, made up of 10 faculty and staff members from various campus units, will continue to meet to address future issues related to AI use at Stanford.\n\n## Balancing experimentation with principles\n\nWhile the committee acknowledged legitimate concerns about AI, it sought to avoid rigid policies that might deter the technology’s potential benefits. “We wanted to first encourage experimentation in safe spaces to learn what it can do and how it might help us pursue our mission,” said Committee Chair Russ Altman. “But then we also wanted to establish clear ‘hot button’ areas where people should proceed with an abundance of caution.”\n\nThe report highlights potential policy gaps for the university, noting that many situations and challenges can’t be anticipated, and provides general principles to guide AI usage at the university, such as requiring human oversight and ethical considerations in all AI usage.\n\n“There should always be professionalism and personal responsibility. Whenever somebody uses AI, even if the AI does some work, they need to take responsibility for the output, and if there’s mistakes, it’s on them,” said Altman, the Kenneth Fong Professor and professor of bioengineering, of genetics, of medicine, of biomedical data science, and senior fellow at the Stanford Institute for Human-Centered Artificial Intelligence.\n\nPeople should resist lapsing into AI exceptionalism– assuming that existing laws, regulations, and university policies aren’t applicable to AI, according to the report. “It seems seductively capable,” Altman said, “and so people let their guard down and use it in ways that suggest they are forgetting what it is and how it works.”\n\nAs a guiding principle, the report recommends an “AI golden rule”: use AI with others as you would want them to use AI with you. For example, would you want AI to be used to review your proposal? This assessment would be based on individual judgments, evolving community norms, and combined with other principles to inform AI use.\n\n## Education\n\nOne primary area the committee examined is how AI affects education. Students have already adopted AI technologies such as ChatGPT, meaning the Honor Code and individual classroom policies may need to be revisited, the committee noted.\n\nAt the same time, many faculty aren’t experienced with AI and are unaware of how they can use it, said Dan Schwartz, committee member and dean of the Graduate School of Education (GSE).\n\n“Students have been exploring it much more than the faculty, and that’s why it’s important to find ways to educate the faculty, which I think is also true on the research side,” Schwartz said. “The question is slowly changing from what are our students going to cheat with, to a more sophisticated question of what counts as cheating, how they can use AI in their work, and how to make it part of the assignment.”\n\nTo help faculty navigate these questions, the committee recommends frameworks that can be tailored to different classroom needs. “It was something that students as much as faculty wanted so they can understand what’s permissible and productive,” Schwartz said.\n\nThe GSE has already created the [AI Tinkery], part of the [Stanford Accelerator for Learning], to provide a collaborative space for educators to explore the possibilities of AI.\n\n## Research\n\nIn research, AI raises complex challenges, including the question of whether AI can or should be credited as an author on publications. Other concerns include the use of AI in reviewing and writing proposals, training AI on student work, using data for AI research, addressing potential copyright violations in LLM outputs, and detecting fraudulent behavior.\n\nFor example, the use of AI detectors is already leading to a higher volume of plagiarism allegations, including spurious ones, which creates huge burdens on the offices that handle research misconduct. The university’s misconduct policy and rules for investigating allegations may also need to be updated in accordance with new federal policy.\n\nMany of these issues emerged from conversations with those who work in research administration, Altman said.\n\nResearchers should also be reminded or made aware of potential risks in AI use and updated as legal risk profile changes, the report says. The committee also encourages the university to consider ways to expand computing resources to ensure Stanford remains a leader in the productive use of these technologies.\n\n## Administration\n\nRegarding AI use in university administration processes, the committee found several areas that may require more guidance and additional policies, such as hiring, performance reviews, admissions, communications, and surveillance. The report also included recommendations on education and training for use of sensitive data, a streamlined procurement process for AI sys"
  },
  {
    "query": "Stanford University provost generative AI guidance",
    "title": "Report of the AI at Stanford Advisory Committee | Risman Adnan",
    "url": "https://www.linkedin.com/posts/risman-adnan-bb726b5_report-of-the-ai-at-stanford-advisory-committee-activity-7295224511203811329--FxJ",
    "text": "## LinkedIn respects your privacy\n\nLinkedIn and 3rd parties use essential and non-essential cookies to provide, secure, analyze and improve our Services, and to show you relevant ads (including **professional and job ads**) on and off LinkedIn. Learn more in our [Cookie Policy].\n\nSelect Accept to consent or Reject to decline non-essential cookies for this use. You can update your choices at any time in your [settings].\n\nAccept\n\nReject\n\nAgree & Join LinkedIn\n\nBy clicking Continue to join or sign in, you agree to LinkedIn’s [User Agreement], [Privacy Policy], and [Cookie Policy].\n\n [Skip to main content] \n\n# Risman Adnan’s Post\n\n[Risman Adnan] \n\nChief Technology\n\n7mo\n\n- [Report this post] \n\nCan students and lecturers use AI for education and research? It is a common question recently. Stanford (and similarly some other Universites like Yale, Cornell, Princeton, Harvard, etc) has released their guidance on GenAI for education and research. Detail guidance and policy is posted here: [https://lnkd.in/gKnVNbwi].\nThe overall approach should ensuring that human oversight remain central. AI is \"Augmented Intelligence\" to support rather than replace human capabilities, striving for equitable and ethical deployments that respect privacy, security, and accuracy. Stanford encourages innovation and experimentation with rapidly evolving AI, underscores the need for careful alignment with institutional values and compliance with existing laws and policies. How AI should be integrated into educational contexts?. First, “human oversight” requiring that individuals remain \"accountable\" for any AI tools used, including their deployment, maintenance, and removal if needed. The principle of “human alignment” stipulates that AI solutions must support University mission and be regularly evaluated for risks, such as bias or unintended outcomes. Under the banner of “human professionalism,” faculty, staff, and students share the responsibility for \"critical thinking\", ensuring the authenticity and integrity of any AI-assisted work. Additionally, ethical and safe use stands at the forefront, insisting that AI deployments preserve justice, non-discrimination, and overall well-being. Sensitive or personal information should not be fed into external AI tools without appropriate oversight, and ongoing efforts must ensure data provenance, quality, and risk mitigation. Finally, the “AI Golden Rule” calls for a reciprocal perspective: users should handle and share AI outputs in ways they would accept if roles were reversed, promoting transparency and accountability. For teachers, the university encourages experimentation with AI-driven educational enhancements—such as AI-based tutoring or assessment tools—provided they remain transparent to students and critically assessed for reliability. Any instructor who relies on AI for grading or feedback must disclose its use and confirm that it meets acceptable standards of accuracy and fairness!. Moreover, teachers should be particularly careful about using AI for tasks like letter-of-recommendation writing: while draft assistance may be permissible, they must ensure that the final output remains authentic and reflective of genuine human evaluation.\nOverall it is a very good baseline to be adopted and improved. Not only in Universities, but it applied to all companies to start thinking their internal human-centric AI policy. Like Markov chain, our future in education and research is independent of past give present. AI is our present and future!.\nHope this helps!\n\n[21] [1 Comment] \n\n[Like] [Comment] \n\nShare\n\n- Copy\n- LinkedIn\n- Facebook\n- X\n\n[Akhirudin Fahmi] \n\nProduct Lead @ DOKU \\| Core Finance, E-Wallet, Payment & Fintech\n\n7mo\n\n- [Report this comment] \n\n[Fathiyya Fitriani Refananda] \n\n[Like] [Reply] 1 Reaction\n\nTo view or add a comment, [sign in] \n\n10,321 followers\n\n- [424 Posts] \n\n[View Profile] [Connect] \n\n## Explore content categories\n\n- [Career] \n- [Productivity] \n- [Finance] \n- [Soft Skills & Emotional Intelligence] \n- [Project Management] \n- [Education] \n- [Technology] \n- [Leadership] \n- [Ecommerce] \n- [User Experience] \n\nShow more\n\nShow less\n\nLinkedIn\n\nNever miss a beat on the app\n\nDon’t have the app? Get it in the Microsoft Store.\n\n[Open the app]"
  },
  {
    "query": "Stanford University teaching and learning generative AI guidance",
    "title": "Artificial Intelligence Teaching Guide - Stanford Teaching Commons",
    "url": "https://teachingcommons.stanford.edu/teaching-guides/artificial-intelligence-teaching-guide",
    "text": "Artificial Intelligence Teaching Guide | Teaching Commons\n[Skip to main content] [Skip to secondary navigation] \n[Stanford University(link is external)] \n[\nStanford\nTeaching Commons\n] \nSearch this siteSubmit Search\n# Artificial Intelligence Teaching Guide\n## Main navigation\n[Skip Secondary Navigation] \nMain content start\nArtificial intelligence technology has become increasingly sophisticated and readily available. We believe that educators can contribute to how this important technology is understood and used. We invite you to engage thoughtfully and attentively with this teaching guide as a way to learn about and positively influence the dialogue around artificial intelligence in education.\n## For instructors and teaching teams\nWe offer this guide to all instructors and teaching teams approaching the topic of generative AI tools in education, whether for the first time or as part of your ongoing engagement with the topic, in response to practical concerns that we heard from instructors like yourself. You don't need to be an expert or have prior experience with generative AI to use this resource, though you should have some understanding of or experience with teaching and learning in higher education contexts. We intend this guide to apply to any disciplinary area or teaching modality and to help you structure the work of integrating AI tools into your teaching practice.\n## Goals and scope of this guide\nThe goal of this guide is to help you make informed and intentional decisions as you navigate AI technology in your teaching practice and courses. We cannot comprehensively address the complex topic of artificial intelligence in any short guide. Many campus service providers, such as University Information Technology (UIT), Stanford Accelerator for Learning, and the Institute for Human-Centered Artificial Intelligence (HAI), have developed excellent resources that offer insight into AI in terms of technical aspects, innovative new tools, societal impacts, AI research, and so on. We have chosen to focus on the practical and pedagogical aspects of AI tools in the classroom. We will focus on generative AI chatbots in particular, but you may find the content here can also apply to other generative AI tools, such as image, media, or code generators.\n## Multiple instructional modules for flexibility\nEach page of this guide contains one instructional module, including content, practice tasks, and assessment activities. We suggest that you complete the activities and suggested readings in each section as a self-directed online lesson. We designed each module as a discrete and complete lesson that you can finish in a relatively short amount of time. You can work through the modules in any order. We encourage you to engage fully with each module, completing the recommended activities to reinforce your learning.\nEach module has specific goals and objectives:\n1. **Understanding AI literacy**—Examine a framework that identifies and organizes skills and knowledge useful for navigating generative AI.\n2. **Defining AI and chatbots**—Define common concepts and explain how AI tools work\n3. **Exploring the pedagogical uses of AI chatbots**—Explore educational use cases, describe risks, and access and practice using chatbots.\n4. **Analyzing the implications for your course**—Describe campus AI policy guidance, evaluate, and analyze your course\n5. **Creating your course policy on AI**—Draft a course policy on AI use for your syllabus\n6. **Integrating AI into assignments**—Examine ways to integrate AI tools into assignments and activities used to assess student learning## Complete the modules with your colleagues\nLearning together with others can deepen the learning experience. We encourage you to organize your colleagues to complete these modules together.\n## Facilitate an AI workshop based on these modules\nStanford's Center for Teaching and Learning has also developed[do-it-yourself workshop kits] inspired by these modules. The kits expand upon the topics and strategies covered in these modules. Each workshop kit typically contains a resource list, sample agenda, promotional materials, slide presentation, facilitator's notes, key strategies, and an evaluation tool to assess learning and gather feedback.\n* [Exploring Pedagogic Uses of AI Workshop Kit] \n* [Analyzing the Implications of AI Workshop Kit] \n* [Creating an AI Course Policy Workshop Kit] \n* [Integrating AI into Assignments Workshop Kit] ## Complete the self-paced Canvas resource on critical AI literacy\nStanford's Center for Teaching and Learning has also developed a[Critical AI Literacy for Instructors] self-paced, professional development resource available in Canvas. It is designed for instructors new to generative AI (genAI) technology and aims to provide foundational knowledge and skills to critically and effectively navigate genAI in teaching and learning contexts.\n## How AI was used in the creation of this guide\nWe did not copy and paste any language generated by AI chatbots into this guide. We used AI chatbots, primarily ChatGPT, to generate feedback on the clarity and structure of some of the writing and to clean up some text formatting. We used ChatGPT and other chatbots more extensively in the development of the module \"Exploring the pedagogical uses of AI chatbots\" to mimic how we thought instructors and students might use them in a course and to better understand the pedagogical potential and challenges of such tools.\n## Authors and acknowledgments\nWe are a team of support staff from different parts of the Office of the Vice Provost for Undergraduate Education (VPUE). Our team created the guide in the summer of 2023 through the collective effort of dedicated colleagues from across the university. We want to thank the following people who contributed to this resource.\n* Kenji Ikemoto (Center for Teaching and Learning) for leading the project team and being the primary author of this guide.\n* Marvin Diogenes (Program for Writing and Rhetoric) for brainstorming, editorial feedback, and sage advice on the meaning of language.\n* Sarah Pickett and Kritika Yegnashankaran (CTL) for their pedagogical expertise and teacherly support.\n* Carlos Seligo (CTL), Josh Weiss (GSE), and Andy Saltarelli (VPSA) for being thought partners and providing the initial inspiration for this guide.\n* Laura Otero (California State University Monterey Bay) for sharing her exemplary AI in education Canvas course.\n* John Mitchell (Computer Science) and Glenn Fajardo (d.school) for leading the[Seminar on Generative AI and Education].\n* Merve Tekgurler (History) and Patrick Young (Computer Science) for sharing their teaching experience with us.\n* Anyone who recommended an article, had a random hallway conversation, or cheered us on.\nYou may adapt, remix, or enhance these modules for your own needs. This guide is licensed under[Creative Commons BY-NC-SA 4.0] (attribution, non-commercial, share-alike) and should be attributed to Stanford Teaching Common"
  },
  {
    "query": "Stanford University teaching and learning generative AI guidance",
    "title": "Quick Start Guide to AI and Teaching",
    "url": "https://tlhub.stanford.edu/docs/quick-start-guide-to-ai-and-teaching/",
    "text": "Quick Start Guide to AI and Teaching - Teaching and Learning Hub\n[Skip to content] \nStanford University\n**\nFor immediate technical assistance during class time, call the GSB classroom support hotline: 1(650)736-3342\nOr submit a[support ticket] to Stanford Service-Now\n[![Teaching and Learning Hub, Stanford GSB]] \n[**MenuClose] \n# Quick Start Guide to AI and Teaching\n![] \n# Engaging Students\n9\n* **[Pre-recording Videos to Share with Your Students] \n* **[Handout: Zoom Instructions for Remote Guest Speakers] \n* **[Faculty Tips: Teaching with Presence and Connection] \n* **[Tools and Technology] \n* **[Asynchronous Course Discussions] \n* **[Conducting Role-Plays] \n* **[Using Polls in the GSB Classroom] \n* **[Building Classroom Community] \n* **[Leading Effective Discussions] \n![] \n# Feedback\n1\n* **[Collecting Mid-quarter Student Feedback] \n![] \n# Resources for Remote Teaching\n1\n* **[Teaching in the Virtual Classroom] \n![] \n# Open Discourse\n6\n* **[Teaching During an Election Season] \n* **[Handling Sensitive Topics] \n* **[Writing Content Notices for Sensitive Content] \n* **[GSB Students on Open Discourse: May 24, 2023 Panel Highlights] \n* **[Engaging with Difference in Course Content] \n* **[GSB Faculty on Open Discourse and Participation: April 13, 2022 Panel Highlights] \n![betterdocs-category-icon] \n# Teaching and AI\n6\n* **[GSB Student Voices Panel on Exploring AI: Event Highlights] \n* **[Faculty Showcase on AI and Teaching] \n* **[Starting Small with AI in the Classroom] \n* **[Teaching in the AI Era] \n* **[Course Policies on Generative AI Use] \n* **[Quick Start Guide to AI and Teaching] \n[] \n* [Teaching and AI] \n# Quick Start Guide to AI and Teaching\n* updated onDecember 11, 2025\nTable of contents\n* [Top Tips and Updates: Winter Edition] \n* [GSB AI Course Policies] \n* [Teaching Strategies] \n* [Using AI Tools for Teaching] \n* [Additional Resources] \n**\n## Top Tips and Updates: Winter Edition\nTop highlights:\n* Nectir AI, a secure and customizable chatbot tool that integrates directly into Canvas, is now available (GSB only). Visit this[Canvas site] for how-to guides and to interact with sample bots, or[contact us] to learn more.\n* Starting Dec. 16,[Google Gemini] and[NotebookLM] will be available for Stanford students and faculty as secure tools when authenticated with your SUNet ID.\n* View[survey data] to learn more about how GSB instructors and students have been using AI in the classroom (SUNet Authentication required).\nDive into these top tips below:\n* Familiarize yourself with the[GSB&#8217;s policies on AI use and citation] for MBA/MSx courses, and share your course policies with students.\n* Test your coursework regularly and adopt[AI-aware teaching strategies] to ensure students are learning deeply in an AI world.\n* Use approved[AI tools for teaching], including Nectir AI custom chatbots or Google Gemini and NotebookLM.\n**\n## GSB AI Course Policies\n### Policy on student AI use for coursework\n**MBA/MSx courses:**Instructors may not ban student use of AI tools for take-home coursework, including assignments and exams, but may choose whether to allow student use of AI tools for in-class work. Learn more about the[GSB’s policy for student AI use].\n**PhD/Undergraduate courses:**Follow the[Generative AI Policy Guidance] from Stanford&#8217;s Office of Community Standards.\n### Policy on citing resources for coursework\nStudents are expected to[**cite all resources**] used to prepare their academic work, including AI tools. You can ask students to include AI-disclosure statements, submit chat logs, cite according to industry-specific standards, or take another approach.\nFind more strategies and templates in our resource on[citing generative AI use].\n### Sharing your course policy on AI use and citation\nState your policy in the[Course Policies and Norms form] and refer to this policy in your syllabus. For templates and tips on setting your course approach to AI use, see our resource on[Course Policies on Generative AI Use].\n**\n## Teaching Strategies\n### Design with AI in mind\nStudents today&#8230;**regularly use AI**across all areas of their course prep.\n↳Strategy:**Acknowledge wide AI use and discuss how to use it effectively.**For example:\n* Test your coursework with multiple AI tools every quarter\n* Provide clear assignment goals and guidelines, including the “why” behind your AI approaches\n* Discuss how AI is changing your industry, including limitations and opportunities\n* Ask your students to share their own creative uses\nStudents today&#8230;**have more information and analysis available**at their fingertips.\n↳Strategy:**Lean in to AI&#8217;s affordances.**For example:\n* Expect more (research, prototypes, examples, etc.)\n* Follow up with discussion questions that push beyond pre-prepared answers (ask for justification, counter-arguments, connections to others&#8217; comments, etc.)\n* Include real-world tasks that use AI authentically, in ways that students will encounter in the workplace\n### Promote deep learning\nStudents today&#8230;may**engage less in &#8220;productive struggle&#8221;**needed to deeply learn new material.\n↳Strategy:**Incorporate guardrails to support deep learning.**For example:\n* Hold students accountable with high standards of in-class work\n*Examples: in-person exams; real-time discussion, debate, or problem solving activities*\n* Encourage critique\n*Examples: pushing back on an AI’s work or asking for AI self-critique; verifying AI-generated sources and facts*\n* Emphasize process over product with iterative work\n*Examples: drafts, feedback, review points*\n**\n## Using AI Tools for Teaching\n### Stanford AI Playground\nThe[Stanford AI Playground] is a secure tool approved for[low and medium risk data], available at no cost to students and faculty. This platform offers multiple advanced and widely used AI models, including ChatGPT and Claude. Learn more with the[AI Playground Quick Start Guide].\n### Nectir AI Custom Chatbots\nNectir AI, a secure and customizable chatbot tool that integrates directly into Canvas, is now available for GSB faculty to use in their courses. Visit this[Canvas site] for how-to guides and to interact with sample bots, or[contact the TLHub] to learn more.\n### Google Gemini &amp; NotebookLM\nStarting on Dec. 16,[Google Gemini] and[NotebookLM] will be available for Stanford users, including students and faculty. These tools are approved for[low and medium risk data] when authenticated with your SUNet ID.\nLearn more about[using Stanford AI tools for teaching].\n**\n## Additional Resources\n### Reach out to the Teaching and Learning Hub\nWe can offer one-on-one consultations, answers to questions via email, and even a short presentation with Q&amp;A at your next faculty meeting. Please[reach out] to discuss your questions or specific use cases.\n### TLHub Resources\n* [Teaching in the AI Era] FAQ resource\n* [Canvas AI Course Assistant Gallery], with sample Nectir AI bots and info on getting started\n* [GSB Faculty Showcase on AI and Teaching] (202"
  },
  {
    "query": "Stanford University teaching and learning generative AI guidance",
    "title": "Introducing the New Academic Technology Tools List",
    "url": "https://ctl.stanford.edu/news/introducing-new-academic-technology-tools-list",
    "text": "Introducing the New Academic Technology Tools List | Center for Teaching and Learning\n[Skip to main content] [Skip to secondary navigation] \n[Stanford University(link is external)] \n[\nStanford\nCenter forTeaching and Learning\n] \nSearch this siteSubmit Search\nMain content start\n[Announcements],[Faculty],[Staff] \n# Introducing the New Academic Technology Tools List\nThe new interactive guide is your definitive guide to Stanford’s officially licensed and supported tools.\nNovember 7, 2025\nWe are pleased to announce the release of the[Academic Technology Tools List], a new resource for faculty, instructors, and staff seeking to enhance their teaching with vetted and supported academic technologies.\nDeveloped through a collaborative effort between the Center for Teaching and Learning and[Learning Technologies and Spaces], this list is your definitive guide to the university’s officially licensed and supported tools. The resource provides clear information on how to access and utilize essential platforms like Canvas, Zoom, Gradescope, and Panopto, alongside other valuable technologies for discussion, collaboration, polling, and assessment.\nThis initiative is designed to simplify your search for reliable tools, ensuring you have access to the best resources to support your pedagogical goals.\nThe new list is more than just a directory; it’s a detailed resource that helps you navigate your technology options. Each tool entry includes key details, such as its primary function, whether it integrates with Canvas, and whether it includes generative AI features (which are always off by default, giving instructors full control over their use). You can browse or filter the list by integration level, tool type (e.g., Adaptive Learning, Digital Whiteboard, Polling &amp; Surveys), and AI capabilities to quickly discover the right technology for your course needs. This allows you to efficiently match specific teaching activities or learning outcomes with the most appropriate, university-approved solution.\nWe encourage all faculty to explore the Academic Technology Tools List today to discover new ways to promote student engagement, manage assignments, and facilitate dynamic learning environments. This resource is a commitment to providing a transparent, trustworthy, and evolving suite of technologies that adhere to university policy and receive dedicated support.\nFor guidance on navigating potential benefits and risks, especially concerning AI, please refer to the[Responsible AI at Stanford] and[AI Meets Education at Stanford (AIMES)] resources.\nVisit the[Academic Technology Tools List] on the Teaching Commons website to get started.\n## More News Topics\n[Announcements] [Faculty] [Staff] \n## More News\n* [\n![] \n### Center celebrates 50 years of educational empowerment\n] \nNovember 11, 2025\nSince 1975, Stanford’s Center for Teaching and Learning has supported the university’s educational mission by providing practical, evidence-based resources for faculty, instructors\n* [In the News] \n* [Faculty] \n* [Staff] \n* [\n![] \n### How Stanford educators are bringing AI into the classroom\n] \nOctober 22, 2025\nA new initiative highlights how instructors are leveraging generative AI while imposing constraints on student use to foster critical thinking.\n* [In the News] \n* [Faculty] \n* [\n![] \n### Three projects awarded Curriculum Transformation Project funding\n] \nJune 18, 2025\nTeams in the sciences, engineering, and the humanities bring innovative ideas and teaching expertise to improving introductory undergraduate courses.\n* [Awards] \n* [Faculty] \n* [In the News] \nBack to Top"
  }
]