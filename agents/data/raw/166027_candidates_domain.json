[
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Generative AI Guidelines | Harvard University Information Technology",
    "url": "https://www.huit.harvard.edu/ai/guidelines",
    "text": "Generative AI Guidelines | Harvard University Information Technology[\nSkip to main contentarrow\\_circle\\_down\n] \nSearch\nSearchsearch\n[\n![Harvard University Information Technology] \n![Harvard University Information Technology] \n] \nmenucloseMenu\nSearch\nSearchsearch\n[\n![Harvard University Information Technology] \n![Harvard University Information Technology] \n] \n# Generative AI Guidelines\n# Guidelines for the use of Generative AI tools at Harvard\nPolicies &amp; guidelines\n![Woman works on her laptop in a library.] \n## Overview\nGenerative AI is a type of artificial intelligence that can learn from and mimic large amounts of data tocreate content such as text, images, music, videos, code, and more, based on inputs or prompts.The University supports responsible experimentation with[generative AI tools], but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. These guidelines are updated periodically.\n## Protect confidential data\nYou should not enter data[classified as confidential] (Level 2 and above, including non-public research data, finance, HR, student records, medical information, etc.) into publicly-available generative AI tools, in accordance with the University’s[Information Security Policy]. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties.\nLevel 2 and above confidential data must only be entered into generative AI tools that have been assessed and approved for such use by Harvard’s Information Security and Data Privacy office. See below for more information about approved tools.\n## Review content before publishing or sharing\nAI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called “hallucinations”) or may contain copyrighted material. You are responsible for any content that you publish or share that includes AI-generated material.\n## Adhere to local academic and administrative policies\nReview your School or Unit’s local policies around the use of generative AI. Many Schools have developed or updated policies around the use of generative AI in the classroom.[You can find links to local resources on the University’s generative AI website].\nFaculty should be clear with students they’re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed.\n## Be alert for phishing\nGenerative AI has made it easier for malicious actors to create sophisticated phishing emails and “deepfakes” (i.e., video or audio intended to convincingly mimic a person’s voice or physical appearance without their consent) at a far greater scale. Continue to[follow security best practices] and report suspicious messages to[phishing@harvard.edu].\n## Use approved tools for Harvard work\nHUIT and School IT providers have procured a range of generative AI tools with important contractual protections for use in Harvard work. These include security and privacy protections that ensure the tools are appropriate for use with certain types of confidential data, and assurances that the data entered will not be used to train vendor models.\n* [A list of available tools provided by HUIT can be found here]. More tools may be available from[your School IT provider].\n* AI meeting assistants should not be used in Harvard meetings, with the exception of approved tools with contractual protections. Consult the[AI Assistant Guidelines] for more information and how to manage unwanted AI assistants in meetings.\n* If you are considering procuring a generative AI tool not currently offered or have questions,[please contact HUIT]. All vendor generative AI tools must be[assessed for risk by Harvard's Information Security and Data Privacy office] prior to use in Harvard work.\n## Additional guidelines\n* [**AI Assistant Guidelines**]: Guidance on the use ofautomated meeting assistants (aka “AI note takers” or “bots”) in online meetings.\n* [**EU AI Act Prohibited Use Cases**]: Regulation on the use of AI technologies that may be developed or used in the European Union, or whose output may be used in the European Union.",
    "length": 4359,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "HGSE AI Policy",
    "url": "https://registrar.gse.harvard.edu/AI-policy",
    "text": "HGSE AI Policy | Office of the Registrar[\nSkip to main contentarrow\\_circle\\_down\n] \n[![Harvard Graduate School of Education]] \n[\nOffice of the Registrar\nHarvard Graduate School of Education\n] \nmenucloseMenu\nSearch\nSearchsearch\n[\nOffice of the Registrar\nHarvard Graduate School of Education\n] \n# HGSE AI Policy\n**HGSE POLICY ON STUDENT USE OF GENERATIVE ARTIFICIAL INTELLIGENCE IN ACADEMIC WORK**\n**Academic Year 2025-2026**\nGenerative Artificial Intelligence (AI) poses both great opportunities and great challenges for the field of education. Tools such as ChatGPT, DALL-E, and GitHub Copilot are having a profound influence on teaching and learning –and on your role as education practitioners and leaders. Your time at HGSE is an opportunity to learn to leverage such tools to produce more equitable access to and deeper engagement in education.\nHGSE encourages responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, copyright issues, the trustworthiness of the content they generate, and academic integrity. The implications of using generative AI for your own learning are equally salient. It might be possible to use this technology to complete some class assignments while doing little work yourself, but short cutting the process of thinking and writing in this way would rob you of the learning you came to HGSE to experience. At its best, generative AI can be like a tutor or thought partner with unlimited time to help you learn –but it should not be used to do the cognitive work for you, or else your own learning will be greatly diminished.\nThe following guidelines aim to ensure that, in your academic work at HGSE, you use generative AI when it can help you learn and not when it is a hindrance. If you have any doubt about whether a specific use of generative AI is permitted for an assignment or course, you are responsible for discussing it with your instructor prior to using it.\n1. Unless otherwise specified by your instructor, it is a violation of the HGSE Academic Integrity Policy to use generative AI to create all or part of an assignment for a course (e.g., a paper, memo, presentation, or short response) and submit it as your own. This rule parallels other rules. You may not ask another person to complete your assignment for a course. You may not copy from something someone else has created or re-write in your own words something someone else has written, without proper attribution.\n2. Permissible uses of generative AI in HGSE coursework include seeking clarification on concepts, brainstorming ideas, or generating scenarios that help contextualize what you are learning. For instance, it is fine to use AI-powered web search and to have “conversations” with tools like ChatGPT to help you explore ideas, refine your thinking, identify examples, and better understand course material. It is also acceptable to use generative AI to draft emails to instructors, students, and others in the HGSE community that are not being submitted as coursework.\n3. For any permitted use of GenAI tools, you must acknowledge and document that use in your assignment submission by explaining what tool(s) you used, prompts you provided (if applicable), and how you integrated the output into your work. If you cite directly from the tool, use proper citation format to credit the source. For more details and examples, see these APA guidelines:[How to cite ChatGPT].\n4. Keep in mind that the information provided by generative AI tools like ChatGPT is generated from unverified crowd-sourced information. Large language models can produce false claims or “hallucinations” and will regenerate any biases in the corpus of texts on which they are trained. You therefore should not trust the information as if it were equivalent to published research. You are ultimately responsible for the accuracy of the work you submit.\n5. The use of generative AI may also have implications for the protection of your own intellectual property. For example, if you upload your own original content to a generative AI tool, that content may become part of the tool’s models, which others may encounter and use. Conversely, if you use generative AI to develop your own original work, it may unexpectedly include others' copyrighted material.\n6. Certain uses of AI also infringe on copyright laws applicable to U.S. universities or contravene existing expectations for student conduct in HGSE courses. For example, the HGSE Student Handbook notes that “Students may not post, publish, sell, or otherwise publicly distribute course materials without the written permission of the course instructor. Such materials include, but are not limited to, the following: lecture slides, video, or audio recordings, assignments, problem sets, examinations, other students’ work, and answer keys. Students may not make recordings of course material for their own use without written permission of the instructor.” In keeping with these guidelines:\n7. Uploading any substantial course content —including text, video, readings, discussion-board pages, or audio recordings —is only allowable through the[Harvard-approved AI Sandbox], which ensures data entered is kept in a secure environment and not used to train public AI tools. The Sandbox is available through individual courses; if you have questions about the Sandbox, discuss with your instructor.\n8. It is forbidden to make your own recording of any course meetings, with or without AI tool integrations. If you require or would prefer that course meetings be recorded, discuss this request with your instructor. More broadly, if you require AI technology as part of an assistive technology solution to enable you to participate fully in the course, you must coordinate your usage with the Office of Student Affairs.\n9. Given the wide range of learning goals in courses at HGSE, individual instructors may create course-specific policies that differ from and supersede these guidelines. Again, if you have any doubt about whether a specific use of generative AI is permitted for an assignment or course, you are responsible for discussing it with your instructor prior to using it.\nNew ways of teaching and learning will emerge as generative AI becomes increasingly ubiquitous and robust. We thus anticipate that this policy will also evolve, with feedback from students and instructors.\n##### Featured Links\n[arrow\\_forwardmy.Harvard] [arrow\\_forwardStudent Handbook] [arrow\\_forwardRecords Request] [arrow\\_forwardRegistrar Communications]",
    "length": 6614,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Guidelines for Using ChatGPT and other Generative AI tools at ...",
    "url": "https://provost.harvard.edu/guidelines-using-chatgpt-and-other-generative-ai-tools-harvard",
    "text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard | Office of the Provost[\nSkip to main contentarrow\\_circle\\_down\n] \n[![Harvard University]] \n[\nOffice of the Provost\n] \nmenucloseMenu\nSearch\nSearchsearch\n[\nOffice of the Provost\n] \n# Guidelines for Using ChatGPT and other Generative AI tools at Harvard\nDear Members of the Harvard Community,\nWe write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI’s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.\nGenerative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.\n***Initial guidelines for use of generative AI tools:***\n* **Protect confidential data:**You should not enter data[classified as confidential] (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University’s[Information Security Policy]. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties.\n* **You are responsible for any content that you produce or publish that includes AI-generated material:**AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called “hallucinations”), or may contain copyrighted material. Review your AI-generated content before publication.\n* **Adhere to current policies on academic integrity:**Review your School’s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they’re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed.\n* **Be alert for AI-enabled phishing:**Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to[follow security best practices] and report suspicious messages to[phishing@harvard.edu].\n* **Connect with HUIT before procuring generative AI tools:**The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds.\n* If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at[ithelp@harvard.edu].\n* Vendor generative AI toolsmust be[assessed for risk by Harvard’s Information Security and Data Privacy officeprior to use].\nIt is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use,[on the HUIT website], which will be updated as new information becomes available. Sincerely,\nAlan M. Garber\nProvost\nMeredith Weenick\nExecutive Vice President\nKlara Jelinkova\nVice President and University Chief Information Officer",
    "length": 3499,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Responsible use of generative AI",
    "url": "https://it.hms.harvard.edu/about/policies/responsible-use-generative-ai",
    "text": "Generative AI | HMS IT[Skip to main content] \n**\n[![]] \n**\nMenu\n**Search\n# Generative AI\n* [About] \n* [Policies and guidelines] \n* [Email Terms of Use] \n* [Email policy] \n* [External use of HMS High-Performance Computing (HPC) Resources] \n* [Generative AI] \n* [HMS IT Code of Conduct] \n* [Information security] \n* [Privacy] \n* [Use of Computing Facilities] \n## Guidance and tools for secure, ethical use of AI at HMS\nGenerative AI offers new opportunities in research, education, and productivity —but it must be used thoughtfully.\nThis page builds on[Initial guidelines for the use of Generative AI tools at Harvard] to outline HMS recommendations and approved tools to help you use AI responsibly, in line with policies and data security requirements. Learn how to choose the right tool, meet compliance standards, and protect confidential information. [\n### Responsible use\nUse AI tools thoughtfully and within academic boundaries.\nIntegrityAccuracy\n] \nResponsible AI use at HMS includes ethical, transparent practices:\n* **Verify outputs**. AI-generated content may be incorrect or fabricated. Always fact-check before using or publishing results.\n* **Uphold academic integrity**. Follow faculty or student handbook policies. Be transparent with students and colleagues about acceptable use.\n* **Cite AI contributions**appropriately in research and academic work. Refer to guidance from COPE and WAME for citation standards.\n[\n### Data and security\nProtect sensitive information and maintain compliance.\nData levelsPhishing\n] \nBefore using any generative AI tools, ensure you're complying with Harvard’s data and security policies:\n* **Avoid entering**[**confidential information**] (Level 2 and above) into public-facing AI platforms.\n* **Classify your data**appropriately. Most AI tools are approved for**Level 3 and below**. For Level 4 or regulated data, contact[**hms-it-ai@hms.harvard.edu**] before proceeding.\n* **Consult HMS IT**for support with data security, platform vetting, and vendor risk. Use[this form] to request help.\n* **Comply with regulations**such as HIPAA, FERPA, and sponsor-specific data requirements.\n* **Legal and funding agency compliance**is essential. Align AI use with NIH, COPE, and other relevant bodies.\n* **Beware of phishing**. Generative AI can be used for malicious purposes like phishing.[Stay alert] and report suspicious messages to[**phishing@harvard.edu**].\n* **Procurement support**for licensing terms or contracts is available by sending an email message to[**procurement@hms.harvard.edu**].\n[\n### Getting started\nSteps to access and begin using generative AI tools.\nAccessTraining\n] \nTo begin using AI tools at HMS,**choose the right tool**based on your use case:\n* **Text and code creation**– Harvard AI Sandbox, ChatGPT Edu\n* **Image creation**– Harvard AI Sandbox, Adobe Firefly\n* **Compare models**– Harvard AI Sandbox\n* **Custom GPTs**– ChatGPT Edu\n* **Information retrieval**– ChatGPT Edu, Harvard AI Sandbox\n* **App development**– HUIT API Portal, HMS Azure AI, or Longwood Cluster\nAfter you have selected the tools that you need:\n1. **Complete the required training**listed on the tool's service page.\n2. **Submit a request form**to gain access to the tool.\n3. **Contact support teams**with any issues or questions related to the tools.\n[\n### Supported tools\nCompare features, use cases, and access requirements.\nEligibilityUse case\n] \nThe following table summarizes key information about available generative AI tools at HMS:\n|**Tool**|**Use cases**|**Data security level**|**Eligibility**|**Training required**|**Cost**|\n[**AI Sandbox**] |Create content; compare models|Level 3|Quad-based faculty, staff, MD/DMD students; some others via courses|Yes|Covered by HMS IT|\n[**ChatGPT Edu**] |Text, images, custom GPTs, info retrieval|Level 3|Licensed users with specific needs|Yes|Covered by HMS IT|\n[**Adobe Firefly**] |Image creation|Level 3|Users with Adobe Creative Cloud|No|Included in Adobe license|\n[**HMS Azure AI**] |Azure OpenAI API access, large dataset analysis|Level 4|HMS IT, project developers|No|Varies with use|\n**HUIT API Portal**|API access for app development|Level 3|HMS IT, project developers|No|Varies with use|\n[**Longwood Cluster**] |High-compute AI workloads|Level 4|HMS researchers|No|Free for DIA winners|\n## Additional resources\n### Tools and policies\n#### [AI tools at Harvard] \nHarvard-endorsed services for the community.\n#### [Information Security Policy] \nData protection and IT security requirements.\n#### [Confidential data guidance] \nHow to handle confidential data when using AI.\n### Ethics and regulations\n#### [COPE: AI in authorship] \nAuthorship ethics for AI-generated content.\n#### [WAME guidelines] \nEditorial guidance for medical writing.\n#### [NIH guidelines] \nAI usage in NIH-supported research.\n&copy; 2026",
    "length": 4810,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Generative AI Guidance - Office of Undergraduate Education",
    "url": "https://oue.fas.harvard.edu/faculty-resources/generative-ai-guidance/",
    "text": "Generative AI Guidance &#8211; Office of Undergraduate Education\n[Skip to content] \n[![Harvard University homepage]] \n[HARVARD.EDU] \n![Students use laptop comuters.] \n[HOME] /[FACULTY RESOURCES] \n# Generative AI Guidance\nResources for instructors regarding appropriate use of generative AI in courses.\nBackground video of aerial view of Harvard University and other b roll video of the inside of campus buidlings\n## **On This Page**\n[SAMPLE AI POLICIES] \n[AI RESOURCES] \n[EVENT RECORDINGS] \n[FREQUENTLY ASKED QUESTIONS] \nHarvard supports responsible experimentation with generativeAItools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.\n## Announcements\nStarting in Fall 2025, faculty can find Respondus, a new browser lockdown tool, on their Canvas site(s) to use for in-person seated exams and quizzes to ensure that students do not use AI unless the course asks them to do so.[Contact the Academic Technology Group] with questions or to get started.\n## Policies for the use of AI in courses\nAll faculty are required to inform students of the policies governing generative AI use in class.Whether students in your course are forbidden from using ChatGPT or expected to explore its limits, a policy helps ensure that your expectations forappropriate interaction with generative AI tools are clear to students.Once you decide on a policy, make sure you articulate it clearly for your students, so that they know what is expected of them. More specifically,**you should post your policy on your Canvas site**.\nYou can choose from among the below example policies to add to your Canvas site, or you can design your own to suit the needs of your course.\nA maximally restrictive draft policy:\n> *> We expect that all work students submit for this course will be their own. In instances when collaborative work is assigned, we expect for the assignment to list all team members who participated. We specifically forbid the use of ChatGPT or any other generative artificial intelligence (AI) tools at all stages of the work process, including preliminary ones. Violations of this policy will be considered academic misconduct. We draw your attention to the fact that different classes at Harvard could implement different AI policies, and it is the student’s responsibility to conform to expectations for each course.&nbsp;\n*\n> A fully-encouraging draft policy:\n> *> This course encourages students to explore the use of generative artificial intelligence (GAI) tools such as ChatGPT for all assignments and assessments. Any such use must be appropriately acknowledged and cited. It is each student’s responsibility to assess the validity and applicability of any GAI output that is submitted; you bear the final responsibility. Violations of this policy will be considered academic misconduct. We draw your attention to the fact that different classes at Harvard could implement different AI policies, and it is the student’s responsibility to conform to expectations for each course.&nbsp;\n*\n> Mixed draft policy:\n> *> Certain assignments in this course will permit or even encourage the use of generative artificial intelligence (GAI) tools such as ChatGPT. The default is that such use is disallowed unless otherwise stated. Any such use must be appropriately acknowledged and cited. It is each student’s responsibility to assess the validity and applicability of any GAI output that is submitted; you bear the final responsibility. Violations of this policy will be considered academic misconduct. We draw your attention to the fact that different classes at Harvard could implement different AI policies, and it is the student’s responsibility to conform to expectations for each course.&nbsp;\n*\n> ## Additional AI Resources\n![] ### [AI Pedagogy Project] \nVisit the[AI Pedagogy Project (AIPP)], developed by the metaLAB at Harvard, for an introductory guide to AI tools, an LLM Tutorial, additional AI resources, and curated assignments to use in your own classroom. The metaLAB has also published a quick start guide for[Getting Started with ChatGPT] \n![Derek Bok Center for Teaching and Learning logo.] ### [Teaching and Artificial Intelligence] \nVisit the Bok Center for Teaching and Learning website for resources on teaching in the age of AI that includes information ondesigning courses and assessments, communicating with students about AI, and examples for using AI in your teaching.\n### [Teaching at the Faculty of Arts and Sciences] \nThe Teaching at FAS website, a collaborative project between several college and university offices, offers[a list of resources] for Harvard faculty related to designing and teaching courses.\n## Generative AI event recordings\nIn August 2023, Amanda Claybaugh, Dean of Undergraduate Education, and Christopher Stubbs, Dean of Science, hostedinformational sessions on the use of generative AI incourses. In each session, faculty presented examples of new assignments they have developed, as well as advice on how to “AI-proof” familiar assignments, and shared thoughts about how to guide students in using these technologies responsibly.\n### Generative AI and Your Writing Course\nAugust 9, 2023\n[![Embedded YouTube video]] \n### Generative AI and Your STEM Course\nAugust 8, 2023\n[![Embedded YouTube video]] \n## FAQ\nWhat is ChatGPT?\n Generative artificial intelligence (GAI) tools such as Chat-GPT represent a significant advance in natural-language interaction with computers. On the basis of a ‘prompt’ a GAI system can produce surprisingly human-like responses including narrative passages and responses to technical questions. Moreover, an iterative exchange with the AI system can produce refined and tuned responses. This technology is evolving very rapidly. GAI systems have demonstrated the ability to pass the medical licensing exam, pass the bar exam, to generate art and music, answer graduate level problems from physics courses. These GAI systems are far from perfect, and some of the material they provide as factual are incorrect. GAI technology is a disruptive and rapidly changing new technology that will impact many aspects of our lives.\nWeaknesses of many GAIs at present include their inability to perform basic arithmetic calculations, and the propensity for ‘hallucinations’. Also, the GAI responses will reflect the biases and inaccuracies that are contained in the training data. It’s important to realize that there is an intentional ‘random’ element in the responses for most GAI systems. The same input does not always produce the same output. Also, the provenance of information that is used in responses does not flow through to the output, and this limits our ability to perform validation. But GAI capabilities are changing rapidly and we should anticipate ongoing refinements and progress. We currently don’t think twice about use spell-checking and grammar-checking tools in word processors, picking a suggested next-word when composing a text message on a phone, or using electronic calculators and spreadsheets. It will be interesting to observe whether GAI tools will become similarly integrated into everyday workflow.\nHow does this affect our teaching?\n Generative AI systems can produce responses to homework problem sets, to essay assignments, and to take-home exam questions. We should assume that all our students are proficient with these tools and should adjust our expectations accordingly. While it’s true that our students could previously draw upon various resources to avoid doing assignments themselves, the ease-of-use, free access, and high performance of GAI systems have raised this to a new level. Our challenge is to incentivize the level of engagement that leads to a deeper understanding and the development of the habits of mind we hope to instill in our students.\nA good first step is to feed representative assignments from your upcoming courses into a GAI tool such as ChatGPT and take a look at what it produces. Then assume that if given the opportunity, many of the students in your course are likely to do the same thing. Based on this insight, decide how to best adapt to, adjust to, and as appropriate incorporate GAI into your instructional plans. Then decide on and disseminate a course-by-course policy on student use of GAI tools.For more guidance about generative AI in teaching, visit the Bok Center&#8217;&#8217;s AI resources.\n[Bok Center AI Resources] \nAs this situation evolves, we need to learn how to best use these tools to enhance learning. We also need to teach our students how to use these tools in an ethical and responsible manner.\nHow can I get a ChatGPT account?\n Go to[chat.OpenAi.com] and register for an account. It’s free. You can find many quick-start guides online.\nBe sure to review Harvard’s guidelines for use of these tools at[https://huit.harvard.edu/ai].\nIs there a technology that can detect unauthorized use of ChatGPT?\nThere are a variety of tools that claim various degrees of success in finding instances when GAI was used, but this is something of an arms race. It would be inadvisable to count on automated methods for GAI detection. FAS does not plan to provide/license such a tool for use in courses.\nIs there a technology that can block students from accessing the internet so that they can use their laptops for in-class exams?\n While there are technologies for exam proctoring that aim to block students from using the internet, FAS does not plan to provide/license such a tool for use in courses.\nIs it appropriate to enter student work into ChatGPT to generate feedback, or for students to enter their work into ChatGPT?\n No confidential information can be loaded into GAI systems, since there is no expectation of privacy or confidentiality. Faculty must get documented permission from students before putting original student content into any generative AI tool, and students should be made aware of the risks of entering their original work into such tools.\nChatGPT’s[terms of service] allow the company to access any information fed into it.\nWhat tools do we and our students have access to?\nHarvard HUIT has compiled a list of available tools at[https://huit.harvard.edu/ai/tool].\n**close menu\n**close search\nSearch\n**Search Site",
    "length": 10318,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Generative AI Policy - Nieman Foundation - Harvard University",
    "url": "https://nieman.harvard.edu/about/generative-ai-policy/",
    "text": "Generative AI Policy - Nieman Foundation[Skip to content] \n* [Nieman Foundation at Harvard] \n* [Fellowships] \n* [Reports] \n* [Lab] \n* [Storyboard] \n* [Subscribe] \nSearch\n[![Nieman Foundation]] \n* [Fellowships] \n* [Current Fellows] \n* [Awards &amp; Conferences] \n* [Alumni] \n* [News] \n* [About] \n* [Make a Gift] \n[![Nieman Foundation]] \n* [Fellowships] \n* [Current Fellows] \n* [Awards &amp; Conferences] \n* [Alumni] \n* [News] \n* [About] \n* [Make a Gift] \n* [Subscribe] \n* [Nieman Foundation at Harvard] \n* [Fellowships] \n* [Reports] \n* [Lab] \n* [Storyboard] \n## [About] \n# Generative AI Policy\nRecognizing the potential and pitfalls of generative artificial intelligence (AI) tools in the workplace, the Nieman Foundation for Journalism at Harvard has created these guidelines for staff members and contractors hired by the Nieman Foundation, including contributors to Nieman’s three publications:[Nieman Journalism Lab],[Nieman Reports], and[Nieman Storyboard].\nGenerative AI tools, overseen carefully by humans, can be useful and beneficial in parts of our workflow.\nThis policy reflects Nieman’s long-standing commitment to ethical reporting and the journalistic principles of truth, accuracy, and transparency. The guidelines are designed to protect Nieman’s reputation and credibility and safeguard audience trust. This document will be updated and modified as needed as AI technology evolves.\n**The use of AI-generated content, including text, images, audio, and video, is subject to the following guidelines:**\nWe do not publish stories drafted or edited using generative AI tools.\nA staff member or designated person will always vet the following material before it is published or incorporated into a story:\n* Information gathered through AI research tools\n* AI-generated transcriptions and translations\n* AI-generated text for social media posts or headlines\n* Copyedits suggested by generative AI tools\nWe will not use AI image generators to create photorealistic depictions of real people or places, or to alter images of real people or places. Any AI-generated illustrations or composites we publish will be clearly labeled, and the image caption will disclose the method of generation. Every visual must serve a clear editorial purpose and uphold our responsibility to inform, not mislead.\nAny AI-generated material in our publications or on our websites (images, translations, etc.) will be clearly identified for readers.\nIn addition, Nieman adheres to the[guidelines for generative AI use] issued by the Harvard Public Affairs &amp; Communications Office:\n* AI should augment human creativity and decision-making, not replace it. Transparency in how AI is used for content creation and decision-making is critical, especially when it comes to automated content. Ethical concerns such as bias, intellectual property, and AI’s role in content should be addressed upfront. Clear communication about the involvement of AI fosters trust with the audience.\n* AI can help scale content creation, but quality and relevance must remain a top priority.\n* Digital content strategists may use AI to assess what content performs best and optimize content for targeted audiences as well as search engines.\nAll freelance contributors should familiarize themselves with these policies and follow them in their work for the Nieman Foundation and its publications.\n*Last updated September 30, 2025*",
    "length": 3396,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Guidance on AI meeting assistants; data privacy principles | Harvard ...",
    "url": "https://www.huit.harvard.edu/news/guidance-ai-meeting-assistants-data-privacy-principles",
    "text": "Guidance on AI meeting assistants; data privacy principles | Harvard University Information Technology[\nSkip to main contentarrow\\_circle\\_down\n] \nSearch\nSearchsearch\n[\n![Harvard University Information Technology] \n![Harvard University Information Technology] \n] \nmenucloseMenu\nSearch\nSearchsearch\n[\n![Harvard University Information Technology] \n![Harvard University Information Technology] \n] \n# Guidance on AI meeting assistants; data privacy principles\nFebruary 11, 2025\nDear Members of the Harvard Community,\nSince the rapid emergence of generative artificial intelligence (AI) tools, members of our community have embraced experimentation in exciting ways, examples of which you can see on[Harvard’s generative AI website]. As these tools evolve, the University is continuously assessing their use to support responsible experimentation while protecting the privacy and security of Harvard’s data.\nAs a reminder, before using generative AI in your Harvard work, please consult the[University-wide guidelines], which are updated periodically based on community feedback and technological developments. Some Schools, departments, and courses have also issued[additional guidelines or policies].\nWe write today to share additional guidance on the use of AI meeting assistants and to share best practices on data privacy more broadly.\n### **New guidance on AI meeting assistants**\nAI meeting assistants or “bots”—either built into existing collaboration tools such as Zoom or added by individual meeting participants using third-party software—can transcribe and summarize online meetings or enhance meeting accessibility. However, they can also pose substantial privacy, regulatory, and legal risks, and have the potential to stifle conversation and open inquiry.\nAccordingly, the University has issued[new guidance on AI assistants].**AI meeting assistants should not be used in Harvard meetings, with the exception of approved tools with contractual protectionsas outlined in these guidelines.**We will continue to evaluate their use in our environment.\n### **Data privacy principles**\nData privacy considerations —including the collection and use of personal information—apply across the breadth of Harvard’s activities and are not limited to the use of AI. To assist our community in understanding and managing data privacy, HUIT has collaborated with faculty and administrators to develop University-wide[**Privacy Principles**] and online training.\nThese principles are not new policy but rather aspirational values, such as data minimization, transparency, and stewardship. We invite you to familiarize yourself with these principles and[contact HUIT] or[your School’s Information Security and Data Privacy Officer] with any questions.\nWe thank you for continuing to experiment responsibly with generative AI, and for keeping Harvard’s data private and secure.\nSincerely,\nKlara Jelinkova\nVice President and University Chief Information Officer\nMichael Tran Duff\nUniversity Chief Information Security and Data Privacy Officer",
    "length": 3033,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "AI Guidance & FAQs",
    "url": "https://oue.fas.harvard.edu/ai-guidance",
    "text": "[Skip to main content] \n\n- [Main Menu] \n- [Utility Menu] \n- [Search] \n\n[![University Logo]] \n\n[HARVARD.EDU] \n\n[HOME] / [FOR FACULTY & STAFF] / [TEACHING RESOURCES FOR FACULTY] /\n\nHarvard supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.\n\nThe Office of Undergraduate Education has compiled the following resources for instructors regarding appropriate use of generative AI in courses.\n\n## Generative AI Event Recordings\n\nIn August 2023, Amanda Claybaugh, Dean of Undergraduate Education, and Christopher Stubbs, Dean of Science, hosted informational sessions on the use of generative AI in courses. In each session, faculty presented examples of new assignments they have developed, as well as advice on how to “AI-proof” familiar assignments, and shared thoughts about how to guide students in using these technologies responsibly.\n\n### Generative AI in Your STEM Course - August 8, 2023\n\n[iframe] \n\n### Generative AI in Your Writing Course - August 9, 2023\n\n[iframe] \n\n## Policies for the Use of AI in Courses\n\nWe encourage all instructors to include a policy in course syllabi regarding the use and misuse of generative AI. Whether students in your course are forbidden from using ChatGPT or expected to explore its limits, a policy helps ensure that your expectations for appropriate interaction with generative AI tools are clear to students. Once you decide on a policy, make sure you articulate it clearly for your students, so that they know what is expected of them. More specifically, **you should post your policy on your Canvas site**.\n\nBelow is sample language you may adopt for your own policy. Feel free to modify it or create your own to suit the needs of your course.\n\n### A maximally restrictive draft policy:\n\n_We expect that all work students submit for this course will be their own. In instances when collaborative work is assigned, we expect for the assignment to list all team members who participated. We specifically forbid the use of ChatGPT or any other generative artificial intelligence (AI) tools at all stages of the work process, including preliminary ones. Violations of this policy will be considered academic misconduct. We draw your attention to the fact that different classes at Harvard could implement different AI policies, and it is the student’s responsibility to conform to expectations for each course._\n\n### A fully-encouraging draft policy:\n\n_This course encourages students to explore the use of generative artificial intelligence (GAI) tools such as ChatGPT for all assignments and assessments. Any such use must be appropriately acknowledged and cited. It is each student’s responsibility to assess the validity and applicability of any GAI output that is submitted; you bear the final responsibility. Violations of this policy will be considered academic misconduct. We draw your attention to the fact that different classes at Harvard could implement different AI policies, and it is the student’s responsibility to conform to expectations for each course._\n\n### Mixed draft policy:\n\n_Certain assignments in this course will permit or even encourage the use of generative artificial intelligence (GAI) tools such as ChatGPT. The default is that such use is disallowed unless otherwise stated. Any such use must be appropriately acknowledged and cited. It is each student’s responsibility to assess the validity and applicability of any GAI output that is submitted; you bear the final responsibility. Violations of this policy will be considered academic misconduct. We draw your attention to the fact that different classes at Harvard could implement different AI policies, and it is the student’s responsibility to conform to expectations for each course._\n\n## Additional AI Resources\n\n#### [AI Pedagogy Project] \n\nVisit the [AI Pedagogy Project (AIPP)], developed by the metaLAB at Harvard, for an introductory guide to AI tools, an LLM Tutorial, additional AI resources, and curated assignments to use in your own classroom. The metaLAB has also published a quick start guide for [Getting Started with ChatGPT] \n\n#### [Teaching and Artificial Intelligence] \n\nA [Canvas module], created by the Bok Center for Teaching and Learning, for instructors teaching in the age of AI that includes information on [creating syllabus statements], [writing assignments], and [in-class assessments]. The Bok Center also offers [advice and consultations]  for faculty seeking to respond to the challenges and opportunities posed by AI.\n\n#### [Teaching at the Faculty of Arts and Sciences] \n\nThe Teaching at FAS website, a collaborative project between several college and university offices, offers [a list of resources] for Harvard faculty related to designing and teaching courses.\n\n## Frequently-Asked Questions about ChatGPT and Generative AI\n\n### What is ChatGPT?\n\nGenerative artificial intelligence (GAI) tools such as Chat-GPT represent a significant advance in natural-language interaction with computers. On the basis of a ‘prompt’ a GAI system can produce surprisingly human-like responses including narrative passages and responses to technical questions. Moreover, an iterative exchange with the AI system can produce refined and tuned responses. This technology is evolving very rapidly. GAI systems have demonstrated the ability to pass the medical licensing exam, pass the bar exam, to generate art and music, answer graduate level problems from physics courses. These GAI systems are far from perfect, and some of the material they provide as factual are incorrect. GAI technology is a disruptive and rapidly changing new technology that will impact many aspects of our lives.\n\nWeaknesses of many GAIs at present include their inability to perform basic arithmetic calculations, and the propensity for ‘hallucinations’. Also, the GAI responses will reflect the biases and inaccuracies that are contained in the training data. It’s important to realize that there is an intentional ‘random’ element in the responses for most GAI systems. The same input does not always produce the same output. Also, the provenance of information that is used in responses does not flow through to the output, and this limits our ability to perform validation. But GAI capabilities are changing rapidly and we should anticipate ongoing refinements and progress. We currently don’t think twice about use spell-checking and grammar-checking tools in word processors, picking a suggested next-word when composing a text message on a phone, or using electronic calculators and spreadsheets. It will be interesting to observe whether GAI tools will become similarly integrated into everyday workflow.\n\n### How does this affect our teaching?\n\nGenerative AI systems can produce responses to homework problem sets, to essay assignments, and to take-home exam questions. We should assume that all our students are proficient with these tools and should adjust our expectations accordingly. While it’s true that our students could previously draw upon various resources to avoid doing assignments themselves, the ease-of-use, free access, and high performance of GAI systems have raised this to a new level. Our challenge is to incentivize the level of engagement that leads to a deeper understanding and the development of the habits of mind we hope to instill in our students.\n\nA good first step is to feed representative assignments from your upcoming courses into a GAI tool such as ChatGPT and take a look at what it produces. Then assume that if given the opportunity, many of the students in your course are likely to do the same thing. Based on this insight, decide how to best adapt to, adjust to, and as appropriate incorporate GAI into your instructional plans. Then decide on and disseminate a course-by-course policy on student use of GAI tools. For more guidance about generative AI in teaching, visit the [Bok Center’s AI resources].\n\nAs this situation evolves, we need to learn how to best use these tools to enhance learning. We also need to teach our students how to use these tools in an ethical and responsible manner.\n\n### How can I get a ChatGPT account?\n\nGo to [chat.OpenAi.com] and register for an account. It’s free. You can find many quick-start guides online.\n\nBe sure to review Harvard’s guidelines for use of these tools at [https://huit.harvard.edu/ai].\n\n### Is there a technology that can detect unauthorized use of ChatGPT?\n\nThere are a variety of tools that claim various degrees of success in finding instances when GAI was used, but this is something of an arms race. It would be inadvisable to count on automated methods for GAI detection. FAS does not plan to provide/license such a tool for use in courses.\n\n### Is there a technology that can block students from accessing the internet so that they can use their laptops for in-class exams?\n\nWhile there are technologies for exam proctoring that aim to block students from using the internet, FAS does not plan to provide/license such a tool for use in courses.\n\n### Is it appropriate to enter student work into ChatGPT to generate feedback, or for students to enter their work into ChatGPT?\n\nNo confidential information can be loaded into GAI systems, since there is no expectation of privacy or confidentiality. Faculty must get documented permission from students before putting original student content into any generative AI tool, and students should be made aware of the risks of entering their original work into such tools.\n\nChatGPT’s [terms of service] allow the company to access any information fed into it.\n\n### What tools do we and our students have access to?\n\nHarvard HUIT has compiled a list of available tools at [https://huit.harvard.edu/ai/tool].\n\n- [Resources for Faculty] \n - [Start of Term Information for Faculty] \n - [Teaching Resources for Faculty] \n - [Teaching and Learning Support] \n - [Academic Field Trips] \n - [AI Guidance & FAQs] \n - [Mid-Semester Feedback in Blue] \n - [Welcoming Visitors to Courses] \n - [Academic Policy & Handbooks] \n - [Instructional Support] \n - [Funding for Teaching] \n - [Resources for Concentrations] \n - [Climate in the Curriculum Retreat] \n - [Derek Bok Center for Teaching and Learning] \n\nCopyright © 2025 The President and Fellows of Harvard College \\| [Accessibility] \\| [Digital Accessibility] \\| [Report Copyright Infringement]",
    "length": 10494,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Generative AI | HMS IT - Harvard University",
    "url": "https://it.hms.harvard.edu/about/policies-and-guidelines/generative-ai",
    "text": "Generative AI | HMS IT[Skip to main content] \n**\n[![]] \n**\nMenu\n**Search\n# Generative AI\n* [About] \n* [Policies and guidelines] \n* [Email Terms of Use] \n* [Email policy] \n* [External use of HMS High-Performance Computing (HPC) Resources] \n* [Generative AI] \n* [HMS IT Code of Conduct] \n* [Information security] \n* [Privacy] \n* [Use of Computing Facilities] \n## Guidance and tools for secure, ethical use of AI at HMS\nGenerative AI offers new opportunities in research, education, and productivity —but it must be used thoughtfully.\nThis page builds on[Initial guidelines for the use of Generative AI tools at Harvard] to outline HMS recommendations and approved tools to help you use AI responsibly, in line with policies and data security requirements. Learn how to choose the right tool, meet compliance standards, and protect confidential information. [\n### Responsible use\nUse AI tools thoughtfully and within academic boundaries.\nIntegrityAccuracy\n] \nResponsible AI use at HMS includes ethical, transparent practices:\n* **Verify outputs**. AI-generated content may be incorrect or fabricated. Always fact-check before using or publishing results.\n* **Uphold academic integrity**. Follow faculty or student handbook policies. Be transparent with students and colleagues about acceptable use.\n* **Cite AI contributions**appropriately in research and academic work. Refer to guidance from COPE and WAME for citation standards.\n[\n### Data and security\nProtect sensitive information and maintain compliance.\nData levelsPhishing\n] \nBefore using any generative AI tools, ensure you're complying with Harvard’s data and security policies:\n* **Avoid entering**[**confidential information**] (Level 2 and above) into public-facing AI platforms.\n* **Classify your data**appropriately. Most AI tools are approved for**Level 3 and below**. For Level 4 or regulated data, contact[**hms-it-ai@hms.harvard.edu**] before proceeding.\n* **Consult HMS IT**for support with data security, platform vetting, and vendor risk. Use[this form] to request help.\n* **Comply with regulations**such as HIPAA, FERPA, and sponsor-specific data requirements.\n* **Legal and funding agency compliance**is essential. Align AI use with NIH, COPE, and other relevant bodies.\n* **Beware of phishing**. Generative AI can be used for malicious purposes like phishing.[Stay alert] and report suspicious messages to[**phishing@harvard.edu**].\n* **Procurement support**for licensing terms or contracts is available by sending an email message to[**procurement@hms.harvard.edu**].\n[\n### Getting started\nSteps to access and begin using generative AI tools.\nAccessTraining\n] \nTo begin using AI tools at HMS,**choose the right tool**based on your use case:\n* **Text and code creation**– Harvard AI Sandbox, ChatGPT Edu\n* **Image creation**– Harvard AI Sandbox, Adobe Firefly\n* **Compare models**– Harvard AI Sandbox\n* **Custom GPTs**– ChatGPT Edu\n* **Information retrieval**– ChatGPT Edu, Harvard AI Sandbox\n* **App development**– HUIT API Portal, HMS Azure AI, or Longwood Cluster\nAfter you have selected the tools that you need:\n1. **Complete the required training**listed on the tool's service page.\n2. **Submit a request form**to gain access to the tool.\n3. **Contact support teams**with any issues or questions related to the tools.\n[\n### Supported tools\nCompare features, use cases, and access requirements.\nEligibilityUse case\n] \nThe following table summarizes key information about available generative AI tools at HMS:\n|**Tool**|**Use cases**|**Data security level**|**Eligibility**|**Training required**|**Cost**|\n[**AI Sandbox**] |Create content; compare models|Level 3|Quad-based faculty, staff, MD/DMD students; some others via courses|Yes|Covered by HMS IT|\n[**ChatGPT Edu**] |Text, images, custom GPTs, info retrieval|Level 3|Licensed users with specific needs|Yes|Covered by HMS IT|\n[**Adobe Firefly**] |Image creation|Level 3|Users with Adobe Creative Cloud|No|Included in Adobe license|\n[**HMS Azure AI**] |Azure OpenAI API access, large dataset analysis|Level 4|HMS IT, project developers|No|Varies with use|\n**HUIT API Portal**|API access for app development|Level 3|HMS IT, project developers|No|Varies with use|\n[**Longwood Cluster**] |High-compute AI workloads|Level 4|HMS researchers|No|Free for DIA winners|\n## Additional resources\n### Tools and policies\n#### [AI tools at Harvard] \nHarvard-endorsed services for the community.\n#### [Information Security Policy] \nData protection and IT security requirements.\n#### [Confidential data guidance] \nHow to handle confidential data when using AI.\n### Ethics and regulations\n#### [COPE: AI in authorship] \nAuthorship ethics for AI-generated content.\n#### [WAME guidelines] \nEditorial guidance for medical writing.\n#### [NIH guidelines] \nAI usage in NIH-supported research.\n&copy; 2026",
    "length": 4810,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Research with Generative AI",
    "url": "https://www.harvard.edu/ai/research-resources/",
    "text": "Research with Generative AI - Generative AI @ Harvard\n[Skip to main content] \n[![A logo that says Generative AI at Harvard]] \nGenerative AI**\n# Research with Generative AI\nResources for scholars and researchers\nGenerative AI (GenAI) technologies offer new opportunities to advance research and scholarship. This resource page aims to provide Harvard researchers and scholars with basic guidance, information on available resources, and contacts. The content will be regularly updated as these technologies continue to evolve.[Your feedback is welcome.] \n## Leading the way\nHarvard&#8217;s researchers are making strides not only on generative AI, but the larger world of artificial intelligence and its applications. Learn more about key efforts.\n### The Kempner Institute\nThe Kempner Institute is dedicated to revealing the foundations of intelligence in both natural and artificial contexts, and to leveraging these findings to develop groundbreaking technologies.\nLearn more\n[Learn more] \n### Harvard Data Science Initiative\nThe Harvard Data Science Initiative is dedicated to understanding the many dimensions of data science and propelling it forward.\nLearn more\n[Learn more] \n### More AI @ Harvard\nGenerative AI is only part of the fascinating world of artificial intelligence. Explore Harvard’s groundbreaking and cross-disciplinary academic work in AI.\nLearn more\n[Learn more] \n## Frequently asked questions\n### Can I use generative AI to write and/or develop research papers?\nAcademic publishers have a range of policies on the use of AI in research papers. In some cases, publishers may prohibit the use of AI for certain aspects of paper development. You should review the specific policies of the target publisher to determine what is permitted.\nHere is a sampling of policies available online:\n* [Elsevier] \n* [JAMA and the JAMA Network] \n* [PLOS ONE] \n* [Sage] \n* [Springer Nature] \n* [Science] \n* [Wiley] \n### How should AI-generated content be cited in research papers?\nGuidance will likely develop as AI systems evolve, but some leading style guides have offered recommendations:\n* [APA Style] \n* [The Chicago Manual of Style] \n* [MLA Style Guide] \n### Should I disclose the use of generative AI in a research paper?\nYes. Most academic publishers require researchers using AI tools to document this use in the methods or acknowledgements sections of their papers. You should review the specific guidelines of the target publisher to determine what is required.\n### Can I use AI in writing grant applications?\nYou should review the specific policies of potential funders to determine if the use of AI is permitted. For its part, the National Institutes of Health (NIH)[advises caution]: “If you use an AI tool to help write your application, you also do so at your own risk,” as these tools may inadvertently introduce issues associated with research misconduct, such as plagiarism or fabrication.\n### Can I use AI in the peer review process?\nMany funders have not yet published policies on the use of AI in the peer review process. However, the National Institutes of Health (NIH) has[prohibited] such use “for analyzing and formulating peer review critiques for grant applications and R&amp;D contract proposals.” You should carefully review the specific policies of funders to determine their stance on the use of AI\n### Are there AI safety concerns or potential risks I should be aware of?\nYes. Some of the primary safety issues and risks include the following:\n* Bias and discrimination: The potential for AI systems to exhibit unfair or discriminatory behavior.\n* Misinformation, impersonation, and manipulation: The risk of AI systems disseminating false or misleading information, or being used to deceive or manipulate individuals.\n* Research and IP compliance: The necessity for AI systems to adhere to legal and ethical guidelines when utilizing proprietary information or conducting research.\n* Security vulnerabilities: The susceptibility of AI systems to hacking or unauthorized access.\n* Unpredictability: The difficulty in predicting the behavior or outcomes of AI systems.\n* Overreliance: The risk of relying excessively on AI systems without considering their limitations or potential errors.\nSee[Initial guidelines for the use of Generative AI tools at Harvard] for more information.\n## Resources\n### General\n* [Initial guidelines for the use of Generative AI tools at Harvard] \n### Generative AI tools\n* [Explore Tools Available to the Harvard Community] \n* [System Prompt Library] \n* [Request API Access] \n* [Request a Vendor Risk Assessment] \n* [Questions? Contact HUIT] \n### Copyright and intellectual property\n* [Copyright and Fair Use: A Guide for the Harvard Community] \n* [Copyright Advisory Program] \n* [Intellectual Property Policy] \n* [Protecting Intellectual Property] \n### Data security and privacy\n* [Harvard Information Security and Data Privacy] \n* [Data Security Levels &#8211; Research Data Examples] \n* [Privacy Policies and Guidelines] \n### Research support\n* [University Research Computing and Data (RCD) Services] \n* [Research Administration and Compliance] \n* [Research Computing] \n* [Research Data and Scholarship] \n### AI@Harvard\n* [Faculty engaged in AI research] \n* [Centers and initiatives engaged in AI research] \n* [Degree and other education programs in AI]",
    "length": 5323,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Ideas for experimenting with Generative AI: Use cases and things to ...",
    "url": "https://www.huit.harvard.edu/news/ai-use-cases",
    "text": "Ideas for experimenting with Generative AI: Use cases and things to keep in mind | Harvard University Information Technology[\nSkip to main contentarrow\\_circle\\_down\n] \nSearch\nSearchsearch\n[\n![Harvard University Information Technology] \n![Harvard University Information Technology] \n] \nmenucloseMenu\nSearch\nSearchsearch\n[\n![Harvard University Information Technology] \n![Harvard University Information Technology] \n] \n# Ideas for experimenting with Generative AI: Use cases and things to keep in mind\nNovember 15, 2023\nThe University supports responsible experimentation with Generative AI tools. This page provides an overview of possible use cases, and important things to keep in mind before you begin experimenting. It is intended as general information only and is not a comprehensive list of all the ways in which people across the University are exploring the use of AI. You can find more resources, including available tools, at[HUIT’s Generative AI page].\n## **Before you experiment with Generative AI tools**\n* [**Review the University’s initial guidelines for use of Generative AI tools,**] including guidelines around the use of confidential data with publicly-available tools, adhering to your School’s student and faculty policies around the use of Generative AI, and your responsibility to review and verify any AI-generated content that you publish or share with others.\n* [**Explore available tools through HUIT,**] including a[comparison of the appropriate levels of data] that they are approved to handle. The University is working to provide access to more tools, so check back periodically to see if new options are available. Some Schools may also offer additional tools to eligible affiliates.\n* **Consider AI’s limitations:**A good rule of thumb is to retain a healthy skepticism and understand that these tools can and frequently do get things wrong. AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called “hallucinations”). A chatbot can provide a confident-sounding, plausible response to a question that is nonetheless completely false or significantly distorted. Always verify the accuracy of AI-generated output.\n* **Be mindful of copyright issues:**AI-generated content can contain copyrighted material and,[per University guidelines,] you are responsible for any AI-generated content that you publish or make available to others. Don’t upload content that infringes the intellectual property or other legal rights of any third party, and don’t ask LLMs to rewrite or reproduce copyrighted material.## **Ideas for experimenting with Generative AI**\n### **Creating new content**\nAI can create stories, poems, music, images, recipes, travel guides, study guides, lesson plans, surveys and much more in seconds from simple, natural language prompts. It can generate new ideas or help you get creatively unstuck. The key to successful experimentation with text-based AI content generation is learning how to use and improve your prompts.[You can find some ideas and tips for using prompts here.] \n### **Generating or debugging code**\nGenerative AI tools can generate computer code, identify and fix bugs, and enhance code quality and efficiency using natural language prompts, which means that people who don’t already know any programming languages can experiment with code. But[research has shown] that AI-generated computer code can be more likely to contain security vulnerabilities. As with all AI-generated content, review your code before you publish –you are responsible for any content that you publish that includes AI-generated material.\n### **Creating spreadsheet formulas**\nLearning how to create formulas for Excel and other spreadsheet tools can take time. Generative AI tools can help make this easier for non-experts by enabling you to create formulas using natural language prompts. So even if you don’t know the formula you need, or where to find it, you can try asking an AI to, for example, “Write an Excel formula that calculates 10% of the sum of columns B and C,” and then copy and paste that formula into your spreadsheet. Again, be sure to double-check for accuracy.\n### **Search**\nBecause of the vastness of data that LLMs have been trained on, Generative AI tools can quickly sift through information from many different sources. Many people use Generative AI as an advanced search tool, and popular search engines such as Google and Bing are increasingly incorporating AI-generated results into their products. However, if the data on which AI has been trained is outdated, the output you receive will be inaccurate. As an example, OpenAI’s GPT-3.5 was trained on data up to September 2021, which means answers to general knowledge questions (e.g. “Who is the President of Harvard?”) will only reflect its knowledge of information up to that date. Similarly, if an AI model is trained on data that contains biases, unfair stereotypes, etc., these biases can be reflected in its outputs.[Learn more about bias in AI and other ethical considerations in this article from Harvard Business Review.] \n### **Summarizing documents**\nGenerative AI tools can analyze and generate concise summaries of long documents. This can be used for tasks such as highlighting key insights from research papers, summarizing emails, reports, and meeting notes, or identifying key findings from spreadsheet data. Keep in mind that AI may miss key context or misinterpret findings.\n### **Synthesizing information, including survey responses**\nGenerative AI tools can analyze complex datasets and synthesize the information within. A good example is survey responses: if you have results from a survey that include a lot of open text responses from multiple participants, you can feed those into an AI tool and get an overall summary of what people are saying or pick out the most common keywords and phrases.\n### **Translation**\nGenerative AI tools can quickly translate long passages of text into other languages. As with other uses of AI, exercise caution:[it has been reported] that while Generative AI tools are good at translating into English, they are not as good at translating English into another language.\n### **Reviewing or enhancing your work**\nIn addition to creating new content, AI can be a helpful partner in editing and enhancing your writing. You can paste in an email or document that you’re working on and ask the AI to suggest improvements, look for errors, assess flow and readability, and make suggestions of additional content. However,[per University guidelines,] don’t upload any confidential data into publicly-available Generative AI tools.\n### **Boosting productivity**\nAI tools can summarize meeting notes, highlight key takeaways, and generate action items. These uses will become more widespread as common productivity tools begin to incorporate AI-powered features directly into their apps.\n### **Improving accessibility**\nThoughtful application of AI tools can aid in making content more accessible by identifying problems in your documents, suggesting improvements such as alt-text and audio descriptions for images, or converting speech to text for captioning. AI-powered accessibility checkers can be a valuable time saver, but they can’t catch every error. You should still be sure to[manually test your content for accessibility.] It can be overwhelming to assess all the various tools that are available, but Harvard’s[Digital Accessibility Services] can help you navigate the landscape.",
    "length": 7497,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "University-wide working groups",
    "url": "https://www.harvard.edu/ai/working-groups/",
    "text": "University-wide working groups - Generative AI @ Harvard\n[Skip to main content] \n[![A logo that says Generative AI at Harvard]] \nGenerative AI**\n# University-wide working groups\nGenerative AI poses unprecedented challenges and opportunities for higher education. In summer 2023, Harvard's leadership began convening discussions with faculty and administrators to explore how GenAI could impact how we teach, learn, research, and work.\n## Generative AI Teaching and Learning Group\n### Charge\nThe GenAI Teaching and Learning Group seeks to share resources, identify emerging best practices, and craft effective strategies for common challenges. It provides for direct and timely communication between people responsible for implementation at each of the Schools, as well as communication with University leadership and with the other two groups.\n### Members\n* Emily Bottis, Managing Director of Academic Technologies (HUIT)\n* Lance Brisbois, Senior Project Manager\n* Bernard Chang, Dean for Medical Education (HMS)\n* Amanda Claybaugh, Dean for Undergraduate Education (College)\n* Glenn Cohen, Deputy Dean (HLS)\n* Nancy Coleman, Dean of the Division of Continuing Education and University Extension (DCE)\n* Suzanne Cooper, Academic Dean for Teaching and Curriculum (HKS)\n* Erin Driver-Linn, Dean for Education (SPH)\n* Klara Jelinkova, Vice President and University Chief Information Officer (HUIT)\n* Lee LaFleur, Associate Librarian for Research, Teaching, and Learning Services (FAS)\n* David Lamberth, Professor of Philosophy and Theology (HDS)\n* Matt Miller, Senior Associate Dean for Learning and Teaching (GSE)\n* Sang Park, Associate Dean for Dental Education; Associate Professor of Restorative Dentistry and Biomaterials Sciences (HSDM)\n* Rebecca Nesson, Dean for Academic Programs (SEAS)\n* Sebastian Schmidt Dalzon, Assistant Dean of Academic and Strategic Planning (GSD)\n* Chris Stubbs, Dean of Science (FAS)\n* Dustin Tingley, Deputy Vice Provost for Advances in Learning\n* Mitch Weiss, Richard L. Menschel Professor of Management Practice (HBS)\n* Nick Wilson, Director of Teaching Innovation (HILT)\n* William Wisser, Executive Director of the Teaching and Learning Lab\n## Generative AI Research and Scholarship Group\n### Charge\nThe GenAI Research and Scholarship Group focuses on how to enable, and support the integrity of, our scholarly activities with generative AI tools. It considers how to ensure accuracy of information from generative AI tools; how to safeguard confidential data and intellectual property; how to generate and improve research proposals while meeting funder expectations; and how to use these tools to communicate scholarship.\n### Members\n* John Shaw (chair), Vice Provost for Research\n* Demba Ba, Professor of Electrical Engineering (SEAS)\n* Andy Beam, Assistant Professor of Epidemiology and of Biomedical Informatics (SPH/HMS)\n* Molly Brady, Deputy Dean (HLS)\n* Ellen Desmarais, Chief Operations Officer &amp; EVP Higher Ed for Harvard Business Publishing (HBS)\n* Hawazin Elani, Assistant Professor of Oral Health Policy &amp; Epidemiology\n* Sam Gershman, Professor of Psychology (FAS)\n* Sharad Goel, Professor of Public Policy (HKS)\n* Klara Jelinkova, Vice President and University Chief Information Officer (HUIT)\n* Terrence Johnson, Professor of African American Religious Studies (HDS)\n* Sham Kakade, Professor of Computer Science and of Statistics (SEAS)\n* Emre Keskin, University Research Data Officer (OVPR/HUIT)\n* Sam Kou, Professor of Statistics (FAS/SPH)\n* Hanspeter Pfister, Academic Dean for Computing, Information and Mathematical Sciences and Engineering (SEAS)\n* John Quackenbush, Professor of Computational Biology and Bioinformatics (SPH)\n* Allen Sayegh, Design Critic and Senior Interaction Technologies Fellow\n* Stuart Snydman, Associate University Librarian and Managing Director, Library Technology (HUIT/Library)\n* Latanya Sweeney, Professor of the Practice of Government and Technology (FAS/HKS)\n* Marty West, Academic Dean (GSE)\n* Marinka Zitnik, Assistant Professor of Biomedical Informatics (HMS)\n## Generative AI Administration and Operations Group\n### Charge\nThe GenAI Administration and Operations Group works to address information security, data privacy, procurement, and administration\nand organizational efficiencies.\n### Members\n* Klara Jelinkova (chair), Vice President and University Chief Information Officer (HUIT)\n* Kristen Anderson, Administrative Dean (HDS)\n* Richard Bakken, Chief Information Officer (GSD)\n* Chris Bebenek, University Attorney (OGC)\n* Mary Ann Bradley, Associate Dean for Administrative Operations (FAS)\n* Ismael Carreras, Special Project Advisor (FAS)\n* Frank Cafasso, Director of Information Technology (RAD)\n* Sean Caron, Vice President for Campus Services (CS)\n* Beth Clark, Chief Information Officer (HBS)\n* Kyle Courtney, Director of Copyright and Information Policy (HLS)\n* Manuel Cuevas-Trisan, Vice President for Human Resources (HHR)\n* Jeita Deng, Associate Dean, Chief Financial Officer (HKS)\n* Deane Eastwood, Chief Information Officer (SPH)\n* Mark Goble, Administrative Dean (GSD)\n* Lori Gross, Associate Provost for Arts and Culture (OPP)\n* Stephanie Gumble, Chief Operating Officer (HUIT)\n* Tim Harrigan, Associate Director for Research and Assessment (CCL)\n* Kelsey Heroux, Project Manager (OPP)\n* Ritu Kalra, Vice President and Chief Financial Officer (FAD)\n* Stacey Kane, Director of Risk and Compliance and IT Strategy and Security (HSDM)\n* Simone Leary, Chief Human Resource Officer (HMS)\n* Nisha Mongia, Executive Dean (HRI)\n* Siovhan O’Connor, Associate Vice President and Chief Operating Officer (AA&amp;D)\n* Robert Oatman, Chief Information Officer (GSE)\n* Brooke Pulitzer, Senior Assistant Provost for Research (OVPR)\n* Leslie Schaffer, Associate Dean for Finance (SEAS)\n* Mary Straub, AVP of Central Finance and Administration (CADM)\n* Vaughn Waters, Chief of Administrative Operations (Library)\n* Tracee Whitley, Dean for Administration (HLS)\n* Jenn Wickman, Chief of Staff, Office of the UCIO (HUIT)",
    "length": 6020,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Best Practices: Generative AI (Community Members)",
    "url": "https://privsec.harvard.edu/best-practices-generative-ai-community-members",
    "text": "Best Practices: Generative AI (Community Members) | University Information Security and Data Privacy[\nSkip to main contentarrow\\_circle\\_down\n] \n[![Harvard University]] \n[\n![University Information Security and Data Privacy] \n![University Information Security and Data Privacy] \n] \nmenucloseMenu\nSearch\nSearchsearch\n[\n![University Information Security and Data Privacy] \n![University Information Security and Data Privacy] \n] \n# Best Practices: Generative AI (Community Members)\n![robot hands and human hands on keyboard] \n## What is Generative AI?\nArtificial Intelligence (AI) refers to the capability of a computer or machine to mimic human-like cognitive functions such as learning, problem solving, creativity, and decision-making.\nGenerative AI tools, such as chatbots, writing assistants, image generators, and embedded AI features in everyday software, can help with learning, research, and administrative tasks. However, these tools can also introduce privacy, security, and academic integrity risks if used carelessly.\nThese best practices are intended for**students, faculty, staff, researchers, and affiliates**who use generative AI tools as part of their Harvard work or studies.\n## What to Know Before Using Generative AI\nWhen you use generative AI tools, it’s important to remember:\n* Anything you enter into an AI tool may be**stored, reviewed, or reused**by the service provider.\n* AI tools may be**inaccurate or misleading**, even when responses sound confident.\n* Some AI features are**built into tools you already use**(browsers, email, document editors) and may operate in the background.\n* Not all AI tools are approved for use with University data.\n## Common Risks\nSort\n|Category|Why It Matters|\nSensitive Information Exposure\n|\nInformation you enter into AI tools, such as personal details, student information, research data, or internal documents, may be stored or shared outside Harvard’s control.\n|\nInaccurate or “Hallucinated” Outputs\n|\nAI-generated content may contain errors, outdated information, or fabricated details that can affect learning, research, or decision-making.\n|\nBias and Fairness Concerns\n|\nAI systems may unintentionally reinforce stereotypes or unfair patterns.\n|\nOver-Reliance on AI\n|\nDepending solely on AI without human judgment can undermine learning, research quality, or fairness.\n|\nAcademic or Legal Violations\n|\nMisuse of AI tools may conflict with academic integrity rules, copyright law, or Harvard policies.\n|\nPhishing and Social Engineering\n|\nAttackers leverage AI generated deceptive messages to trick users into revealing personal information.\n|\nMalicious or Unsafe AI Tools\n|\nUnsafe AI tools may collect data insecurely or introduce malware risks to your device.\n|\nMeeting Transcripts and Recordings\n|\nAI meeting tools may record or store private information without consent or adequate security.\n|\nAI Browser Data Exposure\n|\nAI-enabled browsers may retain web content, prompts, or activity for processing, creating risks if confidential or personal information is displayed or entered.\n|\nUnintended Automation\n|\n\"Agent\" or automation modes can act on your behalf, submitting forms or sharing data, without full review.\n|\n### Best Practices\n#### Be Careful What You Share\n* Do**not**enter sensitive personal information, student records, HR data, health information, or confidential University materials into public AI tools.\n* Avoid pasting unpublished research, internal communications, or proprietary documents into AI prompts.\n* When possible, use**de-identified or hypothetical examples**instead of real data.\n* Turn off “memory” or “agent mode” features if available, and clear chat or browsing history regularly.\n#### Use Approved or Recommended Tools\n* Use AI tools that your School, program, or central IT has reviewed or approved.\n* Be cautious with browser extensions, plugins, or “free” AI tools that request broad access to your accounts or files.\n* If you’re unsure whether a tool is appropriate, ask your instructor, supervisor, or local IT support.#### Check Accuracy and Bias\n* Treat AI-generated content as a**starting point**, not a final answer.\n* Verify important information using trusted, authoritative sources.\n* Watch for biased, stereotypical, or incomplete responses, especially in sensitive topics.\n#### Follow Academic Integrity and Copyright Rules\n* Follow your course, program, or School’s guidance on acceptable AI use.\n* Do not submit AI-generated work as your own unless explicitly permitted.\n* Cite or acknowledge AI use when required by instructors or policies.\n* Respect copyright and licensing rules when generating or reusing content.\n* Verify security and regulatory compliance (e.g. National Defense Authorization Act)#### Understand Embedded AI Features\n* Be aware that AI may be built into tools like document editors, email, search, or browsers.\n* Review settings to understand what features are enabled and what data they may use.\n* Disable AI features if you are uncomfortable with how data is handled or if they are not appropriate for your work.#### Protect Your Accounts and Devices\n* Use strong, unique passwords and enable MFA for accounts used with AI tools.\n* Watch for fake or lookalike AI websites designed to steal credentials.\n* Keep your devices updated and secure, especially if using AI tools frequently.#### Ask for Help When Unsure\n* If you’re unsure whether an AI tool is appropriate for a task,**ask before using it**.\n* Contact your instructor, supervisor, School Security &amp; Privacy Officer (SPSO), or IT support for guidance.\n* For sensitive data or unusual use cases, consult your SPSO or PrivSec.\n## Related Resources\n* [Initial University Guidelines] \n* [HUIT Guidelines and Resources] \n* [Generative AI @ Harvard] \n* [Research with Generative AI] \n* [Teach with Generative AI] \n* [NDAA 2026 Guidance and Prohibition on use of certain Artificial Intelligence] \n* For additional academic guidelines, search school specific websites.",
    "length": 5966,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Generative Artificial Intelligence (AI) | Harvard University Information ...",
    "url": "https://www.huit.harvard.edu/ai",
    "text": "Generative Artificial Intelligence (AI) | Harvard University Information Technology[\nSkip to main contentarrow\\_circle\\_down\n] \nSearch\nSearchsearch\n[\n![Harvard University Information Technology] \n![Harvard University Information Technology] \n] \nmenucloseMenu\nSearch\nSearchsearch\n[\n![Harvard University Information Technology] \n![Harvard University Information Technology] \n] \n# Generative Artificial Intelligence (AI)\n# Generative Artificial Intelligence (AI)\nPrograms &amp; initiatives\n![2019 Harvard University IT Summit] \nGenerative AI is a type of artificial intelligence that can learn from and mimic large amounts of data tocreate content such as text, images, music, videos, code, and more, based on inputs or prompts.The University supports responsible experimentation with[Generative AI tools], but there are[important considerations] to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.\n## AI tools &amp; resources\nhistory\\_edu\n[### Review guidelines for use of AI tools\n] \nmanage\\_search\n[### Compare AI tools and check appropriate data levels\n] \nchat\n[### AI assistant guidelines\n] \ntips\\_and\\_updates\n[### Ideas and use cases for experimenting with AI\n] \nimage\\_search\n[### Get started with prompts for image-based AI tools\n] \nformat\\_quote\n[### Get started with prompts for text-based AI tools\n]",
    "length": 1392,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "AI Meeting Assistant Guidelines",
    "url": "https://www.huit.harvard.edu/ai-assistant-guidelines",
    "text": "AI Assistant Guidelines | Harvard University Information Technology[\nSkip to main contentarrow\\_circle\\_down\n] \nSearch\nSearchsearch\n[\n![Harvard University Information Technology] \n![Harvard University Information Technology] \n] \nmenucloseMenu\nSearch\nSearchsearch\n[\n![Harvard University Information Technology] \n![Harvard University Information Technology] \n] \n# AI Assistant Guidelines\n# AI Assistant Guidelines\nPolicies &amp; guidelines\n![Woman works on her laptop in a library.] \n## Contents\n* [Overview] \n* [Risks to consider when using these tools] \n* [Guidance for approved use of AI assistants] \n* [General guidance on preventing AI assistants from joining meetings] \n* [Related references] \n## Overview\nUsing generative artificial intelligence (AI) technology, automated meeting assistants (aka “AI note takers” or “bots”) are now available to transcribe and summarize online meetings. These assistants may be built into existing collaboration tools such as Zoom, while others are third-party software agents used by individual meeting participants. \nAlthough these tools have legitimate uses (e.g., for accessibility and capturing notes for later reference), they can also pose substantial privacy, regulatory, and legal risks, and have the potential to stifle conversation and open inquiry. More details on risks are outlined below. **AI meeting assistants should not be used in Harvard meetings, with the exception of approved tools with contractual protections:**\n* Use only AI assistants for which Harvard has an enterprise agreement with the vendor including appropriate security and privacy protections, including:\n* Approved tools as part of limited HUIT-directed pilot programs to evaluate the use of AI assistants within the Harvard environment.[You can register your interest using this form].\n* Individuals who wish to request access to AI supported notetaking for accessibility accommodations should contact their[Local Accommodations Coordinator] to discuss options.\n* To inquire about or request other tools,[please contact HUIT].\nWe will continue to evaluate their use in our environment.\n## Risks to consider when using these tools\n* Creating and distributing AI-generated transcripts and summaries could stifle open conversation and discourage some meeting attendees from fully participating, akin to the video or audio recording of a meeting. * Personal and Harvard confidential information may be exposed to third parties or used to train AI models, without the necessary contractual protections in place.\n* Discussion summaries may be inaccurate (e.g., “hallucinations” or missing nuances, sarcasm, irony, and important context).\n* Discussion content may be preserved indefinitely or shared with third parties, increasing the risks of inappropriate distribution and unauthorized access.\n* Recorded discussions and summaries/transcripts may be subject to legal discovery in future litigation.\n* Meeting participants may be recorded without their awareness or consent. \n## Guidance for approved use of AI assistants\n* **Use only approved tools with contractual protections:**Use only AI assistants for which Harvard has an enterprise agreement with the vendor including appropriate security and privacy protections, including:\n* Approved tools as part of limited HUIT-directed pilot programs to evaluate the use of AI assistants within the Harvard environment.[You can register your interest using this form].\n* Individuals who wish to request access to AI supported notetaking for accessibility accommodations should contact their[Local Accommodations Coordinator] to discuss options.\n* To inquire about or request other tools,[please contact HUIT].\n* **Consider the risks:**Before using an AI assistant in a meeting (or when joining a meeting where an AI assistant is active), carefully consider the risks above and ask yourself whether the benefit of using the technology outweighs the risks, taking into account the nature of the meeting.\n* **Proactively inform participants:**Ensure that, at the beginning of the meeting, all meeting participants are aware of and consent to the meeting being recorded / transcribed. Only begin recording once all participants have consented.\n* If not automatically announced to all participants, clearly announce your intention to use a bot in the meeting: “Please note that we plan to use an AI assistant to transcribe and summarize this meeting. Please let us know if you have any objections.”\n* If a participant objects to the use of an AI assistant, do not use the assistant (those receiving the assistant as an accommodation for disability should consult with their[Local Accommodations Coordinator] about how to proceed in such instances). * When joining a third party’s meeting, explicitly ask if it is being recorded or transcribed. If the anticipated discussion involves sensitive information, request that the meeting not be recorded/transcribed.\n* **Avoid use in sensitive meetings:**Explicitly ask participants to disable AI assistants in meetings involving[Level 3] or higher information (i.e., sensitive nonpublic information such as patient information, identifiable student information, employee performance or employment actions, personally identifiable records, sensitive research, donor strategy, and privileged legal advice). This holds true whether the meeting is hosted by Harvard or a third party.\n* Suggested statement in a meeting when sensitive matters arise: “Given the sensitive nature of this discussion, I suggest that we discontinue the use of AI assistants for the time being. If there are any accessibility needs, please let us know so we can accommodate them.”\n* **Moderate yourself:**If an AI assistant is active in the meeting, be mindful of what you say, with the conservative assumption that your words may be preserved indefinitely.\n* **Review and correct summaries, share sparingly, and delete when no longer required:**Prior to sharing AI-generated meeting summaries, carefully review the document, correcting any errors, then share sparingly on a need-to-know basis. Keep in mind that summaries may include information that was shared before or after certain participants joined the meeting. Include a notice at the top of the document that the summary was prepared by an AI assistant and may contain inaccuracies, along with instructions for how to submit corrections, e.g. “*The meeting notes below were generated by an AI assistant and may contain inaccuracies. Please submit any corrections to the meeting host.*”\n* **Delete transcripts and meeting summaries when no longer needed**for the purpose for which they were created. See the[General Records Schedule] for more information on retention requirements.\n* If you are subject to a litigation hold, AI-generated transcripts or summaries from meetings you host must be preserved. * **Be inclusive in your presentation:**Review[Digital Accessibility Services] (DAS)[guidance on hosting accessible remote meetings] for more information on how to make virtual meetings more inclusive. \n## General guidance on preventing AI assistants from joining meetings\n* **Prevent AI assistants from joining and expel unwanted assistants:**For virtual meetings where you want to ensure AI assistants do not join, enable the waiting room and selectively admit participants (see[instructions] for Zoom meetings). Third-party AI assistants that are not built into the meeting platform often have an indicative participant name such as “Michael’s OtterPilot”, but this default name can be changed. If an unwanted AI assistant joins, you can remove them from the meeting, or ask that they be removed:\n* In Zoom, meeting hosts can expel unwanted participants by opening the Participants panel, hovering over the unwanted participant, clicking More, then clicking Remove.\n* Suggested statement to ask that an unwanted AI assistant is removed from a meeting: “I’m sorry, given the nature of this discussion, we cannot allow AI assistants to record or transcribe this meeting. If there are any accessibility needs, please let us know so we can accommodate them.”  \n## Related references\n* [Harvard guidance on Zoom meeting records and transcripts] \n* [Guidance on hosting accessible remote meetings] \n* [Harvard Library Records Management Services] \n* [Guidelines for the use of AI tools at Harvard] \n* [AI tools approved for use at Harvard] \n* [Harvard Generative AI website] \n* [Harvard Data Classifications]",
    "length": 8448,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Exploring generative AI at Harvard",
    "url": "https://news.harvard.edu/gazette/story/2024/04/exploring-potential-benefits-pitfalls-of-generative-ai/",
    "text": "Exploring potential benefits, pitfalls of generative AI &#8212; Harvard Gazette\n[![The Harvard Gazette logo]] \n* ![Close]![Open] Menu\n## Sections\n## Featured Topics\n## ![] Featured series\n### [Wondering] \nA series of random questions answered by Harvard experts.\n## Explore the Gazette\n## Read the latest\n* ![Rémi Drolet and Tess Johnson.] \n### [A second shot at Olympic glory] \n* ![Caitlyn Kukulowicz] \n### [Journey on ice and water] \n* ![Carter Eckert.] \n### [Carter Joel Eckert, 79] \n* ![Search] Search\nSearch the Harvard Gazette\nGo\n[Campus &amp; Community] # Exploring generative AI at Harvard\nJessica McCann\nHarvard Correspondent\nApril 3, 2024long read\n![NASA image of Earth.] \nNASA\n## Leaders weigh in on where we are and what’s next\nThe explosion of generative AI technology over the past year and a half is raising big questions about how these tools will impact higher education. Across Harvard, members of the community have been[exploring] how GenAI will change the ways we teach, learn, research, and work.\nAs part of this effort, the Office of the Provost has convened three[working groups]. They will discuss questions, share innovations, and evolve guidance and community resources. They are:\n* **The Teaching and Learning Group**, chaired by[Bharat Anand], vice provost for advances in learning and the Henry R. Byers Professor of Business Administration at Harvard Business School. This group seeks to share resources, identify emerging best practices, guide policies, and support the development of tools to address common challenges among faculty and students.\n* **The Research and Scholarship Group**, chaired by[John Shaw], vice provost for research, Harry C. Dudley Professor of Structural and Economic Geology in the Earth and Planetary Sciences Department, and professor of environmental science and engineering in the Paulson School of Engineering and Applied Science. It focuses on how to enable, and support the integrity of, scholarly activities with generative AI tools.\n* **T****he Administration and Operations Group**, chaired by[Klara Jelinkova], vice president and University chief information officer. It is charged with addressing information security, data privacy, procurement, and administration and organizational efficiencies.![Headshots of Klara Jelinkova, Bharat Anand, and John Shaw.] \nKlara Jelinkova, Bharat Anand, and John Shaw.\nPhotos by Kris Snibbe/Harvard Staff Photographer; Evgenia Eliseeva; and courtesy of John Shaw\nThe Gazette spoke with Anand, Shaw, and Jelinkova to understand more about the work of these groups and what’s next in generative AI at Harvard.\n**When generative AI tools first emerged, we saw universities respond in a variety of ways —from encouraging experimentation to prohibiting their use. What was Harvard’s overall approach?**\n**Shaw:**From the outset, Harvard has embraced the prospective benefits that GenAI offers to teaching, research, and administration across the University, while being mindful of the potential pitfalls. As a University, our mission is to help enable discovery and innovation, so we had a mandate to actively engage. We set some initial, broad policies that helped guide us, and have worked directly with groups across the institution to provide tools and resources to inspire exploration.\n**Jelinkova:**The rapid emergence of these tools meant the University needed to react quickly, to provide both tools for innovation and experimentation and[guidelines] to ensure their responsible use. We rapidly built an[AI Sandbox] to enable faculty, students, and staff to experiment with multiple large language models in a secure environment. We also worked with external vendors to acquire enterprise licenses for[a variety of tools] to meet many different use cases. Through working groups, we were able to learn, aggregate and collate use cases for AI in teaching, learning, administration, and research. This coordinated, collective, and strategic approach has put Harvard ahead of many peers in higher education.\n**Anand:**Teaching and learning are fundamentally decentralized activities. So our approach was to ask: First, how can we ensure that local experimentation by faculty and staff is enabled as much as possible; and second, how can we ensure that it’s consistent with University policies on IP, copyright, and security? We also wanted to ensure that novel emerging practices were shared across Schools, rather than remaining siloed.\n**What do these tools mean for faculty, in terms of the challenges they pose or the opportunities they offer? Is there anything you’re particularly excited about?**\n**Anand:**Let’s start with some salient challenges. How do we first sift through the hype that’s accompanied GenAI? How can we make it easy for faculty to use GenAI tools in their classrooms without overburdening them with yet another technology? How can one address real concerns about GenAI’s impact?\nWhile we’re still early in this journey, many compelling opportunities —and more importantly, some systematic ways of thinking about them —are emerging. Various Harvard faculty have leaned into experimenting with LLMs in their classrooms. Our team has now interviewed over 30 colleagues across Harvard and curated short videos that capture their learnings. I encourage everyone to[view these materials] on the new GenAI site; they are remarkable in their depth and breadth of insight.\nHere’s a sample: While LLMs are commonly used for Q&amp;A, our faculty have creatively used them for a broader variety of tasks, such as simulating tutors that guide learning by asking questions, simulating instructional designers to provide active learning tips, and simulating student voices to predict how a class discussion might flow, thus aiding in lesson preparation. Others demonstrate how more sophisticated prompts or “prompt engineering” are often necessary to yield more sophisticated LLM responses, and how LLMs can extend well beyond text-based responses to visuals, simulations, coding, and games. And several faculty show how LLMs can help overcome subtle yet important learning frictions like skill gaps in coding, language literacy, or math.\n**Do these tools offer students an opportunity to support or expand upon their learning?**\n**Anand:**Yes. GenAI represents a unique area of innovation where students and faculty are working together. Many colleagues are incorporating student feedback into the GenAI portions of their curriculum or making their own GenAI tools available to students. Since GenAI is new, the pedagogical path is not yet well defined; students have an opportunity to make their voices heard, as co-creators, on what they think the future of their learning should look like.\nBeyond this, we’re starting to see other learning benefits. Importantly, GenAI can reach beyond a lecture hall. Thoughtful prompt engineering can turn even publicly available GenAI tools into tutorbots that generate interactive practice problems, act as expert conversational aids for material review, or increase TA teams’ capacity. That means both that the classroom is expanding and that more of it is in students’ hands. There’s also evidence that these bots field more questions than teaching teams can normally address and can be more comfortable and accessible for some students.\nOf course, we need to identify and counter harmful patterns. There is a risk, in this early and enthusiastic period, of sparking over-reliance on GenAI. Students must critically evaluate how and where they use it, given its possibility of inaccurate or inappropriate responses, and should heed the areas where their style of cognition outperforms AI. One other thing to watch out for is user divide: Some students will graduate with vastly better prompt engineering skills than others, an inequality that will only magnify in the workforce.\n**What are the main questions your group has been tackling?**\n**Anand:**Our group divided its work into three subgroups focused on policy, tools, and resources. We’ve helped guide initial policies to ensure safe and responsible use; begun curating resources for faculty in a[One Harvard repository]; and are exploring which tools the University should invest in or develop to ensure that educators and researchers can continue to advance their work.\nIn the fall, we focused on supporting and guiding HUIT’s development of the AI Sandbox. The[Harvard Initiative for Learning and Teaching’s annual conference], which focused exclusively on GenAI, had its highest participation in 10 years. Recently, we’ve been working with the research group to inform the development of tools that promise broad, generalizable use for faculty (e.g., tutorbots).\n**What has your group focused on in discussions so far about generative AI tools’ use in research?**\n**Shaw:**Our group has some incredible strength in researchers who are at the cutting edge of GenAI development and applications, but also includes voices that help us understand the real barriers to faculty and students starting to use these tools in their own research and scholarship. Working with the other teams, we have focused on supporting development and use of the GenAI sandbox, examining IP and security issues, and learning from different groups across campus how they are using these tools to innovate.\n**Are there key areas of focus for your group in the coming months?**\n**Shaw:**We are focused on establishing programs —such as the new[GenAI Milton Fund track] — to help support innovation in the application of these tools across the wide range of scholarship on our campus. We are also working with the College to develop new programs to help support students who wish to engage with faculty on GenAI-enabled projects. We aim to find ways to convene students and scholars to share their experiences and build a stronger community of practitioners across campus.\n**What types of administration and operations questions are your group is exploring, and what type of opportunities do you see in this space?**\n**Jelinkova:**By using the group to share learnings from across Schools and units, we can better provide technologies to meet the community’s needs while ensuring the most responsible and sustainable use of the University’s financial resources. The connections within this group also inform the guidelines that we provide; by learning how generative AI is being used in different contexts, we can develop best practices and stay alert to emerging risks. There are new tools becoming available almost every day, and many exciting experiments and pilots happening across Harvard, so it’s important to regularly review and update the guidance we provide to our community.\n**Can you talk a bit about what has come out of these discussions, or other exciting things to come?**\n**Jelinkova:**Because this technology is rapidly evolving, we are continually tracking the release of new tools and working with our vendors as well as open-source efforts to ensure we are best supporting the University’s needs. We’re developing more guidance and hosting information sessions on helping people to understand the AI landscape and how to choose the right tool for their task. Beyond tools, we’re also working to build connections across Harvard to support collaboration, including a recently launched[AI community of practice]. We are capturing valuable findings from emerging technology pilot programs in[HUIT], the[EVP area], and across Schools. And we are now thinking about how those findings can inform guiding principles and best practices to better support staff.\n**While the GenAI groups are investigating these questions, Harvard faculty and scholars are also on the forefront of research in this space. Can you talk a bit about some of the interesting research happening across the University in**[**AI more broadly**] **?**\n**Shaw:**Harvard has made deep investments in the development and application of AI across our campus, in our Schools, initiatives, and institutes —such as the Kempner Institute and Harvard Data Science Initiative. In addition, there is a critical role for us to play in examining and guiding the ethics of AI applications —and our strengths in the Safra and Berkman Klein centers, as examples, can be leading voices in this area.\n**What would be your advice for members of our community who are interested in learning more about generative AI tools?**\n**Anand:**I’d encourage our community to view the resources available on the new[Generative AI @ Harvard website], to better understand how GenAI tools might benefit you.\nThere’s also no substitute for experimentation with these tools to learn what works, what does not, and how to tailor them for maximal benefit for your particular needs. And of course, please know and respect University policies around copyright and security.\nWe’re in the early stages of this journey at Harvard, but it’s exciting.\n#### Share this article\n* ![] [Share on Facebook] \n* ![] [Share on LinkedIn] \n* ![] [Email article] \n* ![] [Print/PDF] \n## You might like\n* ![Rémi Drolet and Tess Johnson.] \n[Campus &amp; Community] ### [A second shot at Olympic glory] \nBattle-tested current, former students return to Winter Games\n4 min read\n* ![Caitlyn Kukulowicz] \n[Campus &amp; Community] ### [Journey on ice and water] \nFormer figure skating star Caitlyn Kukulowicz still hits the triple lutz but has found new place at boathouse\n4 min read\n* ![Carter Eckert.] \n[Campus &amp; Community] ### [Carter Joel Eckert, 79] \nMemorial Minute —Faculty of Arts and Sciences\n6 min read\n## Trending\n1. [Science &amp; Tech] ### [How AI deepfakes have skirted revenge porn laws] \nLimits unclear when explicit images of individuals look real, but are digitally generated\n8 min read\n2. [Health] ### [Turns out inherited eye diseases aren’t a sure thing] \nStudy finds only fraction of those with mutated gene develop malady —a finding that could lead to better treatments (and could apply to other such illnesses)\n6 min read\n3. [Science &amp; Tech] ### [Is AI dulling our minds?] \nExperts weigh in on whether tech poses threat to critical thinking, pointing to cautionary tales in use of other cognitive labor tools\nlong read",
    "length": 14170,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Policies & guidelines",
    "url": "https://www.huit.harvard.edu/policies-and-guidelines",
    "text": "Policies &amp; guidelines | Harvard University Information Technology[\nSkip to main contentarrow\\_circle\\_down\n] \nSearch\nSearchsearch\n[\n![Harvard University Information Technology] \n![Harvard University Information Technology] \n] \nmenucloseMenu\nSearch\nSearchsearch\n[\n![Harvard University Information Technology] \n![Harvard University Information Technology] \n] \n# Policies &amp; guidelines\nBrowse or use the search bar to find IT-related policies, guidelines, and standards. Guidance may vary by School. For assistance,[contact your local IT service desk.] \nSearch Within Results\nSearch Within Resultssearch\n## 30 results\nShow filtersfilter\\_alt\nSort byNewestOldestAlphabetical A-ZAlphabetical Z-Asort\nlist\ngrid\\_view\n## 30 results\n[### Access to Electronic Information Policy\n] \nThis policy sets out guidelines and processes for University access to user electronic information stored in or transmitted through any University system\n[### AI Assistant Guidelines\n] \nThis statement covers the use of AI assistants in meetings\n[### AI Sandbox Privacy Statement\n] \nThis statement describes the ways in which Harvard may access and use personal information with the AI Sandbox\n[### Canvas Acceptable Use Policy\n] \nThisarticle states Harvard’s policy for acceptable use of Canvas by instructors, staff, and students\n[### Canvas Privacy Statement\n] \nThis statement describes the ways in which Harvard may access and use personal information with Canvas\n[### CCURE Physical Access Control System Privacy Statement\n] \nThis statement describes how Harvard may access and use personal information in connection with providing the CCURE Physical Access Control System (“CCURE”)\n[### Digital Accessibility Policy\n] \nThis policy is established to promote equal access to information technology and digital content and to improve the user experience of IT and digital / online media for all users, including persons with disabilities\n[### EU AI Act Prohibited Use Cases\n] \nThe EU AI Act is a legislative framework to regulate the use of artificial intelligence technologies\n[### General Data Protection Regulation (GDPR)\n] \nThe General Data Protection Regulation (GDPR) is a regulation that applies to any organization that controls or processes the personal data of individuals in the European Economic Area\n[### Generative AI Guidelines\n] \nThese guidelines cover information security and data privacy, compliance, copyright, and academic integrity regarding the use of Generative AI\n[### Generative AI Tool Comparison\n] \nExplore and compare the Generative AI tools currently available from HUIT\n[### Gmail Privacy Statement\n] \nThis statement describes the ways in which Harvard may access and use personal information with Gmail",
    "length": 2716,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "AI",
    "url": "https://ari.hms.harvard.edu/ai",
    "text": "AI | Academic and Research Integrity[Skip to main content] \n**\n[![]] \n**\nMenu\n**Search\n# AI\n## **Harvard Guidance on Use of AI in Research**\n* HMS:[https://it.hms.harvard.edu/about/policies/responsible-use-generative-ai] \n* Harvard:[https://www.huit.harvard.edu/ai/guidelines] ## **AI in Grant Proposal/Peer Review Process**\n* NIH Peer Review:[https://grants.nih.gov/grants/guide/notice-files/NOT-OD-23-149.html] \n* NSF Reviews and Proposals:[https://new.nsf.gov/news/notice-to-the-research-community-on-ai] ## **AI in Authorship and Publications**\n* Committee on Publication Ethics (COPE) Authorship Guidelines:[https://publicationethics.org/cope-position-statements/ai-author] \n* World Association of Medical Editors (WAME) Recommendations for Scholarly Publications:[https://wame.org/page3.php?id=106] ## **AI in Human Genomic Data Research**\n* Protecting Human Genomic Data when Developing Generative Artificial Intelligence Tools and Applications:[https://grants.nih.gov/grants/guide/notice-files/NOT-OD-25-081.html] \n## **AI Tools available at Harvard**\n* HUIT Generative AI Tools Comparison:[https://huit.harvard.edu/ai/tools] \n* Level 3 data and below can be used with the following tools:\n* [Harvard AI Sandbox] \n* [OpenAI ChatGPT Edu] \n* [AI Meeting Assistants] \n* [Adobe Firefly] ## **AI Best Practices**\n1. You should not enter data[classified as confidential] (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University’s[Information Security Policy].\n2. AI Tools cannot be listed as an author on a paper.\n3. The NIH prohibits NIH scientific peer reviewers from using natural language processors, large language models, or other generative Artificial Intelligence (AI) technologies for analyzing and formulating peer review critiques for grant applications and R&amp;D contract proposals.\n4. NSF reviewers are prohibited from uploading any content from proposals, review information and related records to non-approved generative AI tools.\n5. Authors should be transparent when AI tools are used and provide information about how AI tools were used.\n&copy; 2026 by the President and Fellows of Harvard College",
    "length": 2198,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "AI Sandbox Privacy Statement",
    "url": "https://www.huit.harvard.edu/privacy/ai-sandbox",
    "text": "AI Sandbox Privacy Statement | Harvard University Information Technology[\nSkip to main contentarrow\\_circle\\_down\n] \nSearch\nSearchsearch\n[\n![Harvard University Information Technology] \n![Harvard University Information Technology] \n] \nmenucloseMenu\nSearch\nSearchsearch\n[\n![Harvard University Information Technology] \n![Harvard University Information Technology] \n] \n# AI Sandbox Privacy Statement\n# AI Sandbox Privacy Statement\nPolicies &amp; guidelines\n![Woman works on her laptop in a library.] \n## Contents\n* [Introduction] \n* [Users] \n* [Information gathering] \n* [Access to content] \n* [Use of information] \n* [Sharing of information] \n* [Cookies and related technologies] \n* [Information protection] \n* [Contact information] \n* [Users located outside the US] \n* [Changes to this Privacy Statement] \n## Introduction\nThis Privacy Statement describes the ways in which Harvard University may access and use personal information about individuals in connection with providing the AI Sandbox application managed by Harvard University Information Technology (HUIT). In this Privacy Statement, “AI Sandbox” refers to the HUIT-managed generative AI chatbot accessible by authorized users at[sandbox.ai.huit.harvard.edu].\n## Users\nThis Privacy Statement applies to all users of the AI Sandbox service, regardless of their relationship to Harvard.\n## Information gathering\nThe personal information about users that may be accessed by Harvard in connection with providing the AI Sandbox includes name, email address, affiliation, and user chat history.\n## Access to content\nWe view log data about individuals (whenever possible, separate from an actual person’s name) only as permitted by, and subject to the requirements and restrictions of, the[University Policy on Access to Electronic Information]. ## Use of information\nThe personal information that can be compiled by Harvard in connection with providing the AI Sandbox is used for internal learning and usage analytics. \nHarvard may review and share internally within the University users’ IP addresses, role type, School or unit, and the activities they perform using the AI Sandbox to administer the service, provide technical support, and help diagnose problems. Harvard may use and retain email or other communications to staff in order to process inquiries, respond to requests, and improve services. \nOther uses of personal information by Harvard are described in other parts of this Privacy Statement. \n## Sharing of information\nIf the user discloses personal information in the context of a query submitted to the AI Sandbox, the AI Sandbox will, consistently with its ordinary function, share that information with Large Language Model (LLM) provider(s) designated by the user.\nEach such LLM provider may have its own privacy policy. Harvard is not responsible for the privacy practices of such third-party resources or for the content they generate. Please see below (“Information Protection”) for more information about assurances Harvard has obtained from LLM providers.\nOther sharing of personal information by Harvard is described in other parts of this Privacy Statement. \nIn addition, Harvard may share and disclose personal information, including outside of Harvard, if Harvard believes that doing so is necessary or appropriate to: satisfy any applicable law, regulation, legal process or governmental request; investigate compliance with or enforce this Privacy Statement or any acceptable use policy for the AI Sandbox; detect, prevent or otherwise address fraud, security or technical issues; or protect, defend or enforce the operations, property, rights and safety of users, Harvard, Harvard affiliates or others. We notify users as soon as possible when we take an action that impacts their specific account or system.\n## Cookies and related technologies\nHarvard may use cookies and related technologies (“Cookies”) in the AI Sandbox application to gather information when users navigate through the AI Sandbox, in order to enhance and personalize the experience, to help provide services available through the AI Sandbox, to analyze service usage, and to help improve the service. To find out more about cookies, visit[https://www.internetcookies.com].\nYou can review your Internet browser settings, typically under the sections “Help” or “Internet Options,” to exercise choices you have for certain Cookies, or you can opt out of the collection and use of some Cookies through tools like the Network Advertising Initiative opt-out page. If you disable or delete certain Cookies in your settings, you may not be able to use features of our websites.\n## Information protection\nThere are security measures in place to help protect against the loss, misuse, and alteration of personal information under Harvard’s control. No data transmission, processing, sharing or storage by any party can be guaranteed to be completely secure.\nThe AI Sandbox offers a single interface that enables access to Large Language Models (LLM) provided by OpenAI, Amazon Web Services, Google, and Microsoft.\n* Harvard has enterprise agreements in place with all LLM providers to protect Harvard data and ensure certain privacy protections are in place for users of the AI Sandbox.\n* Per their published service descriptions and agreements with Harvard, LLM providers do not store any user input or generated responses.\n* Per their published service descriptions and agreements with Harvard, LLM providers do not utilize any user input to train their models.\n* All connections between the user’s browser and the AI Sandbox, and from AI Sandbox to the models, are encrypted.\n* Chat sessions and associated data are stored in a HUIT-managed database that is protected by enterprise IT policies and can be used to store data up to Harvard data classification level 3.\n* Chat sessions are private by default and can only be shared through direct user interaction. Even when shared, your name and any messages you add after sharing stay private.\n## Contact information\nPlease contact[ithelp@harvard.edu] with any questions or concerns about this Privacy Statement.\n## Users located outside the US\nIf you are located outside the United States, please click[here] for additional Harvard privacy disclosures that may apply to you.\n## Changes to this Privacy Statement\nThis Privacy Statement may be amended from time to time. Any such changes will be posted on this page. This Privacy Statement was last updated on September 30, 2024.",
    "length": 6476,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "A Red Flag? China’s Generative AI Dilemma - Harvard Journal of Law & Technology",
    "url": "https://jolt.law.harvard.edu/digest/a-red-flag-chinas-generative-ai-dilemma",
    "text": "A Red Flag? China’s Generative AI Dilemma - Harvard Journal of Law &amp; Technology\n[digest-logo] \n[Login] \nSearch by Category:Select a category...AntitrustArtificial IntelligenceCopyrightCybersecurityCommentaryNotesReportsFederal Circuit CommentFirst AmendmentFlash DigestFourth AmendmentNational SecurityPatentPrivacyTelecommunicationsTrademarkTrade Secrets\n#### [Submit to Digest] \n# A Red Flag? China’s Generative AI Dilemma\n## By Gilad Abiri &amp; Yue Huang\nJanuary 22, 2024\n[Notes] \n[View PDF] \nGilad Abiri is an Assistant Professor of Law at Peking University School of Transnational Law, and an Affiliate Fellow at the Information Society Project of Yale Law School. He holds a J.S.D. and L.L.M. from Yale Law School.\nYue Huang is an Assistant Professor of Law at Guangzhou University. He holds a J.S.D. and L.L.M. from Yale Law School.\nThe authors thank Sun Fanshu for her invaluable help with researching and finalizing this Note.\nWe are currently faced with a generative artificial intelligence (“AI”) dilemma. All jurisdictions must weigh the potential for tremendous economic and scientific progress on the one hand, while on the other, they must consider the very real possibility of severe social and political upheaval. This includes the mass dissemination of misinformation, the perpetuation of bias, the endangering of jobs, and the stoking of fears surrounding an AI apocalypse.\nThe way the great powers of the internet &mdash; the United States, the European Union, and China &mdash; choose to regulate generative AI will undoubtedly seriously impact our lives and politics.\nChina is the first power to move. This Note investigates the People’s Republic of China’s forthcoming generative AI regulatory measures, which went into effect on August 15, 2023. We contend that this regulation signifies a cautious decision, motivated mainly by internal political reasons, to decelerate the development of generative AI within China. Chinese regulators view this calculated slowdown as a response to the risks of generative AI.\nChina’s move to affirmatively regulate generative AI will have global implications &mdash; notably, its potential ability to mitigate the international “race to the bottom” in generative AI development. This result is not just welcome but a necessary precondition for the responsible advancement of AI. China’s choice, inadvertently, serves to grant the rest of the world a much-needed respite for contemplating the regulation of generative AI.\n## Recent Articles\nFebruary 09, 2026[Redefining the Standard of Human Oversight for AI Negligence] January 11, 2026[Entrepreneurial Liberty in the Age of AI] January 05, 2026[AI as the New Front Door to Legal Services: What Google’s AI Overview Means for Law Firm Visibility and Professional Responsibility] November 30, 2025[Geofence Searches Are Not (Necessarily) Carpenter Searches]",
    "length": 2871,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Harvard University Information Technology",
    "url": "https://www.huit.harvard.edu/news-types/ai",
    "text": "[Skip to main content] \n\n- [Main Menu] \n- [Utility Menu] \n- [Search] \n\n[![University Logo]] \n\n[HARVARD.EDU] \n\n[HOME] /\n\nFebruary 11, 2025\n\nDear Members of the Harvard Community,\n\nSince the rapid emergence of generative artificial intelligence (AI) tools, members of our community have embraced experimentation in exciting ways, examples of which you can see on [Harvard’s generative AI website]. As these tools evolve, the University is continuously assessing their use to support responsible experimentation...\n\n[Read more about Guidance on AI meeting assistants; data privacy principles] \n\nSee also: [AI], [PrivSec] \n\nOctober 11, 2024\n\nSee also: [AI] \n\nJuly 25, 2024\n\nOn August 1, HUIT is launching a new and improved version of the [Harvard AI Sandbox], including new features such as image generation and data visualization.... [Read more about Coming soon: New and improved Harvard AI Sandbox] \n\nSee also: [AI], [HUIT News] \n\nNovember 16, 2023\n\n[Just as with text-based AI tools], image generation tools such as [Adobe Firefly] rely on a user’s prompts to create good images. After you enter a prompt, the AI model analyzes your input and generates a response based on the patterns it has learned through its training. More descriptive prompts can improve the quality of the outputs. This guide gives you a basic overview of how to generate prompts for image-based AI...\n\n[Read more about Getting started with prompts for image-based Generative AI tools] \n\nSee also: [AI] \n\nNovember 15, 2023\n\nThe University supports responsible experimentation with Generative AI tools. This page provides an overview of possible use cases, and important things to keep in mind before you begin experimenting. It is intended as general information only and is not a comprehensive list of all the ways in which people across the University are exploring the use of AI. You can find more resources, including available tools, at [HUIT’s Generative AI page].\n\n## Before you experiment with Generative AI tools\n\n... [Read more about Ideas for experimenting with Generative AI: Use cases and things to keep in mind] \n\nSee also: [AI] \n\nOctober 31, 2023\n\nSee also: [AI] \n\nSeptember 27, 2023\n\nSee also: [AI] \n\nAugust 30, 2023\n\nThe information, sentences, or questions that you enter into a [Generative AI] tool (“prompts”) are a big influence on the quality of outputs you receive. After you enter a prompt, the AI model analyzes your input and generates a response based on the patterns it has learned through its training. More descriptive prompts can improve the quality of the outputs.\n\n...\n\n[Read more about Getting started with prompts for text-based Generative AI tools] \n\nSee also: [AI] \n\nAugust 25, 2023\n\nSee also: [AI] \n\nSeptember 4, 2023\n\nHUIT, in collaboration with VPAL, the FAS Division of Science, and colleagues across the University, has developed an AI Sandbox tool to enable Harvard community members to securely access Large Language Models (LLM). The AI Sandbox provides a “walled-off,” secure...\n\n[Read more about AI Sandbox pilot launches] \n\nSee also: [AI] \n\nJuly 13, 2023\n\nDear Members of the Harvard Community,\n\nWe write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI’s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.\n\n...\n\n[Read more about Initial guidelines for using ChatGPT and other generative AI tools at Harvard] \n\nSee also: [AI] \n\n## Filter By\n\n- [AI (11)] \n- [Digital Accessibility (10)] \n- [Project Delivery Forum (17)] \n- [PrivSec (3)] \n- [HUIT Stakeholders’ Forum (9)] \n- [HUIT News (10)] \n- [Featured (1)] \n- [Digital Digest (5)] \n- [Microsoft 365 Outlook Updates (5)] \n- [Microsoft Teams Updates (3)] \n- [Zoom Updates (3)] \n\nCopyright © 2025 The President and Fellows of Harvard College \\| [Accessibility] \\| [Digital Accessibility] \\| [Report Copyright Infringement]",
    "length": 4092,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Generative Artificial Intelligence (AI)",
    "url": "https://www.vpal.harvard.edu/links/check-out-huit-generative-artificial-intelligence-guidelines-and-resources-use-ai",
    "text": "[Skip to main content] \n\n- [Main Menu] \n- [Utility Menu] \n- [Search] \n\n[HARVARD.EDU] \n\n[HOME] /\n\n## Generative Artificial Intelligence (AI)\n\n- [Guidelines, tools, news, and more aboutGenerative AI] \n\n- [More resources] \n\n\n## What's new?\n\n[Guidance on AI meeting assistants]  // [EU AI Act Prohibited Uses] \n\n[Home]  / **Generative Artificial Intelligence (AI)**\n\nGenerative AI is a type of artificial intelligence that can learn from and mimic large amounts of data to create content such as text, images, music, videos, code, and more, based on inputs or prompts. The University supports responsible experimentation with [Generative AI tools], but there are [important considerations] to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.\n\n## [Review initial guidelines for use of AI tools] \n\n## [Ideas and use cases for experimenting with AI] \n\n## [Compare AI tools and check appropriate data levels] \n\n## [Get started with prompts for text-based AI tools] \n\n## [Get started with prompts for image-based AI tools] \n\n## More resources\n\n## Policies and Guidelines\n\n- [AI meeting assistant guidelines] \n- [EU AI Act] \n- [Information Security] \n- [Copyright Policy] \n- [School-based resources and policies] \n\n## More AI at Harvard\n\n- [Generative AI @ Harvard] \n- [AI @ Harvard] \n- [In Focus: Artificial Intelligence] \n- [Kempner Institute for the Study of Natural & Artificial Intelligence] \n- [metaLAB: AI Pedagogy Project] \n- [AI @ the FAS] \n\n## Training / Education\n\n- [Bok Center] \n- [Office of Undergraduate Education: Guidance and FAQs] \n- [LinkedIn Learning] \n- [Center for Workplace Development (CWD)] \n\n## Around the Web\n\n- [HBR: AI and technology articles] \n- [NYT: AI glossary] \n\nCopyright © 2025 The President and Fellows of Harvard College \\| [Accessibility] \\| [Digital Accessibility] \\| [Report Copyright Infringement]",
    "length": 1913,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "AI Sandbox Privacy Statement | Harvard University Information Technology",
    "url": "https://www.huit.harvard.edu/ai-sandbox-privacy",
    "text": "AI Sandbox Privacy Statement | Harvard University Information Technology[\nSkip to main contentarrow\\_circle\\_down\n] \nSearch\nSearchsearch\n[\n![Harvard University Information Technology] \n![Harvard University Information Technology] \n] \nmenucloseMenu\nSearch\nSearchsearch\n[\n![Harvard University Information Technology] \n![Harvard University Information Technology] \n] \n# AI Sandbox Privacy Statement\n# AI Sandbox Privacy Statement\nPolicies &amp; guidelines\n![Woman works on her laptop in a library.] \n## Contents\n* [Introduction] \n* [Users] \n* [Information gathering] \n* [Access to content] \n* [Use of information] \n* [Sharing of information] \n* [Cookies and related technologies] \n* [Information protection] \n* [Contact information] \n* [Users located outside the US] \n* [Changes to this Privacy Statement] \n## Introduction\nThis Privacy Statement describes the ways in which Harvard University may access and use personal information about individuals in connection with providing the AI Sandbox application managed by Harvard University Information Technology (HUIT). In this Privacy Statement, “AI Sandbox” refers to the HUIT-managed generative AI chatbot accessible by authorized users at[sandbox.ai.huit.harvard.edu].\n## Users\nThis Privacy Statement applies to all users of the AI Sandbox service, regardless of their relationship to Harvard.\n## Information gathering\nThe personal information about users that may be accessed by Harvard in connection with providing the AI Sandbox includes name, email address, affiliation, and user chat history.\n## Access to content\nWe view log data about individuals (whenever possible, separate from an actual person’s name) only as permitted by, and subject to the requirements and restrictions of, the[University Policy on Access to Electronic Information]. ## Use of information\nThe personal information that can be compiled by Harvard in connection with providing the AI Sandbox is used for internal learning and usage analytics. \nHarvard may review and share internally within the University users’ IP addresses, role type, School or unit, and the activities they perform using the AI Sandbox to administer the service, provide technical support, and help diagnose problems. Harvard may use and retain email or other communications to staff in order to process inquiries, respond to requests, and improve services. \nOther uses of personal information by Harvard are described in other parts of this Privacy Statement. \n## Sharing of information\nIf the user discloses personal information in the context of a query submitted to the AI Sandbox, the AI Sandbox will, consistently with its ordinary function, share that information with Large Language Model (LLM) provider(s) designated by the user.\nEach such LLM provider may have its own privacy policy. Harvard is not responsible for the privacy practices of such third-party resources or for the content they generate. Please see below (“Information Protection”) for more information about assurances Harvard has obtained from LLM providers.\nOther sharing of personal information by Harvard is described in other parts of this Privacy Statement. \nIn addition, Harvard may share and disclose personal information, including outside of Harvard, if Harvard believes that doing so is necessary or appropriate to: satisfy any applicable law, regulation, legal process or governmental request; investigate compliance with or enforce this Privacy Statement or any acceptable use policy for the AI Sandbox; detect, prevent or otherwise address fraud, security or technical issues; or protect, defend or enforce the operations, property, rights and safety of users, Harvard, Harvard affiliates or others. We notify users as soon as possible when we take an action that impacts their specific account or system.\n## Cookies and related technologies\nHarvard may use cookies and related technologies (“Cookies”) in the AI Sandbox application to gather information when users navigate through the AI Sandbox, in order to enhance and personalize the experience, to help provide services available through the AI Sandbox, to analyze service usage, and to help improve the service. To find out more about cookies, visit[https://www.internetcookies.com].\nYou can review your Internet browser settings, typically under the sections “Help” or “Internet Options,” to exercise choices you have for certain Cookies, or you can opt out of the collection and use of some Cookies through tools like the Network Advertising Initiative opt-out page. If you disable or delete certain Cookies in your settings, you may not be able to use features of our websites.\n## Information protection\nThere are security measures in place to help protect against the loss, misuse, and alteration of personal information under Harvard’s control. No data transmission, processing, sharing or storage by any party can be guaranteed to be completely secure.\nThe AI Sandbox offers a single interface that enables access to Large Language Models (LLM) provided by OpenAI, Amazon Web Services, Google, and Microsoft.\n* Harvard has enterprise agreements in place with all LLM providers to protect Harvard data and ensure certain privacy protections are in place for users of the AI Sandbox.\n* Per their published service descriptions and agreements with Harvard, LLM providers do not store any user input or generated responses.\n* Per their published service descriptions and agreements with Harvard, LLM providers do not utilize any user input to train their models.\n* All connections between the user’s browser and the AI Sandbox, and from AI Sandbox to the models, are encrypted.\n* Chat sessions and associated data are stored in a HUIT-managed database that is protected by enterprise IT policies and can be used to store data up to Harvard data classification level 3.\n* Chat sessions are private by default and can only be shared through direct user interaction. Even when shared, your name and any messages you add after sharing stay private.\n## Contact information\nPlease contact[ithelp@harvard.edu] with any questions or concerns about this Privacy Statement.\n## Users located outside the US\nIf you are located outside the United States, please click[here] for additional Harvard privacy disclosures that may apply to you.\n## Changes to this Privacy Statement\nThis Privacy Statement may be amended from time to time. Any such changes will be posted on this page. This Privacy Statement was last updated on September 30, 2024.",
    "length": 6476,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Harvard University Information Technology",
    "url": "https://www.huit.harvard.edu/news/news-types/ai",
    "text": "[Skip to main content] \n\n- [Main Menu] \n- [Utility Menu] \n- [Search] \n\n[![University Logo]] \n\n[HARVARD.EDU] \n\n[HOME] / [NEWS] /\n\nFebruary 11, 2025\n\nDear Members of the Harvard Community,\n\nSince the rapid emergence of generative artificial intelligence (AI) tools, members of our community have embraced experimentation in exciting ways, examples of which you can see on [Harvard’s generative AI website]. As these tools evolve, the University is continuously assessing their use to support responsible experimentation...\n\n[Read more about Guidance on AI meeting assistants; data privacy principles] \n\nSee also: [AI], [PrivSec] \n\nOctober 11, 2024\n\nSee also: [AI] \n\nJuly 25, 2024\n\nOn August 1, HUIT is launching a new and improved version of the [Harvard AI Sandbox], including new features such as image generation and data visualization.... [Read more about Coming soon: New and improved Harvard AI Sandbox] \n\nSee also: [AI], [HUIT News] \n\nNovember 16, 2023\n\n[Just as with text-based AI tools], image generation tools such as [Adobe Firefly] rely on a user’s prompts to create good images. After you enter a prompt, the AI model analyzes your input and generates a response based on the patterns it has learned through its training. More descriptive prompts can improve the quality of the outputs. This guide gives you a basic overview of how to generate prompts for image-based AI...\n\n[Read more about Getting started with prompts for image-based Generative AI tools] \n\nSee also: [AI] \n\nNovember 15, 2023\n\nThe University supports responsible experimentation with Generative AI tools. This page provides an overview of possible use cases, and important things to keep in mind before you begin experimenting. It is intended as general information only and is not a comprehensive list of all the ways in which people across the University are exploring the use of AI. You can find more resources, including available tools, at [HUIT’s Generative AI page].\n\n## Before you experiment with Generative AI tools\n\n... [Read more about Ideas for experimenting with Generative AI: Use cases and things to keep in mind] \n\nSee also: [AI] \n\nOctober 31, 2023\n\nSee also: [AI] \n\nSeptember 27, 2023\n\nSee also: [AI] \n\nSeptember 4, 2023\n\nHUIT, in collaboration with VPAL, the FAS Division of Science, and colleagues across the University, has developed an AI Sandbox tool to enable Harvard community members to securely access Large Language Models (LLM). The AI Sandbox provides a “walled-off,” secure...\n\n[Read more about AI Sandbox pilot launches] \n\nSee also: [AI] \n\nAugust 30, 2023\n\nThe information, sentences, or questions that you enter into a [Generative AI] tool (“prompts”) are a big influence on the quality of outputs you receive. After you enter a prompt, the AI model analyzes your input and generates a response based on the patterns it has learned through its training. More descriptive prompts can improve the quality of the outputs.\n\n...\n\n[Read more about Getting started with prompts for text-based Generative AI tools] \n\nSee also: [AI] \n\nAugust 25, 2023\n\nSee also: [AI] \n\n- 1 of 2\n- [››] \n\n## Latest News\n\n- [Introducing Okta: The new way to verify your identity with HarvardKey] \n- [Project Delivery Forum - Smartsheet Visits On-Campus on March 20!] \n- [Digital Digest Issue #5] \n- [EVP-AIIP program pilots conclude with valuable learnings on the application of AI] \n- [Changes to the HarvardKey sign-in experience from February 26] \n- [Guidance on AI meeting assistants; data privacy principles] \n\n[See All News] \n\n## AI\n\n- [February 2025] \n(1)\n- [October 2024] \n(1)\n- [July 2024] \n(1)\n- [November 2023] \n(1)\n- [November 2023] \n(1)\n\n- 1 of 3\n- [»] \n\nCopyright © 2025 The President and Fellows of Harvard College \\| [Accessibility] \\| [Digital Accessibility] \\| [Report Copyright Infringement]",
    "length": 3794,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Harvard designs AI sandbox that enables exploration, interaction without compromising security",
    "url": "https://news.harvard.edu/gazette/story/newsplus/harvard-designs-ai-sandbox-that-enables-exploration-interaction-without-compromising-security/",
    "text": "Harvard designs AI sandbox that enables exploration, interaction without compromising security &#8212; Harvard Gazette\n[![The Harvard Gazette logo]] \n* ![Close]![Open] Menu\n## Sections\n## Featured Topics\n## ![] Featured series\n### [Wondering] \nA series of random questions answered by Harvard experts.\n## Explore the Gazette\n## Read the latest\n* ![twin graphic] \n### [Your digital twin might save your life] \n* ![Holly Fernandez Lynch (from left), Matthew “Whiz” Buckley, Cat Packer, and David Yaden.] \n### [Time to legalize psychedelics?] \n* ![dna disorder] \n### [How a toxin from the gut microbiome may help spark colorectal cancer] \n* ![Search] Search\nSearch the Harvard Gazette\nGo\n[News+] # Harvard designs AI sandbox that enables exploration, interaction without compromising security\n![Computer on counter] \nContact[tim\\_bailey@harvard.edu] [https://huit.harvard.edu/] \nSeptember 26, 20236 min read\nGenerative AI tools such as OpenAI’s ChatGPT, Microsoft’s Bing Chat, and Google’s Bard have rapidly emerged as the most talked-about topic in technology, provoking a range of reactions —from excitement to fear —and sparking conversations about their role in higher education. In July, Harvard announced its[initial guidelines] for the use of generative AI tools, and strong community demand presented University administrators with the challenge of meeting this emerging need while providing safeguards against security and privacy deficiencies of many consumer tools.\n“At Harvard, we want to innovate and be at the vanguard of new technological developments, while ensuring that we honor our commitment to the security and privacy of University data and intellectual property,” said Klara Jelinkova, vice president and University chief information officer.\n![Klara Jelinkova, ] Klara Jelinkova, vice president and University chief information officer. Photo by Neal Adolph Akatsuka/Harvard IT\nTo address this challenge, Jelinkova, together with Bharat Anand, vice provost for advances in learning (VPAL), and Christopher Stubbs, dean of science in the Faculty of Arts and Sciences (FAS), proposed an idea to a group of faculty and staff advisers: a generative AI “sandbox” environment, designed and developed at Harvard, that would enable exploration of the capabilities of various large language models (LLMs) while mitigating many of the security and privacy risks of consumer tools.\n“Our aspiration is to see how we can use generative AI tools and LLMs to elevate, rather than merely preserve or defend, our teaching and learning practices,” explained Anand. “For this, we must be able to try new things, to experiment, to see what one can do with these technologies in new ways. It’s a very important first step here for Harvard.”\n“The idea of the sandbox,” Jelinkova added, “is to provide a space for experimentation with multiple LLMs —both commercial and open source —so we can learn more about them to inform our strategic procurement efforts, while meeting an immediate need for our community.”\nIn late July, a project team —led by Erica Bradshaw, chief technology officer for Harvard, and Emily Bottis, managing director for academic technology at HUIT —was quickly assembled, comprising more than 40 IT professionals in partnership with VPAL, the FAS Division of Science, and faculty and staff advisers from across the University. IT engineers, architects, and administrators came together to design and develop the secure sandbox and supporting materials with the goal of launching a pilot in time for the beginning of the academic year.\n![Bharat Anand,] “Our aspiration is to see how we can use generative AI tools and LLMs to elevate, rather than merely preserve or defend, our teaching and learning practices,” said Bharat Anand, vice provost for advances in learning. Photo by Evgenia Eliseeva\n“The biggest challenge was meeting the need in the short timeframe, but fortunately we were well positioned to meet this moment,” said Bradshaw. “HUIT’s[Emerging Technology and Innovation Program] blazed the trail by funding small scale AI experiments that enabled us to prototype and create the sandbox within a matter of weeks.”\n“It was invigorating to watch people from across the University coming together to collaborate with a very focused goal and tight timeline,” added Bottis.\nThe result: an AI sandbox tool with a single interface enabling users to easily switch between four LLMs: Azure OpenAI GPT-3.5 and GPT-4, Anthropic Claude 2, and Google PaLM 2 Bison. The prompts and data entered by the user are viewable only to that individual; they are not shared with LLM vendors, nor can the data be used to train those models. As such, pilot participants are permitted to enter a[higher classification of confidential data] into the sandbox than consumer tools.\nThe pilot formally launched on Sept. 1, with an initial group of 50 faculty participants. Approximately half of the initial requests for access were to simply learn how generative AI works. More specific use cases included researching various topics, writing assistance, feedback summarization, and advanced problem-solving, particularly in STEM fields.\n“These technologies are only as good as the creativity of the use cases we envision for them”, said Anand. “The range of ideas is already interesting, even in these early days. They cover use cases such as aggregating anonymized reading summaries from different students into one that reflects their diverse opinions, using LLMs to enhance critical thinking and ideation skills, using them as interactive teaching tools or personalized tutors, refining assessments and quizzes, and enhancing course outline or teaching plans for classes.”\nJohn H. Shaw, vice provost for research, also highlighted the “very positive impact” of the sandbox in the University working group on generative AI in Research &amp; Scholarship. “[The sandbox will] inform and enable University recommendations on effective ways to use generative AI to advance our research activities across so many fields.”\n“It’s an exciting time,” added Bottis. “Our job is to provide the tools and expertise to support this innovative work —and the sandbox is our first product in this area.”\nSome limitations emerged, including vendor rate restrictions necessitating a phased rollout. And some pilot participants highlighted the tendency for LLMs to “hallucinate”: providing confident, plausible-sounding outputs that are nonetheless false, emphasizing the need for discerning use.\nIn addition to the sandbox, the University will also pursue enterprise agreements with commercial vendors. “In many cases,” said Bradshaw, “consumer generative AI tools with appropriate privacy and security protections may be a more suitable choice. We’re continuing to negotiate with vendors to be able to offer a broader range of tools, and exploring tools where AI is embedded as part of the feature set.”\nJelinkova expects that the University will expand access to the sandbox in the fall semester based on use cases. “By documenting use cases and outcomes, we can use this pilot to inform our broader strategy. Our goal is to learn, and then make that learning visible to faculty and staff leaders across the University who are thinking about how to chart a path forward.”\n“This journey promises to be interesting,” said Anand. “And we’re only in the first inning.”\n#### Share this article\n* ![] [Share on Facebook] \n* ![] [Share on LinkedIn] \n* ![] [Email article] \n* ![] [Print/PDF] \n## You might like\n* ![twin graphic] \n[Health] ### [Your digital twin might save your life] \nAI, statistics offer new possibilities for personalized medicine\n6 min read\n* ![Holly Fernandez Lynch (from left), Matthew “Whiz” Buckley, Cat Packer, and David Yaden.] \n[Health] ### [Time to legalize psychedelics?] \nCampus debate weighs therapeutic need vs. safety questions\n6 min read\n* ![dna disorder] \n[Health] ### [How a toxin from the gut microbiome may help spark colorectal cancer] \nFindings suggest colibactin may be promising target for disease prevention\n[![Findings series]] \n4 min read\n## Trending\n1. [Health] ### [Nighttime exposure to light may raise cardiovascular risk by up to 50%] \nNew research suggests that it disrupts biological clock\n5 min read\n2. [Work &amp; Economy] ### [Anxious about retirement savings? Avoid these mistakes.] \nComplexity of finance system sets up many for failure, argues economics professor\n7 min read\n3. [Science &amp; Tech] ### [How memory works (and doesn’t)] \nIn podcast, scientists explain why remembering is more reconstruction than replay\nlong read",
    "length": 8566,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "",
    "url": "https://www.harvard.edu/ai/work-resources/",
    "text": "Work with Generative AI - Generative AI @ Harvard\n[Skip to main content] \n[![A logo that says Generative AI at Harvard]] \nGenerative AI**\n# Work with Generative AI\nResources for staff\nGenerative AI offers numerous opportunities to enhance our administrative work. Harvard staff have embraced those opportunities through explorations and pilots across Units and Schools. The[Generative AI Administration and Operations Group] helps provide visibility into these pilots through the[AI Community of Practice wiki (log in with your HarvardKey)]. This resource page aims to provide Harvard staff and administrators with basic guidance, information on available resources, and contacts. The content will be regularly updated as these technologies continue to evolve.[**Your feedback is welcome.**] \n## Policies and guidance\nSee[Initial guidelines for the use of Generative AI tools at Harvard] for important considerations to keep in mind when using GenAI tools, including information security and data privacy, compliance, copyright, and academic integrity.\n## Tips, tools, and best practices\nThere are many[exciting potential use cases for generative AI], including productivity enhancements, creative design, coding, search, and much more. The University supports responsible experimentation with generative AI. Be sure to[review the University’s guidelines] on safe and effective use and explore the resources below to get started.\n### Compare AI tools and check appropriate data levels\nLearn more\n[Learn more] \n### Ideas and use cases for experimenting with AI\nLearn more\n[Learn more] \n### Get started with prompts for text-based AI tools\nLearn more\n[Learn more] \n### Get started with prompts for image-based AI tools\nLearn more\n[Learn more] \n## Other resources\n### Generative AI at Harvard\n* [Derek Bok Center for Teaching and Learning: Generative A.I.] \n* [Introduction to Generative AI] \n* [MetaLAB at Harvard] \n* [AI Pedagogy Project] \n* [Getting Started with ChatGPT] \n* [Office of the Vice Provost for Advances in Learning] \n* [Harvard Initiative for Learning &amp; Teaching] \n* [HILT Conference 2023: Teaching in the Age of AI: Nurturing Connections and Empowering Learners] \n* [AI @ Harvard] \n* [In Focus: Artificial Intelligence] \n* [EU AI Act Prohibited Use Cases] \n### Learn more\n* [LinkedIn Learning: Artificial Intelligence] \n* [Harvie: Learn about Generative AI] \n![] \nExploring Generative AI Administrative Applications\n[Click to Play Video] \nResource spotlight\n## Videos for staff\nIsmael Carreras, Associate Dean for Strategic Analysis for FAS Administration &amp; Finance, discusses his use of generative AI models like ChatGPT and FAS AI Sandbox for tasks such as drafting documents, summarizing long texts, and creating visuals and code.\n[Learn more and watch other videos on FAS&#8217; Generative AI website]",
    "length": 2827,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Generative AI in Teaching and Learning at the GSD",
    "url": "https://www.gsd.harvard.edu/resources/ai/",
    "text": "Generative AI in Teaching and Learning at the GSD - Harvard Graduate School of Design[Skip to main content] \n[Harvard Graduate School of Design] \nPrimary Menu\n[] \n[] \nClose Primary Menu\nOpen Search\nSubmit SearchClose Search\n# Generative AI in Teaching and Learning at the GSD\n![A computer-generated image spells the words Made by A I with pink mylar balloons] Image generated by Adobe Firefly (beta)\nThis page provides policies, information, and guidance for courses regarding the use of generative AI in teaching and learning at the GSD. Generative AI is artificial intelligence trained on large data sets that can predict letters, words, sounds, images, code, etc., based on the likelihood of those so-called ‘tokens’ occurring together. Note that generative AI is not intelligent and doesn’t think or apply reason as we understand it but produces output that mimics the data it was trained on, flaws included.\n*Updated 10/24/2024*\n## Policies\n*What do I have to keep in mind when using generative AI in teaching and learning?*\nInformation security and data privacy\n* Never submit personal information or any information[classified by HUIT as Level 2 or higher] into publicly available generative AI tools.\n* Be vigilant and[follow security best practices] as AI-driven scams are becoming more sophisticated.\nCompliance and copyright\n* You are responsible for the accuracy and compliance of your content.\n* Depending on the tools and parameters you use, AI-generated content can be inaccurate, misleading, entirely fabricated, or may contain material protected by copyright.\n* Do not submit work to which you don’t have rights into a generative AI tool and always be prepared to disclose your usage of any such tool.\nAcademic integrity\n* The GSD’s academic integrity policy can be found in the Student Handbook and any reference to unauthorized human or non-human support and aids in producing academic work applies equally to generative AI tools.\n* As a rule of thumb, wherever it is inappropriate for you to ask a human contributor to do work for you or where you don’t have explicit permission to share the work of one person with another, it is equally inappropriate for you to prompt an AI tool to do work for you or upload the work of others into an AI tool.\n* Instructors determine what constitutes appropriate use of generative AI in their courses just like they have the authority to determine what constitutes appropriate use of established technological aids and reliance on human collaboration.\n* Students are expected to be familiar with and abide by the School’s standards for academic integrity and conduct, and consult their instructor if they need clarification.\n* It is suggested that instructors be proactive and communicate expectations for academic conduct and the use of generative AI tools for their courses (see “Guidance for GSD courses” below).\n## Tools and General Information\n*What tools are available, and where can I learn more about generative AI?*\nGenerative AI tools available at Harvard\n* Harvard University IT provides up-to-date[information about available AI tools online].\n* If you have questions about the risk of using a specific tool or are interested in learning whether HUIT may be able to provide a secure environment for experimenting with a specific tool, please contact[HUIT] \nHarvard Resources on Generative AI\n* [Harvard’s central information hub around generative AI] provides a wealth of information, recommendations, scenarios, and testimonials for different user groups: faculty, students, scholars and researchers, and staff.\n* [HUIT’s website on generative AI] provides up-to-date information about available tools, usage considerations, and related resources.\n* [The Derek Bok Center’s Resources on Teaching and Artificial Intelligence] in Canvas include a wealth of useful information about AI in teaching settings.\n* [metaLAB (at) Harvard’s Proposed Harvard AI Code of Conduct] provides key points and suggestions for the responsible use of AI in alignment with the Harvard College Honor Code.\nNon-Harvard Resources on Generative AI\n* [TheresAnAIForThat.com] is an AI-generated database that indexes and tracks 15,000+ (and counting) publicly available AI tools across the internet.\n* [“A Generative AI Primer” by Michael Webb] explains generative AI technology and its expected impact on higher education.\n* [unesco’s quick start guide on “ChatGPT and Artificial Intelligence in higher education”] illustrates the challenges and ethical implications of AI.\n## Guidance for GSD Courses\n*What usage of AI tools is appropriate in GSD courses?*\nPolicy for the use of AI in courses\nWe encourage all instructors to include a policy in course syllabi regarding the use and misuse of generative AI.Whether students in your course are forbidden from using ChatGPT or expected to explore its limits, a policy helps ensure that your expectations forappropriate interaction with generative AI tools are clear to students. Once you decide on a policy, make sure you articulate it clearly for your students, so that they know what is expected of them.Below is sample language you may adopt for your own policy. Feel free to modify it or create your own to suit the needs of your course.\n* **Restrictive draft policy:***We expect that all work students submit for this course will be their own. In instances when collaborative work is assigned, we expect for the assignment to list all team members who participated.*We strictly prohibit the use of ChatGPT, AI-based image generators, or any generative artificial intelligence (GAI) tool at any stage of the work process, including initial or preliminary stages.*Violations of this policy will be considered academic misconduct. We draw your attention to the fact that different classes at Harvard could implement different AI policies, and it is the student’s responsibility to conform to expectations for each course.*\n* **Fully encouraging draft policy:***This course encourages students to explore the use of generative artificial intelligence (GAI) tools such as ChatGPT or an AI-based image generator for all assignments and assessments. Any such use must be appropriately acknowledged and cited. It is each student’s responsibility to assess the validity and applicability of any GAI output that is submitted; you bear the final responsibility. Violations of this policy will be considered academic misconduct. We draw your attention to the fact that different classes at Harvard could implement different AI policies, and it is the student’s responsibility to conform to expectations for each course.*\n* **Mixed draft policy:***Certain assignments in this course will permit or even encourage the use of generative artificial intelligence (GAI) tools such as ChatGPT or an AI-based image generator. The default is that such use is disallowed unless otherwise stated. Any such use must be appropriately acknowledged and cited. It is each student’s responsibility to assess the validity and applicability of any GAI output that is submitted; you bear the final responsibility. Violations of this policy will be considered academic misconduct. We draw your attention to the fact that different classes at Harvard could implement different AI policies, and it is the student’s responsibility to conform to expectations for each course.*\nCommon uses for students\nIn alignment with course policy requiring disclosure and excluding certain scenarios, students might want to use AI tools in coursework for tasks such as\n* Formulating initial ideas and starting points for research and asking high-level non-specialized questions about their goals\n* Proofreading or correcting existing text similar to what is provided by tools such as Grammarly (which relies on AI in its main functionality)\n* Gathering references and resources for research, with great caution towards unreliable and fabricated content (sometimes called “hallucinations”)\n* Summarizing large datasets that are either publicly available or don’t otherwise violate data privacy policies. An example would be extracting verdicts from hundreds or thousands of publicly available legal cases\n* Analyzing existing and non-protected sets of data for correlations or possible patterns\n* Generating images with caution towards possible copyright infringement (note that Adobe Firefly is trained exclusively on licensed or freely available content and thus poses no risk regarding copyright)\nCommon uses for instructors\nInstructors are expected to use great caution in employing AI technologies in teaching and are responsible for ensuring accuracy. However, like traditional internet searches, AI-generated content can provide a useful starting point and inspiration for\n* Drafting lesson plans, exercises, or quizzes; note that output will almost always contain problems and requires careful review\n* Summarizing, simplifying, or customizing existing material such as lecture notes; when asked to edit text, AI generally won’t introduce new or misleading concepts, but vigilance is key\n* Note that while it is inappropriate for instructors to use AI for providing feedback on student work, it may be appropriate to ask students to seek AI-generated feedback on their work as part of a carefully framed assignment (e.g., AI tools may be getting better at auditing a design project for compliance with codes or other specifications)\nCovering the cost of AI tools\n* Instructors are encouraged to utilize AI tools in their courses that Harvard or the GSD provides on enterprise agreements as they become available. In partnership with HUIT, the GSD is actively exploring how to make desirable tools available at no additional cost to users, and contract negotiations are ongoing\n* Instructors may also require students to purchase individual licenses to a specific generative AI tool not provided by Harvard or the GSD and are asked to include information and the expected cost on their syllabus, treating it the same as any other expected expense associated with courses, such as materials for fabrication\n* Instructors may not use course budgets to pay for or reimburse students for using AI tools in their courses but may use available research funds to pay for AI tools for their own research and experimentation; contact the[GSD Help Desk] to learn whether IT can offer support or access to a desired tool for your course needs\n*For more information about AI platform access and license requirements, please contact the[GSD Help Desk].*\n[] \nSearchSubmit\n## Results for\nShow Filters\n### Type\n* Pages\n* People\n* Courses\n* Events &amp; Exhibitions\n* Publications\n* News\n* Resources\n* Projects\n* Podcasts\n* Harvard Design Magazine\n### Sort by\nSort byRelevanceDate, oldest to newestDate, newest to oldestLast updated\nResults\nClear Filters ()\nLoad More",
    "length": 10803,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard - July 13, 2023",
    "url": "https://provost.harvard.edu/links/guidelines-using-chatgpt-and-other-generative-ai-tools-harvard-july-13-2023",
    "text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard - July 13, 2023 | Office of the Provost[\nSkip to main contentarrow\\_circle\\_down\n] \n[![Harvard University]] \n[\nOffice of the Provost\n] \nmenucloseMenu\nSearch\nSearchsearch\n[\nOffice of the Provost\n] \n# Guidelines for Using ChatGPT and other Generative AI tools at Harvard - July 13, 2023\nDear Members of the Harvard Community,\nWe write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI’s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.\nGenerative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.\n***Initial guidelines for use of generative AI tools:***\n* **Protect confidential data:**You should not enter data[classified as confidential] (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University’s[Information Security Policy]. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties.\n* **You are responsible for any content that you produce or publish that includes AI-generated material:**AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called “hallucinations”), or may contain copyrighted material. Review your AI-generated content before publication.\n* **Adhere to current policies on academic integrity:**Review your School’s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they’re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed.\n* **Be alert for AI-enabled phishing:**Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to[follow security best practices] and report suspicious messages to[phishing@harvard.edu].\n* **Connect with HUIT before procuring generative AI tools:**The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds.\n* If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at[ithelp@harvard.edu].\n* Vendor generative AI toolsmust be[assessed for risk by Harvard’s Information Security and Data Privacy officeprior to use].\nIt is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use,[on the HUIT website], which will be updated as new information becomes available. Sincerely,\nAlan M. Garber\nProvost\nMeredith Weenick\nExecutive Vice President\nKlara Jelinkova\nVice President and University Chief Information Officer\nSee also:\n* [Reports]",
    "length": 3553,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Teach with Generative AI",
    "url": "https://www.harvard.edu/ai/teaching-resources/",
    "text": "Teach with Generative AI - Generative AI @ Harvard\n[Skip to main content] \n[![A logo that says Generative AI at Harvard]] \nGenerative AI**\n# Teach with Generative AI\nResources for faculty\n## Overview\nWhen it comes to the future of education, virtually no recent technology has sparked as much debate as generative AI (GenAI) and large language models (LLMs). Some have seen this technology as destructive, with school districts from[Los Angeles] to[New York] initially banning its use, and others have touted its[transformative impact] and possibility of changing the game for educators and students alike.\nHarvard has consistently tried to embrace new technology across our classrooms, residential and virtual. GenAI has been no different.\nFaculty and students have access to a range of tools. Some of these tools are free and open to all faculty, students and staff behind Harvard Key (Harvard Sandbox) other tools require a license or approval.[See a list of tools].\nAs our faculty and students have engaged with these technologies we have invited faculty to reflect on questions such as the following:\n* What is the challenge you were trying to address?\n* How did you use generative AI tools to tackle it?\n* What did you learn?\nOut of this there have been several learnings worth considering:\n1. GenAI tools have raised concerns about how they may compromise student assessments, promote academic dishonesty, and facilitate “lazy learning.” Our faculty colleagues who experimented with these tools were not oblivious to these concerns; indeed, many share them. At the same time, faculty are looking to understand how GenAI tools can enhance the educational experience and build more vibrant classrooms.\n2. Several colleagues are leveraging LLM features that everyone should keep in mind:\n1. **Beyond text:**For[visual aids] and[images],[coding],[analysis],[games],[simulations], and more.\n2. **Prompt design:**There’s an old saying: &#8220;garbage in, garbage out.&#8221; The output of LLMs is only as good as the input, and it’s essential to learn (and perhaps teach) how to write a prompt that works. This is highlighted through discussions on the critical role of deliberate prompt formulation, from[having students iterate on their prompts through the course], to engaging students in debate on the[ethics of AI use], to[making advanced statistical concepts accessible to diverse learners]. Our new**[System Prompt Library] **offers a range of effective prompts that can be used by educators.\n3. **Interrogate hallucinations**: Errors arise not just because of algorithmic or data limitations but, importantly, because LLMs are fundamentally probabilistic. Faculty have found that errors can be reduced through[detailed prompt engineering] and[balanced AI-human partnership].\n4. Some consistent patterns and learnings emerge from how GenAI has been implemented for use by our colleagues:\n1. **Going beyond the simple question-and-answer interface:**Sal Khan popularized the idea of using LLMs to ask questions of a student, not just answer them. Several faculty colleagues take this further, illustrating how LLMs can be used to simulate any persona you want, and to ask anything of them. Examples include simulating[experts],[peers],[graders],[course designers,] and[personal chatbots].\n2. **More than the “first draft”:**GenAI needn’t compromise student creativity; in fact, it can augment it. Some colleagues are using it to help students[refine project prototypes] and[polish final drafts].\n3. **Work alongside what you already have:**Many faculty used LLMs to improve different (and sometimes mundane) aspects of existing teaching and learning “workflows,” such as[producing course materials],[personalizing feedback],[generating assignments],[summarizing real-time student responses], and[tutoring students].\n4. **Identify, and overcome, hidden or invisible barriers:**Students and educators sometimes confront hidden prerequisites that present barriers for teaching and learning. GenAI can assist with overcoming these skill gaps:[coding for a business class], foreign languages for research, art skills for building visual aids, and even[building games for class engagement.] \n5. **Reimagining the classroom**: While we’re still in the early days of GenAI, some of these examples already start to surface more profound questions: What does a class with GenAI at its core look like? Ultimately, what is the role of a teacher?\n6. Questions around genAI’s efficacy on learning arise: can we use GenAI tools—specifically tutor bots—to improve the way students learn? One faculty member created a tutor bot that[answered questions like course staff]. Beyond such research on students’ interactions with genAI tools, it might be helpful to imagine how it can help you and your students now, in other ways as well: by increasing task efficiency, improving student engagement, increasing their confidence, or even improving learning outcomes.\n7. The risks of LLMs present valid concerns. While popular debates often focus on “big picture” concerns like algorithmic biases, digital divides, and fake content, some faculty explore the risks[at a micro scale], within our classrooms, such as hallucinations,[failed reasoning], or superficial thinking,[pushing students] to understand these issues more deeply.\nVideo interviews that fed these reflections are featured here in the first[**Harvard GenAI Library for Teaching and Learning**]. And across Harvard there have and will continue to be convenings large and small bringing together faculty, students, staff, and administrators. As you think about what’s relevant for your course, support exists across Harvard to help you and your team experiment as well.\n## Frequently asked questions\nThe following information offers advice for educators interested in using generative AI tools in their teaching and course preparation. As this technology is constantly evolving, this page will be updated frequently with new resources and advice.\n### How can I use generative AI to help prepare my curriculum?\nCreating materials for courses—syllabi, lesson plans, assignments—takes time. Generative AI tools aren’t just useful at broad, general prompts, but, as our colleagues have shown, they are useful in tackling the preparations before a student even arrives in the classroom:\n* **Preparing to teach:**Starting a syllabus or a lesson plan from a blank page is daunting. AI can help you[outline your course, create learning objectives, and suggest assignments or in-class activities,] while making content that fits your course by[feeding it specific reference materials].\n* **Assignments:**Reusing the same assignments across multiple years can be time efficient, but it creates challenges for assessments. Some faculty have explored how AI tools can help write, modify, or create question sets.[The more information] you put in about the structure and concepts you want it to use,[the better it will be]. And it can even[make a rubric] for it.\n* **Course content:**Classes often include large amounts of content for reading, from case studies to readings to slideshow text. Just a few sentences of a prompt using GenAI can[generate summaries of relevant readings] or videos you might want to include for students,[brand new cases] to discuss, and even what your[slideshow for a lecture] should include. Some faculty have gone a step further, inputting all the course materials in the materials that trained a teaching assistant chatbot. Hear what[this faculty member] learned from students’ interactions with this “faculty copilot.”\n### How can I use generative AI to help me teach in the classroom, day to day?\nHow to engage your students in the classroom is an age-old question. Some faculty have used GenAI’s real-time thinking and range of outputs to help offer contemporary solutions, such as a content-generator, a data analyst, or a personal tutorbot.\n* **Activity leader:**Creating interactive classes is easy to advocate for, but hard to do. Some of our faculty have used GenAI to[act like a peer student] to stimulate critical thinking,[perform real-time analysis of student responses] to make them feel heard, or even help[make games that align with class content],no coding needed!\n* **Personal tutor:**You (or your TA) can’t be everywhere at once, but GenAI might be able to. Feeding it your syllabus, lectures, and an in-depth prompt can help make a personal tutorbot,[generate unlimited practice problems,] or even[remind students of course-specific information].\n* **Custom reviewer:**LLMs can be used to provide initial personalized feedback to your students, so you can focus on the big picture. Some faculty used them to[quickly summarize student responses] before office hours, or even[point out areas in student responses that need improvement].\n* **Skill leveler:**Classes often have hidden prerequisites: familiarity with coding, texts barred in foreign languages, or even art skills. GenAI tools can be leveraged creatively to help students overcome such barriers, like[teaching a business school class about data analysis] without requiring every MBA student to learn code, or[analyzing trends of thousands of photographs] without having to do so manually.\n### How can I redesign assessments to reduce the misuse of generative AI?\nSince LLMs can write essays, respond to readings, and finish problem sets, how can one not be concerned about misuse? For you and your students, addressing this concern means first making sure we know what GenAI can and can’t do—then creating assessments that emphasize skills where AI tools fall short.\nHere are some strategies faculty have found useful:\n* **Human-based learning:**Design tasks and assessments that require creativity, practical application of concepts, and critical thinking. For example, instead of asking your students to summarize perspectives on a given issue, you may ask them to critically analyze which perspective is most convincing to them and explain why; to relate their answers to class discussions; or to assess their peers’ performance during a live problem-solving session.\n* **Process-based assessments:**Another approach is to test intermediate steps in the learning process, instead of just the final product. (It’s easy to fudge your report card to your parents; it’s harder to fudge not having gone to school for the past two months.) Testing evidence of original thinking, planning, peer-to-peer conversations, etc. can make relying on genAI less attractive.\n* **Establishing norms**: Emphasize original work and academic honesty, and at a minimum provide clear guidelines about the use of AI-generated content in assignments and assessments.\n### What are the most important risks of using GenAI tools in my classroom?\nHere are the most important ones to keep in mind:\n* **AI models can make mistakes**: We’ve all had an incident (or two!) where a GenAI tool seems to have lost its mind, yielding garbled or entirely made-up answers. These are called AI hallucinations. It’s tempting to think these will get eliminated over time as technology improves. But since LLMs are fundamentally probabilistic rather than deterministic, this may not be the case.\n* **AI models can be biased:**AI adopts the biases of the material and data it was trained on. Good AI use involves being aware of, checking for, and making efforts to correct such biases.\n* **AI models can violate privacy:**AI is very good at doing what you want, but it is also very bad at knowing if what you want it to do is allowed. Personal data is not supposed to be fed to GenAI models. Make sure you are aware of Harvard’s[data protection policies] and[FERPA].\n* **AI models can be misused**: Of course, AI could be used to plagiarize assignments. Unless you are interested in grading robots, you should shift the kind of assignments you are providing students (see above) as well as enforce academic dhonesty policies.\n### How do I know whether GenAI is even effective for teaching?\nGenAI’s ability to meet learners where they are, both in terms of prior knowledge and learning progress, can[increase students’ understanding], and AI-powered educational games can[increase student motivation and engagement], particularly in STEM courses. One fascinating study showed that when students tutored by AI are pitted against students taught in the traditional classroom setting, the[AI-tutored students performed as well or better].\nIt is natural for instructors, particularly successful ones, to wonder:*GenAI may be useful for the average educator. But my classes are great; why would I need it?*\nOne way to think about this is in terms of the efficiency benefits of GenAI tools—they can save time, facilitate meaningful non-classroom learning experiences, and make classroom discussions more interactive. For example:\n* Utilize[highly thoughtful prompt engineering] with GenAI to build a[tutorbot] that gives students an unlimited number of interactive statistical problems.\n* Challenge students with DIY[interactive simulation games] created on short order—and without any coding prerequisite!\n* Empower students to experiment with[visualizations of interactions] with just a few minutes prompt engineering with DALL-E.\n### How can I become a GenAI whiz?\n* **Where to start experimenting**: If you’re ready to jump right in, then the best place is to start is the[Harvard AI Sandbox]. Like any sandbox, it’s a great place to play around with tools; unlike any sandbox, it has five large-language-models to choose from and is accessible[by request here].\n* **Where to modify images:**If you’re looking to use GenAI to manipulate and create images, then you can use download Adobe Firefly through the[Harvard Adobe Creative Cloud license]. For more information, see[Getting started with prompts for image-based Generative AI tools].\n* **Where to learn more**: If you want to level up your GenAI knowledge before you start creating, check out the[FAS Division of Science Generative AI Resources] or theBok Center&#8217;s[Artificial Intelligence] page.\n* **Where to find higher level tools**: API access to tools like Azure OpenAI, Google Vertex, and AWS services is[available by request] from HUIT. If you don’t know what these are, that’s okay; teachers have made remarkable tools just using ChatGPT and a well-crafted prompt!\n### What policies should I keep in mind when using LLMs in my classroom?\nWhen using large language models (LLMs) in your classroom, it&#8217;s essential to be aware of the[University’s guidelines] designed to ensure responsible and effective use of these technologies and to refer to School-specific policies and resources. It is also important to remember that other existing policies, such as Harvard’s[Information Security Policy],[Digital Accessibility Policy], and[Intellectual Property Policy], also apply to GenAI and the use of LLMs.\nAcross Harvard, there&#8217;s a strong emphasis on using LLMs[ethically and in ways that uphold academic integrity]. The use of generative AI must align with the principles of honesty, respect, and responsibility, ensuring that students&#8217; work remains original and reflective of their understanding and skills. In crafting a response to the use of LLMs in the classroom, it&#8217;s crucial to strike a balance between leveraging these tools for educational enhancement and ensuring that they do not compromise educational objectives.\nAt the course level, Schools within Harvard allow for the[customization of AI usage policies by individual instructors], provided these are[clearly communicated]. Each encourages innovative and thoughtful[integration of AI in teaching and learning practices], including[learning to use generative AI productively]. Above all, students and faculty are encouraged to be transparent about the use of generative AI in academic work. This includes proper[attribution] when AI-generated content or assistance is utilized in the creation of academic materials. Consider co-creating course-specific norms around the use of generative AI with your students.\nFor any faculty who wish to simply explore the use of these tools, we encourage you to engage with peers and organizations at Harvard that are interested in these topics.\n![] \n## School-based resources\nVisit your School&#8217;s website for the latest policies and guidance around using GenAI in the classroom. This list will be updated as further School-specific guidance becomes available.\n* [Harvard Business School] \n* [Harvard College] \n* [Harvard Division of Continuing Education] \n* [Harvard Graduate School of Design] \n* [Harvard Divinity School] \n* [Harvard Kenneth C. Griffin Graduate School of Arts and Sciences] \n* [Harvard John A. Paulson School of Engineering and Applied Sciences] \n* [Harvard Kennedy School] \n* [Harvard Medical School]",
    "length": 16816,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "AI Syllabus Policies | The Derek Bok Center for Teaching and Learning",
    "url": "https://bokcenter.harvard.edu/ai-syllabus-policies",
    "text": "AI Syllabus Policies | The Derek Bok Center for Teaching and Learning[\nSkip to main contentarrow\\_circle\\_down\n] \nannouncement\n**The Bok Center is currently developing new programming and new resources for the FAS community. Thank you for your patience as we revise our website to reflect these changes.**\nclose\n[![Harvard University]] \n[\n![The Derek Bok Center for Teaching and Learning] \n![The Derek Bok Center for Teaching and Learning] \n] \nmenucloseMenu\nSearch\nSearchsearch\n[\n![The Derek Bok Center for Teaching and Learning] \n![The Derek Bok Center for Teaching and Learning] \n] \n# AI Syllabus Policies\nAll instructors are encouraged to include a policy on course syllabi outlining acceptable and unacceptable uses of AI.\n**Syllabus statement advice**is available through:\n* [The Office of Undergraduate Education website] \n* [The Bok Center’s Illustrated Rubric for Syllabus Statements on Generative AI] \nIn addition to including an AI policy on your syllabus, also:\n* Post the statement on your Canvas site\n* Reiterate the statement on assignment prompts\n* Discuss the policy in course meetings and office hours\nShare resources with students like the[Harvard Libraries Artificial Intelligence for Research and Scholarship Guide], which includes information about citing AI.\n**The Bok Center is currently developing new programming and new resources for the FAS community. Thank you for your patience as we revise our website to reflect these changes.**",
    "length": 1459,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Strategy and Initiatives",
    "url": "https://www.huit.harvard.edu/strategy",
    "text": "About | Harvard University Information Technology[\nSkip to main contentarrow\\_circle\\_down\n] \nSearch\nSearchsearch\n[\n![Harvard University Information Technology] \n![Harvard University Information Technology] \n] \nmenucloseMenu\nSearch\nSearchsearch\n[\n![Harvard University Information Technology] \n![Harvard University Information Technology] \n] \n# About\n# About\nHarvard University Information Technology\n![Audience members listen to a keynote speech at the 2024 IT Summit.] \n## About HUIT\nTechnology underpins nearly every aspect of Harvard’s academic and administrative activities. As the University’s central IT organization, HUIT delivers technologies, strategy, and policies that support more than 50,000 faculty, students, staff, and researchers.\nexplore\n[### Mission and values\n] \nOur work is guided by our mission and core valuesthat were developed collaboratively by our team\nbook\n[### Annual Report\n] \nLearn more about our areas of focus and recent accomplishments\ngroups\n[### Our team\n] \nLearn about our organizational structure and how to join us\naccount\\_box\n[### Senior Leadership Team\n] \nIncluding the Vice President and University Chief Information Officer and leaders of HUIT's technical and administrative teams\n## IT at Harvard\nlanguage\n[### IT Community\n] \nHarvard's IT Community features programs for connection, collaboration, and celebration across all of Harvard's Schools and units\nshield\n[### CIO Council\n] \nLeading and advancing Harvard-wide IT strategy in support of the missions of both the individual schools and the University",
    "length": 1552,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Diplomacy, artificial intelligence, and war - Harvard Law School | Harvard Law School",
    "url": "https://hls.harvard.edu/today/diplomacy-artificial-intelligence-and-war/",
    "text": "Diplomacy, artificial intelligence, and war - Harvard Law School | Harvard Law School[Skip to content] \n##### [National &amp; World Affairs] \n![United Nations headquarters in New York.] \n# Diplomacy, artificial intelligence, and war\nHarvard Law&#8217;s Program on International Law and Armed Conflict (PILAC) collaborates on a landmark workshop on AI and international humanitarian law\nMar 17, 2025\nIn an era where artificial intelligence (AI) is reshaping global security dynamics, more than 90 diplomats from dozens of U.N. member and observer states convened at the Permanent Mission of Egypt to the United Nations in New York on Feb. 28 for a workshop on AI and International Humanitarian Law (IHL). Organized in collaboration with Harvard Law School’s[Program on International Law and Armed Conflict (PILAC)] and the African Union Office of the Legal Counsel, the event provided a unique opportunity for representatives from around the world to engage in an open and wide-ranging discussion on the technological, legal, and ethical implications of AI in armed conflict.\n## A call for informed discussion\nThe workshop opened with welcoming remarks from H.E. Ambassador Osama Abdelkhalek, Permanent Representative of Egypt to the United Nations; H.E. Elinor Hammarskjöld, the new Under-Secretary-General for Legal Affairs and United Nations Legal Counsel; Dr. Hajer Gueldich, legal counsel of the African Union; Dr. Mohamed Helal LL.M. ’10, S.J.D. ’16, member of the African Union’s Commission of International Law; and Harvard Law School Professor of Practice[Naz K. Modirzadeh ’02], founding director of PILAC. Their collective insights set the tone for the day’s discussions, emphasizing the urgent need for diplomatic and legal engagement on AI’s evolving role in war.\n“Before we delve into our discussions, let me underscore an important aspect of IHL that can often be lost in abstract discussions about law and that can become even more attenuated in discussions about AI. At its core, IHL is about human beings,” Modirzadeh said. “As we discuss the use of AI in armed conflict, we must always return to central questions: What are the roles and responsibilities of humans in ensuring that AI remains a tool to help uphold the law and its normative commitments, not just a framework for technological advancement? And how does this affect human beings caught up in armed conflict?”\nShe emphasized that the workshop was meant to foster open inquiry rather than impose rigid conclusions. “At PILAC, we operate on the assumption that all states make and interpret international law and that all states benefit from access to reliable information, independent research, and opportunities for dialogue on areas of global public concern.”\n![] \nDustin Lewis (left), PILAC research director, and Naz Modirzadeh, founding director of PILAC.## AI, explained\nThe workshop provided a structured introduction to AI’s technological aspects and its increasing role on some modern battlefields.\nA session led by Julia Stoyanovich, the institute associate professor of computer science and engineering and associate professor of data science at New York University, provided a deep dive into what underpins AI. Stoyanovich —who serves as director of the Center for Responsible AI at NYU —summarized the key ingredients of AI, drawing on everyday analogies, such as cooking recipes and fraud-detection classifiers, to demystify AI’s core attributes and functions. Her interactive presentation also addressed the distinctions between rule-based and learning algorithms, the role of data in decision-making, and the ethical implications of AI deployment in high-stakes environments, such as the military.\n[Dustin A. Lewis], research director of PILAC, illustrated an array of AI applications being developed for wars. Those developments include AI-based decision-support systems (DSS) to process battlefield information, AI systems for target recognition, AI-reliant defenses against adversarial cyber operations, and AI-based predictive analytics to assist humanitarian actors in better allocating resources.\n## The legal framework: Harmonizing innovation and responsibility\nFollowing the introduction to AI, Modirzadeh and Lewis turned to the international legal dimensions of armed conflict, covering the foundations of IHL, rules regulating attacks and detention operations involving AI, and how to uphold legal responsibility when warring parties rely on AI.\nThey drew extensively on PILAC’s January 2025 legal concept paper,[Exercising Cognitive Agency: A Legal Framework Concerning Natural and Artificial Intelligence in Armed Conflict], which Lewis co-authored with Hannah Sweeney ’24. That analysis grounds the examination of AI in war in the roles and responsibilities of humans in performing IHL obligations. In recent years, numerous Harvard Law School research assistants —including Camila Castellanos Forero LL.M. ’25, Erica Chen ’25, Emma Davies ’25, Eoin Jackson LL.M. ’23, Elizabeth Peartree ’25, Emma Plankey ’24, Tamar Ruseishvili LL.M. ’25, Elliot Serbin J.D./M.P.P. ’24, Zoe Shamis ’24, Sima Sweidat LL.M. ’25, Dominique Virgil LL.M. ’25, and Cecilia Wu ’24 —have supported PILAC’s research on AI and IHL.\nBeyond the late-February workshop for diplomats, Lewis recently briefed government, military, humanitarian, and U.N. actors on PILAC’s research in Geneva, The Hague, Kyiv, Seoul, and Stockholm. At those engagements, he emphasized the global relevance of these discussions and the pressing need for clarity on how governments, armed forces, humanitarian actors, and international courts should approach the range of legal, ethical, and policy challenges and opportunities concerning AI in war.\n## Diplomatic collaboration and future initiatives\nThe workshop concluded with an overview of ongoing international legal and policy initiatives on military AI governance. Representatives from Egypt, the Netherlands, and the United Nations Office for Disarmament Affairs discussed multilateral efforts to establish legal and ethical guidelines for the use of AI in armed conflict.\nAs discussions drew to a close, the gathering underscored the urgency of addressing AI’s role in modern warfare within the confines of international law. Participants expressed commitments to continued dialogue and deepening collaboration with PILAC to remain informed on emerging technological and legal developments in this rapidly evolving field.\n**Want to stay up to date with Harvard Law Today? Sign up for our weekly newsletter.**\n[Sign up for the Harvard Law Today newsletter] \n## Modal Gallery\nClose modal gallery\nof\nPreviousNext\n## Gallery Block Modal Gallery\nClose modal gallery\nof\nPreviousNext",
    "length": 6695,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Harvard College AI Guidance & FAQs",
    "url": "https://careerservices.fas.harvard.edu/resources/harvard-college-ai-guidance-faqa/",
    "text": "Harvard College AI Guidance &amp; FAQs &#8211; Harvard FAS | Mignone Center for Career Success\nZoom drop-ins (1-2pm) will not be held on Mon, Dec 8, but will be available Tue through Fri.\n[Skip to content] [Skip to main nav] \n[![Harvard University]] \nSearch Keywords\n[![Office of Career Services | Harvard University –Faculty of Arts and Science] \n![Mignone Center for Career Success logo] \n] \n[**] \n[Crimson Careers] \n[Resources] \n[For Employers] \n* [Crimson Careers] \n* [Resources] \n* [For Employers] \n* [People We Serve] \n* [Audiences] \n* [Harvard College] \n* [Harvard Kenneth C. Griffin Graduate School of Arts &amp; Sciences] \n* [Harvard Extension School] \n* [Premed / Pre-Health] \n* [Alumni] \n* [Employers] \n* [Families &amp; Supporters] \n* [Faculty &amp; Staff] \n* [Prospective Students] \n* [Identities &amp; Affinities] \n* [BGLTQ+] \n* [First Generation / Low Income] \n* [International Students] \n* [Students of Color] \n* [Students with Disabilities] \n* [Undocumented Students] \n* [Varsity Athletes] \n* [Veterans] \n* [Women] \n* [Build Career Skills] \n* [Explore Interests &amp; Make Career Decisions] \n* [Create a Resume/CV or Cover Letter] \n* [Expand Your Network] \n* [Engage with Employers] \n* [Search for a Job / Internship] \n* [Summer Experiences (College)] \n* [January Experiences (College)] \n* [Find &amp; Apply for Summer Opportunities Funding] \n* [Prepare for an Interview] \n* [Negotiate an Offer] \n* [Apply to Graduate or Professional School] \n* [Access Resources] \n* [AI for Professional Development and Exploration] \n* [Discover Career Pathways] \n* [Arts &amp; Entertainment] \n* [Business &amp; Entrepreneurship] \n* [Climate, Sustainability, Environment, Energy] \n* [Government, Int’l Relations, Education, Law, Nonprofits] \n* [Life Sciences &amp; Health] \n* [Technology &amp; Engineering] \n* [Still Exploring] \n* [Events] \n* [MCS Programs &amp; Workshops] \n* [MCS Career Fairs &amp; Expos] \n* [Employer &amp; Graduate School Events] \n* [All Calendar Events] \n* [Talk to an Advisor] \n* # Harvard College AI Guidance &amp; FAQs\n* Share This:[Share Harvard College AI Guidance &amp; FAQs on Facebook] [Share Harvard College AI Guidance &amp; FAQs on LinkedIn] [Share Harvard College AI Guidance &amp; FAQs on X] \n* Copy Link\nThe Office of Undergraduate Education has compiled the following resources for instructors regarding appropriate use of generative AI in courses.\n[View Resource] \n## Related Resources\n## MCS Online Platforms\nShow more MCS Online Platforms\n* [MUSE] \n* [Job Market Insights] \n* [Case Questions Interactive\n; This content is restricted to certain users. Please login or sign up to see if you are eligible to view this content.\n] \n* [CaseQuestions.com Video Vault\n; This content is restricted to certain users. Please login or sign up to see if you are eligible to view this content.\n] \n* * [Beyond the Professoriate] \n* [Global Career Accelerator] \n* [Big Resume] Show more MCS Online Platforms\n## MCS Guides &amp; Publications\nShow more MCS Guides &amp; Publications\n* [House Prelaw Tutors and Websites\n; This content is restricted to certain users. Please login or sign up to see if you are eligible to view this content.\n] \n* [Pre-Med and Pre-Health Volunteer Opportunities in the Greater Boston Area] \n* [U.S. Medical School Course Requirements for 2025 Matriculants\n; This content is restricted to certain users. Please login or sign up to see if you are eligible to view this content.\n] \n* [U.S. Medical School Admissions Information for 2025 Matriculants\n; This content is restricted to certain users. Please login or sign up to see if you are eligible to view this content.\n] \n* * [Pursuing a PhD in the Arts and Sciences] Show more MCS Guides &amp; Publications\n## Resume/CV/Cover Letter Templates\n* [Big Resume] \n## Recommended Sites &amp; Tools\nShow more Recommended Sites &amp; Tools\n* [HES Self-Assessment Resources] \n* [House Prelaw Tutors and Websites\n; This content is restricted to certain users. Please login or sign up to see if you are eligible to view this content.\n] \n* [Pre-Med and Pre-Health Volunteer Opportunities in the Greater Boston Area] \n* [Leadership Development and Rotational Programs] \n* * [CARC Career Webinar Video Library] \n* [GSAS Career Exploration and Job Search Links] \n* [HES Career Services Overview] \n* [Out for Undergrad (O4U) Conferences] \n* [LGBT Connect] \n* [The Point Foundation Scholarship] \n* [IMDiversity] \n* [True Platform] \n* [NCAI Fellowships] \n* [American Association for the Advancement of Science (AAAS) Entry Point Program] \n* [Undocumented at Harvard] \n* [Career One Stop] \n* [Recruit Military] \n* [Federal Diversity Internship Initiative] \n* [Grace Hopper Celebration] \n* [Career Contessa] \n* [U.S. Medical School Course Requirements for 2025 Matriculants\n; This content is restricted to certain users. Please login or sign up to see if you are eligible to view this content.\n] \n* [U.S. Medical School Admissions Information for 2025 Matriculants\n; This content is restricted to certain users. Please login or sign up to see if you are eligible to view this content.\n] \n* [Technology Internships for First Years and Sophomores] \n* [Lime Connect] \n* [Forage] \n* [Parker Dewey] \n* [Payscale] \n* [Salary.com] \n* [Glassdoor] \n* [Harvard Access to the Wall Street Journal] \n* [Candor.Co Salary Negotiation Guide for Tech] \n* [LinkedIn] \n* [Choosing Between Offers Worksheet] \n* [Student Groups &amp; Additional Student Advertising] \n* [Firsthand Get Started Guide for Alumni] \n* [Graduate Bioscience Career Cognitive City] \n* [Career Navigator for Bioscientists] \n* [MUSE] \n* [GoGovernment.org career guides] \n* [Technical Interviews] \n* [Job Market Insights] \n* [LinkedIn Learning @Harvard] \n* [MCS Professional Attire Guide] \n* [Case Questions Interactive\n; This content is restricted to certain users. Please login or sign up to see if you are eligible to view this content.\n] \n* [CaseQuestions.com Video Vault\n; This content is restricted to certain users. Please login or sign up to see if you are eligible to view this content.\n] \n* [Case Questions Exchange Platform\n; This content is restricted to certain users. Please login or sign up to see if you are eligible to view this content.\n] \n* [FastMath\n; This content is restricted to certain users. Please login or sign up to see if you are eligible to view this content.\n] \n* [Top Corporate Inclusion Programs (by Gorick Ng)] \n* [USAjobs.gov] \n* [Idealist.org] \n* [Harvardwood Jobs Board] \n* [Handshake] \n* [Indeed] \n* [Built In] \n* [Guidestar] \n* [NACE&#8217;s Guide to Internships] \n* [Prometheus] \n* [Beyond the Professoriate] \n* [Career Coach GPT: The Complete Guide to ChatGPT Resume, Cover Letter, Interview, and Job Search Success] \n* [AI at Work: Linkedin Future of Work Report (pdf)] \n* [AI Policy Guidance on Copyright (US) (pdf)] \n* [Superhuman.ai AI newsletter] \n* [The Job Insiders AI Career Tools] \n* [Harvard University IT AI Resources Hub] \n* [All Tech is Human Responsible Tech Job Board] \n* [RA Capital Biological Sciences Career Map] \n* [Foundation Directory Online] \n* [Interstride&#8217;s Guide to Hiring International Students] \n* [Interstride: Sponsoring Your First H-1B] \n* [Global Career Accelerator] \n* [Jelper Club] \n* [A Harvard Griffin GSAS Student&#8217;s Guide to Academic Job Market Resources] \n* [Resources for PhD Careers from Scholarly Societies] \n* [NCAA Job Seekers] \n* [Harvard Varsity Alumni Database] \n* [National Association of Collegiate Directors of Athletics Job Center] \n* [Harvard Varsity Club Career Resources] \n* [Wellfound.com] \n* [BioSpace] \n* [Casing Machine] \n* [InterSECT Job Simulations] \n* [BioPharmGuy] \n* [Rora Technical Interview Guide for Research Scientists] \n* [The Engineering Resource] \n* [Central Application for Teaching Sections (CATS)] \n* [NASA L&#8217;SPACE Program] \n* [Nicole Kelner Climate Jobs Resources] \n* [Vault 2025 Best Internships for Diversity with Respect to Women] \n* [Where Women Work] \n* [Fairygodboss] \n* [AAUW Salary Negotiation] \n* [AmplifyME University Pathways (Financial Services Training Simulation)\n; This content is restricted to certain users. Please login or sign up to see if you are eligible to view this content.\n] \n* [The Disability Index] \n* [NSF AccessComputing Team] \n* [abilityJOBS] \n* [Summer Architecture Career Program Directory] \n* [American Planning Association Career Center] \n* [Core77 Design Directory] \n* [Green Jobs Network] \n* [Human Rights Campaign] \n* [Trans Road Map Transition at Work Resources] \n* [LGBTQ+ Career Network] \n* [Campus to Career: A Practical Guide to Employment-Based Visas for Noncitizen Students in Higher Education] \n* [Design Your Career: Build a Life that Aligns with Your Values] \n* [SparkPath Career Discovery Digital Challenge Cards] \n* [O\\*NET Resource Center] \n* [My Next Move] \n* [Big Resume] \n* [Beyond Graduate School] \n* [Milkround] \n* [Bright Network] \n* [Gradcracker] \n* [BroadFutures Internships] \n* [Pursuing a PhD in the Arts and Sciences] \n* [Bandana Jobs] \n* [50strong] Show more Recommended Sites &amp; Tools\n## MCS Recruiting\nShow more MCS Recruiting\n* [MCS Professional Attire Guide] \n* [Case Questions Interactive\n; This content is restricted to certain users. Please login or sign up to see if you are eligible to view this content.\n] \n* [CaseQuestions.com Video Vault\n; This content is restricted to certain users. Please login or sign up to see if you are eligible to view this content.\n] \n* [Case Questions Exchange Platform\n; This content is restricted to certain users. Please login or sign up to see if you are eligible to view this content.\n] \n* * [FastMath\n; This content is restricted to certain users. Please login or sign up to see if you are eligible to view this content.\n] Show more MCS Recruiting\n## Job Boards\nShow more Job Boards\n* [Leadership Development and Rotational Programs] \n* [GSAS Career Exploration and Job Search Links] \n* [BioPharmGuy] \n* [50strong] \n* * [The Engineering Resource] \n* [Central Application for Teaching Sections (CATS)] \n* [Nicole Kelner Climate Jobs Resources] \n* [Vault 2025 Best Internships for Diversity with Respect to Women] \n* [Where Women Work] \n* [Fairygodboss] \n* [American Planning Association Career Center] \n* [Green Jobs Network] \n* [Milkround] \n* [Bright Network] \n* [Gradcracker] \n* [BroadFutures Internships] \n* [Bandana Jobs] \n* [LGBT Connect] \n* [BioSpace] \n* [Wellfound.com] \n* [Harvard Varsity Club Career Resources] \n* [National Association of Collegiate Directors of Athletics Job Center] \n* [Jelper Club] \n* [All Tech is Human Responsible Tech Job Board] \n* [Guidestar] \n* [Built In] \n* [Indeed] \n* [Handshake] \n* [Harvardwood Jobs Board] \n* [Idealist.org] \n* [USAjobs.gov] \n* [Top Corporate Inclusion Programs (by Gorick Ng)] \n* [LinkedIn] Show more Job Boards\n## **Contact & Location\nPhone**617-495-2595\nEmail**[&#109;c&#115;&#064;&#102;&#097;&#115;.&#104;&#097;&#114;v&#097;&#114;d.e&#100;u] \nAddress**\nHarvard University\n54 Dunster Street\nCambridge, MA 02138\n## **MCS Hours\n|MMonday|9:00 am - 5:00 pm|\nTTuesday|9:00 am - 5:00 pm|\nWWednesday|9:00 am - 5:00 pm|\nTHThursday|9:00 am - 5:00 pm|\nFFriday|9:00 am - 5:00 pm|\n**[Blogs] |[Employers] |[Events] |[Jobs] |[Resources] |[Videos] |[Meet the Team] |[Maps & Directions] **\nThe Mignone Center for Career Success (MCS) is committed to ensuring access to a broad range of information and opportunities across all sectors. Our website contains external content that may be useful to our learners. The inclusion of external content does not necessarily constitute endorsement, recommendation, or agreement with the information.\n[**Give A Gift**] \nHarvard Faculty of Arts &amp; Sciences\nHarvard FAS Mignone Center for Career Success\n[Instagram] [YouTube] \nHarvard University\n54 Dunster Street\nCambridge, MA 02138\n617-495-2595\n[&#109;&#099;s&#064;fas.h&#097;r&#118;ar&#100;&#046;ed&#117;] \n[Maps & Directions\n[uConnect Privacy Policy] |[Terms of Service] \n&copy; 2025 President and Fellows of Harvard College\n[![Powered by uConnect]]",
    "length": 11958,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Policy on Access to Electronic Information",
    "url": "https://provost.harvard.edu/links/policy-access-electronic-information",
    "text": "[Skip to main content] \n\n- [Main Menu] \n- [Utility Menu] \n- [Search] \n\n[![University Logo]] \n\n[HARVARD.EDU] \n\nCopyright © 2024 The President and Fellows of Harvard College \\| [Accessibility] \\| [Digital Accessibility] \\| [Report Copyright Infringement]",
    "length": 252,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Generative AI @ FAS",
    "url": "https://www.fas.harvard.edu/gai",
    "text": "Generative AI @ FAS | Faculty of Arts &amp; Sciences[\nSkip to main contentarrow\\_circle\\_down\n] \n[![Harvard University]] \n[\n![Faculty of Arts and Sciences logo] \n![Faculty of Arts and Sciences logo] \n] \nmenucloseMenu\nSearch\nSearchsearch\n[\n![Faculty of Arts and Sciences logo] \n![Faculty of Arts and Sciences logo] \n] \n# Generative AI @ FAS\n## Your Gateway to Generative AI at FAS\nFAS Initiatives\n![Person at a laptop with floating visuals from the screen] \nAt Harvard we are exploring how GAI tools can open up new ways of teaching and learning, and how natural-language interactions with computational tools can provide improved access to quantitative methods for fields that have not traditionally been computer-intensive. As a rapidly moving disruptive technology, GAI poses significant opportunities and challenges that span the economic, regulatory, ethical, environmental, and societal domains and the Faculty of Arts and Sciences (FAS) is committed to advancing those conversations across the full breadth of our community.\n![Man wearing a white shirt and yellow and black striped tie sits on armchair in front of bookshelves.] \nIncorporating generative AI into the classroom means reconceptualizing the essentials of education that form the crux of the University’s mission.\nChris Stubbs\nSamuel C. Moncher Professor of Physics, Senior Advisor on Artificial Intelligence.\n## AI Resources, Policies, Tools, and Opportunities for Responsible Use\nFeatured Event\n### SEAS Dean&#039;s Dialogue: Reimagining AI Alignment\n**February 5, 2026**\n**3:30–5:30 p.m.**\nScience and Engineering Complex, LL2.221\nThis SEAS Dean's Dialoguewill surface the technical, ethical, and societal stakes of AI alignment, and help the Harvard community better understand the complexity of designing AI that is both safe and inclusive.\n[Event detailsarrow\\_circle\\_right] \n![A futuristic circuit board with the letters &quot;AI&quot; glowing in bright blue, surrounded by intricate electronic components and warm orange accent lights] \n### Generative AI Office Hours\n**For Faculty**\n10 a.m. –1 p.m. Daily | Pierce Hall, Room 110\nTeams from both the Bok Center and HUIT ATG are available for one-on-one consultations. Stop by with questions about assignment design, pedagogical strategies, troubleshooting tool access, or general AI integration advice.\n**For Staff**\nThursdays, 1 p.m. –2 p.m. | Virtual\nThese are informal drop-in sessions where staff can bring specific questions or examples from their work and see how Harvard-supported generative AI tools might help. Staff are welcome to join with a question, a particular task or document, or simply to observe a few live, FAS-based examples.\n![Glowing digital light bulb on books with data graphics in the background.] \n### Resources\nGuidance on which tools are available, key features, and how to request access.\n[Generative AI Tool Comparisonarrow\\_circle\\_right] [Harvard Training Portal –AI Learning Opportunitiesarrow\\_circle\\_right] [For Faculty: Teaching in the Age of AI –Bok Center for Teaching and Learningarrow\\_circle\\_right] \n![Hands typing on a laptop with AI and data icons floating above the keyboard.] \n### Policies\nHarvard University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. These guidelines are updated periodically.\nOpen all sectionsClose all sections\n### University Guidelinesexpand\\_more\nHarvard University Guidelines on t[he use and procurement of generative AI tools] \n### Facultyexpand\\_more\nGuidance for[instructional use and responsible adoption in courses] \n### Staffexpand\\_more\nGuidance for[administrative units using AI] responsibly and securely\n### Undergraduate Studentsexpand\\_more\n[Coursework and academic integrity guidance] for generative AI tools should follow course syllabi\n### Graduate Studentsexpand\\_more\nProgram and department level expectations for[coursework, research, and TA work] \n## AI in Action\n[### Through AI, New Dimensions in Physics\n] \nNovember 25, 2025\n![An event display from a particle collider experiment. Yellow lines indicate the paths of charged particles radiating from a central collision point, while surrounding green and blue blocks represent the energy signatures of particle jets.] \n[### Tapping Into Whale Talk\n] \nDecember 11, 2025\n![Sperm whales] \n[### Can AI Strengthen Democracy and Improve Collective Decision-Making?\n] \nNovember 17, 2025\n![Ariel Procaccia] \n## News\n[### Want to Speed Brain Research? It’s All in How You Look at It\n] \n![Ishaan Chandok (from left), Jeff Lichtman, Yaron Meirovitch, and Aravi Samuel] \n[### Technically, It’s Possible. Ethically, It’s Complicated.\n] \n![Matthew Kopec] \n[### Is AI Dulling Our Minds?\n] \n![Tina Grotzer, Dan Levy, Christopher Dede, Fawwaz Habbal, Karen Thornber, and Jeff Behrends] \n[Read more about GAI at the FASarrow\\_circle\\_right] \n## Contact and Support\nlaptop\n[### Harvard University Information Technology.\n] \nAI resources, trouble-shooting tips, and how-to guides\n[ithelp@harvard.edu] \naccount\\_balance\n[### Bok Center for Teaching and Learning\n] \nInnovative approaches to teaching and learning with AI\n[bokcenter@fas.harvard.edu] \nsupport\n[### Academic Technology for FAS\n] \nGuidance on the thoughtful use of AI toolsin teachingand learning\n[atg@fas.harvard.edu]",
    "length": 5423,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "AI and Public POlicy",
    "url": "https://generative-ai-course.hks.harvard.edu/",
    "text": "The Science and Implications of Generative AI\nSearch this site\nEmbedded Files\nSkip to main content\nSkip to navigation\n[![] The Science and Implications of Generative AI] \n# AI and Public POlicy\n## Harvard Kennedy School\nSharad Goel, Dan Levy, and Teddy Svoronos\nThis site contains links toseveral course offerings about the intersection between generative AI and public policy taught by the three of us at the Harvard Kennedy School. These courses aim to equip any individual with the knowledge of thescienceof generative AI,how to useit effectively, and how it mightshape our future.\nSign up for our courses:\n* Our[asynchronous, six-week course] offered through HKS Executive Education\n* Our[weeklong in-person course] offered through HKS Executive Education\nFor HKS students:\n* Our[in-person course] for HKS students\nFor educators and learners:\n* A free, archived version of our[2024 in-person course].\nGoogle Sites\nReport abuse\nPage details\nPage updated\nGoogle Sites\nReport abuse\nThis site uses cookies from Google to deliver its services and to analyze traffic. Information about your use of this site is shared with Google. By clicking &quot;accept&quot;, you agree to its use of cookies.[Cookie Policy] \nReject\nAccept",
    "length": 1223,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "EVP Artificial Intelligence Innovation Program (AIIP)",
    "url": "https://www.huit.harvard.edu/aiip",
    "text": "EVP Artificial Intelligence Innovation Program (AIIP) | Harvard University Information Technology[\nSkip to main contentarrow\\_circle\\_down\n] \nSearch\nSearchsearch\n[\n![Harvard University Information Technology] \n![Harvard University Information Technology] \n] \nmenucloseMenu\nSearch\nSearchsearch\n[\n![Harvard University Information Technology] \n![Harvard University Information Technology] \n] \n# EVP Artificial Intelligence Innovation Program (AIIP)\n# EVP Artificial Intelligence Innovation Program (AIIP)\nPrograms &amp; initiatives\n![2019 Harvard University IT Summit] \n## Contents\n* [Overview] \n* [Program details] \n* [Key dates and timing] \n* [Office hours] \n* [First round pilots] \n## Overview\nThe Executive Vice President (EVP) Artificial Intelligence Innovation Program (AIIP) is intended to encourage the exploration of generative AI to**improve****operational efficiency**and**administrative systems and processes**.****The AIIP****is an inclusive approach to innovation—soliciting proposals directly from EVP staff—and is designed to foster creativity and collaboration among our respective Units’ teams.\n## Program Details\nStaff are invited tosubmitproposals about an identified AI-related innovation opportunity. Selected proposals will receive:\n* Access end-user AI tools such asAI SandboxandChatGPTEdu,development tools including OpenAI Codex andClaude Code, and AI providers including AWS, OpenAI, and Google.\n* Connect with HUIT and CADM experts and practitioners to shape your proposal and accelerate development of your pilot solution.\n* Up to $15,000 in funding for a6-monthexperiment or pilot.\n* An opportunity toshowcasekey learnings and AI innovations to administrative leadership.\nMore information on the EVP-AIIP Round 1 pilots can be found[here].\n## Key dates and timing\n* **January 26:**Proposal submission opens to EVP areas.\n* **March 6:**Proposal submissions close for review by submission committee; submissions will be prioritized based on alignment to themesidentifiedby the University Generative AI Administration and Operations Committee.\n* **April:**Proposal groups pitch their idea to select members of the Office of theEVPand awards announced for selected proposals.\n* **May - October:**Experimentation withassistancefrom HUIT.\n* **November:**Teams share project outcomes andlearningwith sponsors. For any**questions**or**assistance**in refining proposal ideas, please contact[evp\\_aiip@harvard.edu].\n## Office hours\n[### EVP Artificial Intelligence Innovation Program (AIIP) office hours\n] \nJan. 28, 2026\n11:30AM EST\nVirtual\n[### EVP Artificial Intelligence Innovation Program (AIIP) office hours\n] \nFeb. 2, 2026\n11:30AM EST\nVirtual\n[### EVP Artificial Intelligence Innovation Program (AIIP) office hours\n] \nFeb. 4, 2026\n11:30AM EST\nVirtual\n[### EVP Artificial Intelligence Innovation Program (AIIP) office hours\n] \nFeb. 9, 2026\n11:30AM EST\nVirtual\n[### EVP Artificial Intelligence Innovation Program (AIIP) office hours\n] \nFeb. 11, 2026\n11:30AM EST\nVirtual\n[### EVP Artificial Intelligence Innovation Program (AIIP) office hours\n] \nFeb. 16, 2026\n11:30AM EST\nVirtual\n## First round pilots\n### AI for Administrative Learning Resources\n* Leverage AI to create draft training content for sponsored research topics at Harvard, significantly reducing SMEs' development time.\n* Increase efficiency, accelerating content delivery while reducing non-compliance risks and audit exposure.\n### AI-TIES (Targeted Information Extraction and Summarization)\n* Use generative AI to automate information extraction and create task-specific summaries, improving efficiency and demonstrating AI's ability to filter pertinent content.\n* Aim for precise information extraction and summarization, lessening manual distillation effort.\n### HUHS FirstAIde\n* Develop an AI chatbot for Harvard University Health Services (HUHS) customer support staff, offering digital health plan services and integrating AI for scaled customer service and improved response times.\n* Improve customer service by providing faster, precise responses and increasing operational efficiencies, serving as a learning tool for Member Services staff.\n### Improving Reunion Customer Service with AI\n* Employ AI tools for email review and triage in Alumni Affairs &amp; Development areas across campus, streamlining large volume email processing.\n* Save staff time, enhance efficiency in handling requests and feedback, and provide effective support in donor-related tasks.\n### Productivity, Risk-Mitigation, and Efficiency through AI-Assisted Contract Drafting\n* Utilize AI for contract drafting and analysis at the Office of Contract Management (OCM), enabling faster and more reliable self-generated drafts and risk assessments.\n* Enhance OCM’s efficiency and effectiveness in contract review and negotiation, reducing risks and achieving savings.\n### ServiceRightNow\n* Use AI, particularly ChatGPT, to automate routine service ticket tasks, focusing human expertise on complex issues.\n* Expect a significant reduction in workload, assessed through fewer tickets, quicker response times, and improved customer satisfaction.\n### Use of AI in Streamlining Data Use Agreements​\n* Apply AI to streamline processing of Data Use Agreements (DUAs), focusing on identifying standard language exceptions to expedite the review process.\n* Realize a substantial decrease in initial review time, easing the burden on reviewers and negotiators handling complex negotiations.\n### Using AI to Enhance the HR Policy Creation and Review Processes​\n* Implement AI to analyze data and improve HR policy creation and review at Harvard, focusing on searchability, accuracy, consistency, relevance, accessibility, or inclusivity.\n* Result in a more accessible, organized policy library, facilitating strategic focus for workforce policy leadership.",
    "length": 5810,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "AI Sandbox",
    "url": "https://www.huit.harvard.edu/ai-sandbox",
    "text": "AI Sandbox | Harvard University Information Technology[\nSkip to main contentarrow\\_circle\\_down\n] \nSearch\nSearchsearch\n[\n![Harvard University Information Technology] \n![Harvard University Information Technology] \n] \nmenucloseMenu\nSearch\nSearchsearch\n[\n![Harvard University Information Technology] \n![Harvard University Information Technology] \n] \n# AI Sandbox\n# AI Sandbox\nTools &amp; services\n[Get startedarrow\\_circle\\_right] [Sign inarrow\\_circle\\_right] \n![People on their laptops at the Smith Campus Center.] \n## Overview\nThe AI Sandbox provides a secure environment in which to explore Generative AI, mitigating many security and privacy risks and ensuring the data entered will not be used to train any vendor large language models (LLMs). It offers a single interface that enables access to the latest LLMs from OpenAI, Anthropic, Google, and Meta. Features include image generation, data visualization, and the ability to upload multiple files.\nCompliance with[University guidelines for use of generative AI] required for use.\n### Eligibility\n**Faculty, staff,**and**researchers**in Central Administration, FAS, College, GSAS, SEAS, GSD, GSE, HBS, HDS, HKS, HLS, HMS (Quad), HSDM (Quad), Radcliffe, SPH;**Students**in College, GSAS, SEAS, GSE, HDS, HKS, HLS, HMS (Quad), HSDM (Quad), SPH\n### Data classification level\nApproved for up to[Level 3 confidential data] \n*Data you enter will not be used to train large language models*\n### Quick links\n* [Browse help articles] \n* [Read release notes]",
    "length": 1503,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Generative AI Research Program",
    "url": "https://uraf.harvard.edu/uraf-opportunities/generative-ai-research-program",
    "text": "[Skip to main content] \n\n- [Main Menu] \n- [Utility Menu] \n- [Search] \n\n[HARVARD.EDU] \n\nCopyright © 2025 The President and Fellows of Harvard College \\| [Accessibility] \\| [Digital Accessibility] \\| [Report Copyright Infringement]",
    "length": 229,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Academic & Research Integrity – Harvard Faculty of Arts and Sciences",
    "url": "https://www.fas.harvard.edu/initiatives/generative-artificial-intelligence/academic-research-integrity/",
    "text": "[Skip to content] \n\n[![Harvard University homepage]] \n\n[HARVARD.EDU] \n\n## Our Approach\n\n![] \n\nWhat does it mean to use GAI in an ethical way and how do we incentivize that behavior? These are the questions we need to explore as we begin to incorporate GAI into our teaching, research and administrative processes. Already we know that AI detectors are unreliable and that our efforts need to focus on a culture of integrity rather than one of enforcement. Just as the way in which we use GAI will vary across disciplines, how we define the ethical use of GAI may look very different in different fields.\n\nOur conversations on this topic are guided by the following principles:\n\n- Promote the ethical use of AI in learning and knowledge generation.\n- Increase awareness of how the tools work and their ethical implications.\n- Promote ethical standards for the use of AI in research.\n\n## Informing Our Thinking\n\n- Inside Higher Education’s [ongoing coverage of Artificial Intelligence] \n- The Chronicle of Higher Education’s [coverage of Technology] \n\n## Contact Area Leads\n\n## **Melissa Dell**\n\n**Andrew E. Furer Professor of Economics**\n\n[melissadell@fas.harvard.edu] \n\n## **William Petrick**\n\n**Associate Dean, Office of Academic Integrity and Student Conduct**\n\n[william\\_petrick@fas.harvard.edu]",
    "length": 1298,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Pedagogy – Harvard Faculty of Arts and Sciences",
    "url": "https://www.fas.harvard.edu/initiatives/generative-artificial-intelligence/pedagogy/",
    "text": "[Skip to content] \n\n[![Harvard University homepage]] \n\n[HARVARD.EDU] \n\n## Our Approach\n\n![] \n\nHow can we both contend with and take advantage of generative artificial intelligence in our teaching? GAI systems can produce responses to homework problem sets, to essay assignments, and to take-home exam questions. It has capabilities ranging from producing text, code, music and images to actually solving problems. The question then, is whether GAI is a powerful tool for us to run with or an existential threat to higher education? The answer is both.\n\nWhile it’s true that our students could previously draw upon various resources to assist in the completion of assignments, the ease-of-use, free access, and high performance of GAI systems have raised this to a new level. As teachers, we need to assess and anticipate how best to adapt to, accommodate, incorporate, exploit, innovate with, regulate and evaluate GAI tools in our courses, in our mentoring, and in our assessment of students. With that in mind, there are three principles that are guiding our work this year:\n\n1. Maintaining the integrity of our courses\n2. Ensuring that students and instructors know how to use these tools effectively in the classroom\n3. Preparing our students to use them effectively in the world\n\nAs we seek to abide by these principles, our challenge is to incentivize the level of engagement that leads to a deeper understanding and the development of the habits of mind we hope to instill in our students.\n\n## What’s Next\n\nIn line with the second principle listed above, the Bok Center is offering all FAS faculty and instructional support staff the opportunity to request customized workshops for their departments or programs designed to familiarize instructors with common AI use cases in the discipline and to explore effective practices for teaching and learning with AI. A request form is available [here].\n\n## Informing Our Thinking\n\n- [“One Useful Thing”] A Substack newsletter by Professor Ethan Mollick, Wharton School of the University of Pennsylvania.\n- [AI Pedagogy Project] by by the metaLAB (at) Harvard within the Berkman Klein Center for Internet & Society\n\n## Contact Area Leads\n\n## **Amanda Claybaugh**\n\n**Samuel Zemurray, Jr. and Doris Zemurray Stone Radcliffe Professor of English**\n\n[amanda\\_claybaugh@harvard.edu] \n\n## **Christopher Stubbs**\n\n**Dean of Science**\n\n[stubbs@g.harvard.edu]",
    "length": 2400,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Harvard Open Access Policies",
    "url": "https://osc.hul.harvard.edu/liblab/sites/default/files/305_tagging_archives_library_lab_final_report.pdf",
    "text": "[Skip to main content] \n\n# Harvard Open Access Policies\n\nMaking Harvard research available online without barriers.\n\n## Open Access at Harvard\n\nHarvard's Faculty of Arts and Sciences faculty voted unanimously in 2008 to give Harvard permission to make their scholarly articles available for any non-commercial purpose, in the interest of disseminating the fruits of their research and scholarship as widely as possible. In the years since, the remaining eight Harvard schools and several research centers have voted to establish similar open access policies.\n\nScholarly articles provided to the university are stored, preserved, and made freely accessible from Harvard's institutional open access repository [DASH].\n\n### Harvard Authors\n\nIf you are a Harvard author and have questions about whether and when a school-level policy, center-level policy or the Individual Open Access License applies to your work, please contact Open Scholarship and Research Data Services.\n\n### Beyond Harvard\n\nIf you are considering adopting an open access policy at your institution, we encourage you to consult the annotated [Model Open Access Policy] and the [Good Practices for University Open Access Policies], representing the accumulated experience of multiple institutions that have drafted and implemented open access policies.\n\n### Contact Us\n\nFor questions about open access, the open access policies at Harvard, or making your work available from DASH, reach out to [Open Scholarship and Research Data Services] at Harvard Library.\n\n## Policies by School\n\n**Faculty of Arts and Sciences**\n\nThe Faculty of Arts and Sciences of Harvard University is committed to disseminating the fruits of its research and scholarship as widely as possible. In keeping with that commitment, the Faculty adopts the following policy: Each Faculty member grants to the President and Fellows of Harvard College permission to make available his or her scholarly articles and to exercise the copyright in those articles. In legal terms, the permission granted by each Faculty member is a nonexclusive, irrevocable, paid-up, worldwide license to exercise any and all rights under copyright relating to each of his or her scholarly articles, in any medium, and to authorize others to do the same, provided that the articles are not sold for a profit. The policy will apply to all scholarly articles written while the person is a member of the Faculty except for any articles completed before the adoption of this policy and any articles for which the Faculty member entered into an incompatible licensing or assignment agreement before the adoption of this policy. The Dean or the Dean's designate will waive application of the policy for a particular article upon written request by a Faculty member explaining the need.\n\nTo assist the University in distributing the articles, each Faculty member will provide an electronic copy of the final version of the article at no charge to the appropriate representative of the Provost's Office in an appropriate format (such as PDF) specified by the Provost's Office.\n\nThe Provost's Office may make the article available to the public in an open-access repository. The Office of the Dean will be responsible for interpreting this policy, resolving disputes concerning its interpretation and application, and recommending changes to the Faculty from time to time. The policy will be reviewed after three years and a report presented to the Faculty.\n\n**Adopted February 12, 2008**\n\n**This policy is not included under the Creative Commons Attribution 4.0 International License (CC BY) to which other textual content on this web page may be subject.**\n\n**Graduate School of Design**\n\nThe Faculty of the Harvard Graduate School of Design is committed to disseminating the fruits of its research and scholarship as widely as possible. In keeping with that commitment, the Faculty adopts the following policy: Each Faculty member grants to the President and Fellows of Harvard College permission to make available his or her scholarly articles and to exercise the copyright in those articles. More specifically, each Faculty member grants to the President and Fellows a nonexclusive, irrevocable, worldwide license to exercise any and all rights under copyright relating to each of his or her scholarly articles, in any medium, provided that the articles are not sold for a profit, and to authorize others to do the same. The policy applies to all scholarly articles authored or co-authored while the person is a member of the Faculty except for any articles completed before the adoption of this policy and any articles for which the Faculty member entered into an incompatible licensing or assignment agreement before the adoption of this policy. The Dean or Dean's designate will waive application of the license for a particular article or delay access for a specified period of time upon express direction by a Faculty member.\n\nEach Faculty member will provide an electronic copy of the author's final version of each article no later than the date of its publication at no charge to the appropriate representative of the Provost's Office in an appropriate format (such as PDF) specified by the Provost's Office. The Provost's Office may make the article available to the public in an open-access repository. The Office of the Dean will be responsible for interpreting this policy, resolving disputes concerning its interpretation and application, and recommending changes to the Faculty from time to time. The policy will be reviewed after three years and a report presented to the Faculty.\n\n**Adopted March 30, 2011**\n\n**This policy is not included under the Creative Commons Attribution 4.0 International License (CC BY) to which other textual content on this web page may be subject.**\n\n**Graduate School of Education**\n\nThe Faculty of the Harvard University Graduate School of Education is committed to disseminating the fruits of its research and scholarship as widely as possible. In keeping with that commitment, the Faculty adopts the following policy: Each Faculty member grants to Harvard University permission to make available his or her scholarly articles and to exercise the copyright in those articles. More specifically, each Faculty member grants to Harvard University a nonexclusive, irrevocable, worldwide license to exercise any and all rights under copyright relating to each of his or her scholarly articles, in any medium, provided that the articles are not sold for a profit, and to authorize others to do the same. The policy will apply to all scholarly articles authored or co-authored while the person is a member of the Faculty except for any articles completed before the adoption of this policy and any articles for which the Faculty member entered into an incompatible licensing or assignment agreement before the adoption of this policy. The Dean or Dean's designate will waive application of the license for a particular article upon express direction by a Faculty member.\n\nEach Faculty member will provide an electronic copy of the author's final version of each article no later than the date of its publication at no charge to the appropriate representative of the Provost's Office in an appropriate format (such as PDF) specified by the Provost's Office. The Provost's Office may make the article available to the public in an open-access repository. The Office of the Dean will be responsible for interpreting this policy, resolving disputes concerning its interpretation and application, and recommending changes to the Faculty from time to time. The policy will be reviewed after three years and a report presented to the Faculty.\n\n**Adopted June 1, 2009**\n\n**This policy is not included under the Creative Commons Attribution 4.0 International License (CC BY) to which other textual content on this web page may be subject.**\n\n**Harvard Business School**\n\nThe Faculty of the Harvard Business School is committed to disseminating the fruits of its research and scholarship as widely as possible. In keeping with that commitment, the Faculty adopts the following policy: Each Faculty member grants to the President and Fellows of Harvard College permission to make available articles that he or she has prepared for journal peer review and to exercise the copyright in those articles. More specifically, each Faculty member grants to the President and Fellows a nonexclusive, irrevocable, worldwide license to exercise any and all rights under copyright relating to each of these articles, in any medium, and to authorize others to do the same, provided that the articles are not sold for a profit. The policy will apply to all such articles authored or co-authored while the person is a member of the Faculty except for any articles completed before the adoption of this policy and any articles for which the Faculty member entered into an incompatible licensing or assignment agreement before the adoption of this policy.\n\nSince the policy will apply only to articles prepared for peer review, it thus does not apply to Harvard Business School Cases and Notes, or to articles written for the Harvard Business Review or other publications that are not peer-reviewed. The Dean or the Dean's designate will waive application of the license for a particular article upon express direction by a Faculty member.\n\nEach Faculty member will provide an electronic copy of the author's final version of each article to the Division of Research and Faculty Development (DRFD) no later than the date of its publication. DRFD will submit the article to the Harvard University open access repository; the Provost's Office may make it available to the public.\n\nThe Office of the Dean will be responsible for interpreting this policy, resolving disputes concerning its interpretation and application, and recommending changes to the Faculty from time to time. Effects of the policy will be continuously monitored, and after three years it will be reviewed and a report presented to the Faculty.\n\n**Adopted February 12, 2010**\n\n**This policy is not included under the Creative Commons Attribution 4.0 International License (CC BY) to which other textual content on this web page may be subject.**\n\n**Harvard Divinity School**\n\nThe Faculty of the Harvard Divinity School is committed to disseminating the fruits of its research and scholarship as widely as possible. In keeping with that commitment, the Faculty adopts the following policy: Each Faculty member grants to the President and Fellows of Harvard College permission to make available his or her scholarly articles and to exercise the copyright in those articles. More specifically, each Faculty member grants to the President and Fellows a nonexclusive, irrevocable, worldwide license to exercise any and all rights under copyright relating to each of his or her scholarly articles, in any medium, provided that the articles are not sold for a profit, and to authorize others to do the same. The policy applies to all scholarly articles authored or co-authored while the person is a member of the Faculty except for any articles completed before the adoption of this policy and any articles for which the Faculty member entered into an incompatible licensing or assignment agreement before the adoption of this policy. The Dean or Dean's designate will waive application of the license for a particular article or delay access for a specified period of time upon express direction by a Faculty member.\n\nEach Faculty member will provide an electronic copy of the author's final version of each article no later than the date of its publication at no charge to the appropriate representative of the Provost's Office in an appropriate format (such as PDF) specified by the Provost's Office. The Provost's Office may make the article available to the public in an open-access repository.\n\nThe Office of the Dean will be responsible for interpreting this policy, resolving disputes concerning its interpretation and application, and recommending changes to the Faculty from time to time. The policy will be reviewed after three years and a report presented to the Faculty.\n\n**Adopted November 15, 2010**\n\n**This policy is not included under the Creative Commons Attribution 4.0 International License (CC BY) to which other textual content on this web page may be subject.**\n\n**Harvard Kennedy School**\n\nThe Faculty of the Harvard Kennedy School of Government is committed to disseminating the fruits of its research and scholarship as widely as possible. In keeping with that commitment, the Faculty adopts the following policy: Each Faculty member grants to the President and Fellows of Harvard College permission to make available his or her scholarly articles and to exercise the copyright in those articles. More specifically, each Faculty member grants to the President and Fellows a nonexclusive, irrevocable, worldwide license to exercise any and all rights under copyright relating to each of his or her scholarly articles, in any medium, and to authorize others to do the same, provided that the articles are not sold for a profit. The policy will apply to all scholarly articles authored or co-authored while the person is a member of the Faculty except for any articles completed before the adoption of this policy and any articles for which the Faculty member entered into an incompatible licensing or assignment agreement before the adoption of this policy. The Dean or the Dean's designate will waive application of the license for a particular article upon express direction by a Faculty member.\n\nEach Faculty member will provide an electronic copy of the author's final version of each article at no charge to the appropriate representative of the Provost's Office in an appropriate format (such as PDF) specified by the Provost's Office no later than the date of its publication. The Provost's Office may make the article available to the public in an open-access repository.\n\nThe Office of the Dean will be responsible for interpreting this policy, resolving disputes concerning its interpretation and application, and recommending changes to the Faculty from time to time. The policy will be reviewed after three years and a report presented to the Faculty.\n\n**Adopted March 10, 2009**\n\n**This policy is not included under the Creative Commons Attribution 4.0 International License (CC BY) to which other textual content on this web page may be subject.**\n\n**Harvard Law School**\n\nThe Faculty of the Harvard Law School is committed to disseminating the fruits of its research and scholarship as widely as possible. In keeping with that commitment, the Faculty adopts the following policy: Each Faculty member grants to the President and Fellows of Harvard College permission to make available his or her scholarly articles and to exercise the copyright in those articles. More specifically, each Faculty member grants to the President and Fellows a nonexclusive, irrevocable, worldwide license to exercise any and all rights under copyright relating to each of his or her scholarly articles, in any medium, and to authorize others to do the same, provided that the articles are not sold for a profit. The policy will apply to all scholarly articles authored or co-authored while the person is a member of the Faculty except for any articles completed before the adoption of this policy and any articles for which the Faculty member entered into an incompatible licensing or assignment agreement before the adoption of this policy. The Dean or the Dean's designate will waive application of the policy to a particular article upon written request by a Faculty member explaining the need.\n\nEach Faculty member will provide an electronic copy of the final version of the article at no charge to the appropriate representative of the Provost's Office in an appropriate format (such as PDF) specified by the Provost's Office no later than the date of its publication. The Provost's Office may make the article available to the public in an open-access repository.\n\nThe Office of the Dean will be responsible for interpreting this policy, resolving disputes concerning its interpretation and application, and recommending changes to the Faculty from time to time. The policy will be reviewed after three years and a report presented to the Faculty.\n\n**Adopted May 1, 2008**\n\n**This policy is not included under the Creative Commons Attribution 4.0 International License (CC BY) to which other textual content on this web page may be subject.**\n\n**Harvard Medical School**\n\nThe Faculty of the Harvard Medical School is committed to disseminating the fruits of its research and scholarship as widely as possible. In keeping with that commitment, the Faculty adopts the following policy: Each Faculty member grants to the President and Fellows of Harvard College permission to make available his or her scholarly articles and to exercise the copyright in those articles. More specifically, each Faculty member grants to the President and Fellows a nonexclusive, irrevocable, worldwide license to exercise any and all rights under copyright relating to each of his or her scholarly articles, in any medium, provided that the articles are not sold for a profit, and to authorize others to do the same. The policy applies to all scholarly articles authored or co-authored while the person is a member of the Faculty except for any articles completed before the adoption of this policy and any articles for which the Faculty member entered into an incompatible licensing or assignment agreement before the adoption of this policy. The Dean or Dean's designate will waive application of the license for a particular article or delay access for a specified period of time upon express direction by a Faculty member.\n\nEach Faculty member will provide an electronic copy of the author's final version of each article no later than the date of its publication at no charge to the appropriate representative of the Provost's Office in an appropriate format (such as PDF) specified by the Provost's Office. The Provost's Office may make the article available to the public in an open-access repository.\n\nThe Office of the Dean will be responsible for interpreting this policy, resolving disputes concerning its interpretation and application, and recommending changes to the Faculty from time to time. The policy will be reviewed after three years and a report presented to the Faculty.\n\n**Adopted June 18, 2014**\n\n**This policy is not included under the Creative Commons Attribution 4.0 International License (CC BY) to which other textual content on this web page may be subject.**\n\n**Harvard T.H. Chan School of Public Health**\n\nThe Faculty of the Harvard School of Public Health is committed to disseminating the fruits of its research and scholarship as widely as possible. In keeping with that commitment, the Faculty adopts the following policy: Each Faculty member grants to the President and Fellows of Harvard College permission to make available his or her scholarly articles and to exercise the copyright in those articles. More specifically, each Faculty member grants to the President and Fellows a nonexclusive, irrevocable, worldwide license to exercise any and all rights under copyright relating to each of his or her scholarly articles, in any medium, provided that the articles are not sold for a profit, and to authorize others to do the same. The policy applies to all scholarly articles authored or co-authored while the person is a member of the Faculty except for any articles completed before the adoption of this policy and any articles for which the Faculty member entered into an incompatible licensing or assignment agreement before the adoption of this policy. The Dean or Dean's designate will waive application of the license for a particular article or delay access for a specified period of time upon express direction by a Faculty member.\n\nEach Faculty member will provide an electronic copy of the author's final version of each article no later than the date of its publication at no charge to the appropriate representative of the Provost's Office in an appropriate format (such as PDF) specified by the Provost's Office. The Provost's Office may make the article available to the public in an open-access repository.\n\nThe Office of the Dean will be responsible for interpreting this policy, resolving disputes concerning its interpretation and application, and recommending changes to the Faculty from time to time. The policy will be reviewed after three years and a report presented to the Faculty.\n\n**Adopted November 26, 2012**\n\n**This policy is not included under the Creative Commons Attribution 4.0 International License (CC BY) to which other textual content on this web page may be subject.**\n\n## Policies by Center\n\n**Ash Center**\n\nThe Ash Center for Democratic Governance and Innovation is committed to disseminating the fruits of its research and scholarship as widely as possible. In keeping with that commitment, the Ash Center adopts the following policy: Each faculty, fellow, and staff member grants to Harvard University permission to make available his or her scholarly articles and to exercise the copyright in those articles. More specifically, each faculty, fellow, and staff member grants to Harvard a nonexclusive, irrevocable, worldwide license to exercise any and all rights under copyright relating to each of his or her scholarly articles, in any medium, provided that the articles are not sold for a profit, and to authorize others to do the same. The policy applies to all scholarly articles authored or co-authored while the person is a member of the Ash Center except for any articles completed before the adoption of this policy and any articles for which the faculty, fellow, and staff member entered into an incompatible licensing or assignment agreement before the adoption of this policy. The Provost or Provost’s designate will waive application of the license for a particular article or delay access for a specified period of time upon express direction by a faculty, fellow, and staff member.\n\nEach faculty, fellow, and staff member will provide an electronic copy of the author’s final version of each article no later than the date of its publication at no charge to the appropriate representative of the Provost’s Office in an appropriate format (such as PDF) specified by the Provost’s Office.\n\nThe Provost’s Office may make the article available to the public in an open-access repository, including the open Digital Access to Scholarship at Harvard (DASH) repository. Furthermore, the Ash Center may also make such articles available to the public without charge. The Office of the Provost in consultation with the Director of the Ash Center will be responsible for interpreting this policy, resolving disputes concerning its interpretation and application, and recommending changes to the faculty, fellow, and staff member from time to time.\n\n**Adopted June 28, 2018**\n\n**This policy is not included under the Creative Commons Attribution 4.0 International License (CC BY) to which other textual content on this web page may be subject.**\n\n**Berkman Klein Center for Internet & Society**\n\nThe Berkman Center for Internet & Society is committed to disseminating the fruits of its research and scholarship as widely as possible. In keeping with that commitment, the faculty directors and staff including employee fellows (henceforth \"covered members\") adopt the following policy: Each covered member grants to Harvard University permission to make available his or her scholarly articles whose subject relates to the purview of research at the Berkman Center, and to exercise the copyright in those articles. More specifically, each covered member grants to Harvard University a nonexclusive, irrevocable, worldwide license to exercise any and all rights under copyright relating to each of his or her scholarly articles, in any medium, provided that the articles are not sold for a profit, and to authorize others to do the same. The policy applies to all scholarly articles authored or co-authored while the person is a covered member except for any articles completed before the adoption of this policy and any articles for which the covered member entered into an incompatible licensing or assignment agreement before the adoption of this policy. The Provost or Provost's designate will waive application of the license for a particular article or delay access for a specified period of time upon express direction by a covered member author.\n\nEach covered member will provide an electronic copy of the author's final version of each article no later than the date of its publication at no charge to the appropriate representative of the Provost's Office in an appropriate format (such as PDF) specified by the Provost's Office.\n\nThe Provost's Office may make the article available to the public in an open-access repository. The Office of the Provost will be responsible for interpreting this policy, resolving disputes concerning its interpretation and application, and recommending changes to the Center from time to time. The policy will be reviewed by the Berkman Center for Internet & Society after three years and a report presented to the faculty directors.\n\n**Adopted October 9, 2014**\n\n**This policy is not included under the Creative Commons Attribution 4.0 International License (CC BY) to which other textual content on this web page may be subject.**\n\n**Harvard University Center for the Environment**\n\nHarvard has pioneered measures to promote open access and move toward a publishing system that is in keeping with the University's goals of disseminating the fruits of its research and scholarship as widely as possible. One of these measures is an open-access policy that has the effect of allowing open distribution of research articles.\n\nHUCE is committed to advancing these university-wide goals, and its research community, including its postdoctoral Environmental Fellows, will provide an electronic copy of the author's final version of each scholarly article to the appropriate representative of the Provost's Office, to be deposited into the open Digital Access to Scholarship at Harvard (DASH) repository.\n\nHUCE Environmental Fellows grant the President and Fellows of Harvard College permission to make available his or her scholarly articles and to exercise the copyright in those articles. More specifically, each Environmental Fellow grants to the President and Fellows of Harvard College a nonexclusive, irrevocable, worldwide license to exercise any and all rights under copyright relating to each of his or her scholarly articles, in any medium, provided that the articles are not sold for a profit, and to authorize others to do the same. The policy applies to all scholarly articles authored or co-authored while the person is a member of HUCE except for any articles completed before the adoption of this policy and any articles for which the member entered into an incompatible licensing or assignment agreement before the adoption of this policy. The Provost or Provost's designate will waive application of the license for a particular article or delay access for a specified period of time upon express direction by a Fellow.\n\n**Adopted July 1, 2015**\n\n**This policy is not included under the Creative Commons Attribution 4.0 International License (CC BY) to which other textual content on this web page may be subject.**\n\n**Harvard-China Center on Energy, Economy and Environment**\n\nThe Harvard-China Project on Energy, Economy and Environment of the Harvard John A. Paulson School of Engineering and Applied Sciences (henceforth “Harvard-China Project”) is committed to disseminating the fruits of its research and scholarship as widely as possible. In keeping with that commitment, the Harvard-China Project adopts the following policy: Each member of the Harvard-China Project research community including faculty, staff, postdoctoral fellows, students, and visiting researchers with research appointments with the Harvard-China Project (henceforth “covered member”) grants to Harvard University permission to make available his or her scholarly articles and to exercise the copyright in those articles. More specifically, each covered member grants to Harvard University a nonexclusive, irrevocable, worldwide license to exercise any and all rights under copyright relating to each of his or her scholarly articles, in any medium, provided that the articles are not sold for a profit, and to authorize others to do the same. The policy applies to all scholarly articles authored or co-authored while the person is a covered member of the Harvard-China Project, except for any articles completed before the adoption of this policy and any articles for which the covered member entered into an incompatible licensing or assignment agreement before the adoption of this policy. The Provost or Provost’s designate will waive application of the license for a particular article or delay access for a specified period of time upon express direction by a covered member.\n\nEach covered member will provide an electronic copy of the author’s final version of each article at the time of acceptance for publication at no charge to the appropriate representative of the Provost’s Office in an appropriate format (such as PDF) specified by the Provost’s Office. Distribution of peer-reviewed articles scheduled for publication will be postponed until the date of publication.\n\nThe Provost’s Office may make the article available to the public in an open-access repository. The Office of the Provost in consultation with the Executive Director of the Harvard-China Project will be responsible for interpreting this policy, resolving disputes concerning its interpretation and application, and recommending changes to the covered members from time to time.\n\n**Adopted September 21, 2017**\n\n**This policy is not included under the Creative Commons Attribution 4.0 International License (CC BY) to which other textual content on this web page may be subject.**\n\n**Mossavar-Rahmani Center for Business and Government**\n\nCommitment to Open Knowledge\n\nThe Mossavar-Rahmani Center for Business and Government (M-RCBG) is actively committed to the openness of information relevant to the public, particularly for issues at the intersection of business and government. This value is embodied in the traditions of a free press and is consistent with the idea that quality information is vital to the functioning of democracy.\n\nWe therefore support and reaffirm Harvard's Open Access Policy, enacted and implemented by all of the nine individual Harvard faculties all by unanimous or nearly unanimous faculty votes. This includes an affirmative vote by the Harvard Kennedy School, under which M-RCBG operates.\n\nHarvard has pioneered measures to promote open access and move toward a publishing system that is in keeping with the University's goals of promoting the civic good. One of these measures is an open-access policy that has the effect of allowing open distribution of research articles.\n\nM-RCBG Open Access Policy\n\nM-RCBG is actively committed to the openness of information relevant to the public, and has a long-established tradition of offering all research materials produced by the Center and its Fellows free to the public on its website. Further, we will provide an electronic copy of the author's final version of each scholarly article to the appropriate representative of the Provost's Office, to be deposited in an open-access repository, including the open Digital Access to Scholarship at Harvard (DASH) repository.\n\nEach member of the Mossavar-Rahmani Center for Business and Government grants to the President and Fellows of Harvard College permission to make available their scholarly articles and to exercise the copyright in those articles. More specifically, each member grants to the President and Fellows of Harvard College a nonexclusive, irrevocable, worldwide license to exercise any and all rights under copyright relating to each of their scholarly articles, in any medium, provided that the articles are not sold for a profit, and to authorize others to do the same. The policy applies to all scholarly articles authored or co-authored while the person is a member of M-RCBG, except for any articles completed before the adoption of this policy and any articles for which the member entered into an incompatible licensing or assignment agreement before the adoption of this policy. The Provost or Provost's designate will waive application of the license for a particular article or delay access for a specified period of time upon express direction by member.\n\n**Adopted June 6, 2024**\n\n**This policy is not included under the Creative Commons Attribution 4.0 International License (CC BY) to which other textual content on this web page may be subject.**\n\n**Shorenstein Center**\n\nThe Shorenstein Center on Media, Politics and Public Policy is actively committed to the openness of information relevant to the public. This value is embodied in the traditions of a free press, protected under the First Amendment, and is consistent with the idea that quality information is vital to the functioning of democracy.\n\nWe therefore support and reaffirm Harvard's Open Access Policy, enacted and implemented by all of the nine individual Harvard faculties all by unanimous or nearly unanimous faculty votes. This includes an affirmative vote by the Harvard Kennedy School, under which the Shorenstein Center operates.\n\nHarvard has pioneered measures to promote open access and move toward a publishing system that is in keeping with the University's goals of promoting the civic good. One of these measures is an open-access policy that has the effect of allowing open distribution of research articles.\n\nThe Shorenstein Center is committed to advancing these university-wide goals, and has a long-established tradition of offering all research materials produced by the Center and its Fellows free to the public on its website. Further, we will provide an electronic copy of the author's final version of each scholarly article to the appropriate representative of the Provost's Office, to be deposited into the open Digital Access to Scholarship at Harvard (DASH) repository.\n\nEach member of the Shorenstein Center grants to the President and Fellows of Harvard College permission to make available his or her scholarly articles and to exercise the copyright in those articles. More specifically, each member grants to the President and Fellows of Harvard College a nonexclusive, irrevocable, worldwide license to exercise any and all rights under copyright relating to each of his or her scholarly articles, in any medium, provided that the articles are not sold for a profit, and to authorize others to do the same. The policy applies to all scholarly articles authored or co-authored while the person is a member of the Shorenstein Center except for any articles completed before the adoption of this policy and any articles for which the member entered into an incompatible licensing or assignment agreement before the adoption of this policy. The Provost or Provost's designate will waive application of the license for a particular article or delay access for a specified period of time upon express direction by member.\n\n**Adopted October 9, 2014**\n\n**This policy is not included under the Creative Commons Attribution 4.0 International License (CC BY) to which other textual content on this web page may be subject.**\n\n## HARVARD INDIVIDUAL OPEN ACCESS LICENSE\n\n**About the Individual Open Access License**\n\nThis a voluntary open access license that any Harvard researchers may choose for their own scholarly articles. Because Harvard faculty are already covered by open access licenses through the school-level open access policies, in practice the Individual Open Access License is for non-faculty researchers, such as administrators, librarians, staff, postdocs, fellows, and students.\n\nThrough the school-level open access policies, faculty grant Harvard certain nonexclusive rights to their future scholarly articles. These enable Harvard to make a certain version of those articles open access through DASH, Harvard's institutional open access repository. Moreover, they enable Harvard to grant the same rights back to the authors. The Individual Open Access License grants Harvard exactly the same set of nonexclusive rights, for exactly the same purposes. In that sense, it gives non-faculty researchers the same benefits that the school-level policies give faculty.\n\nJust as faculty may obtain waivers from their open access license, those who sign the Individual Open Access License may also obtain waivers.\n\nThe License works best when authors sign it before they sign publishing contracts. For that reason, and because it permits waivers for any given work, we encourage interested authors to sign the Individual Open Access License soon, or before they publish their next scholarly article.\n\nAuthors only need to sign the License once for the rest of their career at Harvard, not once per paper. It lasts as long as the author is affiliated with Harvard.\n\n**Individual Open Access License (Opt-In)**\n\n**The Individual Open Access License is an opt-in, voluntary license for individuals who are not already covered by the (opt-out) open access policies at the school- and center-levels.**\n\nAuthors at Harvard University are committed to disseminating the fruits of their research and scholarship as widely as possible. In keeping with that commitment, by signing this license, I hereby grant to Harvard University the same non-exclusive rights that faculty grant to Harvard under the faculty open-access (OA) policies. More specifically, I grant to Harvard University a non-exclusive, irrevocable, worldwide license to exercise any and all rights under copyright relating to each of my scholarly articles, in any medium, provided that the articles are not sold for a profit, and to authorize others to do the same. This license applies to all scholarly articles I author or co-author while I am a Harvard affiliate, except for any articles completed before I signed this form and any articles for which I entered into an incompatible licensing or assignment agreement before I signed this form. Harvard Library or its designate, will waive application of this license for a particular article or delay access for a specified period of time upon my express direction\n\nI will provide an electronic copy of the author’s accepted manuscript of each article no later than the date of its publication at no charge to the appropriate representative of Harvard Library in an appropriate format (such as PDF) specified by Harvard Library or its designate.\n\nIf I am a non-faculty employee or affiliate of Harvard University, and my work is normally covered by the work-for-hire doctrine, then I understand that this license only applies to my scholarly articles not covered by that doctrine.\n\nTo sign and enroll in the Individual Open Access License, simply accept the DASH Submitter Agreement when you first log in to the [DASH repository] as a Harvard author.\n\n**Adopted June 7, 2018**\n\n**This license is not included under the Creative Commons Attribution 4.0 International License (CC BY) to which other textual content on this web page may be subject.**\n\n## Frequently Asked Questions\n\n### Why Open Access?\n\n**Why are we doing this?**\n\nHarvard University has long had a policy that \"when entering into agreements for the publication and distribution of copyrighted materials, authors will make arrangements that best serve the public interest\" (from the [Statement of Policy in Regard to Intellectual Property]). In the eyes of many, including the Provost's Committee on Scholarly Publishing and large numbers of individual faculty members, this goal is best served by using the unified action of the faculty to enable individual faculty to distribute their scholarly writings freely.\n\nOther organizations with a vested interest in advancing research are independently supporting such efforts as well. For instance, funders increasingly require scholarly articles resulting from the research they fund to be made openly accessible, whether by their own accord or by congressional legislation.\n\n**What do the Harvard open access policies provide?**\n\nThe policies have two basic provisions. First, covered authors commit to deposit a certain version of their future scholarly articles in DASH, Harvard's institutional open access repository. Second, covered authors grant certain nonexclusive rights over their future scholarly articles to Harvard, authorizing it to make those deposited articles open access. This grant of nonexclusive rights is not equivalent to a grant of ownership. It includes waiver and embargo options to enhance author freedom and control over their work.\n\nOften, we call this grant of nonexclusive rights the Harvard open access license or the license granted to Harvard.\n\nThe policies do _not_ require covered authors to submit new scholarly articles to any particular type of journals, such as open-access journals. On the contrary, the policies deliberately allow authors to submit new work to the journals of their choice.\n\nAll Harvard schools have open access policies, each one adopted by faculty vote. In their major provisions, they are identical. Many centers have also adopted open access policies covering their members. And any member of the Harvard community who is not a member of faculty, given faculty are already covered, may opt in to the Individual Open Access License (IOAL) to be covered by the provisions described above.\n\n**Why are the school- and center-level policies opt-out?**\n\nFirst, mere encouragement to distribute articles open access has little effect. For instance, before Congress made it a requirement, participation in the NIH Public Access Policy was optional. During that period, there was only a 4% level of compliance. During the same period studies showed that the low level of compliance was not due to opposition so much as preoccupation, busyness, and forgetfulness.\n\nSecond, opt-out systems achieve much higher degrees of participation than opt-in systems, even while remaining noncoercive.\n\nThird, by making policies at the school and center levels, individual faculty benefit from their membership in the policy-making group. Without a policy covering many authors, we could not take full advantage of the benefit of unified action.\n\n**What are the advantages for authors?**\n\nThe school-level policies, center-level policies, and the Individual Open Access License:\n\n- Give authors permission to make their work open access without the difficulty or uncertainty of negotiating with publishers;\n- Enable the university to help authors make their works open access;\n- Preserve author freedom to publish in the journals of their choice;\n- Preserve author freedom to decide for or against open access for each publication; and\n- Enhance author rights to reuse their work, and give authors more rights over their own work than standard publishing contracts.\n\nThe chief benefit is the way these policies (and the Individual License) foster open access itself. Research has shown that [articles that are free online are cited more often] than articles that are not free online, and that these articles also [receive more citations across disciplines].\n\n### Policy and License Basics\n\n**What kinds of writings do the policies cover?**\n\nWhile DASH, Harvard's institutional open access repository, welcomes scholarly works other than articles, the policies only cover articles.\n\nWe focus on scholarly articles because, in the language of the [Budapest Open Access Initiative], these are the primary works that scholars publish \"for the sake of inquiry and knowledge\" and \"give to the world without expectation of payment.\" Scholarly articles are typically presented in peer-reviewed scholarly journals and conference proceedings.\n\nLike the school- and center-level policies, the Individual Open Access License is also limited to scholarly articles.\n\n**What version of a scholarly article should authors deposit in the repository?**\n\nCovered authors should deposit the accepted author manuscript. The accepted author manuscript of a work is the version approved by peer review, or the last version the author sends to the publisher after peer review. It does not include unilateral edits made by the journal after peer review, or the journal's look and feel.\n\nThe school- and center-level policies use the term \"final version of the article.\" But since the policies were adopted, the more self-explanatory term \"accepted author manuscript\" has become more widely accepted for describing the version we have in mind.\n\n**Do the policies apply to articles written before the policies were adopted?**\n\nNo. The school-level and center-level policies do not apply to any articles that were completed before their adoption at the author's school or center, nor to any articles for which the author entered into an incompatible publishing agreement before the respective policy was adopted.\n\nFor authors who have signed the Individual Open Access License, it too will not apply to articles written before the individual signed the license.\n\n**Do the policies apply to articles written after leaving Harvard?**\n\nNo. Once authors are no longer affiliated with Harvard, any articles they write will not be subject to the Harvard open access policies or to the Individual Open Access License.\n\n**How do I request a waiver?**\n\nA covered author simply needs to complete the appropriate form, on an article-by-article basis, to [waive the license granted to Harvard] under the Harvard open-access policies or the Individual Open Access License.\n\n**Do the policies apply to co-authored papers?**\n\nYes. If you are a co-author of an article, you should inform your fellow co-authors about the nonexclusive license that you have granted Harvard under the open access policies or the Individual Open Access License. If they object to the license, and cannot be convinced it is beneficial, then remember that you can obtain a waiver for the article.\n\nEach joint author of an article holds copyright in the article and, individually, has the authority to grant Harvard a nonexclusive license. However, one waiver from one covered author is sufficient to waive the license to Harvard under the policies for that article. The same is true for authors who signed the Individual Open Access License.\n\n**Is scholarship by students made available open access at Harvard?**\n\nIt depends. Theses and dissertations by graduate students will usually become open access. And, any Harvard student may volunteer to be covered by signing the Individual Open Access License.\n\n### Common Concerns\n\n**Can my articles be used as the basis of derivative works by other authors?**\n\nYes. Harvard may permit you and others to make derivative works based on the articles covered by a school-level policy, center-level policy, or the Individual Open Access License. However, Harvard would only exercise this right with permission from the author and in order to advance the aims of the policy.\n\nHarvard recognizes authors' interest in the integrity of their scholarly articles. If Harvard allows the display or distribution of any derivative work that modifies the substance of the original document (e.g., an abridgment), it will require that the derivative work include a citation, hyperlink or similar reference to the original document and that it appropriately identify the nature of the revision (e.g., \"abridged from…\").\n\nAs technology and modes of distributing and using scholarly content evolve, it may serve the interests of research and education for Harvard to adjust the derivative-work rights granted to the public. In any such decisions, Open Scholarship and Research Data Services will be guided by its advisors.\n\n**Is the university acquiring ownership of my writing?**\n\nNo. Authors still retain ownership and control of the copyright in their writings, subject only to the prior, nonexclusive rights granted to Harvard under the policies and, upon its signing, the Individual Open Access License.\n\n**Will Harvard ever sell articles for profit or allow others to do so?**\n\nNo. Harvard does not have the right to sell for a profit the articles under the policies or the Individual Open Access License, and cannot grant this right to others. The same applies to a course pack or book containing such articles.\n\n**What if a journal publisher refuses to publish my article?**\n\nIf you are covered by a Harvard open access policy or the Individual Open Access License and a journal publisher refuses to publish your article because of this, you have a number of options. You may:\n\n- Obtain a waiver for that article and let the publisher know that you have done so; or\n- Obtain an embargo to delay deposit of the work in DASH and let the publisher know you have done so; or\n- Try to persuade the publisher that it should accept Harvard's nonexclusive license under the policies (or the Individual Open Access License) as a condition of publication; or finally,\n- Try to seek a different publisher and consult with colleagues on alternative journals.\n\nWe have not heard of a single case in which a journal has refused to publish an article merely because of the open access policies. This is because the waiver and embargo options offer complete protection to publishers who wish to take advantage of them.\n\n**Where can I find information about funder open access policies?**\n\nThis website pertains only to the Harvard Open Access Policies and the Individual Open Access License.\n\nContact your funder and your Submitting Office at Harvard about any funder policy to which you may be subject and the terms of your award. More information and resources about the [updated public access policies at federal agencies] are also available from Open Scholarship and Research Data Services.\n\n**Does this website provide legal advice to me?**\n\nNo. This website provides information and resources to help covered authors and others understand the open access policies and to assist in compliance, but does not provide individual legal advice. Open Scholarship and Research Data Services and its staff also are not able to provide individual legal advice. If you wish legal advice about your copyrights or individual situation, you should consult your own attorney.\n\n### Using an Author Addendum\n\n**What is an author addendum?**\n\nAn author addendum is a proposed modification of a publishing agreement. If accepted by the publisher, it modifies the agreement, for example, in order to take proper account of the nonexclusive rights granted to Harvard under the open access policies and the Individual Open Access License. (See the Sample Addendum.)\n\nPublishing agreements often contain provisions that are inconsistent with the prior license granted to Harvard under the Harvard open access policies and the Individual Open Access License. For instance, a publishing agreement may specify the author transfers all rights under copyright in the article to the publisher and that the author warrants there are no prior licenses. The existence of the prior license to Harvard means that this warranty would not be true. If an author were to sign the publishing agreement without appropriate amendment, the author may be in breach of the agreement.\n\nTo avoid a conflicting transfer of copyright to the publisher and to protect oneself from breach of contract, an author can use the Sample Addendum. Even without the attachment of an addendum, however, the prior license granted to Harvard under the Harvard open access policies and the Individual Open Access License will still have force unless it is waived for a particular article.\n\n**What if the journal publisher doesn't accept my addendum, wants to negotiate it, or tells me I don't need it?**\n\n**Publisher refuses to accept or wants to negotiate the addendum**\n\nWhether you are covered by the school- or center-level policies or the Individual Open Access License, you may obtain a waiver for that article.\n\nAlternatively, you can work to persuade the publisher that it should accept Harvard's license under the policies or the Individual Open Access License, or you may seek a different publisher.\n\n**Publisher states the addendum is unnecessary**\n\nA publisher may tell you the addendum is unnecessary because the publishing agreement already permits immediate posting of the article in an institutional open access repository. In that case, it may still be a good idea to use the addendum. The license to Harvard enables the University to allow you and others to make various beneficial uses of the article, which may be in conflict with provisions of the publishing agreement. To avoid a conflicting transfer of copyright to the publisher and to protect yourself from breach of contract, you may still want to attach an addendum.\n\nIf the publishing agreement, however, is wholly consistent with Harvard's license under the policies (or the Individual Open Access License), you would not need to use the addendum.\n\n**Sample Addendum**\n\n**Complete an appropriate form of the addendum, sign and date the form, add a statement to the publisher's agreement making it subject to the addendum, and attach the addendum to the publisher's agreement. The two paragraphs in italics (i.e. 4.b and 4.d) are optional to be used at the author's discretion.**\n\n**SAMPLE ADDENDUM TO PUBLICATION AGREEMENT** v.2 1/27/09\n\n1\\. This Addendum modifies and supplements the attached publication agreement (the \"Publication Agreement\") concerning the article titled \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_ (including any supplementary materials, the \"Work\") in \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_.\n\n2\\. The parties to the Publication Agreement as modified and supplemented by this Addendum are: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_ (corresponding author) and any other authors listed on the Work (individually or, if more than one author, collectively, \"Author\") and \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_ (\"Publisher\").\n\n3\\. The parties agree that wherever there is any conflict between this Addendum and the Publication Agreement, the provisions of this Addendum will control and the Publication Agreement will be construed accordingly.\n\n4\\. Notwithstanding any terms in the Publication Agreement to the contrary, Author and Publisher agree as follows:\n\n4.a. All of the terms and conditions of the Publication Agreement, including but not limited to all grants, agreements, representations and warranties, are subject to and qualified by a non-exclusive license previously granted by Author to Harvard University. Under that license, Harvard may make the Work available and may exercise all rights under copyright relating to the Work, and may authorize others to do the same, provided that the Work is not sold for a profit. In the exercise of that license, Harvard may use the Author's final manuscript of the Work (including all modifications from the peer review process), but will not use a facsimile of the final published version of the Work unless Publisher permits use of that version. When Harvard makes the Work available in an on-line repository under that license, Harvard will cite to Publisher's definitive version of the Work, and will link to Publisher's version if it is available on-line.\n\n_4.b. In addition to any rights retained by or granted to Author in the Publication Agreement, Author retains the non-exclusive right to make the Work available and to exercise all rights under copyright relating to the Work, in any medium, in connection with Author's teaching, conference presentations, lectures, other works of authorship, and professional activities, and to authorize others to do the same._\n\n4.c. Where applicable, all of the terms and conditions of the Publication Agreement, including but not limited to all grants, agreements, representations and warranties, are subject to and qualified by non-exclusive rights previously granted, or required to be granted, by Author to a funding entity that financially supported the research reflected in the Work as part of an agreement between Author or Author's employing institution and such funding entity, such as an agency of the United States government, and/or to Author's employing institution.\n\n_4.d. Publisher agrees to provide to Author within 14 days of first publication and at no charge an electronic copy of the published Work in a format, such as the Portable Document Format (.pdf), that preserves final page layout, formatting and content of the final published version. No technical restriction, such as security settings, will be imposed to prevent copying or printing of that copy. The Publisher permits a copy of the final published version to be used in the exercise of the rights and licenses referred to in the paragraphs above._\n\n4.e. Nothing in the Publication Agreement will impose any limitation on the rights and licenses referred to in the paragraphs above or any obligation in connection with their exercise. Neither the existence nor the exercise of those rights and licenses will be deemed to violate any representation or warranty or to breach the Publication Agreement.\n\n5. Either publication of the Work or Publisher's signature below will constitute Publisher's acceptance of and agreement to this Addendum.\n\nAUTHOR\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_(corresponding author on behalf of all authors)\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_Date\n\nPUBLISHER\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_ Date\n\n## OBTAIN A WAIVER\n\nTo waive the license granted to Harvard under the Harvard open-access policies or the Individual Open Access License, simply complete a [Harvard Open Access Policy Waiver Request] on an article-by-article basis. One waiver from one covered author is sufficient.\n\nEven if you obtain a waiver for a given article, you should still deposit the accepted author manuscript in DASH. It can be made available after a delay, depending on the nature of your publication agreement. Open Scholarship and Research Data Services can help determine what kind of distribution is appropriate for the work.\n\nIf your article is subject to an open access policy at a funding agency, it is possible your obligations under those policies cannot be waived. Check with your funder for more information.\n\nStay in the know\n\nSign up",
    "length": 58758,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Federal Guidance",
    "url": "https://catalyst.harvard.edu/regulatory/ai-human-participant-research/federal-guidance",
    "text": "- [About] \n- [News & Highlights] \n- [Calendar] \n- [Contact Us] \n\n- [About] \n - [People] \n - [Programs] \n - [Publications and Documents] \n- [Train] \n - [C/T Research Academy] \n - [Education in C/T Science] \n - [Browse Our Courses] \n - [K12 Training Award] \n - [Harvard Catalyst On-Demand] \n- [Connect] \n - [Connector] \n - [Catalyst Connect] \n - [SMART IRB Reliance Request] \n- [Support] \n - [Biostatistics Consulting] \n - [Regulatory Support] \n - [Pilot Funding] \n - [Informatics Program] \n- [Include] \n - [Community Engagement] \n - [Enhancing Research Enrollment] \n- [News & Highlights] \n- [Harvard Catalyst Profiles] \n- [Calendar] \n- [Contact Us] \n\n# Regulatory Foundations, Ethics, and Law Program\n\nSupporting researchers to navigate clinical and translational regulatory processes\n\nAll\n\n- [Overview] \n- [Digital Accessibility: Utilizing Third-Party Devices or Software] \n- [Stay Informed: Artificial Intelligence & Human Participant Research] \n - [Editorials] \n - [Journal Articles] \n - [Informational Videos] \n - [Events] \n - [Federal Guidance] \n - [Regulatory AI Symposium] \n- [Our Expertise] \n- [Publications & Documents] \n- [Contacts] \n\n## Federal Guidance\n\n[NIH Notice NOT OD 25 132: Supporting Fairness and Originality in NIH Research Applications] \n\n[NIH Notice NOT‑OD‑25‑132: Supporting Fairness and Originality in NIH Research Applications] \n\nThis NIH policy forbids grant applications that are substantially drafted or developed by generative AI is effective for submissions from **September 25, 2025**, onwards. Its stated aim is to preserve originality and fairness in research proposals and prevent peer-review overload. Proposals for human participant research must reflect authentic scientific thinking—not AI outputs—and rigorous ethical planning and integrity in study design.\n\nAuthor: NIH\n\n[FDA Announces Completion of First AI-Assisted Scientific Review Pilot and Aggressive Agency-Wide AI Rollout Timeline] \n\n[FDA Announces Completion of First AI-Assisted Scientific Review Pilot and Aggressive Agency-Wide AI Rollout Timeline] \n\nThis FDA news release highlights the completion of the FDA’s first AI-assisted scientific review pilot, demonstrating the ability to reduce tasks that previously took days down to minutes. FDA plans to deploy a secure generative AI tool called Elsa across all centers by **June 30, 2025**. This initiative highlights AI’s transformative role in increasing regulatory efficiency for drug, biologic, and device reviews, signaling a shift toward faster approvals. However, as AI streamlines review processes, it also raises critical questions about maintaining scientific integrity, securing confidential data, ensuring human judgment remains central, and protecting participant safety. For human participant research, the rollout underscores the importance of transparency and human oversight in AI-enhanced regulatory decisions to preserve trust and equitable outcomes.\n\nAuthor: FDA\n\n[Artificial Intelligence in Research: Policy Considerations and Guidance] \n\n[Artificial Intelligence in Research: Policy Considerations and Guidance] \n\nThis resource from the National Institutes of Health (NIH) outlines the policies and guidelines related to the use of AI in research funded by the National Institutes of Health (NIH). It provides a framework for how AI should be integrated into research projects, with a focus on maintaining ethical standards and ensuring responsible use. The NIH’s AI policies provide essential guidance for integrating AI into research involving human participants, ensuring that ethical considerations, data security, and regulatory compliance are prioritized in the research process.\n\nAuthor: National Institutes of Health (NIH)\n\n[FDA Guidelines on AI/ML in Medical Devices] \n\n[FDA Guidelines on AI/ML in Medical Devices] \n\nThis guidance from the FDA outlines the development of the FDA’s regulatory framework for AI and machine learning in medical devices, ensuring that any AI-driven medical devices undergo rigorous testing, including clinical trials with human participants to affirm their safety and efficacy before these technologies are widely adopted.\n\nAuthor: U.S. Food and Drug Administration (FDA)\n\n[IRB Considerations for AI in Human Subjects Research] \n\n[IRB Considerations for AI in Human Subjects Research] \n\nThis resource from Office for Human Research Protections (OHRP) outlines IRBs’ role in overseeing AI-driven research, ensuring that human subjects are protected, and ethical principles are followed, particularly with respect to informed consent, data privacy, and the potential risks associated with AI systems in research.\n\nAuthor: Office for Human Research Protections (OHRP)",
    "length": 4677,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Evidence at the Core: How Policy Can Shape AI’s Future",
    "url": "https://d3.harvard.edu/evidence-at-the-core-how-policy-can-shape-ais-future/",
    "text": "[Skip to content] \n\n[Visit hbs.edu] \n\nSearch for:\n\nAs AI technology advances, policymakers will face the crucial task of how to steer its development responsibly. In the new paper published in _Science_, “ [Advancing science- and evidence-based AI policy],” a multidisciplinary group of experts, including [Himabindu Lakkaraju], Assistant Professor of Business Administration at Harvard Business School and PI in the [Trustworthy AI Lab] at the [Digital Data Design Institute at Harvard (D^3)], argue that the future of AI governance depends on robust support for evidence utilization and generation.\n\n## **Key Insight: Evidence Must Drive AI Policy**\n\n> “Defining what counts as (credible) evidence is the first hurdle for applying evidence-based policy to an AI context.” [\\[1\\]] \n\nThe authors stress that the idea of evidence itself is not simple or straightforward. What qualifies as evidence can vary across fields: in health policy, randomized control trials serve as the gold standard, while in economics, forecasts and theoretical models hold weight. And history illustrates that evidence will often be questioned or ignored: the tobacco industry leaned on inconclusive studies to stall public health measures, and fossil fuel companies downplay climate risks despite knowing otherwise. These examples show that the tasks of defining, evaluating, and acting on evidence are urgent and complex. In response to these challenges, the authors encourage the US government to utilize the Foundations for Evidence-Based Policymaking Act (Evidence Act).\n\n## **Key Insight: Policy Can Accelerate Evidence Generation**\n\n> “We recommend that policy-makers require major AI companies to disclose more information about their safety practices to governments and, especially, to the public.” [\\[2\\]] \n\nThe authors propose several mechanisms to make policy the driver of evidence creation. Policymakers should incentivize pre-release evaluations, ensuring that risks (such as using AI for malicious purposes, the likelihood of AI hallucinations, or the prevalence of AI generating copyrighted material) are measured before companies deploy new models. They also call for increased transparency, citing findings from the 2024 Foundation Model Transparency Index that top AI companies fall short when it comes to publicly reporting their risk-mitigation practices. They recommend post-deployment monitoring, such as adverse-event reporting systems that track concrete instances of harm once models are in use. Finally, they encourage protections for third-party research, noting that independent investigators often face legal and contractual barriers when probing AI systems. Safe harbor provisions, modeled on cybersecurity law, would enable such research to proceed in the public interest. Together, these measures would expand the evidence base and allow AI policy to evolve in step with the technology itself.\n\n## **Key Insight: Consensus in a Fragmented Field**\n\n> “Scientific consensus, including on areas of uncertainty or immaturity, is a powerful primitive for better AI policy.” [\\[3\\]] \n\nThe AI research and policy community is currently divided, with divergent views on the seriousness of risks and the speed of technological progress. This lack of alignment makes it difficult to establish clear, effective policy responses. Drawing from precedents in climate governance and disaster policy, the authors call for deliberate processes that foster consensus, even amid uncertainty. Global initiatives, such as the UN’s High-Level Advisory Board on AI and proposals for an International Scientific Panel, aim to provide shared baselines of evidence. Such consensus would not eliminate debate but would ensure that disagreements unfold with a common evidentiary framework, strengthening the legitimacy and durability of policy decisions.\n\n## **Why This Matters**\n\nAs AI becomes more central to business operations, having trustworthy and reliable systems will be crucial. Business leaders and executives will benefit from understanding the growing landscape of AI policy, supporting evidence-based foundations for AI technology, and following the guidance of institutions that produce independent research. By aligning with these principles, companies will not only be ready to comply with emerging regulations, but will also be a step ahead to build trust with customers and stakeholders. As the authors conclude, governing AI will be one of the grand challenges of the 21st century, and informed business leaders will have an important role to play facing it.\n\n## **References**\n\n[\\[1\\]] Rishi Bommasani et al., “Advancing science- and evidence-based AI policy,” _Science_ 389 (2025): 459. DOI: [10.1126/science.adu8449] \n\n[\\[2\\]] Bommasani et al., “Advancing science- and evidence-based AI policy,” 460.\n\n[\\[3\\]] Bommasani et al., “Advancing science- and evidence-based AI policy,” 461.\n\n## **Meet the Authors**\n\n[Himabindu Lakkaraju] is an Assistant Professor of Business Administration at Harvard Business School and PI in D^3’s [Trustworthy AI Lab]. She is also a faculty affiliate in the Department of Computer Science at Harvard University, the Harvard Data Science Initiative, Center for Research on Computation and Society, and the Laboratory of Innovation Science at Harvard. Professor Lakkaraju’s research focuses on the algorithmic, practical, and ethical implications of deploying AI models in domains involving high-stakes decisions such as healthcare, business, and policy.\n\n**Additional Authors:** Rishi Bommasani, Sanjeev Arora, Jennifer Chayes, Yejin Choi, Mariano-Florentino Cuéllar, Li Fei-Fei, Daniel E. Ho, Dan Jurafsky, Sanmi Koyejo, Arvind Narayanan, Alondra Nelson, Emma Pierson, Joelle Pineau, Scott Singer, Gaël Varoquaux, Suresh Venkatasubramanian, Ion Stoica, Percy Liang, and Dawn Song\n\nDid you find this insight to be useful?\n\n- Yes\n- No\n\nWould you recommend this insight to others?\n\n- Yes\n- No\n\nSubmit\n\nEngage With Us\n\n## Join Our Community\n\nReady to dive deeper with the Digital Data Design Institute at Harvard? Subscribe to our newsletter, contribute to the conversation and begin to invent the future for yourself, your business and society as a whole.\n\nEmail \\*\n\nSubscribe",
    "length": 6211,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Letter Sent to Harvard 2025-04-11",
    "url": "https://www.harvard.edu/research-funding/wp-content/uploads/sites/16/2025/04/Letter-Sent-to-Harvard-2025-04-11.pdf",
    "text": "April 11, 2025\nDr. Alan M. Garber\nPresident\nHarvard University\nOffice of the President\nMassachusetts Hall\nCambridge, MA 02138\nPenny Pritzker\nLead Member, Harvard Corporation\nHarvard Corporation\nMassachusetts Hall\nCambridge, MA 02138\nDear Dr. Garber:\nThe United States has invested in Harvard University’s operations because of the value to the\ncountry of scholarly discovery and academic excellence. But an investment is not an entitlement. It\ndepends on Harvard upholding federal civil rights laws, and it only makes sense if Harvard fosters\nthe kind of environment that produces intellectual creativity and scholarly rigor, both of which are\nantithetical to ideological capture.\nHarvard has in recent years failed to live up to both the intellectual and civil rights conditions that\njustify federal investment. But we appreciate your expression of commitment to repairing those\nfailures and welcome your collaboration in restoring the University to its promise. We therefore\npresent the below provisions as the basis for an agreement in principle that will maintain Harvard’s\nfinancial relationship with the federal government.\nIf acceptable to Harvard, this document will constitute an agreement in principle, which the parties\nwill work in good faith to translate into a more thorough, binding settlement agreement. As you will\nsee, this letter incorporates and supersedes the terms of the federal government’s prior letter of April\n3, 2025.\n● Governance and leadership reforms. By August 2025, Harvard must make meaningful\ngovernance reform and restructuring to make possible major change consistent with this\nletter, including: fostering clear lines of authority and accountability; empowering tenured\nprofessors and senior leadership, and, from among the tenured professoriate and senior\nleadership, exclusively those most devoted to the scholarly mission of the University and\ncommitted to the changes indicated in this letter; reducing the power held by students and\nuntenured faculty; reducing the power held by faculty (whether tenured or untenured) and\nadministrators more committed to activism than scholarship; and reducing forms of\ngovernance bloat, duplication, or decentralization that interfere with the possibility of the\nreforms indicated in this letter.\n● Merit-Based Hiring Reform. By August 2025, the University must adopt and implement\nmerit-based hiring policies, and cease all preferences based on race, color, religion, sex, or\nnational origin throughout its hiring, promotion, compensation, and related practices among\nfaculty, staff, and leadership. Such adoption and implementation must be durable and\ndemonstrated through structural and personnel changes. All existing and prospective faculty\nshall be reviewed for plagiarism and Harvard’s plagiarism policy consistently enforced. All\nhiring and related data shall be shared with the federal government and subjected to a\ncomprehensive audit by the federal government during the period in which reforms are\nbeing implemented, which shall be at least until the end of 2028.\n● Merit-Based Admissions Reform. By August 2025, the University must adopt and\nimplement merit-based admissions policies and cease all preferences based on race, color,\nnational origin, or proxies thereof, throughout its undergraduate program, each graduate\nprogram individually, each of its professional schools, and other programs. Such adoption\nand implementation must be durable and demonstrated through structural and personnel\nchanges. All admissions data shall be shared with the federal government and subjected to a\ncomprehensive audit by the federal government—and non-individualized, statistical\ninformation regarding admissions shall be made available to the public, including\ninformation about rejected and admitted students broken down by race, color, national\norigin, grade point average, and performance on standardized tests—during the period in\nwhich reforms are being implemented, which shall be at least until the end of 2028. During\nthis same period, the dean of admissions for each program or school must sign a public\nstatement after each admissions cycle certifying that these rules have been upheld.\n● International Admissions Reform. By August 2025, the University must reform its\nrecruitment, screening, and admissions of international students to prevent admitting\nstudents hostile to the American values and institutions inscribed in the U.S. Constitution\nand Declaration of Independence, including students supportive of terrorism or\nanti-Semitism. Harvard will immediately report to federal authorities, including the\nDepartment of Homeland Security and State Department, any foreign student, including\nthose on visas and with green cards, who commits a conduct violation. As above, these\nreforms must be durable and demonstrated through structural and personnel changes;\ncomprehensive throughout all of Harvard’s programs; and, during the reform period, shared\nwith the federal government for audit, shared on a non-individualized basis with the public,\nand certified by deans of admissions.\n● Viewpoint Diversity in Admissions and Hiring. By August 2025, the University shall\ncommission an external party, which shall satisfy the federal government as to its\ncompetence and good faith, to audit the student body, faculty, staff, and leadership for\nviewpoint diversity, such that each department, field, or teaching unit must be individually\nviewpoint diverse. This audit shall begin no later than the summer of 2025 and shall proceed\non a department-by-department, field-by-field, or teaching-unit-by-teaching-unit basis as\nappropriate. The report of the external party shall be submitted to University leadership and\nthe federal government no later than the end of 2025. Harvard must abolish all criteria,\npreferences, and practices, whether mandatory or optional, throughout its admissions and\nhiring practices, that function as ideological litmus tests. Every department or field found to\nlack viewpoint diversity must be reformed by hiring a critical mass of new faculty within\nthat department or field who will provide viewpoint diversity; every teaching unit found to\nlack viewpoint diversity must be reformed by admitting a critical mass of students who will\nprovide viewpoint diversity. If the review finds that the existing faculty in the relevant\ndepartment or field are not capable of hiring for viewpoint diversity, or that the relevant\nteaching unit is not capable of admitting a critical mass of students with diverse viewpoints,\nhiring or admissions within that department, field, or teaching unit shall be transferred to the\nclosest cognate department, field, or teaching unit that is capable of achieving viewpoint\ndiversity. This audit shall be performed and the same steps taken to establish viewpoint\ndiversity every year during the period in which reforms are being implemented, which shall\nbe at least until the end of 2028.\n● Reforming Programs with Egregious Records of Antisemitism or Other Bias. By\nAugust 2025, the University shall commission an external party, which shall satisfy the\nfederal government as to its competence and good faith, to audit those programs and\ndepartments that most fuel antisemitic harassment or reflect ideological capture.\no The programs, schools, and centers of concern include but are not limited to the\nDivinity School, Graduate School of Education, School of Public Health, Medical\nSchool, Religion and Public Life Program, FXB Center for Health & Human Rights,\nCenter for Middle Eastern Studies, Carr Center for Human Rights at the Harvard\nKennedy School, Department of Near Eastern Languages and Cultures, and the\nHarvard Law School International Human Rights Clinic.\no The report of the external party shall include information as to individual faculty\nmembers who discriminated against Jewish or Israeli students or incited students to\nviolate Harvard’s rules following October 7, and the University and federal\ngovernment will cooperate to determine appropriate sanctions for those faculty\nmembers within the bounds of academic freedom and the First Amendment.\no The report of the external party shall be submitted to University leadership and the\nfederal government no later than the end of 2025 and reforms undertaken to repair\nthe problems. This audit shall be performed and the same steps taken to make repairs\nevery year during the period in which reforms are being implemented, which shall be\nat least until the end of 2028.\n● Discontinuation of DEI. The University must immediately shutter all diversity, equity, and\ninclusion (DEI) programs, offices, committees, positions, and initiatives, under whatever\nname, and stop all DEI-based policies, including DEI-based disciplinary or speech control\npolicies, under whatever name; demonstrate that it has done so to the satisfaction of the\nfederal government; and demonstrate to the satisfaction of the federal government that these\nreforms are durable and effective through structural and personnel changes. By August\n2025, the University must submit to the government a report—certified for accuracy—that\nconfirms these reforms.\n● Student Discipline Reform and Accountability. Harvard must immediately reform its\nstudent discipline policies and procedures so as to swiftly and transparently enforce its\nexisting disciplinary policies with consistency and impartiality, and without double\nstandards based on identity or ideology. Where those policies are insufficient to prevent the\ndisruption of scholarship, classroom learning and teaching, or other aspects of normal\ncampus life, Harvard must develop and implement disciplinary policies sufficient to prevent\nthose disruptions. This includes but is not limited to the following:\no Discipline at Harvard must include immediate intervention and stoppage of\ndisruptions or deplatforming, including by the Harvard police when necessary to stop\na disruption or deplatforming; robust enforcement and reinstatement of existing time,\nplace, and manner rules on campus, including ordering the Harvard police to stop\nincidents that violate time, place, and manner rules when necessary; a disciplinary\nprocess housed in one body that is accountable to Harvard’s president or other\ncapstone official; and removing or reforming institutional bodies and practices that\ndelay and obstruct enforcement, including the relevant Administrative Boards and\nFAS Faculty Council.\no Harvard must adopt a new policy on student groups or clubs that forbids the\nrecognition and funding of, or provision of accommodations to, any student group or\nclub that endorses or promotes criminal activity, illegal violence, or illegal\nharassment; invites non-students onto campus who regularly violate campus rules; or\nacts as a front for a student club that has been banned from campus. The leaders or\norganizers of recognized and unrecognized student groups that violate these policies\nmust be held accountable as a matter of student discipline and made ineligible to\nserve as officers in other recognized student organizations. In the future, funding\ndecisions for student groups or clubs must be made exclusively by a body of\nUniversity faculty accountable to senior University leadership. In particular, Harvard\nmust end support and recognition of those student groups or clubs that engaged in\nanti-Semitic activity since October 7th, 2023, including the Harvard Palestine\nSolidarity Committee, Harvard Graduates Students 4 Palestine, Law Students 4\nPalestine, Students for Justice in Palestine, and the National Lawyers Guild, and\ndiscipline and render ineligible the officers and active members of those student\norganizations.\no Harvard must implement a comprehensive mask ban with serious and immediate\npenalties for violation, not less than suspension.\no Harvard must investigate and carry out meaningful discipline for all violations that\noccurred during the 2023-2024 and 2024-2025 academic years, including the\nHarvard Business School protest of October 2023, the University Hall sit-in of\nNovember 2023, and the spring encampment of 2024. This must include\npermanently expelling the students involved in the October 18 assault of an Israeli\nHarvard Business School student, and suspending students involved in occupying\nuniversity buildings, as warranted by the facts of individual cases.\no The Harvard president and police chief must publicly clarify that the Harvard\nUniversity Police Department will enforce University rules and the law. Harvard\nmust also commit to cooperating in good faith with law enforcement.\n● Whistleblower Reporting and Protections. The University must immediately establish\nprocedures by which any Harvard affiliate can report noncompliance with the reforms\ndetailed in this letter to both university leadership and the federal government. Any such\nreporter shall be fully protected from any adverse actions for so reporting.\n● Transparency and Monitoring. The University shall make organizational changes to\nensure full transparency and cooperation with all federal regulators. No later than June 30,\n2025, and every quarter thereafter during the period in which reforms are being\nimplemented, which shall be at least until the end of 2028, the University shall submit to the\nfederal government a report—certified for accuracy—that documents its progress on the\nimplementation of the reforms detailed in this letter. The University must also, to the\nsatisfaction of the federal government, disclose the source and purpose of all foreign funds;\ncooperate with the federal government in a forensic audit of foreign funding sources and\nuses, including how that money was used by Harvard, its agents, and, to the extent available,\nthird parties acting on Harvard’s campus; report all requested immigration and related\ninformation to the United States Department of Homeland Security; and comply with all\nrequirements relating to the SEVIS system.\nWe expect your immediate cooperation in implementing these critical reforms that will enable\nHarvard to return to its original mission of innovative research and academic excellence.",
    "length": 14090,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Initial guidelines for using ChatGPT and other generative AI tools at Harvard",
    "url": "https://www.huit.harvard.edu/news/ai-guidelines",
    "text": "Generative AI Guidelines | Harvard University Information Technology[\nSkip to main contentarrow\\_circle\\_down\n] \nSearch\nSearchsearch\n[\n![Harvard University Information Technology] \n![Harvard University Information Technology] \n] \nmenucloseMenu\nSearch\nSearchsearch\n[\n![Harvard University Information Technology] \n![Harvard University Information Technology] \n] \n# Generative AI Guidelines\n# Guidelines for the use of Generative AI tools at Harvard\nPolicies &amp; guidelines\n![Woman works on her laptop in a library.] \n## Overview\nGenerative AI is a type of artificial intelligence that can learn from and mimic large amounts of data tocreate content such as text, images, music, videos, code, and more, based on inputs or prompts.The University supports responsible experimentation with[generative AI tools], but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. These guidelines are updated periodically.\n## Protect confidential data\nYou should not enter data[classified as confidential] (Level 2 and above, including non-public research data, finance, HR, student records, medical information, etc.) into publicly-available generative AI tools, in accordance with the University’s[Information Security Policy]. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties.\nLevel 2 and above confidential data must only be entered into generative AI tools that have been assessed and approved for such use by Harvard’s Information Security and Data Privacy office. See below for more information about approved tools.\n## Review content before publishing or sharing\nAI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called “hallucinations”) or may contain copyrighted material. You are responsible for any content that you publish or share that includes AI-generated material.\n## Adhere to local academic and administrative policies\nReview your School or Unit’s local policies around the use of generative AI. Many Schools have developed or updated policies around the use of generative AI in the classroom.[You can find links to local resources on the University’s generative AI website].\nFaculty should be clear with students they’re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed.\n## Be alert for phishing\nGenerative AI has made it easier for malicious actors to create sophisticated phishing emails and “deepfakes” (i.e., video or audio intended to convincingly mimic a person’s voice or physical appearance without their consent) at a far greater scale. Continue to[follow security best practices] and report suspicious messages to[phishing@harvard.edu].\n## Use approved tools for Harvard work\nHUIT and School IT providers have procured a range of generative AI tools with important contractual protections for use in Harvard work. These include security and privacy protections that ensure the tools are appropriate for use with certain types of confidential data, and assurances that the data entered will not be used to train vendor models.\n* [A list of available tools provided by HUIT can be found here]. More tools may be available from[your School IT provider].\n* AI meeting assistants should not be used in Harvard meetings, with the exception of approved tools with contractual protections. Consult the[AI Assistant Guidelines] for more information and how to manage unwanted AI assistants in meetings.\n* If you are considering procuring a generative AI tool not currently offered or have questions,[please contact HUIT]. All vendor generative AI tools must be[assessed for risk by Harvard's Information Security and Data Privacy office] prior to use in Harvard work.\n## Additional guidelines\n* [**AI Assistant Guidelines**]: Guidance on the use ofautomated meeting assistants (aka “AI note takers” or “bots”) in online meetings.\n* [**EU AI Act Prohibited Use Cases**]: Regulation on the use of AI technologies that may be developed or used in the European Union, or whose output may be used in the European Union.",
    "length": 4359,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "HGSE AI Policy | Office of the Registrar",
    "url": "https://registrar.gse.harvard.edu/learning/policies-forms/ai-policy",
    "text": "HGSE AI Policy | Office of the Registrar[\nSkip to main contentarrow\\_circle\\_down\n] \n[![Harvard Graduate School of Education]] \n[\nOffice of the Registrar\nHarvard Graduate School of Education\n] \nmenucloseMenu\nSearch\nSearchsearch\n[\nOffice of the Registrar\nHarvard Graduate School of Education\n] \n# HGSE AI Policy\n**HGSE POLICY ON STUDENT USE OF GENERATIVE ARTIFICIAL INTELLIGENCE IN ACADEMIC WORK**\n**Academic Year 2025-2026**\nGenerative Artificial Intelligence (AI) poses both great opportunities and great challenges for the field of education. Tools such as ChatGPT, DALL-E, and GitHub Copilot are having a profound influence on teaching and learning –and on your role as education practitioners and leaders. Your time at HGSE is an opportunity to learn to leverage such tools to produce more equitable access to and deeper engagement in education.\nHGSE encourages responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, copyright issues, the trustworthiness of the content they generate, and academic integrity. The implications of using generative AI for your own learning are equally salient. It might be possible to use this technology to complete some class assignments while doing little work yourself, but short cutting the process of thinking and writing in this way would rob you of the learning you came to HGSE to experience. At its best, generative AI can be like a tutor or thought partner with unlimited time to help you learn –but it should not be used to do the cognitive work for you, or else your own learning will be greatly diminished.\nThe following guidelines aim to ensure that, in your academic work at HGSE, you use generative AI when it can help you learn and not when it is a hindrance. If you have any doubt about whether a specific use of generative AI is permitted for an assignment or course, you are responsible for discussing it with your instructor prior to using it.\n1. Unless otherwise specified by your instructor, it is a violation of the HGSE Academic Integrity Policy to use generative AI to create all or part of an assignment for a course (e.g., a paper, memo, presentation, or short response) and submit it as your own. This rule parallels other rules. You may not ask another person to complete your assignment for a course. You may not copy from something someone else has created or re-write in your own words something someone else has written, without proper attribution.\n2. Permissible uses of generative AI in HGSE coursework include seeking clarification on concepts, brainstorming ideas, or generating scenarios that help contextualize what you are learning. For instance, it is fine to use AI-powered web search and to have “conversations” with tools like ChatGPT to help you explore ideas, refine your thinking, identify examples, and better understand course material. It is also acceptable to use generative AI to draft emails to instructors, students, and others in the HGSE community that are not being submitted as coursework.\n3. For any permitted use of GenAI tools, you must acknowledge and document that use in your assignment submission by explaining what tool(s) you used, prompts you provided (if applicable), and how you integrated the output into your work. If you cite directly from the tool, use proper citation format to credit the source. For more details and examples, see these APA guidelines:[How to cite ChatGPT].\n4. Keep in mind that the information provided by generative AI tools like ChatGPT is generated from unverified crowd-sourced information. Large language models can produce false claims or “hallucinations” and will regenerate any biases in the corpus of texts on which they are trained. You therefore should not trust the information as if it were equivalent to published research. You are ultimately responsible for the accuracy of the work you submit.\n5. The use of generative AI may also have implications for the protection of your own intellectual property. For example, if you upload your own original content to a generative AI tool, that content may become part of the tool’s models, which others may encounter and use. Conversely, if you use generative AI to develop your own original work, it may unexpectedly include others' copyrighted material.\n6. Certain uses of AI also infringe on copyright laws applicable to U.S. universities or contravene existing expectations for student conduct in HGSE courses. For example, the HGSE Student Handbook notes that “Students may not post, publish, sell, or otherwise publicly distribute course materials without the written permission of the course instructor. Such materials include, but are not limited to, the following: lecture slides, video, or audio recordings, assignments, problem sets, examinations, other students’ work, and answer keys. Students may not make recordings of course material for their own use without written permission of the instructor.” In keeping with these guidelines:\n7. Uploading any substantial course content —including text, video, readings, discussion-board pages, or audio recordings —is only allowable through the[Harvard-approved AI Sandbox], which ensures data entered is kept in a secure environment and not used to train public AI tools. The Sandbox is available through individual courses; if you have questions about the Sandbox, discuss with your instructor.\n8. It is forbidden to make your own recording of any course meetings, with or without AI tool integrations. If you require or would prefer that course meetings be recorded, discuss this request with your instructor. More broadly, if you require AI technology as part of an assistive technology solution to enable you to participate fully in the course, you must coordinate your usage with the Office of Student Affairs.\n9. Given the wide range of learning goals in courses at HGSE, individual instructors may create course-specific policies that differ from and supersede these guidelines. Again, if you have any doubt about whether a specific use of generative AI is permitted for an assignment or course, you are responsible for discussing it with your instructor prior to using it.\nNew ways of teaching and learning will emerge as generative AI becomes increasingly ubiquitous and robust. We thus anticipate that this policy will also evolve, with feedback from students and instructors.\n##### Featured Links\n[arrow\\_forwardmy.Harvard] [arrow\\_forwardStudent Handbook] [arrow\\_forwardRecords Request] [arrow\\_forwardRegistrar Communications]",
    "length": 6614,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "D^3 and Microsoft Launch Accelerated AI Research Initiative | Digital Data Design Institute at Harvard",
    "url": "https://d3.harvard.edu/d3-and-microsoft-launch-accelerated-ai-research-initiative/",
    "text": "D^3 and Microsoft Launch Accelerated AI Research Initiative | Digital Data Design Institute at Harvard\n[Skip to content] \n[] [Visit hbs.edu] \nSearch for:\n[] \nSearch for:Search\nMenu\n# D^3 and Microsoft Launch Accelerated AI Research Initiative\n[![Avatar photo] Written by\nD^3 Institute\n]![] November 17, 2025\n*Harvard Business School faculty, in collaboration with Microsoft andits**clients, will study human-AI work, publish evidence-based blueprints, and deliver custom workshops for executives to rapidly reinvent global businesses as Frontier Firms;Eli Lilly and Company*,*EY, Lumen Technologies, and Nestlé among 14 organizations in the inaugural cohort.*\nBOSTON,November18, 2025 –Faculty at the[Digital Data Design Institute at Harvard] today announced the launch of the Frontier Firm AI Initiative, a collaboration with Microsoft anditsclients that aims to deepen understanding and accelerate the practice of building Frontier Firms.As defined by theDigital Data Design Institute at Harvard,Frontier Firms are human led, agent operated organizations that put AI at the core of their strategy to transform operations, accelerate innovation, and amplify human capacity. The research intothe journey ofFrontier Firms will be a catalyst for redefining long-held paradigms of work. Hosted by the Digital Data Design Institute at Harvard, this Initiative will develop applied research on human-AI collaboration, upskill global C-suite leadership, and deliver new insights and tools to disrupt conventional business thinking.\nTheChair of Digital Data Design Instituteat Harvardand Dorothy and Michael Hintze Professor of Business Administrationat Harvard Business School (HBS), Karim Lakhani, will be joined with fellow HBSfaculty members Iavor Bojinov, Rafaella Sadun, Rem Koning, Shunyuan Zhang, and Kadeem Noray to drive forward the portfolio of experiments. Their efforts will focus in five key areas:future-state operating models for effective human-AI collaboration in core business functions, agent boss (an initial management theory for AI agents), agentic workflows, building a Frontier Firm radar based on AI-native startups, and the effect of new technologies on firm demands for skills and labor.\n> “Executives that go all in on AI without a clear path forward risk falling into a frustrating cycle of pilots that don’t deliver value and have no impact, with this Initiative, we are collaborating with trailblazing organizations who are pushing the limits of agentic AI to deliver value to their customers, reimagine work patterns, reinvent operations, and generate new business models. Together in collaboration with Microsoft and&nbsp;its&nbsp;customers, we aim to create rigorous, evidence-based blueprints for high-performing human-AI workplaces, bridging the gap between ambition and true competitive advantage.”\n> > Karim Lakhani, The&nbsp;Chair of Digital Data Design Institute&nbsp;at Harvard&nbsp;and Dorothy and Michael Hintze Professor of Business Administration&nbsp;at Harvard Business School (HBS)\nJared Spatro, Chief Marketing Officer, AI at Work at Microsoft articulated, “It’s no longer a question of ‘if’ AI is right for business—leaders today are grappling with ‘how’ to become a Frontier Firm. This Frontier Firm AI Initiative is addressing a critical gap in the marketplace, giving leaders the education and practical tools they need to help their people and organizations navigate this transformation.”\nThe inaugural cohort of organizations embarking on the path to become Frontier Firms include Barclays, BNY, Clifford Chance, DuPont, Eaton, Eli Lilly and Company, EY, GHD, Kantar, Levi Strauss &amp; Co., Lumen Technologies, Mastercard, Nestlé, and others. Organizations will participate in large-scale field-based experiments in AI that explore AI-first work patterns as well as participate in custom workshops that translate the results of the research into practical guidelines for organizations innovating their operating models with AI.\n> &#8220;AI has given business new ways to create value and a thousand new ways to get lost doing it. Academia’s role is to chart the tide so leaders can navigate with more reliable information about the surrounding environment.&nbsp;&nbsp;We’re grateful for our organizational relationships, which make it possible to curate this knowledge at a moment when best practices are urgently needed yet still unwritten&#8221;&nbsp;&nbsp;\n> > Jen Stave, the founding Director of the Digital Data Design Institute at Harvard Business School.\n[Learn More] \n**About Digital Data Design Institute at Harvard**\nThe Digital Data Design Institute at Harvard (D^3) provides research-driven insights, accessible to anyone in the world, on using AI and digital technologies to advance business and society. Emerging from Harvard Business School under the leadership of Dean Srikant Datar and founded on the premise that AI technology is only half of the answer and that businesses must also revamp their processes to harness AI&#8217;s potential, D^3 is made up of a global network of multidisciplinary faculty, researchers and scientists, business leaders, and entrepreneurs. For more information, please visit d3.harvard.edu.\nEngage With Us\n## Join Our Community\nReady to dive deeper with the Digital Data Design Institute at Harvard? Subscribe to our newsletter, contribute to the conversation and begin to invent the future for yourself, your business and society as a whole.\nEmail\\*\nSubscribe![Loading]",
    "length": 5467,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Guidelines for Using AI tools at Harvard",
    "url": "https://hcsra.sph.harvard.edu/news/guidelines-using-ai-tools-harvard",
    "text": "# Connecting to\n\nSign in with your account to access SPH Intranet\n\n## Your OneDrive version is not supported\n\nUpgrade now by installing the OneDrive for Business Next Generation Sync Client to login to Okta\n\n[Learn how to upgrade] \n\n## Cookies are required\n\nCookies are disabled on your browser. Please enable Cookies and refresh this page.\n\n[Refresh] \n\n## The page has timed out\n\nIf this page does not reload automatically, please refresh your browser.",
    "length": 453,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Harvard University - Message",
    "url": "https://links.repoint.harvard.edu/servlet/MailView?j=Mjg4Mzc0MTA0OQS2&ms=MzU5MDA2NDcS1&mt=1&r=MjMxNTEwNDEzNTYS1&rt=0",
    "text": "Harvard University - Message\nThis message contains graphics. If you do not see the graphics,[click here to view].\n![The Office of President] \nDear Members of the Harvard Community,\nOver the course of the past week, the federal government has taken several actions following Harvard&rsquo;s refusal to comply with its illegal demands. Although some members of the administration have said their April 11 letter was sent by mistake, other statements and their actions suggest otherwise. Doubling down on the letter&rsquo;s sweeping and intrusive demands&mdash;which would impose unprecedented and improper control over the University&mdash;the government has, in addition to the initial freeze of $2.2 billion in funding, considered taking steps to freeze an additional $1 billion in grants, initiated numerous investigations of Harvard&rsquo;s operations, threatened the education of international students, and announced that it is considering a revocation of Harvard&rsquo;s 501(c)(3) tax-exempt status. These actions have stark real-life consequences for patients, students, faculty, staff, researchers, and the standing of American higher education in the world.\nMoments ago, we filed a lawsuit to halt the funding freeze because it is unlawful and beyond the government&rsquo;s authority. I encourage you to[read our complaint].\nThe consequences of the government&rsquo;s overreach will be severe and long-lasting. Research that the government has put in jeopardy includes efforts to improve the prospects of children who survive cancer, to understand at the molecular level how cancer spreads throughout the body, to predict the spread of infectious disease outbreaks, and to ease the pain of soldiers wounded on the battlefield. As opportunities to reduce the risk of multiple sclerosis, Alzheimer&rsquo;s disease, and Parkinson&#39;s disease are on the horizon, the government is slamming on the brakes. The victims will be future patients and their loved ones who will suffer the heartbreak of illnesses that might have been prevented or treated more effectively. Indiscriminately slashing medical, scientific, and technological research undermines the nation&rsquo;s ability to save American lives, foster American success, and maintain America&rsquo;s position as a global leader in innovation.\nThe government has cited the University&rsquo;s response to antisemitism as a justification for its unlawful action. As a Jew and as an American, I know very well that there are valid concerns about rising antisemitism. To address it effectively requires understanding, intention, and vigilance. Harvard takes that work seriously. We will continue to fight hate with the urgency it demands as we fully comply with our obligations under the law. That is not only our legal responsibility. It is our moral imperative.\nBefore taking punitive action, the law requires that the federal government engage with us about the ways we are fighting and will continue to fight antisemitism.Instead, the government&rsquo;&rsquo;s April 11 demands seek to control whom we hire and what we teach. Today, we stand for the values that have made American higher education a beacon for the world. We stand for the truth that colleges and universities across the country can embrace and honor their legal obligations and best fulfill their essential role in society without improper government intrusion. That is how we achieve academic excellence, safeguard open inquiry and freedom of speech, and conduct pioneering research&mdash;&mdash;and how we advance the boundless exploration that propels our nation and its people into a better future.\nWe acknowledge that we have unfinished business. We need to ensure that the University lives up to its ideals by taking concrete steps to reaffirm a culture of free inquiry, viewpoint diversity, and academic exploration; making changes to our disciplinary systems so they will be more consistent and more effective in ensuring that our students, faculty, and staff take responsibility for their actions; implementing measures to ensure that all members of our community are safe and respected; and adopting important adjustments to the ways we build community&mdash;continuing to focus on individuals and their unique characteristics rather than their race. In the days ahead, I will say more about our progress in each of these areas.\nWe will also soon release the reports of the Task Force on Combating Antisemitism and Anti-Israeli Bias and the Task Force on Combating Anti-Muslim, Anti-Arab, and Anti-Palestinian Bias. I established these groups last year as part of our efforts to address intolerance in our community. The reports are hard-hitting and painful. They also include recommendations with concrete plans for implementation, which we welcome and embrace. No one in our community should experience bias, intolerance, or bigotry. We believe adoption of the recommendations and other measures will go far toward eradicating those evils on our campus.\nThe time ahead will demand much from each of us, but I am as confident as ever in our ability to meet our challenges with integrity and resolve, our minds set on the work before us and our hearts committed to the future of our beloved University.\nSincerely,\nAlan M. Garber\n&copy;&copy;2025President and Fellows of Harvard College |[Harvard.edu] \n|\n|\n![]",
    "length": 5347,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Automating Content Policy | Berkman Klein Center",
    "url": "https://cyber.harvard.edu/events/automating-content-policy",
    "text": "Automating Content Policy | Berkman Klein Center\n[Skip to the main content] \nMenu[Harvard Berkman Klein Center Logo] \n[Harvard Berkman Klein Center Logo] \nSearcho-icon\\_\\_search\n![Automating Content Policy] \n# Automating Content Policy\nFall Speaker Series\n![Dave Willner] \n[\nDave Willner\n] \n![Meg Marco] \n[\nMeg Marco\n] \nShare To[icon--facebook] [icon--twitter] \n[![Embedded YouTube video]] \nAI is no longer just moderating individual posts —it is learning how to interpret and enforce policy itself. Dave Willner —who has led trust and safety teams at Facebook, Airbnb, and OpenAI —joins journalist Meg Marco for a conversation about the shifting terrain of moderation in the age of generative AI.\nFrom the tipping points of past technologies, like the cell phone, to today’s new “policy-rewriting machines” that can run millions of classifications in an hour, Willner and Marco will explore how automation is changing the scale, speed, and stakes of online governance.\nNew systems can ingest a company’s policy documents, apply them to vast datasets, and make millions of classifications in hours. In doing so, they both replicate the human task of drawing boundaries —deciding what falls inside or outside the rules —and extend that task far beyond traditional concerns like hate speech or misinformation.\n### Speakers\n#### Dave Willner\nDave Willner is a Co-Founder at Zentropi, the company behind CoPE, a best-in-class small language model capable of accurate and steerable content classification.\nHe was previously a non-resident fellow in Stanford Cyber Policy Center, and worked in industry as Head of Trust &amp; Safety at OpenAI, as Head of Community Policy at Airbnb, and as Head of Content Policy at Meta (formerly Facebook).\n#### Meg Marco\nMeg is the Senior Director of the Applied Social Media Lab, focusing on building public interest technology that helps make information available and understandable to researchers, journalists, civil society organizations and the general public. She has held senior editorial positions at WIRED, ProPublica and The Wall Street Journal.\n**Past Event**Wednesday, October 22, 2025\n**Time**\n12:30 PM - 1:30 PM ET\n**Location**\n1557 Massachusetts Ave.\n5th Floor\nCambridge,MA02138US\n*Last updatedNov 17, 2025*\n## Events03\n[] \nOct 1, 2025 @ 12:30 PM\n### [Belief, Uncertainty, and Truth in Language Models] \nFall Speaker Series\nWhat does it mean for a language model to “know” something—and how should it communicate uncertainty to the people who use it? In this talk, Jacob Andreas, Associate Professor of…\n[] \nSep 24, 2025 @ 12:30 PM\n### [What Is Intelligence? Lessons from AI About Evolution, Computing, and Minds] \nFall Speaker Series\n&amp;nbsp;“Life” and “intelligence” are terms with heavily contested meanings.This discussion will offer a novel, unified perspective on both, as described in Blaise Agüera y Arcas’…\n[] \nNov 5, 2025 @ 12:30 PM\n### [AI Triad: A Dialogue Across Differences] \nFall Speaker Series\n&amp;nbsp;Artificial intelligence isn’t just a technology—it’s a battleground of competing values, incentives, and worldviews.Accelerationists see AI as a force for human progress,…",
    "length": 3139,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Kempner Institute for the Study of Natural and Artificial Intelligence Open Science Policies",
    "url": "http://kempnerinstitute.harvard.edu/open-science-policies/",
    "text": "Kempner Institute for the Study of Natural and Artificial Intelligence Open Science Policies - Kempner Institute\n[Skip to content] \n[![Kempner Institute for the study of natural and artificial intelligence at Harvard University]![Kempner Institute Logo]] \n[![Kempner Institute for the study of natural and artificial intelligence at Harvard University]![Kempner Institute Logo]] [![Harvard Shield]] MenuClose\n[![Kempner Institute Logo]] \n[![Harvard University Logo]] \n# Kempner Institute for the Study of Natural and Artificial Intelligence Open Science Policies\nThe Kempner Institute seeks to build a community of scholars who work across boundaries and fields to advance our understanding of Natural and Artificial Intelligence. To support this endeavor, we commit to making research funded by the Kempner Institute as widely and easily available as possible.\nPrior to receiving any funding from the Kemper Institute, all recipients will be required to sign the Harvard University Participation Agreement.\nThe requirements below apply only to intellectual property and publications for which the investigator is a primary or corresponding author or inventor and the research was funded in whole or in part by the Kempner Institute. Work, including research and code development, is not considered to be funded by the Kempner merely because the investigator or researcher receives salary or other general support from the Gift.\nIf the recipient receives research funding the following policies will apply:\n### Data Access\nWe are committed to developing and using platforms that disseminate data openly and freely. In support of these goals, any datasets either curated or generated using the Kempner-funding will be made publicly available**on or before first publication**, including preprint publication, and easily accessible online under an Open Definition-Conformant License.\nThat said, we understand the need for subsets of data to be protected or provided with controlled access. In these cases data shall be handled in accordance with national and international standards, including all privacy regulations (including, without limitation, the Health Insurance Portability and Accountability Act of 1996 and the Health Information Technology for Economic Clinical Health Act, as far as applicable).\nAll raw data (sequencing, imaging, etc.) and metadata not otherwise restricted should be deposited into a publicly accessible repository.\n### Intellectual Property\nWe are committed to ensuring that Kempner funded technologies have the broadest reach and impact. In support of that goal, and in alignment with existing Harvard University policy, intellectual property generated using Kempner funding shall be made freely available for all academic and noncommercial use.\nKempner-funded projects may result in the creation of software and other works of authorship (“Developed Software”) and may also result in other intellectual property, both non-software inventions and patentable inventions, as described below.\n### Developed Software\nWhenever possible, to encourage sharing and reuse, code produced for new software and tools will be made available through open-source licenses.\nTo support this goal, any third-party or pre-existing Code used for a Kempner-funded research project should be licensed under the most permissive terms possible and used or incorporated in such a way as it will allow for further distribution.\nWhere possible, all code will be developed in the open using a code sharing site like GitHub from the start of the Kempner funded project to promote open collaboration.\nAll Kempner Developed Code shall be released under a permissive open-source license.\n### Patentable Inventions\nWe seek the broadest distribution and dissemination of technology developed from Kempner funded research. In some cases, commercialization of intellectual property rights in the form of patents will provide the best route for dissemination.\nHarvard’s policies and procedures for intellectual property management will be followed, and in all cases, non-exclusive patent licensing shall be given preference over exclusive licensing.\nAll patentable and non-patentable intellectual property shall be made freely and openly available for all academic use and openly available for non-commercial use, in alignment with Harvard policy. Kempner funded research will also be made openly available for non-commercial use by for-profit companies.\n### Reagent Sharing\nWe commit to reproducible science. In support of that effort, all data and replicable materials shall be made available, in a timely manner and on reasonable, cost reimbursable basis,**from the date of publication.**\nWe encourage the use of existing community repositories where possible. The requirement for sharing applies to clones, and to transgenic organisms and cell lines including monoclonal antibodies.\n### Publication\nWe support full publication and public availability of research findings without conditions or restrictions on academic and publication freedom.\nAll manuscripts for any publications that were funded in whole or in part by the Kempner Institute shall be submitted as preprints to bioRxiv, or a similar service for sharing preprints,**before or upon first submission to a journal.**\nExperimental protocols shall be made publicly available through a protocol sharing service such as https://protocols.io. If there is some reason that will make this impossible or counterproductive the investigator shall contact Kempner Institute Directors.\n### Acknowledgments\nAcknowledgement of support will be included in publications referencing research sponsored under the Kempner Institute.\nWe suggest the following wording: “This work has been made possible in part by a gift from the Chan Zuckerberg Initiative Foundation to establish the Kempner Institute for the Study of Natural and Artificial Intelligence.”\n*Drafted: 12/12/20222*\n*Approved: 1/24/2023*\n!",
    "length": 5944,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Federal Lawsuits - Federal Lawsuits",
    "url": "https://www.harvard.edu/federal-lawsuits/",
    "text": "Federal Lawsuits - Federal Lawsuits\n[Skip to main content] \n[\nFederal Lawsuits\n] \nMain Menu**\n# Federal Lawsuits\n## UpholdingOurValues, DefendingOurUniversity\n### Research Funding\nLearn more\n[Learn more] \n### International Students and Scholars\nLearn more\n[Learn more] \n## &#8220;No government—regardless of which party is in power—should dictate what private universities can teach, whom they can admit and hire, and which areas of study and inquiry they can pursue.&#8221;\nHarvard President Alan Garber\n## By the numbers\n* 402\n**Innovations reported by Harvard researchers in the 2024 fiscal year**\n* 155\n**U.S. patents issued in 2024**\n* $526 million\n**Research funded[directly by the University] in 2024**\n* 27.2%\n**Of Harvard&#8217;s student body are international students**\n* 147\n**Countries that Harvard&#8217;s international students come from**\n* 4,000+\n**International scholars are hosted by Harvard annually, more than any other university in the United States**\n![A dropper going into a test tube] \n## Research Funding\nResearch at Harvard—from medicine to technology to education and business—touches countless lives, moving us closer to disease cures, next-generation technology, and a more secure future for millions of people.\n[Learn how Research Powers Progress] \n![sets of hands covered in colorful dust.] \n## Supporting international students and scholars\nOur thousands of international students, who hail from more than 140 countries, enrich the University community—and this nation—immeasurably with their presence and contributions.\n[Learn more about the Harvard International Office] \n## The Latest\n![An illustration of people in a lab creating new drugs] \n* In the Media### Trump slashed funding for universities that helped create these vital drugs\nThe Washington PostOctober 7, 2025\n[Trump slashed funding for universities that helped create these vital drugs] \n* In the Media### After funding halt, Harvard nurses health study scrambles to save 50 years of samples\nWCVB BostonOctober 6, 2025\n![A person in a lab taking samples out of cold storage] [After funding halt, Harvard nurses health study scrambles to save 50 years of samples] \n* News### Court victory for Harvard in research funding fight\nHarvard GazetteSeptember 4, 2025\n![Widener Library on Harvard campus] [Court victory for Harvard in research funding fight] \n* Statements### An Update on Our Litigation\nSeptember 4, 2025\n![Harvard campus against blue skies, and the Charles River.] [An Update on Our Litigation] \n* In the Media### Judge reverses Trump administration’s cuts of billions of dollars to Harvard University\nAssociated PressSeptember 3, 2025\n![A tower with a red roof on the Charles River.] [Judge reverses Trump administration’s cuts of billions of dollars to Harvard University] \n* News### Funding cuts upend projects piecing together saga of human history\nHarvard GazetteAugust 26, 2025\n![Christina Warinner in a museum] [Funding cuts upend projects piecing together saga of human history]",
    "length": 2995,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Dataverse Joins NIH in Increasing Access to Biomedical Data | The Institute for Quantitative Social Science",
    "url": "https://www.iq.harvard.edu/news/dataverse-joins-nih-data-repository-initiative",
    "text": "Dataverse Joins NIH in Increasing Access to Biomedical Data | The Institute for Quantitative Social Science[\nSkip to main contentarrow\\_circle\\_down\n] \n[![Harvard University]] \n[\n![The Institute for Quantitative Social Science] \n![The Institute for Quantitative Social Science] \n] \nmenucloseMenu\nSearch\nSearchsearch\n[\n![The Institute for Quantitative Social Science] \n![The Institute for Quantitative Social Science] \n] \n# Dataverse Joins NIH in Increasing Access to Biomedical Data\nJanuary 26, 2022\nThe Dataverse Project at IQSS is joining the[Office of Data Science Strategy (ODSS) at the National Institutes of Health] (NIH) and five other data repositoriesin launching a new data curation, sharing, and interoperability initiative. Through this collaboration, the Generalist Repository Ecosystem Initiative (GREI), the Dataverse Project plans to facilitate access to NIH-funded data by building on the existing Harvard Dataverse Repository. In order to supplement the NIH’s existing domain-specific repositories, the goal of the GREI is to expand its data ecosystem into additional repositories so that researchers can more easily and effectively find and share data from studies funded by the NIH.\n[The Dataverse Project] will expand its services and teams–including the UX/UI, development, research computing, and data curation and management teams–using recently awarded NIH funding. The funding is designed to be flexible, and will allow the Dataverse Project team to work with NIH and the other collaborating repositories to determine the highest priority areas of focus and impact, and then quickly build new workflows to support biomedical researchers. Some areas being explored are:\n* **Supporting very large datasets**by integrating metadata records in the repository with the data in research computing storage, allowing data to be discovered in the Harvard Dataverse and viewed, explored, and analyzed directly in the research computing environment;\n* **Increasing support for biomedical and cross-domain metadata standards**and controlled vocabularies–taking advantage of the Dataverse Project’s extensive support for metadata standards and additional custom metadata;\n* **Facilitating researchers’ efforts to share and publish**their entire workflows or containers that describe the main transformations and analysis of the data, following the FAIR (Findable, Accessible, Interoperable, and Reusable) principles;\n* **Improving the existing harvesting functionality**in the Dataverse software based on the Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH) standard, and coordinate with other repository packaging standards to share or move metadata and data;\n* **Standardizing usage metrics**within the repository and across other repositories and coordinating the implementation of the metrics with other repositories, so that the values and assumptions are comparable;\n* **Supporting the sharing and discovery of sensitive data**using privacy-preserving tools such as those from[the OpenDP project at IQSS];\n* **Improving the user experience and interface**(UX&amp;UI) and Application Programming Interfaces (APIs) for depositing, viewing, and accessing data and metadata in the repository; and\n* **Providing curation services, outreach, and training**for managing and sharing NIH-funded research assets in the repository; the Dataverse team at IQSS will provide overall guidance and support in UX&amp;&amp;UI efforts, technical design, and implementation.\nLaunched in 2006, the[Harvard Dataverse Repository] is powered by the Dataverse open-source software, which was developed at the Institute for Quantitative Social Science at Harvard University. Of the 76 Dataverse repositories that are now deployed worldwide the Harvard Dataverse Repository is the largest, with more than 100,000 datasets containing about 1 million files available to users who continue to share, explore, cite, and analyze every day.\nThe Dataverse Project was founded by IQSS Director Gary King in 2006. Its software platform provides a preservation and archival infrastructure, and allows researchers to share, keep control of, and get recognition for their data through an easy to access web browser interface.\nFor more information about the Dataverse Project and its platform, visit:[https://dataverse.org/] \nThe Office of Data Science Strategy leads implementation of the NIH Strategic Plan for Data Science through scientific, technical, and operational collaboration with the institutes, centers, and offices that comprise the National Institutes of Health. More information about the Generalist Repository Ecosystem Initiative (GREI) can be found in[the recent ODSS announcement on the initiative].\nSee also:\n* [Institute for Quantitative Social Science] \nShare on:\n* [Facebook] \n* [Twitter] \n* [Linkedin] \n## Latest News\n[### The art of the online interface\n] \nFebruary 02, 2026\n![Sid Research Computing] \n[### Why self-appraisals may not be best way to judge job performance\n] \nDecember 17, 2025\n![Iris Bohnet] \n[### Science needs contrarians, and contrarians need support\n] \nDecember 10, 2025\n![Richard McNally at work in his office] \n[More Newsarrow\\_circle\\_right] \n## Do you have news to share?\nWe're always interested in hearing the latest work and accomplishments fromIQSS affiliates and would be happyto help you share your news. If you have any updates, let us know!\n[arrow\\_forwardSubmit affiliate news]",
    "length": 5433,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "The Antitrust Case Against AI Overviews - Harvard Journal of Law & Technology",
    "url": "https://jolt.law.harvard.edu/digest/the-antitrust-case-against-ai-overviews",
    "text": "The Antitrust Case Against AI Overviews - Harvard Journal of Law &amp; Technology\n[digest-logo] \n[Login] \nSearch by Category:Select a category...AntitrustArtificial IntelligenceCopyrightCybersecurityCommentaryNotesReportsFederal Circuit CommentFirst AmendmentFlash DigestFourth AmendmentNational SecurityPatentPrivacyTelecommunicationsTrademarkTrade Secrets\n#### [Submit to Digest] \n![The Antitrust Case Against AI Overviews] \n# The Antitrust Case Against AI Overviews\n## By Madhavi Singh - Edited By Shriya Srikanth\nOctober 06, 2025\n[Antitrust] [Artificial Intelligence] [Commentary] \n*Madhavi Singh is the Deputy Director of the Thurman Arnold Project and a Resident Fellow at the Information Society Project at Yale Law School. She was previously a Research Associate at the National University of Singapore and a Visiting Lecturer at the National Law School of India University. She holds law degrees from Harvard (LL.M.), Oxford (BCL), and the National Law School of India University (B.A. LL.B.).*\n**Introduction**\nAI-powered answer engines (in the form of AI chatbots and other generative features in search) are fast replacing search engines as the gateway to the internet. Google understands these winds of change and is trying to avoid becoming[redundant] in this rapidly changing market by[aggressively building] a flurry of AI products like AI Overviews, AI Mode, and Gemini.[] [It is also dedicating] [a large (and increasing)] percentage of the search engine results page to AI Overviews. Google’s acquisition of[DeepMind], its[pseudo-acquisition] of Character AI, and its large investments in[Anthropic] and many others are also part of its bigger strategy to[co-opt creative destruction] rather than succumb to it.\nAlthough other Big Tech companies are also racing to expand their AI portfolios, what sets Google apart is that it has already been found to have engaged in illegal monopolization of the search market.[[1]] We[] are now witnessing it leveraging this illegally maintained monopoly position to gain a foothold in and monopolize the market for AI-powered answer engines too. This essay argues that such acts of leveraging constitute an antitrust violation. Antitrust enforcement should strive to prevent this second sin, which would compound Google’s first and facilitate the continuity of its monopolization strategies.\n[**Monopolization**] \nThe offense of monopolization under section 2 of the Sherman Act requires proof of two elements: (1) the defendant must possess monopoly power in the relevant market; and (2) there must be willful acquisition or maintenance of monopoly power through exclusionary conduct.[[2]] In the recent Google search antitrust case, the District Court for the District of Columbia provided a detailed analysis establishing that Google holds monopoly power in the relevant market for general search services.[[3]] As the court has comprehensively addressed this issue, the article does not examine it further.\nThe second element of proving exclusionary conduct under a rule of reason analysis consists of a four-step burden-shifting framework: first, the plaintiff bears the burden to show anti-competitive effect; second, the burden shifts to the defendant to prove the existence of pro-competitive justifications; third, the burden shifts back to the plaintiff to demonstrate that these procompetitive objectives could be achieved through less restrictive means; and finally, the court balances the conduct’s procompetitive and anticompetitive effects.[[4]] Google’s exclusionary conduct consists of denying publishers and content creators the[3 Cs (consent, credit and compensation]). Google[scrapes websites] without consent or compensation to train its AI models, and publishers have no recourse to resist such behaviour, given Google’s criticality in ensuring the discovery and distribution of their content. It then uses this non-consensually scraped data to generate AI-powered responses, which sometimes[fail to attribute or misattribute] sources. Finally, it[prominently places] these AI-generated answers at the top of the search results page, assigning them an[ever-expanding] percentage of the screen.\nAnti-competitive effects\nThe anti-competitive effects of this exclusionary conduct are fast becoming apparent. The number of zero-click searches has[increased] significantly as more queries are being answered through AI Overviews. According to[one study], about 80% of search users rely on AI summaries at least 40% of the time, and about 60% of searches result in zero clicks. Although AI summaries have started including some citations to sources,[click-through rates] have still[drastically declined]. The growing popularity of AI Overviews and chatbots as a mainstream way of answering queries has been accompanied by a concomitant[decline] in[website traffic] (by almost 34.5%) and[revenue for publishers]. This shift from a search-driven web to an AI-driven web has made it[extremely difficult] for creators to monetize their content and threatens to break the internet. Publishers who were already enfeebled in an ad-driven ecosystem where Google and Facebook controlled discovery, distribution and monetization of content, risk being further crippled by the proliferation of AI Overviews.\nPro-competitive justifications\nUndoubtedly, Google will offer some pro-competitive justifications for its exclusionary conduct. For example, it could try to justify the integration of AI Mode and AI Overview as a simple product design change aimed at improving the functionality and value of its search function by adding new features. Notably, Microsoft in its antitrust case advanced a similar argument that the various acts to integrate Internet Explorer in the Windows Operating System were a design choice or product improvement.[[5]] However, the court rejected this argument, noting that a monopolist’s design choices are not*per se*lawful and would still be subject to antitrust scrutiny. Further, Google could argue that the non-consensual scraping of publisher data (some categories of which have recently been deemed to be “fair use” under copyright law)[[6]] also has pro-competitive justifications, since such data scraping is the*sine qua non*for training generative AI models, which in turn improve consumer experience and increase consumer welfare.\nProof of any pro-competitive effects would then have to be assessed against the benchmark of whether these could be achieved through less restrictive means, and a final step of balancing the pro and anti-competitive effects.[[7]] All of these, however, are fact-intensive exercises requiring large swathes of empirical and other evidence and therefore, could only get a fair shake at trial. At this stage, though the anti-competitive effects of Google’s exclusionary conduct are abundantly clear and therefore, the threshold for a regulator to investigate this matter has been reached.\n**Tying**\nAnother theory of harm which could encapsulate Google’s exclusionary conduct is tying. Tying cases can be brought under section 1 or section 2 of the Sherman Act (or under section 3 of the Clayton Act, in case of physical goods).[[8]] This must be accompanied by the caveat that tying law in the US is a wreck and needs to be[reconceptualized] to reflect the realities of the digital age. Indeed, the tying claim in both*Microsoft*and*Epic*cases were unsuccessful, even though the same conduct was found illegal under the monopolization claim.[[9]] Nevertheless, from that jurisprudential wreckage, we can extract the following elements of a typical tying claim: (1) the defendant tied together the sale of two*distinct*products or services; (2) the sale of the tying product is*conditioned*on the sale of the tied product; (3) the seller has*appreciable economic power*in the tying product market; and (4) the tying affects a*not insubstantial volume of commerce*in the tied product market.[[10]] The bundling of AI-powered answer engines (such as AI Mode and AI Overview) with Google’s search engine satisfies all four elements. Evidence of the final two elements, namely, market power and anticompetitive effects, has been discussed above.\nDistinct products\nTo ascertain the distinctiveness of the products, courts apply a consumer-demand test to assess whether there is sufficient consumer demand to make it efficient for a firm to offer the two products separately.[[11]] In this case, the tying product i.e., general search services has been offered as a standalone product for a long time as exemplified by platforms like Google, Bing and DuckDuckGo. Similarly, there are several providers of the tied product i.e., AI-powered answer engines like ChatGPT, Perplexity, and Claude, that provide these as a standalone product. Indeed, Google itself offers Gemini as a standalone product, in addition to integrating it into Google search through AI Overview and AI Mode. Additionally, the court in the*Google Search*case considered whether generative AI (or AI-powered answer engines) serve as substitutes for general search. It concluded that general search services constitute a distinct relevant market, noting that generative AI has not yet eliminated its need.[[12]] Thus, the two are distinct products which satisfy the first prong of the tying test.\nConditioning/ forcing\nThe second prong of tying, which requires that the sale of the tying product is*conditioned*on the sale of the tied product, is trickier. It is clear that Google bundles both products together; that is, Google’s search feature automatically produces an AI Overview. The AI Mode button is also[available by default] on the Google search page (just below the search bar alongside the Images, Videos, News, and other buttons) in the countries in which the feature has been rolled out. Unfortunately, some courts have required proof of “forcing” or “coercion” for a tying arrangement to be deemed illegal.[[13]] Courts vary in their standards for establishing coercion: some consider proof of market power in the tying market sufficient;[[14]] others examine whether the bundle was the buyer’s “only viable economic option;”[[15]] and still others assess voluntariness based on whether the buyer had knowledge of and accepted the tie.[[16]] \nThere are still unanswered questions about whether modern manifestations of tying in the form of default placement, product integration or design nudges are sufficient to establish conditional sale/ coercion or whether evidence of something more forceful is required. For instance, Google could argue that there is no coercion (narrowly construed) because users could simply scroll past the AI Overview on the results page or choose not to use the AI Mode. This would be a very narrow construction of the requirement, though, because there is ample evidence to indicate that even without express coercion, subtle behavioral manipulations like nudges, default placements, etc. have the same effect. As discussed above, even though users could scroll past the AI Overview, its premium placement at the top of the results page invariably results in[lower click-through rates] for all websites. Fortunately, as witnessed in the*Google search*case, courts have started becoming sophisticated in their analysis of subtle behavioural manipulation and accounting for concepts like default bias, status quo bias, stickiness, etc. in assessing the effects of exclusionary conduct.[[17]] \nOther jurisdictions like the EU have adapted their law to accommodate this: in the EU, if consumers do not have the choice to obtain the tying product without the tied product, then that is sufficient to show conditioning or coercion.[[18]] Additionally, instances of technological tying and bundling through product design and integration have become so pervasive that antitrust law must eventually confront how tying doctrine should be[adapted] to address these developments. The time is therefore ripe to align tying law with technological realities, and this case presents an ideal opportunity to do so.\n**Echoes of Microsoft**\nAlthough it has become somewhat of a cliché to invoke*Microsoft*as a historical lesson for every platform antitrust case, there are striking similarities between that historical moment and Google’s attempts here. Firstly, there are factual similarities between the two cases. The Microsoft case stemmed from the integration of Internet Explorer into the Windows operating system which Microsoft argued was a product design choice that improved functionality and enhanced user experience. Similarly, here, one of the causes for concern (apart from non-consensual data scraping, lack of attribution and compensation) is that Google has integrated AI Overview and AI Mode into its search home page, which Google would surely try to defend on similar grounds.\nSecondly, and more importantly, both cases involve a monopoly’s attempt to extend its control from one generation of information distribution to the next. As the famous[Gates’ memo] (unearthed in the prolonged antitrust trial) outlined, Microsoft in the 1990s was at an inflection point. The true power of the Windows OS was that it was the platform on which all apps were installed, and it served as the site for all user activity (like listening to music, watching movies, and most productivity tasks). With the rise of the internet, Windows OS was about to be dethroned by browsers, as users switched from using apps locally on their PC to simply executing all tasks directly on the internet through their browsers. Losing its status as the gatekeeper of the dominant ecosystem in which all tasks are executed is any monopoly’s nightmare. Google is now faced with the same threat. Google and its suite of websites (including YouTube) are amongst the[most-visited websites] in the world. Today, Google search is the starting point for[most user queries] (nearly 93.57%) and is considered the gateway to the internet. However, AI-powered answer engines are expected to dethrone Google search soon, becoming the next gateway for information access. Just as the Gates’ memo revealed Microsoft’s perception of the internet as an urgent existential threat and the company’s intention to quell and conquer even this new ecosystem, Google’s declaration of[Code Red] with the release of ChatGPT should give us a sense of déjà vu.\nThe invocation of the Microsoft antitrust case and its parallels to the present moment is meant to underscore the urgency of bringing prompt antitrust action against Google. This is not merely another instance of a dominant firm leveraging its monopoly to gain power in an adjacent market; what is at stake is the future architecture of the internet and the next generation of technology. The Justice Department’s antitrust suit against Microsoft at that seminal moment has[been credited with] unleashing innovation and fostering a vibrant and competitive (at least temporarily) ecosystem. Similarly, timely action against Google is essential to ensure that the emerging gateway technologies are not captured by the platform monopolies of the previous era. That Google is a repeat offender only heightens the concern; entrusting the future of the internet to such a company is a profoundly troubling prospect.[[19]] \n**Conclusion**\nModern antitrust scholarship about Big Tech often bemoans the failure of enforcement to act swiftly to*prevent*rather than*remediate*monopolization and associated[loss of innovation]. The[remedies phase] of the Google search trial has also exposed the difficulty in[restoring competition that is once lost]. With AI, we are once again at a crossroads where antitrust could play a role in preventing monopolization of emerging technology. Several senators have also[urged] the Department of Justice and the Federal Trade Commission to investigate the competition risks posed by generative AI features.\nNotably, private actors who are the most directly affected have started taking action too. Cloudflare, one of the biggest content delivery networks in the world (responsible for routing roughly[16% of global internet traffic]), recently[announced] that it would block AI crawlers by default and use a new ‘pay per crawl’ model. Penske Media, the publisher of Rolling Stone, The Hollywood Reporter, and several other major outlets, recently[filed an antitrust lawsuit] against Google, alleging that it unlawfully used and republished content from their websites to train its generative AI programs &mdash; conduct that, they argue, amounts to illegal monopolization. Chegg, an online education company, has also[brought a suit] claiming that Google’s AI Overviews have caused a[drop in traffic] and revenue. In the EU too, a group of independent publishers have brought a[competition complaint] and demanded an interim measure to prevent irreparable harm from Google’s AI Overviews.\nHowever, given the scale and significance of the problem, private actions alone are insufficient. Regulatory intervention is urgently needed. Even if a case were to take years to reach trial and produce a ruling on liability and remedies, the mere initiation of an investigation could exert meaningful pressure on Google to curb its monopoly-maintenance strategies or even produce a settlement. A substantial body of modern antitrust scholarship, coupled with the recent flexing of regulatory muscle in Big Tech cases, has been preparing us for precisely this moment: when antitrust enforcement might finally arrive in time for the party, rather than showing up several years late, after most of the damage is done.\n[] [1] United States v. Google LLC, 747 F.Supp.3d 1 (D.D.C. 2024) [Google search case].\n[] [2] United States v. Grinnell Corp., 384 U.S. 563 (1966).\n[] [3] United States v. Google LLC, 747 F.Supp.3d 1 (D.D.C. 2024).\n[] [4] United States v. Microsoft Corp., 253 F.3d 34 (D.C. Cir. 2001).\n[] [5] United States v. Microsoft Corp., 253 F.3d 34 (D.C. Cir. 2001).\n[] [6] Bartz v. Anthropic, 3:24-cv-05417, (N.D. Cal. Jun 23, 2025); Richard Kadrey v. Meta Platforms, 3:23-cv-03417, (N.D. Cal. Jun 25, 2025).\n[] [7] United States v. Microsoft Corp., 253 F.3d 34 (D.C. Cir. 2001).\n[] [8] Int’l Bus. Machines Corp. v. United States, 298 U.S. 131 (1936).\n[] [9] United States v. Microsoft Corp., 253 F.3d 34 (D.C. Cir. 2001); Epic Ganes v. Apple, Inc., 67 F.4th 946 (9th Cir. 2023).\n[] [10] Jefferson Parish Hosp. Dist. No 2 v. Hyde, 466 U.S. 2 (1984).\n[] [11] Eastman Kodak Co. v. Image Tech. Servs., Inc., 504 U.S. 451 (1992); Jefferson Parish Hosp. Dist. No. 2 v. Hyde, 466 U.S. 2 (1984).\n[] [12] United States v. Google LLC, 747 F.Supp.3d 1 (D.D.C. 2024).\n[] [13] Jefferson Par. Hosp. Dist. No. 2 v. Hyde, 466 U.S. 2, 12 (1984).\n[] [14] Tic-X-Press, Inc. v. Omni Promotions Co. of Ga., 815 F.2d 1407 (11th Cir. 1987); Kentucky Fried Chicken Corp. v. Diversified Packaging Corp., 549 F.2d 368 (5th Cir. 1977).\n[] [15] Amerinet, Inc. v. Xerox Corp., 972 F.2d 1483 (8th Cir. 1992).\n[] [16] Suburban Propane v. Proctor Gas, Inc., 953 F.2d 780 (2d Cir. 1992).\n[] [17] United States v. Google LLC, 747 F.Supp.3d 1 (D.D.C. 2024).\n[] [18] Case T-201/04, Microsoft Corp. v. Comm'n, 2007 E.C.R. 11-3601 (Ct. First Instance).\n[] [19] United States v. Google LLC, 747 F.Supp.3d 1 (D.D.C. 2024); United States v. Google LLC, 2025 WL 1132012 (E.D. Va 2025).\n[\n] \n## Recent Articles\nFebruary 09, 2026[Redefining the Standard of Human Oversight for AI Negligence] January 11, 2026[Entrepreneurial Liberty in the Age of AI] January 05, 2026[AI as the New Front Door to Legal Services: What Google’s AI Overview Means for Law Firm Visibility and Professional Responsibility] November 30, 2025[Geofence Searches Are Not (Necessarily) Carpenter Searches]",
    "length": 19699,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "AI Policies and Course Examples",
    "url": "https://tll.gse.harvard.edu/ai-policies-and-course-examples",
    "text": "AI Policies and Course Examples | Teaching &amp; Learning Lab[\nSkip to main contentarrow\\_circle\\_down\n] \n[![Harvard Graduate School of Education]] \nSearch\nSearchsearch\n[\n![Teaching and Learning Lab] \n![Teaching and Learning Lab] \n] \nmenucloseMenu\nSearch\nSearchsearch\n[\n![Teaching and Learning Lab] \n![Teaching and Learning Lab] \n] \n# AI Policies and Course Examples\n![Engaged person using AI] \nLearn more about AI policies at Harvard and HGSE, and see examples of how faculty are implementing these policies in courses.\n[### AI Literacy Tutorial\n] \n***(Harvard Key login required)***\nThe resource features sections on Functional and Ethical AI literacy, and on Using Generative AI at HGSE, with HGSE-specific examples and guidance including videos from faculty members Elizabeth Bonawitz, Chris Dede and...\n![layered lightbulb illustration] \n[### HGSE-Focused Course Policies\n] \n![HGSE-Focused Course Policies] \n[### HGSE Policy on Student Use of Generative Artificial Intelligence in Academic Work\n] \nStudent-facing document establishing general guidelines to encourage students to use generative AI when it can help them learn and not when it is a hindrance. Given the wide range of learning goals across HGSE courses, individual instructors may create...\n![Screengrab of the HGSE AI Policy webpage] \n![Decorative image of abstract pencil drawing; shapes, pencils and smart phones]",
    "length": 1384,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Towards Positive Outcomes in the AI Economy: Mitigating Algorithmic Collusion and Enabling Fair Recourse",
    "url": "https://dash.harvard.edu/entities/publication/78b94a86-f30a-45d4-bd7d-73b24e15ce77",
    "text": "Towards Positive Outcomes in the AI Economy: Mitigating Algorithmic Collusion and Enabling Fair RecourseSkip to main content\n[\nDASH\n] \n[![Repository logo]] \n**\n[Log In] \n**Log in via HarvardKey\n# Publication:\nTowards Positive Outcomes in the AI Economy: Mitigating Algorithmic Collusion and Enabling Fair Recourse\nLoading...\n![Thumbnail Image] \n## Open/View Files\n[PrimaryPhD\\_Thesis\\_and\\_Dissertation\\_EMM.pdf(3.02 MB)] \n## Date\n2024-05-13\n## Authors\n[Mibuari, Eric] \n## Published Version\n## Published Version\n## Journal Title\n## Journal ISSN\n## Volume Title\n## Publisher\n**Share your DASH Story\nThe Harvard community has made this article openly available. Please share how this access benefits you.\n## Research Projects\n## Organizational Units\n## Journal Issue\n## Citation\nMibuari, Eric. 2024. Towards Positive Outcomes in the AI Economy: Mitigating Algorithmic Collusion and Enabling Fair Recourse. Doctoral dissertation, Harvard University Graduate School of Arts and Sciences.\n## Research Data\n## Abstract\nThe rise of Artificial Intelligence (AI) promises to solve many important problems in the world. At the same time, awareness has been increasing about its potential and real harms. How can we extract maximum benefit from the promise of AI while minimizing present harms and mitigating future risks?\nIn this thesis, I frame and answer this question from the perspective of enabling and promoting positive outcomes in the AI-enabled economy, where markets are facilitated using AI algorithms, including agent behavior, pricing, and matching and clearing. The goal of my research is to find and create the conditions under which the benefits of AI are preserved or even enhanced while the tendency to diminish welfare, perhaps to particular groups, is contained to the greatest possible extent.\nIn lending domains, machine learning can be used to form a predictive model of the probability of default (a ``risk score\"), this driving loan decisions. For simple models, this brings the benefits of transparency and explainability, as well as guidance in regard to recourse. An alternative is to use policy learning, that is, learning a policy from borrower characteristics to loan decisions directly, and without explicit risk scoring. This emphasizes profit and can speed up learning, as a lender understands a borrower population, but with a concomitant loss of transparency. I introduce a risk-score based policy learning method, as well as a new metric of recourse effort fairness, and demonstrate that this risk-score based policy learning achieves optimal profits, explainability and transparency, as well as recourse effort fairness.\nThere are a number of problems where economic actors follow sequential behaviors, for example, in making pricing adjustments over time on e-commerce platforms, or power trading and storage optimization through a typical day in an electricity market. In this thesis, I work with the Stackelberg POMDP framework (partially observable Markov decision process) to design interventions in these kinds of sequential settings, seeking to improve economic welfare. I am especially focused on concerns that can arise in regard to tacit collusion between AI-mediated pricing or trading and storage decisions by automated agents.\nIn a first setting, I study algorithmic pricing on e-commerce platforms, where\nreinforcement learning (RL) algorithms have been shown to learn to set collusive prices with nothing more than profit feedback.\nThis raises the question as to whether collusive pricing can be prevented through the design of suitable ``buy boxes,\" i.e., through the design of the rules that govern the promotion of particular products and prices to consumers. I show that RL can also be used by platforms to learn buy box rules that are effective in preventing collusion by RL sellers. For this, I adopt the Stackelberg POMDP framework, and demonstrate success in learning robust rules that provide high consumer welfare.\nIn a second setting, I study trading and storage decisions by battery operators in electricity power markets. %requiring power storage are an example of a multi-agent dynamic storage optimization application in which artificial intelligence (AI) algorithms are actively applied by prosumers (battery operators who both produce and consume power).\nIn this application, agents who correspond to battery operators buy and sell power in a market with producers and price-elastic consumers, and where power can be stored at a negligible cost up to an agent-specific capacity. The use of RL algorithms has been shown in this setting to lead to outcomes where battery operators arbitrage the market over time, and that correspond to tacit collusion. I again appeal to the Stackelberg POMDP framework, and demonstrate success in learning collusion-mitigation policies by a regulator, in particular through the use of a network-flow thresholding intervention.\n## Description\n## Other Available Sources\n## Keywords\nComputer science\n#### Terms of Use\nThis article is made available under the terms and conditions applicable to Other Posted Material (LAA), as set forth at[Terms of Service] \n## URI\n[https://nrs.harvard.edu/URN-3:HUL.INSTREPOS:37379051] \n## Collections\n[FAS Theses and Dissertations] \n## Endorsement\n## Review\n## Supplemented By\n## Referenced By\n[**Full item page] \n## Related Stories",
    "length": 5357,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Guidance for Faculty on Addressing AI-Related Academic Integrity Issues",
    "url": "https://bokcenter.harvard.edu/guidance-faculty-addressing-ai-related-academic-integrity-issues",
    "text": "Guidance for Faculty on Addressing AI-Related Academic Integrity Issues | The Derek Bok Center for Teaching and Learning[\nSkip to main contentarrow\\_circle\\_down\n] \nannouncement\n**The Bok Center is currently developing new programming and new resources for the FAS community. Thank you for your patience as we revise our website to reflect these changes.**\nclose\n[![Harvard University]] \n[\n![The Derek Bok Center for Teaching and Learning] \n![The Derek Bok Center for Teaching and Learning] \n] \nmenucloseMenu\nSearch\nSearchsearch\n[\n![The Derek Bok Center for Teaching and Learning] \n![The Derek Bok Center for Teaching and Learning] \n] \n# Guidance for Faculty on Addressing AI-Related Academic Integrity Issues\nThe following guidance is intended to help faculty prevent AI-related academic integrity issues in their courses and address them should they occur. Ideally, faculty can manage AI-related academic integrity by structuring course assignments and assessments that are in alignment with their policies. However, when academic integrity issues do arise, clear policies, expectations, and processes can help faculty engage in fair and effective communication and resolution of the issue.\n## Get Ahead of the Problem\n### Clearly State Course AI Policy in the Course Syllabus\nThe[Office of Undergraduate Education AI Guidance] provides several sample policies with different approaches to limiting/allowing AI use in coursework.\nIn addition to setting the policy in the syllabus,**discuss**your policy in class and**remind**students of it particularly when they are working on assignments.\nExplain what**counts**as AI in your course all the way from fully generative AI tools like ChatGPT to Grammarly, predictive text in Google Docs or Microsoft Word. If any of these are allowed, encourage the student to**cite**the use of the tool. Encourage students to ask, at any point in the term, if they are unsure about whether a particular use of AI is in violation of the policy.\nBoth in your syllabus and verbally in class, indicate that one purpose of the assignments is to assess student understanding of the material. As such, students may be asked to demonstrate their understanding of the work they have submitted, including via oral exam after submission.\n### Match Your Assignments to Your AI Policy\n#### **If your AI policy is permissive:**\n1. design assignments where success requires demonstrable student learning beyond what AI can generate, and\n2. to the extent that output quality depends on skillful use of AI tools, provide resources that show students how to use the tools effectively in disciplinary context\nThe Bok Center offers concrete strategies for[teaching in the age of AI] that can help you adapt your assignments.\n#### **If your AI policy is restrictive:**\n1. limit use of assignments such as take-home work that can be completed successfully with the aid of generative AI; and\n2. include other assignments that directly assess student mastery of content in contexts in which access to generative AI is not possible, for example, in-class assignments and/or tests/exams.\nThe first is important because these assignments put students in a difficult position: students are expected to operate on the honor system in a context in which they perceive that their peers may be gaining an unfair advantage.\nThe second is important because it incentivizes students not to rely too heavily on outside help, from generative AI or other sources because they will have to individually perform on other assessments. Additionally, it provides a fuller picture of the mastery of a given student so that anomalous performance can more easily be detected.\n### Design Your Assessments to Verify Student Understanding\nInstructors are encouraged to design assignments that include measures that require students to demonstrate understanding of the work they have submitted. We recommend that instructors thoughtfully integrate this approach into their course design by linking an assignment’s grade not only to the work submitted but also to the student’s demonstrated understanding of the material.\nFor example, an instructor might require students to deliver an oral presentation or engage in a discussion about the content and process involved in completing an assignment. This practice becomes particularly effective if the grading weight of the assignment incorporates the post-submission check of understanding. So long as checks are equitably applied to students, they need not be applied to every student on every submission.\n## Address Problems That Arise\n### Meet with Students to Assess Understanding\nWe recommend that instructors who think a student may have inappropriately used generative AI begin by meeting with the student to discuss their submitted work. It is recommended that this meeting take place soon after the work is submitted. In this meeting we suggest that instructors:\n1. Share the concern they have about the assignment and ask the student whether they used generative AI or another aid to produce the work,\n2. discuss the reasons the work raised concerns and give the student an opportunity for explanation, and\n3. ask the student some questions about the content of the work and/or process of completing the work that give the instructor a sense of whether the student has a sufficient understanding of what they submitted to have plausibly authored it without aid of AI. Here are a few examples of the types of questions one might ask:\n* Can you walk me through your answer to question X?\n* Tell me about how/why you chose these sources.\n* You have used/applied X term/theory in your essay/answer. Can you explain it to me?\nIf the instructor believes AI (or other unauthorized means) were used in contravention of course policy, they are encouraged to refer the issue to the Honor Council along with an account of the meeting with the student. It is important to keep in mind that the Honor Council members are not disciplinary experts. The instructor should be as clear as possible about what in the student’s responses convinced them that the student submitted work authored by or with the aid of AI. Instructors who have questions about the Honor Council process are encouraged to contact the Honor Council directly.\n#### **Other potentially useful indicators of AI use**\nWhen submitting a case to the Honor Council, keep in mind these potentially useful indicators of AI use:\n* Evidence of a significant discrepancy in style or content mastery between different assignments submitted by a single student can be useful in determining whether AI or other inappropriate aid may have been used. For example, a student may submit an assignment that appears to show significant mastery of a subject but then demonstrate a lack of understanding of the same material on an in-class assessment that follows shortly after.\n* Concrete data such as the document revision history for a paper or timestamps from other educational technology systems used by a course can be helpful supporting evidence.\n* Citation of hallucinated sources or quotations.#### **Unreliable/Non-Actionable Indicators of AI Use**\n* AI “checkers” are not, to date, reliable and often over-identify AI use. At this time they do not produce reliable evidence.\n* Use of specific sentence structures, punctuation, or formatting in isolation are not necessarily sufficient to accurately indicate AI use. A significant pattern of or combination of these can be some evidence of AI use.\n* Citation of topics, sources, or techniques from outside the course materials are not necessarily conclusive evidence of use of AI. Some students use a wide range of sources beyond course materials to self-teach. This behavior should be directly discouraged if it is detrimental to student learning.### Spot Other Academic Integrity Issues\nWhen problematic work is submitted, identify whether the problem is solely that the student used generative AI in contravention of a policy or whether there are other issues with the work, such as non-existent sources or lack of citations, for instance, that raise academic integrity issues regardless of how the student produced the work.\n**The Bok Center is currently developing new programming and new resources for the FAS community. Thank you for your patience as we revise our website to reflect these changes.**",
    "length": 8344,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "A Message About AI & Campus Services",
    "url": "https://intranet.campusservices.harvard.edu/2023/07/13/a-message-about-ai-campus-services/",
    "text": "A Message About AI &#038; Campus Services &#8211; Our Campus Services\n[Skip to content] \n[![Harvard University homepage]] \n[HARVARD.EDU] \n# A Message About AI &#038; Campus Services\n* Publication date**July 13, 2023\n![Computer Motherboard] \nDear Colleagues,\nI’m following-up on the University email recently sent (see below) about the rapidly expanding use of publicly available Artificial Intelligence (AI). While AI is not new, the growth in easily accessible generative AI tools has the potential to permeate all facets of life, including the workplace. Today’s message is the beginning of a conversation we will have to better understand AI and what it can, and should, mean for Campus Services and the broader University. Our approach will be to balance the exciting and innovative possibilities with sensible and appropriate implementations that enhance the way our employees deliver services to the Harvard community.\nFor now, it’s important that we adhere to the University’s guidelines. Employees should:\n* Use care when handling[confidential information], including being careful not to share financial and employee-related information. Please don’t enter confidential information into publicly available tools like ChatGPT.\n* Continue to own your work by ensuring any content (i.e. websites, reports, presentations, etc.) generated with the support of AI is accurate.\n* Stay vigilant for suspicious emails and other digital communications, and don’t fall for increasingly challenging phishing scams.\n* Consult with CSIT before purchasing any new AI technology.\nWhile our employees have consistently done well in these areas, it’s important to understand that AI adds a layer of complexity to all of them. If you have any questions about how Artificial Intelligence may interface with our workplace, reach out to your primary IT contact or to[ben\\_gaucherin@harvard.edu] directly. Please pass this message along to others as you deem necessary.\nWe will strive to have an ongoing dialogue so that we can leverage these tools to maximum effectiveness while ensuring employees have the knowledge to grow alongside them. I hope you’re eager to participate in this conversation as we go forward.\nSincerely,\nSean\n&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8211;\n***Original University Message Sent July 13th***\nDear Members of the Harvard Community,\nWe write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI’s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.\nGenerative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.\n***Initial guidelines for use of generative AI tools***\n* **Protect confidential data:**You should not enter data[classified as confidential] (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University’s[Information Security Policy]. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties.\n* **You are responsible for any content that you produce or publish that includes AI-generated material:**AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called “hallucinations”) or may contain copyrighted material. Review your AI-generated content before publication.\n* **Adhere to current policies on academic integrity:**Review your School’s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they’re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed.\n* **Be alert for AI-enabled phishing:**Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to[follow security best practices] and report suspicious messages to[phishing@harvard.edu].\n* **Connect with HUIT before procuring generative AI tools:**The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds.\n* If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at[ithelp@harvard.edu].\n* Vendor generative AI toolsmust be[assessed for risk by Harvard’s Information Security and Data Privacy office] prior to use.\nIt is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use,[on the HUIT website], which will be updated as new information becomes available.\nSincerely,\nAlan M. Garber\nProvost\nMeredith Weenick\nExecutive Vice President\nKlara Jelinkova\nVice President and University Chief Information Officer\nSearch\n**Search Site\n## Recent Posts\n* [Restoring a Piece of Harvard&#8217;s History] \n* [A Zero Waste Future] \n* [New Trees for Harvard Yard] \n* [2025 Campus Services Harvard Heroes] \n* [Activating Allston] \n**close menu\n**close search\nSearch\n**Search Site",
    "length": 6106,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Harvard University Information Technology",
    "url": "https://www.huit.harvard.edu/file/29ecb85251ce97c65bdb0dfc7f417c6e",
    "text": "[Skip to main content] \n\n- [Main Menu] \n- [Utility Menu] \n- [Search] \n\n[![University Logo]] \n\n[HARVARD.EDU] \n\n[HOME] /\n\n[29ecb85251ce97c65bdb0dfc7f417c6e] \n\n| Tool | Overview | Availability | Data Classification Level |\n| --- | --- | --- | --- |\n| AI tools for general use |\n| This category of tools includes “chatbots” and AI assistants for general use and productivity. They are designed to understand and generate human-like responses to text-based, natural language prompts. They can generate text, code, and images, translate languages, write different kinds of creative content, or integrate with productivity and collaboration tools. |\n| [Harvard AI Sandbox] | Experiment with multiple LLMs in secure environment. **Features:** Code generation, Creative writing, Data analysis, Summarizing, Text generation and editing, Image generation, Translation | Access coordinated by School and Units; contact HUIT for details. | [**Level 3 data and below.**] |\n| [OpenAI ChatGPT Edu] | Versatile chatbot able to generate text, code, images, and more. **Features:** Chatbot customization, Code generation, Creative writing, Data analysis, Image generation, Summarizing, Text generation and editing, Translation | Access provided and coordinated by Schools and Units; contact your local IT department for details. | [**Level 3 data and below.**] |\n| [Adobe Firefly] | Generate images and text effects by simply typing key words or a description. Trained on stock images, openly licensed and public domain content. Also integrated into Adobe apps. **Features:** Image generation, Image editing | Available to Harvard faculty, staff, students, and researchers as part of Harvard Adobe Creative Cloud license. | [**Level 3 data and below.**] |\n| [AI Meeting Assistants] | AI meeting assistants (aka “AI note takers” or “bots”) can transcribe and summarize online meetings. These assistants may be built into existing collaboration tools such as Zoom, while others are third-party software agents used by individual meeting participants. Although these tools have legitimate uses (e.g., for accessibility and capturing notes for later reference), they can also pose substantial privacy, regulatory, and legal risks, and have the potential to stifle conversation and open inquiry. | AI meeting assistants should not be used in Harvard meetings, with the exception of approved tools as part of limited HUIT-directed pilot programs to evaluate the use of AI assistants within the Harvard environment, or approved tools for accessibility accommodations. We expect to expand these pilots in the coming weeks. | [**See guidelines for more information about risks and availability.**] |\n| AI developer tools |\n| This category of tools includes AI assistants and API access to enable developers to integrate Large Language Models (LLMs) into their own applications, products, or services. This includes chatbot creation and customization, building and testing applications, access to model training and deployment, coding, predictive analytics, and more. Code and low/no-code offerings are available. These tools are subject to change based on availability. Contact HUIT for more information and advice based on use case. |\n| Current limited access offerings include: | - Amazon Q - Amazon Bedrock - Amazon Sagemaker - Azure AI Studio - Copilot Studio - Google Vertex AI - OpenAI APIs | Available by [request from HUIT]. | [**Level 3 data and below.**] |\n\nCopyright © 2025 The President and Fellows of Harvard College \\| [Accessibility] \\| [Digital Accessibility] \\| [Report Copyright Infringement]",
    "length": 3582,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Generative AI Tool Comparison",
    "url": "https://www.huit.harvard.edu/ai/tools",
    "text": "Generative AI Tool Comparison | Harvard University Information Technology[\nSkip to main contentarrow\\_circle\\_down\n] \nSearch\nSearchsearch\n[\n![Harvard University Information Technology] \n![Harvard University Information Technology] \n] \nmenucloseMenu\nSearch\nSearchsearch\n[\n![Harvard University Information Technology] \n![Harvard University Information Technology] \n] \n# Generative AI Tool Comparison\n# Generative AI tool comparison\nPrograms &amp; initiatives\n![Professor gives a lecture to students on their laptops.] \nThe below table shows Generative AI tools currently available from HUIT, including the level of[confidential Harvard data] for which these tools are approved. Your School’s[local IT department] may also offer additional tools.\nMany other AI tools are available to the public, including tools that are free to use. However,[per University guidelines], you should not enter data classified as confidential ([Level 2 and above]) into publicly-available Generative AI tools.\nAs always, if you’re considering using Generative AI tools for Harvard work, you must[follow the University's initial guidelines for use].\n## AI tools for general use\nThis category of tools includes “chatbots” and AI assistants for general use and productivity. They are designed to understand and generate human-like responses to text-based, natural language prompts. They can generate text, code, and images, translate languages, write different kinds of creative content, or integrate with productivity and collaboration tools.\nAI meeting assistants (aka “AI note takers” or “bots”) can transcribe and summarize online meetings.  Although these tools have legitimate uses (e.g., for accessibility and capturing notes for later reference), they can also pose substantial privacy, regulatory, and legal risks, and have the potential to stifle conversation and open inquiry.**AI meeting assistants should not be used in Harvard meetings, with the exception of approved tools with contractual protections.**[**See guidelines for more information about risks and availability.**] \n***Note:**all data classification levels listed below apply**only**to the Harvard-offered versions of these tools and not to publicly-available versions of these tools (which should not be used for Harvard work).*\nSort\n|Tool|Overview|Availability|Data classification level|\n[Harvard AI Sandbox] \n|\nExperiment with multiple LLMs in secure environment.\n**Features:**Code generation, Creative writing, Data analysis, Summarizing, Text generation and editing, Image generation, Translation\n|\nFaculty**,**staff**,**andresearchersin Central Administration, FAS, College, GSAS, SEAS, GSD, GSE, HBS, HDS, HKS, HLS, HMS (Quad), HSDM (Quad), Radcliffe, SPH;Studentsin College, GSAS, SEAS, GSE, HDS, HKS, HLS, HMS (Quad), HSDM (Quad), SPH\n|\n[**Level 3 data and below.**] \n|\n[Google Gemini] \n(via Harvard Google account)\n|\nVersatile chatbot integrated with Google Workspace.\n**Features:**Chat, search, coding, writing, data analysis, image generation, translation and more. Includes Deep Research and NotebookLM.\n|\nBasic Gemini features are available in all Harvard Google accounts, including Harvard College.\n|\n[**Level 3 data and below.**] \n|\n[Microsoft Copilot Chat] \n(via Harvard Microsoft 365 account)\n|\nVersatile chatbot integrated with Microsoft 365.\n**Features:**Chat, search, coding, writing, data analysis, image generation, translation and more. Includes Deep Research.\n|\nBasic Copilot Chat features available in all Harvard Microsoft 365 accounts.\n|\n[**Level 3 data and below.**] \n|\n[OpenAI ChatGPT Edu] \n|\nVersatile chatbot able to generate text, code, images, and more.\n**Features:**Chatbot customization, Code generation, Creative writing, Data analysis, Image generation, Summarizing, Text generation and editing, Translation\n|\nAccess provided and coordinated by Schools and Units; contact your local IT department for details.\n|\n[**Level 3 data and below.**] \n|\n[Adobe Firefly] \n|\nGenerate images and text effects by simply typing key words or a description. Trained on stock images, openly licensed and public domain content. Also integrated into Adobe apps.\n**Features:**Image generation, Image editing\n|\nAvailable to Harvard faculty, staff, students, and researchers as part of Harvard Adobe Creative Cloud license.\n|\n[**Level 3 data and below.**] \n|\n## AI developer tools\nThis category of tools includes AI assistants and API access to enable developers to integrate Large Language Models (LLMs) into their own applications, products, or services. This includes chatbot creation and customization, building and testing applications, access to model training and deployment, coding, predictive analytics, and more. Code and low/no-code offerings are available. These tools are subject to change based on availability. Contact HUIT for more information and advice based on use case.\nSort\n|Tool|Overview|Availability|Data classification level|\n[Self-service AI APIs] (HarvardKey-protected)\n|\nSelf-service AI API access for eligible users is available through[Harvard’s API Platform]. These include pay-as-you-go or limited-access, credit-redemption options.\n|\nSome self-service AI APIs in the portal have limited availability. Where this is the case, it's noted in the documentation for the API.\n|\n[**Level 3 data and below.**] \n|\n[Additional limited access offerings] \n|\nAI platforms, assistants, and APIs for building and using AI.\n|\nAvailable by[request from HUIT].\n|\n[**Level 3 data and below.**] \n|",
    "length": 5476,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Best Practices: Generative AI (System Administrators)",
    "url": "https://privsec.harvard.edu/best-practices-generative-ai-system-administrators",
    "text": "Best Practices: Generative AI (System Administrators) | University Information Security and Data Privacy[\nSkip to main contentarrow\\_circle\\_down\n] \n[![Harvard University]] \n[\n![University Information Security and Data Privacy] \n![University Information Security and Data Privacy] \n] \nmenucloseMenu\nSearch\nSearchsearch\n[\n![University Information Security and Data Privacy] \n![University Information Security and Data Privacy] \n] \n# Best Practices: Generative AI (System Administrators)\n![robot hands and human hands on keyboard] \n## What is Generative AI?\nGenerative AI (Gen AI) refers to AI systems that can create new content (text, images, code, etc.) from training data.\nFor system administrators, managing AI systems involves additional responsibilities: ensuring proper data governance, applying technical safeguards, monitoring system behavior, and maintaining compliance with Harvard’s policies and regulatory requirements.\nAI functionality is increasingly embedded in enterprise web browsers like ChatGPT Atlas and Perplexity Comet, which integrate generative assistants, memory, and automation. While these features boost productivity, they also introduce new risks, including prompt injection, agentic automation vulnerabilities, extension threats, and uncontrolled data flow to vendor clouds.\n## Common Risks\nSort\n|Category|Description|Mitigation / Best Practice|\nSensitive Data Exposure\n|\nPII, credentials, or business secrets may be ingested, logged, or retained.\n|\nRedact or anonymize data; restrict memory retention; enforce zero-trust architecture.\n|\nPrompt Injection / Goal Hijacking\n|\nMalicious inputs override instructions or exploit logic.\n|\nSanitize inputs, validate outputs, use guardrails, isolate system/user prompts.\n|\nModel/Data Poisoning\n|\nAdversarial or low-quality data alters model behavior.\n|\nValidate data pipelines; detect anomalies; restrict untrusted sources.\n|\nImproper Classification Handling\n|\nHigher-classified data (e.g., Level 4) processed in lower-classified systems.\n|\nEnforce strict controls; align system classification with data classification.\n|\nInsecure Integrations\n|\nAPIs, plugins, or external connectors mishandle authentication or encryption.\n|\nUse scoped API keys, TLS, access controls; contract only with approved vendors.\n|\nAI Browser and Automation Risks\n|\nAI browser features (memory, automation, agent mode) can access or transmit internal data outside managed controls.\n|\nRestrict AI browser use to managed endpoints; disable memory, agent mode, and external connector access by policy; monitor browser logs.\n|\nHallucinations / Inaccurate Outputs\n|\nAI may produce incorrect or misleading information.\n|\nRequire human review of AI-generated decisions or recommendations.\n|\nOver-Privileged Access\n|\nAI connectors or integrations may expose shared data or repositories beyond a user’s intended permissions.\n|\nLimit connector access to user-level data; review permissions regularly; apply least privilege and RBAC.\n|\nOver-Privileged Execution\n|\nAgents or pipelines have excessive permission.\n|\nApply least privilege, RBAC, sandboxing, scoped tokens.\n|\nInadequate Logging / Auditing\n|\nLack of transparency in AI system behavior.\n|\nLog inputs, outputs, and tool usage; ensure traceability.\n|\nThird-Party Data Sharing\n|\nData sent to vendors or hosted services without proper safeguards.\n|\nVet vendors; include contractual clauses; restrict to approved services.\n|\n### General Best Practices\n#### Documentation &amp; Governance\n* Record all AI systems in ServiceNow or unit-specific CMDB.\n* Review whether AI browsers are permitted; apply policy restrictions (e.g., disable memory, restrict agent mode).\n* Document approved browser types and ensure MDM/Intune/JAMF can manage updates and extension policies.#### Data Handling\n* Limit use of PII or confidential data; apply de-identification, anonymization, or PETs where possible.\n* Verify training/fine-tuning data to detect adversarial inputs.\n* Do not train models with higher-risk data on lower-classified systems.\n* For Retrieval-Augmented Generation (RAG), ensure vector databases follow the same access controls as the source data and encrypt Level 3+ content.#### Vendor &amp; Contract Management\n* Only use vendors with Harvard-approved contracts.\n* Add contractual clauses restricting vendors from training on Harvard data.\n* Ensure contracts explicitly cover AI features, privacy, and security measures.\n#### Technical Safeguards\n* Apply least privilege to APIs, tools, and integrations.\n* Enforce TLS, strong authentication (e.g., OAuth), and scoped API keys.\n* Segment AI systems and their supporting infrastructure.\n* Disable unnecessary capabilities in agentic AI systems.#### Monitoring &amp; Incident Response\n* Validate accuracy and fairness of AI outputs; require human review.\n* Monitor logs for anomalies, misuse, or unexpected behaviors.\n* Apply retention policies to AI system memory and outputs.\n* Prepare escalation paths for suspected AI misuse or compromise.\n## Related Resources\n* [Initial University Guidelines] \n* [HUIT Guidelines and Resources] \n* [PrivSec Guidelines for Secure and Responsible Use of AI Systems] \n* [EU Artificial Intelligence Act] \n* Seek additional guidance from your local IT department as needed",
    "length": 5250,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Harvard Data Policy",
    "url": "https://projects.iq.harvard.edu/odap/harvard-data-policy",
    "text": "[Skip to main content] \n\n- [Main Menu] \n- [Utility Menu] \n- [Search] \n\n[![University Logo]] \n\n[HARVARD.EDU] \n\nCopyright © 2024 The President and Fellows of Harvard College \\| [Accessibility] \\| [Digital Accessibility] \\| [Report Copyright Infringement]",
    "length": 252,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Academic Integrity and Teaching With(out) AI",
    "url": "https://oaisc.fas.harvard.edu/academic-integrity-and-teaching-without-ai/",
    "text": "Academic Integrity and Teaching With(out) AI &#8211; Office of Academic Integrity and Student Conduct\n[Skip to content] \n[![Harvard University homepage]] \n[HARVARD.EDU] \n# Academic Integrity and Teaching With(out) AI\n![] \n## **Resources forcourse design in the age of AI.**\nArtificial intelligence presents a dual-edged sword. On one hand, it offers unprecedented opportunities for pedagogical innovation, enabling personalized learning experiences, automating administrative tasks, and facilitating advanced data analysis. On the other hand, the integration of AI in academic settings raises pressing concerns about academic integrity, as these powerful tools can sometimes undermine the authenticity of student work and blur the lines of original thought. The list of resources below provides resources and strategies tailored for faculty members who either encourage or discourage the use of AI in their classes. By focusing on maintaining academic integrity, these resources and strategies can help faculty navigate the integration or exclusion of AI in a way that reinforces academic integrity, caters to their teaching philosophy, and addresses students’ ethical development.\nIn addition to the resources below, you may also want to view the[list of resources] for instructors regarding appropriate use of generative AI in courses compiled by the Office of Undergraduate Education, the Bok Center’s resources for[teaching with AI], the metaLAB’s [AI Pedagogy Project], the Teaching at FAS website’s[list of resources] for designing and teaching courses, Generative AI @ Harvard[resources to faculty] and[resources for students], as well as[the University guidelines] on the use and procurement of AI tools.\nFaculty with questions about academic integrity may contact Qussay Al-Attabi, Assistant Dean for Academic Integrity and Secretary of the Harvard College Honor Council.\n### On This Page:\n**[Resources for Faculty Encouraging AI Use in Their Courses] **\n**[Resources for Faculty Discouraging AI Use in Their Courses] **\n[BACK TO RESOURCES OVERVIEW] \n## **Resources for Faculty Encouraging AI Use in Their Courses**\nArtificial intelligence may be used as a powerful catalyst for pedagogical innovation, offering novel ways to enhance learning and engage students. However, alongside its promising opportunities, AI also introduces significant challenges, particularly concerning academic integrity. As educators, it is crucial to maintain a vigilant approach, thoughtfully integrating AI into course design and instruction. This ensures that we harness its potential while upholding the principles of honesty, fairness, and ethical conduct. By remaining mindful of these considerations, instructors can create enriching educational experiences that respect the integrity of the academic environment. If you intend to integrate AI into your courses, we hope that you find the following resources helpful.\n*Course Design*\n* **Understand AI Tools**: Stay informed about the latest AI applications relevant to your field. Resources like AI literacy workshops and toolkits can be invaluable.\n* **Design with AI in Mind**: Develop assignments that promote critical thinking and creativity, encouraging students to use AI for idea generation or problem-solving.\n* **Case Studies on AI**: Integrate case studies discussing ethical AI use, helping students understand both potential benefits and pitfalls.\n* **Collaborative Projects**: Use projects that leverage AI tools for collaboration, teaching students how to use these tools responsibly.\n*Syllabus Structure*\n* **Explicit AI Guidelines**: Clearly define how AI can be used in the course syllabus. Include examples of acceptable AI applications, such as data analysis or language translation tools.\n* **Ethical AI Use Statement**: Offer a detailed statement on ethical AI use, reinforcing integrity while encouraging experimentation within set boundaries.\n* **Resource Provision**: Provide students with a well-curated list of AI tools that can support their learning, along with guidelines for their ethical use.\n*Assignments*\n* **Integration of AI Tasks**: Create assignments that involve AI tools, tasking students with evaluating AI-generated outputs or improving on them.\n* **Reflective AI Use Journals**: Ask students to maintain a journal documenting how they utilized AI throughout the course, underscoring ethical considerations.\n* **Scaffolded Assignments**: Design multi-part assignments where AI can be used for specific stages, but not others, providing a balanced approach to technology use.\n*Assessments*\n* **AI-Enhanced Projects**: Encourage the creation of projects that include AI components, emphasizing innovation and ethical tool use in assessments.\n* **Oral Defenses**: Use oral presentations to evaluate comprehension, allowing students to explain their process, including AI application.\n* **Ethical Use Evaluations**: Have students submit a section with their assignments where they outline how AI assisted their work, focusing on ethics.\n*Classroom Practices*\n* **AI Literacy Promotion**: Offer sessions on AI functionality and ethical use, empowering students to use technology responsibly.\n* **Peer Review Focus**: Structure peer reviews to critique both the technical and ethical aspects of AI use in projects.\n*Feedback and Reflection*\n* **Frequent Feedback**: Provide regular feedback on AI usage, helping students refine their skills and understanding of appropriate applications.\n* **Self-Assessment**: Incorporate self-assessment tools that encourage students to reflect on their ethical use of AI.\n* **Ethical Debates**: Facilitate debates on AI’s role in specific disciplines, fostering a nuanced understanding of its ethical implications.\n## ****Resources for Faculty Discouraging AI Use in Their Courses****\nFrom an academic integrity perspective, prohibiting AI use in course requires careful consideration of several critical factors. The challenge lies not merely in the existence of AI technology, but in its widespread presence, sophisticated capabilities, and the ease with which it can be accessed and used. Faculty who wish to prevent their students from using AI tools must be especially deliberate in their approach to course design, ensuring that both requirements and assessments are structured in a way that intrinsically discourages the misuse of AI. This might include creating assignments that require unique, personalized responses or demonstrating processes that AI cannot easily replicate. Clear communication of expectations and the reasons behind AI restrictions can also help cultivate an understanding of the importance of independent learning. Additionally, implementing assessments that focus on critical thinking and problem-solving skills can effectively uphold the integrity of the educational process. By being intentional in approaching restricting or banning AI in their courses, instructors can foster a learning environment that prioritizes authenticity and academic honesty, even in the face of advancing technology. If you plan to restrict or prohibit the use of AI in your courses, we hope that you find the following resources helpful.\n*Course Design*\n* **AI-Free Design**: Structure courses with assignments based on personal reflection, problem-solving, or hands-on experiences that AI finds challenging to replicate.\n* **Ethics Discussion**: Include modules on the importance of developing skills without AI aid, focusing on personal growth and ethical implications.\n* **Originality Emphasis**: Design tasks that require creativity and personal insight, making AI assistance less viable.\n*Syllabus Structure*\n* **Strict AI Policies**: Clearly prohibit AI tools&#8217; use in your syllabus and explain the specific risks associated with AI dependability.\n* **Definition of Misuse**: Provide students with clear examples of what constitutes AI misuse in coursework.\n* **Integrity Pledge**: Ask students to sign an academic integrity pledge specifically stating they won’t use AI.\n*Assignments*\n* **Original Thought Assignments**: Develop assignments requiring unique perspectives, such as diaries, journals, or case study reflections.\n* **Process Explanation**: Require students to submit process documentation, detailing their steps from idea conception to completion.\n* **Frequent Drafts**: Implement a drafting process where students submit progressive drafts, highlighting their developmental journey.\n*Assessments*\n* **Controlled Assessment Environments**: Conduct in-person assessments where AI assistance is untenable, such as written tests under strict supervision.\n* **Interactive Assessments**: Use methods that test students’ on-the-spot thinking, like simulations or problem-solving sessions.\n* **Critical Justification Essays**: Include essay portions in exams where students justify their reasoning or solutions, focusing on individual logic.\n*Classroom Practices*\n* **Integrity Workshops**: Host workshops on the value of academic integrity and the expected skills beyond academia that require independent thinking.\n* **Scenario-Based Learning**: Use scenarios that challenge students to solve problems without technology, demonstrating real-world application.\n* **Role Play Exercises**: Engage in activities that emphasize communication, negotiation, and reasoning skills without AI assistance.\n*Feedback and Reflection*\n* **Reflective Writing**: Encourage reflective writing assignments focusing on learning processes and personal contributions without AI.\n* **Peer Feedback**: Use peer feedback sessions to help students learn from one another’s methods, focusing on human-driven insights.\n* **Ethical Reflection Sessions**: Incorporate sessions where students can discuss their thoughts on academic integrity and the temptations of AI use.\n**close menu\n**close search\nSearch\n**Search Site",
    "length": 9839,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "",
    "url": "https://provost.harvard.edu/files/provost/files/policy_on_access_to_electronic_information.pdf",
    "text": "1\nPolicy on Access to Electronic Information\nAs voted by the President and Fellows of Harvard College on March 31, 2014; amended \nMay 8, 2015. Current policy amendments proposed by the ECPOC and approved by the \nProvost on October 6, 2023.\nScope of Policy\nThis policy sets out guidelines and processes for University access to user electronic \ninformation stored in or transmitted through any University system. This policy applies to \nall Schools and units of the University.\nGeneral Statement\nMembers of the Harvard community rely on technology in multiple aspects of their work,\nteaching, research, study, and other activity. In doing so, they use electronic systems, \nnetworks, and devices that the University owns, provides, or administers. The University \nmakes these systems available for the purpose of carrying out the University’s various \nactivities. To promote trust within the University community, the University should be \ntransparent about its policy regarding the circumstances in which it may access user \nelectronic information stored in or transmitted through these systems. This policy \ntherefore sets out guidelines and processes that apply when the University seeks \naccess to such electronic information, consonant with the University’s interest in \nmaintaining an environment in which free academic inquiry thrives. This policy is \nintended to establish internal standards and procedures governing such access by the\nUniversity; it is not intended to create, nor does it create, any contractual or other legal \nobligation on Harvard’s part, or any contractual or other legal right or expectation in or \nfor any individual person.\nThe policy is grounded on, and should be interpreted and applied in accordance with, \nthese six important principles:\n● Access should occur only for a legitimate and important University purpose.\n● Access should be authorized by an appropriate and accountable person.\n● In general, notice should be given when user electronic information will be or has \nbeen accessed.\n● Access should be limited to the user electronic information needed to accomplish \nthe purpose.\n● Sufficient records should be kept to enable appropriate review of compliance with \nthis policy.\n● Access should be subject to ongoing, independent oversight by a committee that \nincludes faculty representation.\n2\nTerminology\nThe following terms are used in this policy with the following meanings:\n“University systems” refers to all services, networks, and devices owned, provided, or\nadministered by any unit of the University, such as email services, Internet access, file \nservers, voice message services, storage devices and services, laptop and desktop \ncomputers, phones and other mobile devices, and usage and access logs.\n“Users” refers to Harvard faculty, others holding academic appointments at Harvard, \nstudents, staff, and other employees.\n“User electronic information,” for any particular user, refers to:\n(i) Documents and communications, including emails, voice mails and text messages, \nand their associated metadata, which are generated by or created or received on \nUniversity systems and located in files and accounts associated with a particular user. \nFor example, this would include all emails and their attachments in a user’s inbox, sent \nitems folder, or other email folders that are recognized as part of the account associated \nwith that user, and all documents in that user account’s document folders; and\n(ii) Information generated by automated processes triggered by that user’s use of \nUniversity systems, such as tracks of Internet use and logs of access to facilities (“Log \nInformation”),\nExclusions\nThis policy does not limit or restrict access to or use or disclosure of user electronic \ninformation if such access, use, or disclosure is part of the normal functionality and \npurpose of a University system.\nThis policy does not limit or restrict:\n(a) access to or use or disclosure of (1) records regularly maintained by the University in \nthe ordinary course of business, such as personnel records or student academic \nrecords, records stored in a Harvard archival repository, or information provided by \npersonnel in connection with regular University record-keeping, such as entries in a \nUniversity travel registry; or (2) Log Information accessed by the University without \nidentifying or seeking to identify any particular user; or\n(b) access to or use or disclosure of information otherwise authorized by or pursuant to \na vote of the Harvard Corporation.\n3\nContents\nI. Reasons for Access\nII. Authorization of Access\nIII. Notice\nIV. Scope of Access\nV. Records of Process\nVI. Oversight Committee; Amendments\nI. Reasons for Access\nUniversity personnel do not routinely monitor the content of information transmitted \nthrough or stored in University information systems. The University may obtain access \nto user electronic information in some circumstances, but only for a legitimate \ninstitutional purpose. The paragraphs below describe certain purposes for which the \nUniversity may access such information. While this list is expected to cover most \ninstances of access, the list is not intended to be exhaustive. The University may \naccess user electronic information for reasons that likewise advance a legitimate \ninstitutional purpose, as determined by a person designated to authorize access \npursuant to this policy and subject to review by the oversight committee as described in \nPart VI.\nAlthough this policy applies to the electronic information of faculty, staff, and students \nalike, in evaluating the institutional purpose, the person designated to authorize access \nshould in each case weigh not only the stated reasons for access but also the possible \neffect of access on University values such as academic freedom, trust, and confidence.\nSystem Protection, Maintenance, and Management\nUniversity systems require ongoing maintenance and inspection to ensure that they are \noperating properly; to protect against threats such as attacks, malware, and viruses; \nand to protect the integrity and security of information. University systems also require \nregular management, for example, in order to implement new software or other facilities. \nTo do this work, the University may scan or otherwise access user electronic \ninformation.\nBusiness Continuity\nUser electronic information may be accessed for the purpose of ensuring continuity in \nbusiness operations. This need can arise, for example, if an employee who typically has \naccess to the files in question is unavailable due to illness or vacation, or following the \ndeparture of an employee from the University.\n4\nSafety Matters\nThe University may access user electronic information to deal with exigent situations \npresenting threats to the safety of the campus or to the life, health, or safety of any \nperson.\nLegal Process and Litigation\nThe University may access user electronic information: (1) in connection with \nthreatened or pending litigation, law enforcement investigations, or other government \ninvestigations; (2) to identify and disclose information as required by law or legal \nprocess; (3) to investigate or assist in the investigation of unlawful activity directed at \nthe University or a member of the Harvard community, or (4) to investigate or assist in \nthe investigation of unlawful activity by a member of the Harvard faculty or staff or other \nHarvard employee.\nInternal Investigations of Misconduct\nThe University may access user electronic information in connection with investigations \nof misconduct by students, faculty, staff, and other members of the University \ncommunity, but only when the authorizing person, after weighing the need for access \nwith other University values, has determined that such investigation would advance a \nlegitimate institutional purpose and that there is a sufficient basis for seeking such \naccess. As described in Section VI of this policy, all decisions to access user electronic \ninformation are subject to review by an Oversight Committee.\nThis policy does not apply to reviews of research misconduct allegations conducted \nunder established School-based policies.\nII. Authorization of Access\nAccess to user electronic information should be authorized by an appropriate person, as \nset forth below. In deciding whether to approve access, the authorizing person should \nconsider whether effective alternative means to obtain the information are reasonably \nand timely available. In all cases, access must comply with applicable legal \nrequirements.\nAuthorization for access to user electronic information may be provided by the consent \nof the user.\n5\nOther cases should be handled as follows:\nIf the user is a faculty member or other holder of an academic appointment at Harvard, \nthe dean of the relevant Faculty must authorize access.\nIf the user is an employee other than a faculty member: (1) the human resources officer \nor his/her designee for the relevant School or administrative unit must authorize access \nin business continuity cases; and (2) the dean of the relevant Faculty or the senior \nadministrator of the relevant unit if not a Faculty, or their designees, must authorize \naccess in investigative or other cases.\nIf the user is a student, the School-level dean or the dean’s designee must authorize \naccess. Any authorization of access shall apply only to the particular situation and user \nor users. Any other instance of access must be separately authorized.\nNo independent authorization is required: \n● For access by information technology personnel to conduct routine system \nprotection, maintenance, or management purposes in accord with internal \nprotocols and processes. \n● For access requested by the Office of the General Counsel: (1) in connection \nwith threatened or pending litigation, law enforcement investigations, or other \ngovernment investigations; and (2) to identify and disclose information as \nrequired by law or legal process. \n● For access expressly granted in a legal agreement, Certificate of Gift, or other \nstewardship agreement executed between a Harvard archival repository and a \nuser or their legal representative.\n● For access by Harvard University Archives staff to determine whether records \nshould be transferred to a Harvard archival repository.\nIn exigent situations involving a threat to campus safety or the life, health, or safety of \nany person, access may be authorized by the Chief of Police of Harvard University or \ntheir designees. If emergency conditions do not allow for prior authorization, the matter \nshall be reported to the Executive Vice President as promptly as possible. In addition, \naccess may be authorized by the University's General Counsel or their designees, in \norder (1) to investigate or assist in the investigation of unlawful activity directed at the \nUniversity or a member of the Harvard community, or (2) to investigate or assist in the \ninvestigation of unlawful activity by a member of the Harvard faculty or staff or other \nHarvard employee.\n6\nFor some requests to search user electronic data, it may not be possible to identify any \nparticular user in advance. For example, requests for logs of access to a University \nfacility (swipe card data) often are intended to find out who entered a facility during a \nparticular period; in such cases, the requestor cannot identify a particular user or users \nbecause the goal of the search is to learn those identities. Such data requests may still \nbe subject to one of the prior provisions of this Section II, for example, those relating to \nlaw enforcement investigations or emergencies.\nOtherwise, such data search requests must be authorized by the dean of the relevant \nFaculty or the senior administrator of the relevant unit if not a Faculty, or their \ndesignees, in the School or unit where the requestor works.\nIII. Notice\nWhen the University intends to access user electronic information, notice ordinarily \nshould be given to that user. All reasonable efforts should be made to give notice at the \ntime of access or as soon thereafter as reasonably possible.\nSystem protection, maintenance, and management — Individual notice is not required \nfor ordinary system protection, maintenance, or management. Notice should be given if \nthe access relates specifically to the activity of an individual user.\nBusiness continuity — Individual notice is not required for access to user electronic \ninformation for purposes of business continuity, in accordance with established \nUniversity practice and the common understanding that individual notice in such cases \nis typically not practical.\nLegal restrictions — Individual notice is not required where the University is subject to \nlegal constraints on its ability to give notice.\nEmergencies and other extraordinary cases — Contemporaneous notice is not required \nin cases where there is insufficient time, where giving notice would otherwise interfere \nwith an effective response to an emergency or other compelling need (e.g., at a stage of \nan internal investigation where giving notice may compromise the investigation), or \nwhere it is impractical (e.g., in the case of a former employee). The decision not to give \ncontemporaneous notice must be made by the person designated by this policy to \nauthorize the access. In such cases, notice will ordinarily be given as soon as practical.\nThe person designated by this policy to authorize access may dispense with the notice \nrequirement for good cause. Any such decision, and the reasons for it, shall be \n7\ndescribed in the records described in Part V of this policy and may be reviewed by the \noversight committee, as set forth in Part VI.\nIV. Scope of Access\nThe University shall adopt reasonable steps, whenever practicable, to limit access \nobtained under this policy only to user electronic information that is related to the \nUniversity’s purpose in obtaining access. These steps will vary depending on the \ncircumstances of the search and may include, by way of illustration, designing searches \nto find specifically designated items, as opposed to categories of information.\nParticipation in the search, and access to the information, should be limited to those \npersonnel with a reasonable need to be involved.\nV. Records of Process\nAny person who authorizes access to user electronic information shall provide that \nreasonable records of the decision process and the reasons for the decision are made \nand preserved.\nThe persons who implement access to user electronic information shall make \nreasonable records and logs of the steps taken to access the information. All \nimplementation records shall be delivered to the University Chief Information Officer.\nCopies of the information accessed should be retained only as needed to effectuate the \npurposes of the access and securely disposed of promptly thereafter.\nThe accessed information and the records and logs of the search shall be kept \nappropriately secure.\nIn all instances of access under this policy, records adequate to permit effective review \nas described in Part VI of this policy should be kept.\nVI. Oversight Committee; Amendments\nThis policy, its implementation, and instances of access under this policy shall be \nsubject to review by an oversight committee to be constituted by the University, which \nshall include faculty and senior administrators. The oversight committee shall make \nrecommendations to the President and/or Provost as to the implementation of the policy \nand possible amendments. Amendments to this policy may be approved in writing by \nthe President or Provost at any time, provided that if the President or Provost\ndetermines that an amendment raises material issues warranting the attention of the \n8\nCorporation, such amendment will be presented to the Corporation for its approval. All \namendments will be reported to the Corporation for its information. The oversight \ncommittee shall also make periodic public reports on the implementation of this policy.\nIn carrying out its responsibilities, the oversight committee may review the records \ndescribed in Part V of this policy, subject to redaction as necessary to protect individual \nusers.",
    "length": 16202,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "AI Marketing Guidelines | Harvard John A. Paulson School of Engineering and Applied Sciences",
    "url": "https://seas.harvard.edu/office-communications/ai-marketing-guidelines",
    "text": "AI Marketing Guidelines | Harvard John A. Paulson School of Engineering and Applied Sciences[Skip to main content] \nSearch\n[] \nHelp support Harvard John A. Paulson School of Engineering and Applied Sciences.[Make a gift].\n[] \n[**Search] [Menu] \n[Offices &amp; Services] \n[Office of Communications] \n# AI Marketing Guidelines\nGenerative AI is a type of artificial intelligence that can learn from and mimic large amounts of data to create content such as text, images, music, videos, code, and more, based on inputs or prompts. Harvard University supports responsible experimentation with[Generative AI tools], but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. These are defined online at:[https://huit.harvard.edu/ai/guidelines]. Additional AI guidelines are provided by the[Office of the Provost].\n## AI Use in Marketing and Communications\nArtificial Intelligence (AI) is having a significant impact on the marketing discipline. These guidelines are intended to set boundaries on the sanctioned uses of AI in our marketing and communications activities.\nGenerative AI is a tool, not a replacement for human expertise and judgment.You are responsible for ensuring any content you create using generative AI is accurate and meets quality standards. Always carefully review any AI-generated content before publication.\nPrioritize data privacy and security in all generative AI-related work. You should not enter private or proprietary data into generative AI tools that lack data privacy protections.\nWhen using generative AI, prioritize creating outputs that are accessible and inclusive. For additional information regarding digital accessibility guidelines, visit[https://huit.harvard.edu/ai/guidelines].\n### **Some Acceptable Uses of AI in Marketing &amp; Communications**\n**Content**\n* Creating drafts of written content for blogs, social media, emails, and ads if they are carefully proofread for accuracy and quality by humans.\n* Drafting marketing plans and creative briefs.\n* Developing draftvideo/audio scripts\n* Creating YouTube video descriptions\n* Generating transcripts for audio and video content if reviewed for accuracy and quality.\n* Developing blog content (with attribution)\n* Developing descriptive web content\n**Multimedia (with attribution)**\n* Removing privacy-related or undesirable elements from photographs while preserving the substance/intent of the original photograph\n* Creating illustrations or animationsfor stories and articles as a substitute for stock photography\n* Modifying or enlarging the backgrounds of photographs (while preserving the substance/intent of the original photograph)\n* Developing motion graphics of non-human things, environments, processes as a substitute for stock photography\n* Developing podcasts### **Prohibited Uses of Generative AI**(examples)\n* Replicatingor imitatinga person’s likeness or voice without their written consent.\n* Creatingheadshots or portraits of individuals.\n* Generating images of fictional people without clear AI attribution.\n* Modifying images and video in ways that alter the original intent of the image.## **AI Citation Guidelines**\nTo ensure the authenticity of the content we produce, it is important to be transparent about the use of AI. AI-generated images (e.g. illustrations, data visualizations) must be identified as such by placing the following tag in the lower right corner of the image: Created using AI. Images that are modified, edited, or enhanced (e.g. sharpened, color-corrected) do not require a tag. Content that was primarily developed by AI requires attribution. Content that was inspired by, guided by, or edited with AI does not require attribution.\n### **Text Attribution**\nPlease follow the[**APA Style**for attribution]:\n**Bibliography Format:**Author. (Date).*Name of tool*(Version of tool) [Large language model]. URL\n**Bibliography Example:**OpenAI. (2025).*ChatGPT*(Mar 14 version) [Large language model].[https://chat.openai.com/chat] \n**In-text citation example**: (OpenAI, 2025)\n### **Image Attribution**\nWhen citing AI-generated images in online content, include attribution directly below the image or in the image caption.\n**Example**: (Image generated using Dall-E)\n### **Podcast Attribution**\nCite your podcast episode using APA Style.List the AI tool as author, followed by the label “(Host),” the date, the episode title and number, the description “[Audio podcast episode],” the name of the podcast, the production company, and a URL if available.\n### **Social Media Attribution**\nOn social media platforms, include attribution embedded in the image and in the image caption or post text.\n**Example**: \"AI-generated image of a cosmic landscape using Dall-E.\"\n## **Prompt engineering for consistent brand voice and tone**\nUsing a consistent brand voice and tone in external communications is important for maintaining a consistent and unified voice. As AI generated content becomes part of the content development process, it’s important to approach this process in a consistent manner as well.\nHere are some best practices for AI prompt engineering:\n1. **Be Clear and Specific:**Clearly state the topic, purpose, and desired outcome to guide the AI response accurately.\n2. **Give IA a persona &amp; skills:**To support the goal of being clear, specify a role or persona and set of skills for the AI that aligns with the desired output style or expertise.\n3. **Include Context:**Give background information, the intended audience, or a situation for the AI to consider.\n4. **Define the Tone and Style:**Specify the tone to ensure consistency with our brand\n5. **Use Simple, Direct Language:**Avoid complex or ambiguous language in prompts; clarity yields more relevant responses.\n6. **Guide the Response Length and Format:**Indicate if the response should be brief, detailed, in bullet points, or a summary to control length and format.\n**Our Voice &amp; Tone**:\n* **Inspiring and Visionary:**Our language should reflect a forward-thinking mindset, emphasizing the transformative impact of science and engineering on society. Use aspirational, vivid language that resonates with curiosity and the ambition to shape the future.\n* **Inclusive and Collaborative:**Harvard SEAS values diversity and the collaborative spirit of discovery; our language should reflect openness and respect for all perspectives. Choose inclusive language and frame messages in a way that invites a broad audience.\n* **Authoritative yet Accessible:**Our tone should convey expertise and knowledge while remaining approachable and clear. Use precise, informed language, and explain complex ideas simply when possible.\n* **Tone**: The tone should be warm, professional, and inclusive. While we maintain a sense of authority, it’s essential that the audience feels welcomed and respected. This tone reflects our commitment to excellence and openness.\n## In Office of Communications\n* [People] \n* [Services] \n* [Brand Style Guide] \n* [Brand Messaging] \n* [Logos &amp; Templates] \n* [Our Shield] \n* [Fonts] \n* [Color Palette] \n* [SWAG] \n* [Press Releases] \n* [Press Release Tips] \n* [Working with the Media] \n* [Institutional Voice Guidelines] \n* [Media Resources &amp; PR] \n* [Media, Filming, and Photography Policies] \n* [Harvard News Resources] \n* [Publications &amp; Newsletters] \n* [Website Editing Tutorials] \n* [AI Marketing Guidelines] \n* [Event Promotion] \n* [Social Media Guidelines]",
    "length": 7516,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Letter from Martha Whitehead: Initial Guidelines on Generative AI",
    "url": "https://library.harvard.edu/about/news/2023-07-13/letter-martha-whitehead-initial-guidelines-generative-ai",
    "text": "[Skip to main content] \n\n# Letter from Martha Whitehead: Initial Guidelines on Generative AI\n\nBy Harvard Library Communications Office\nJuly 13, 2023\n\n_Harvard Library Vice President Martha Whitehead shared the following response to University guidelines on using generative AI tools at Harvard._\n\nEarlier today, the University published [initial guidelines]  for the use of generative artificial intelligence (AI) tools such as ChatGPT and Google Bard.\n\nI would like to take this opportunity to reiterate some of the remarks I made when introducing the topic of Generative AI in Libraries at Harvard Library’s All-Staff Meeting on June 29. This topic seemed an appropriate one for our end-of-year meeting, as it is very much about building on past accomplishments and seizing opportunities for the future. It is also about upholding our core values.\n\nGenerative AI and research libraries share a fundamental promise: the ability to draw upon a broad corpus of existing information to answer questions and generate new information. In this equation, Harvard Library brings the fundamental value of access to information, and by that we mean _access to trustworthy information spanning centuries, regions, and voices around the globe._\n\nIn engaging with generative AI, we want to shape the emerging tools to mitigate biases and improve reliability, and we want to apply them appropriately in our work in information discovery and access, information literacy, information management, and preservation.\n\nThese are words worth repeating: _mitigate biases_. The work ahead of us in generative AI is fundamental to our [equity, diversity, inclusion, belonging, and antiracism (EDIBA) goals] and our role as a center for knowledge for academic programs and advanced research.\n\nHarvard Library will be an active partner in University explorations of the opportunities and challenges of current advances in generative AI, bringing our values, our digital expertise, and our understanding of the information landscape.\n\nWithin Harvard Library, we’re planning for a Library-wide initiative to explore various facets of generative AI in libraries, including user research, information literacy and academic success, research integrity and information policy, discovery, open repositories, and administrative innovation. I hope to share further details in the fall.\n\nMartha Whitehead\n\nVice President for the Harvard Library and University Librarian\n\nRoy E. Larsen Librarian for the Faculty of Arts and Sciences\n\nStay in the know\n\nSign up",
    "length": 2524,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Change to Harvard’s web publishing services and OpenScholar websites | Harvard University Information Technology",
    "url": "https://huit.harvard.edu/news/web-services-openscholar",
    "text": "Change to Harvard’s web publishing services and OpenScholar websites | Harvard University Information Technology[\nSkip to main contentarrow\\_circle\\_down\n] \nSearch\nSearchsearch\n[\n![Harvard University Information Technology] \n![Harvard University Information Technology] \n] \nmenucloseMenu\nSearch\nSearchsearch\n[\n![Harvard University Information Technology] \n![Harvard University Information Technology] \n] \n# Change to Harvard’s web publishing services and OpenScholar websites\nMay 03, 2023\nHarvard University Information Technology (HUIT) has previously offered the OpenScholar platform for members of the Harvard community to create websites. As our community’s needs evolve, we’re changing our web services to offer new website platforms, and transitioning away from the existing Harvard OpenScholar platform. This means:\n* **All existing Harvard OpenScholar sites will need to be moved to a new web platform by November 2025.**\n* **HUIT will continue to support all existing Harvard OpenScholar websites that are compliant with**[**Terms of Use**] **through November 2025.**\n* **HUIT no longer offers new OpenScholar websites.**\n### **What will happen to existing Harvard OpenScholar websites?**\nExisting Harvard OpenScholar websites are based on the Drupal 7 web platform technology that is approaching its end of life. In early 2023, HUIT moved all Harvard OpenScholar websites to a new hosting provider to ensure their continued support beyond Drupal 7’s end of life and through November 2025. However, all existing Harvard OpenScholar sites will need to be moved to a new web platform by November 2025.\n### **When and how will existing Harvard OpenScholar websites be moved to a new platform?**\nHUIT and School IT departments will soon begin contacting website owners directly with specific information. Please make sure your Harvard OpenScholar website’s[owners and members are all correct and up-to-date] in order to receive these communications.\n### **What new platforms will be available?**\nHUIT will offer two new platforms, based on modern WordPress and Drupal 10 technologies, that provide:\n* More functionality to meet a variety of needs, including better options for managing multiple websites.\n* Greater security, reliability, support, and up-to-date software.\n* Improved accessibility, design templates, appearance, and overall user experience.\nSome Harvard Schools will also offer additional platforms to eligible affiliates.\n### **Where can I get help?**\nSupport for all existing Harvard OpenScholar websites is now provided by HUIT:\n* You can find more information about this transition, including steps you can take to prepare, at[ostransition.hwp.harvard.edu].\n* If you need help with your existing website, or if you have questions or concerns about this upcoming change, please[contact the HUIT Service Desk].",
    "length": 2833,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "",
    "url": "https://provost.harvard.edu/resource/policyonaccesstoelectronicinformationpdf",
    "text": "1\nPolicy on Access to Electronic Information\nAs voted by the President and Fellows of Harvard College on March 31, 2014; amended \nMay 8, 2015. Current policy amendments proposed by the ECPOC and approved by the \nProvost on October 6, 2023.\nScope of Policy\nThis policy sets out guidelines and processes for University access to user electronic \ninformation stored in or transmitted through any University system. This policy applies to \nall Schools and units of the University.\nGeneral Statement\nMembers of the Harvard community rely on technology in multiple aspects of their work,\nteaching, research, study, and other activity. In doing so, they use electronic systems, \nnetworks, and devices that the University owns, provides, or administers. The University \nmakes these systems available for the purpose of carrying out the University’s various \nactivities. To promote trust within the University community, the University should be \ntransparent about its policy regarding the circumstances in which it may access user \nelectronic information stored in or transmitted through these systems. This policy \ntherefore sets out guidelines and processes that apply when the University seeks \naccess to such electronic information, consonant with the University’s interest in \nmaintaining an environment in which free academic inquiry thrives. This policy is \nintended to establish internal standards and procedures governing such access by the\nUniversity; it is not intended to create, nor does it create, any contractual or other legal \nobligation on Harvard’s part, or any contractual or other legal right or expectation in or \nfor any individual person.\nThe policy is grounded on, and should be interpreted and applied in accordance with, \nthese six important principles:\n● Access should occur only for a legitimate and important University purpose.\n● Access should be authorized by an appropriate and accountable person.\n● In general, notice should be given when user electronic information will be or has \nbeen accessed.\n● Access should be limited to the user electronic information needed to accomplish \nthe purpose.\n● Sufficient records should be kept to enable appropriate review of compliance with \nthis policy.\n● Access should be subject to ongoing, independent oversight by a committee that \nincludes faculty representation.\n2\nTerminology\nThe following terms are used in this policy with the following meanings:\n“University systems” refers to all services, networks, and devices owned, provided, or\nadministered by any unit of the University, such as email services, Internet access, file \nservers, voice message services, storage devices and services, laptop and desktop \ncomputers, phones and other mobile devices, and usage and access logs.\n“Users” refers to Harvard faculty, others holding academic appointments at Harvard, \nstudents, staff, and other employees.\n“User electronic information,” for any particular user, refers to:\n(i) Documents and communications, including emails, voice mails and text messages, \nand their associated metadata, which are generated by or created or received on \nUniversity systems and located in files and accounts associated with a particular user. \nFor example, this would include all emails and their attachments in a user’s inbox, sent \nitems folder, or other email folders that are recognized as part of the account associated \nwith that user, and all documents in that user account’s document folders; and\n(ii) Information generated by automated processes triggered by that user’s use of \nUniversity systems, such as tracks of Internet use and logs of access to facilities (“Log \nInformation”),\nExclusions\nThis policy does not limit or restrict access to or use or disclosure of user electronic \ninformation if such access, use, or disclosure is part of the normal functionality and \npurpose of a University system.\nThis policy does not limit or restrict:\n(a) access to or use or disclosure of (1) records regularly maintained by the University in \nthe ordinary course of business, such as personnel records or student academic \nrecords, records stored in a Harvard archival repository, or information provided by \npersonnel in connection with regular University record-keeping, such as entries in a \nUniversity travel registry; or (2) Log Information accessed by the University without \nidentifying or seeking to identify any particular user; or\n(b) access to or use or disclosure of information otherwise authorized by or pursuant to \na vote of the Harvard Corporation.\n3\nContents\nI. Reasons for Access\nII. Authorization of Access\nIII. Notice\nIV. Scope of Access\nV. Records of Process\nVI. Oversight Committee; Amendments\nI. Reasons for Access\nUniversity personnel do not routinely monitor the content of information transmitted \nthrough or stored in University information systems. The University may obtain access \nto user electronic information in some circumstances, but only for a legitimate \ninstitutional purpose. The paragraphs below describe certain purposes for which the \nUniversity may access such information. While this list is expected to cover most \ninstances of access, the list is not intended to be exhaustive. The University may \naccess user electronic information for reasons that likewise advance a legitimate \ninstitutional purpose, as determined by a person designated to authorize access \npursuant to this policy and subject to review by the oversight committee as described in \nPart VI.\nAlthough this policy applies to the electronic information of faculty, staff, and students \nalike, in evaluating the institutional purpose, the person designated to authorize access \nshould in each case weigh not only the stated reasons for access but also the possible \neffect of access on University values such as academic freedom, trust, and confidence.\nSystem Protection, Maintenance, and Management\nUniversity systems require ongoing maintenance and inspection to ensure that they are \noperating properly; to protect against threats such as attacks, malware, and viruses; \nand to protect the integrity and security of information. University systems also require \nregular management, for example, in order to implement new software or other facilities. \nTo do this work, the University may scan or otherwise access user electronic \ninformation.\nBusiness Continuity\nUser electronic information may be accessed for the purpose of ensuring continuity in \nbusiness operations. This need can arise, for example, if an employee who typically has \naccess to the files in question is unavailable due to illness or vacation, or following the \ndeparture of an employee from the University.\n4\nSafety Matters\nThe University may access user electronic information to deal with exigent situations \npresenting threats to the safety of the campus or to the life, health, or safety of any \nperson.\nLegal Process and Litigation\nThe University may access user electronic information: (1) in connection with \nthreatened or pending litigation, law enforcement investigations, or other government \ninvestigations; (2) to identify and disclose information as required by law or legal \nprocess; (3) to investigate or assist in the investigation of unlawful activity directed at \nthe University or a member of the Harvard community, or (4) to investigate or assist in \nthe investigation of unlawful activity by a member of the Harvard faculty or staff or other \nHarvard employee.\nInternal Investigations of Misconduct\nThe University may access user electronic information in connection with investigations \nof misconduct by students, faculty, staff, and other members of the University \ncommunity, but only when the authorizing person, after weighing the need for access \nwith other University values, has determined that such investigation would advance a \nlegitimate institutional purpose and that there is a sufficient basis for seeking such \naccess. As described in Section VI of this policy, all decisions to access user electronic \ninformation are subject to review by an Oversight Committee.\nThis policy does not apply to reviews of research misconduct allegations conducted \nunder established School-based policies.\nII. Authorization of Access\nAccess to user electronic information should be authorized by an appropriate person, as \nset forth below. In deciding whether to approve access, the authorizing person should \nconsider whether effective alternative means to obtain the information are reasonably \nand timely available. In all cases, access must comply with applicable legal \nrequirements.\nAuthorization for access to user electronic information may be provided by the consent \nof the user.\n5\nOther cases should be handled as follows:\nIf the user is a faculty member or other holder of an academic appointment at Harvard, \nthe dean of the relevant Faculty must authorize access.\nIf the user is an employee other than a faculty member: (1) the human resources officer \nor his/her designee for the relevant School or administrative unit must authorize access \nin business continuity cases; and (2) the dean of the relevant Faculty or the senior \nadministrator of the relevant unit if not a Faculty, or their designees, must authorize \naccess in investigative or other cases.\nIf the user is a student, the School-level dean or the dean’s designee must authorize \naccess. Any authorization of access shall apply only to the particular situation and user \nor users. Any other instance of access must be separately authorized.\nNo independent authorization is required: \n● For access by information technology personnel to conduct routine system \nprotection, maintenance, or management purposes in accord with internal \nprotocols and processes. \n● For access requested by the Office of the General Counsel: (1) in connection \nwith threatened or pending litigation, law enforcement investigations, or other \ngovernment investigations; and (2) to identify and disclose information as \nrequired by law or legal process. \n● For access expressly granted in a legal agreement, Certificate of Gift, or other \nstewardship agreement executed between a Harvard archival repository and a \nuser or their legal representative.\n● For access by Harvard University Archives staff to determine whether records \nshould be transferred to a Harvard archival repository.\nIn exigent situations involving a threat to campus safety or the life, health, or safety of \nany person, access may be authorized by the Chief of Police of Harvard University or \ntheir designees. If emergency conditions do not allow for prior authorization, the matter \nshall be reported to the Executive Vice President as promptly as possible. In addition, \naccess may be authorized by the University's General Counsel or their designees, in \norder (1) to investigate or assist in the investigation of unlawful activity directed at the \nUniversity or a member of the Harvard community, or (2) to investigate or assist in the \ninvestigation of unlawful activity by a member of the Harvard faculty or staff or other \nHarvard employee.\n6\nFor some requests to search user electronic data, it may not be possible to identify any \nparticular user in advance. For example, requests for logs of access to a University \nfacility (swipe card data) often are intended to find out who entered a facility during a \nparticular period; in such cases, the requestor cannot identify a particular user or users \nbecause the goal of the search is to learn those identities. Such data requests may still \nbe subject to one of the prior provisions of this Section II, for example, those relating to \nlaw enforcement investigations or emergencies.\nOtherwise, such data search requests must be authorized by the dean of the relevant \nFaculty or the senior administrator of the relevant unit if not a Faculty, or their \ndesignees, in the School or unit where the requestor works.\nIII. Notice\nWhen the University intends to access user electronic information, notice ordinarily \nshould be given to that user. All reasonable efforts should be made to give notice at the \ntime of access or as soon thereafter as reasonably possible.\nSystem protection, maintenance, and management — Individual notice is not required \nfor ordinary system protection, maintenance, or management. Notice should be given if \nthe access relates specifically to the activity of an individual user.\nBusiness continuity — Individual notice is not required for access to user electronic \ninformation for purposes of business continuity, in accordance with established \nUniversity practice and the common understanding that individual notice in such cases \nis typically not practical.\nLegal restrictions — Individual notice is not required where the University is subject to \nlegal constraints on its ability to give notice.\nEmergencies and other extraordinary cases — Contemporaneous notice is not required \nin cases where there is insufficient time, where giving notice would otherwise interfere \nwith an effective response to an emergency or other compelling need (e.g., at a stage of \nan internal investigation where giving notice may compromise the investigation), or \nwhere it is impractical (e.g., in the case of a former employee). The decision not to give \ncontemporaneous notice must be made by the person designated by this policy to \nauthorize the access. In such cases, notice will ordinarily be given as soon as practical.\nThe person designated by this policy to authorize access may dispense with the notice \nrequirement for good cause. Any such decision, and the reasons for it, shall be \n7\ndescribed in the records described in Part V of this policy and may be reviewed by the \noversight committee, as set forth in Part VI.\nIV. Scope of Access\nThe University shall adopt reasonable steps, whenever practicable, to limit access \nobtained under this policy only to user electronic information that is related to the \nUniversity’s purpose in obtaining access. These steps will vary depending on the \ncircumstances of the search and may include, by way of illustration, designing searches \nto find specifically designated items, as opposed to categories of information.\nParticipation in the search, and access to the information, should be limited to those \npersonnel with a reasonable need to be involved.\nV. Records of Process\nAny person who authorizes access to user electronic information shall provide that \nreasonable records of the decision process and the reasons for the decision are made \nand preserved.\nThe persons who implement access to user electronic information shall make \nreasonable records and logs of the steps taken to access the information. All \nimplementation records shall be delivered to the University Chief Information Officer.\nCopies of the information accessed should be retained only as needed to effectuate the \npurposes of the access and securely disposed of promptly thereafter.\nThe accessed information and the records and logs of the search shall be kept \nappropriately secure.\nIn all instances of access under this policy, records adequate to permit effective review \nas described in Part VI of this policy should be kept.\nVI. Oversight Committee; Amendments\nThis policy, its implementation, and instances of access under this policy shall be \nsubject to review by an oversight committee to be constituted by the University, which \nshall include faculty and senior administrators. The oversight committee shall make \nrecommendations to the President and/or Provost as to the implementation of the policy \nand possible amendments. Amendments to this policy may be approved in writing by \nthe President or Provost at any time, provided that if the President or Provost\ndetermines that an amendment raises material issues warranting the attention of the \n8\nCorporation, such amendment will be presented to the Corporation for its approval. All \namendments will be reported to the Corporation for its information. The oversight \ncommittee shall also make periodic public reports on the implementation of this policy.\nIn carrying out its responsibilities, the oversight committee may review the records \ndescribed in Part V of this policy, subject to redaction as necessary to protect individual \nusers.",
    "length": 16202,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Love Data Week 2026: Where Is the Data? | Harvard Library",
    "url": "https://library.harvard.edu/about/news/2026-02-09/love-data-week-2026-where-data",
    "text": "Love Data Week 2026: Where Is the Data? | Harvard Library[Skip to main content] \n# Love Data Week 2026: Where Is the Data?\nByHarvard Library Communications OfficeFebruary 09, 2026\n***A message from***[***Yuan Li***] ***, University Scholarly Communication Officer and Director of Open Scholarship and Research Data Services, and***[***Emre Keskin***] ***, Assistant Vice Provost and Managing Director for Research Data Strategy***\nEach year,[Love Data Week] invites the global research community to reflect on how data are created, managed, cared for, shared, and sustained. This year’s theme “Where is the Data?” asks a deceptively simple question that sits at the heart of responsible, impactful research and is a way to get people thinking about data’s journey from collection through storage, and preservation. Good research data management is a continuous, end-to-end process, not a single action or tool. It involves thinking intentionally about data from the earliest stages of a project—how data will be created or collected, organized, documented, protected, analyzed, shared, and preserved—through the full research lifecycle. By taking a holistic approach, researchers can work more efficiently, meet policy and ethical obligations, and ensure that their data remain discoverable, accessible. understandable, usable, and valuable over time.\nAt Harvard, this question resonates deeply. Research data live in many places: in repositories (including archival repositories, analog and digital), in active research environments, in public records, in computational systems, and sometimes in fragile or overlooked formats. Ensuring that data are findable, accessible, usable, and preserved over time requires not only technology, but coordination, stewardship, and shared responsibility.\nHarvard Library, in collaboration with partners across the University, especially the Office of the Vice Provost for Research (OVPR) and University Research Computing (URC), is committed to helping researchers answer the question “Where is the data?” at every stage of the research lifecycle. One example is our work on[public data conservation], which focuses on identifying, preserving, and providing access to at-risk public datasets that are essential for research, teaching, public policy, and public knowledge. These efforts reflect a broader commitment to data stewardship as a public good, especially at a time when data provenance cannot be taken for granted.\nAnother key initiative is Harvard Research Data Connect (HRDC), a collaborative effort among Harvard Library, University Research Computing, and the Office of the Vice Provost for Research. HRDC is designed to help researchers navigate Harvard’s complex data ecosystem by connecting them to computing infrastructure, tools, services, and—most importantly—people. The initiative focuses on linking researchers with staff experts and strengthening staff-to-staff connections across the University. In doing so, HRDC helps ensure that research data are not only securely stored, but also properly supported, understood, and responsibly managed. The overarching goal is to enable higher-quality research outputs by improving access to the data, tools, and expertise required for data-intensive research. Currently, HRDC is focusing on two projects: RSpace and Data Discovery. RSpace is an electronic lab notebook (ELN) that supports the management and documentation of research data within active research workflows. Through HRDC, we are developing technical support services, documentation, training, and outreach to encourage broader campus adoption. The Data Discovery project, which is still in the early stages of development, seeks to address the very question of “Where is the data?” Its goal is to create a system that enables the Harvard community to discover and access data, whether generated by Harvard researchers or acquired through third-party licenses and purchases.\nIn addition to these specific collaborative initiatives, Harvard Library provides a wide range of tools, services, and data-related training to the Harvard community. Together, these efforts reflect a shared goal: to ensure that data are visible, discoverable, connected, and cared for, rather than siloed or lost. During Love Data Week, we invite you to join us for[workshops, events, and conversations] that explore where data live, how they move, and how they can be better supported throughout the research lifecycle. These sessions are designed to be practical, collaborative, and relevant—whether you are generating data, analyzing it, managing it, sharing it, or helping others do so.\nAsking “Where is the data?” is not only a technical question, but a shared responsibility. It invites us to think critically and holistically about discoverability, access, sustainability, and stewardship, and to recognize the people, systems, and partnerships that make research data usable and trustworthy over time. Through continued collaboration across the University, we can ensure that data remain a foundation for discovery, transparency, and public knowledge—today and in the future.\nStay in the know\nSign up",
    "length": 5150,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Research Data Management and Policy Compliance Resources",
    "url": "https://research.harvard.edu/research-policies-compliance/research-data-management/",
    "text": "[Skip to content] \n\n[HARVARD.EDU] \n\n[< Back to Research Policies & Compliance] \n\nMany Harvard faculty, staff, scholars, and student members engage in research that involves the collection or use of identifiable, sensitive or private information. Federal law and Harvard policy provide specific guidance and requirements for protecting identifiable research information. Harvard’s Research Data Management framework ensures that research data is handled in compliance with legal, ethical, and institutional requirements.\n\n### Policy Contacts\n\n[Emre Keskin] _Assistant Vice Provost and Managing Director for Research Data Strateg_ y\n\n[Lorena Rosiles] _Regulated Data Compliance Officer_\n\n(First published May 29, 2025) **NIH has accelerated the effective date for compliance with its updated [2024 Public Access Policy]. As announced in [NIH Notice NOT-OD-25-101], the deadline has moved from December 31, 2025, to July 1, 2025. The revamped policy mandates final peer-reviewed manuscripts that result from NIH-funded research be made freely available through PubMed Central (PMC), no later than the article’s official date of publication.** **To help researchers navigate this change, the following resources are available:**– [Updated NIH Public Access Policy FAQs]  – Answers to common questions about key policy changes, manuscript deposits, compliance timelines, and copyright considerations.– [Information Sessions]  – Online sessions offering opportunities to review essential updates and ask questions.(First published April 8 , 2025) **Effective immediately, DUAs with US bulk data in excess of the thresholds should be reported to OVPR for record-keeping.** **Additional information can be found in the [OVPR Bulk Data Guidance (PDF)].**(First published March 28, 2025) **Effective immediately, the NIH requires researchers to obtain prior approval before using controlled-access human genomic data with generative AI tools, including for training AI or large language models. This policy aims to protect participant privacy and ensure responsible data use as AI technologies evolve. For more information, see [NIH Notice NOT-OD-25-081] and OVPR Gen AI and [Genomic Data FAQ (PDF)].**(First Published August 29, 2024) **Effective immediately, any Data Use Agreement (DUA) or grant submission that necessitates a System Security Plan (SSP) compliant with NIST standards (e.g., SP 800-53, SP 800-171) must be reviewed by the Office of the Vice Provost for Research (OVPR) and Harvard University Information Technology (HUIT) prior to submission.** **Additional information can be found in the [OVPR Review of System Security Plans Policy (PDF)].**\n\n### **Key Policies and Guidelines**\n\nHarvard’s policies protect sensitive data and ensure privacy, compliance, and ethical management across disciplines. They set security standards to mitigate risks and uphold academic integrity. Applicable to all Harvard affiliates handling private information, these guidelines mandate compliance for anyone using Harvard’s resources or conducting research under its authority.\n\nHarvard Research Data Security Policy (HRDSP)\n\n**What is essential**: [HRDSP and Associated Guidance] ​ establishes essential security measures tailored to the specific risks associated with each research project. Working alongside the Harvard Enterprise Information Security Policy (HEISP), it ensures robust protection of sensitive research data, including that related to human subjects, Data Use Agreements (DUAs), and data governed by regulatory and intellectual property requirements. This policy is applicable to all forms and media of research data, whether stored at Harvard or managed remotely, covering researchers, research teams, and administrators who handle confidential information.\n\n**How to comply**: To effectively protect research data, researchers, IRBs, Information Security Officers, Negotiating Offices, and administrators must understand and fulfill their data privacy and security responsibilities. The [HRDSP and Associated Guidance] ​ ​ offers guidance on managing research data, along with associated support systems, procedures, and reviews.\n\n**Resources**:\n\n- [HRDSP and Associated Guidance​] \n- [HRDSP Applications Summary and Order of Reviews] \n- [Research Data Services] \n- [Research Data Security Examples] \n\nHarvard Enterprise Information Security Policy (HEISP)\n\n**What is essential:** The [HEISP Policy] highlights its commitment to safeguarding crucial confidential information and IT systems. Faculty, staff, and students, are responsible for securing this information, following specific guidelines for compliance. The policy emphasizes authorized use, secure handling, regular updates, appropriate disposal, and careful third-party evaluation. Any potential data breaches must be reported promptly.\n\n**How to Comply:** Data protection requirements vary based on data classification levels: the more sensitive the data, the stricter the security measures. These information security requirements apply to everyone at Harvard. The [HESIP web page] offers detailed guidance on compliance and should be integrated into daily life at Harvard to protect both Harvard’s confidential data and your personal information.\n\n**Resources**:\n\n- [HEISP Policy] \n- [HESIP web page] \n\nHarvard Genomic Data Sharing Policy (GDSP)\n\n**What is essential:** The [Harvard Genomic Data Sharing Policy] mandates that large-scale human and nonhuman genomic data produced from NIH-funded research be shared with an NIH-designated data repository, provided it aligns with participant consent. This policy sets protocols for regulatory compliance, promoting transparency, collaboration, and reproducibility in genomic research. It balances data accessibility with rigorous privacy protections, ensuring ethical standards and legal obligations are met.\n\n**How to comply**: Researchers must secure IRB certification before data submission to NIH-designated repositories, ensuring data de-identification aligns with participant consent and privacy standards. This ensures that researchers handle human genomic data appropriately, fostering a responsible and open research environment.\n\n**Resources**:\n\n- [Harvard Genomic Data Sharing Policy] \n\nHarvard Research Data Ownership Policy\n\n**What is essential:** The [Research Data Ownership Policy] ​  outlines that Harvard University owns all research data generated through projects conducted under its authority or using its resources. While the PI and researchers manage and safeguard the data, the University is ultimately responsible for compliance with legal and sponsor requirements, ensuring confidentiality and security. The policy outlines roles, responsibilities, and data retention guidelines, and specifies procedures for transferring data if a researcher departs.\n\n**How to comply**: To comply with [Research Data Ownership Policy] ​, researchers must acknowledge the University’s data ownership and collaborate in its management. Principal Investigators are responsible for ensuring proper data management, storage, and accessibility, meeting all University, legal, and sponsor requirements. This involves setting up procedures for data retention, confidentiality, and sharing while respecting data use agreements. Researchers should update records and coordinate with the Vice Provost for Research on data transfers or inquiries to ensure secure, compliant data handling.\n\n**Resources**:\n\n- [Research Data Ownership Policy] ​\n\nRecords Retention Guideline\n\n**What is essential:** Retaining research records is crucial for supporting the integrity and validation of research findings, ensuring accountability in the use of research funds, and protecting intellectual property. Adhering to these guidelines enables researchers to provide clear documentation during audits and reviews, thereby enhancing the transparency and credibility of their research.\n\n**How to comply:** The [Retention and Maintenance of Research Records and Data Frequently Asked Questions (“FAQs”)] has been developed to outline the minimum University requirements for retaining research records and data, organized by principle. Each school is responsible for designating a representative to oversee retention issues and offer discipline-specific guidance. Researchers should exercise prudent judgment and consult field-specific standards to determine which records are critical for retention.\n\n**Resources:**\n\n- [Retention of Research Data and Materials Guidance] \n- [Retention and Maintenance of Research Records and Data Frequently Asked Questions (“FAQs”)] \n\nGeneral Data Protection Regulation (GDPR)\n\n**What is essential:** The GDPR is a comprehensive regulation that imposes various obligations on organizations managing personal data of individuals in the European Economic Area (EEA). For Harvard, this means maintaining high standards of data protection regardless of where the processing takes place.\n\n**How to comply**: The  [GDPR Research Guidance] and the [GDPR Readiness website]  are provided to help researchers prepare for GDPR compliance. These resources provide tools, checklists, and educational materials to reinforce the protection of personal data.\n\n**Resources**:\n\n- [GDPR Research Guidance] \n- [GDPR Readiness website] \n\nControlled Unclassified Information (CUI)\n\n**What is essential:** CUI is defined by Executive Order 13556 as “information possessed by or generated on behalf of the Federal Government that requires safeguarding or dissemination controls pursuant to and consistent with applicable law, regulations, and government-wide policies that is not classified under Executive Order 13526 or the Atomic Energy Act, as amended.” Although it is not classified, CUI requires protection due to its sensitive nature. Managed by the National Archives and Records Administration (NARA), the CUI Program covers diverse data types, including export-controlled information, technical data, and health records. A comprehensive list of CUI categories is available on the [CUI Registry of the National Archives].\n\n**How to comply:** Harvard is considered an entity that acts on behalf of the government and is obligated to properly handle and protect CUI. To comply with security standards, familiarize yourself with 32 CFR Part 2002, which governs CUI handling. Implementing the safeguarding measures outlined in NIST Special Publication 800-171 (NIST SP 800-171) is crucial for researchers under federal contracts. Check your research agreements to determine if compliance is required, particularly if:\n\n- Your sponsor designates certain data as CUI or subject to NIST controls.\n- Your solicitations or contracts include references to:\n - NIST 800-171 Protecting Controlled Unclassified Information in Nonfederal Systems and Organizations\n - 32 CFR 2002 Controlled Unclassified Information\n - 52.204-21 Basic Safeguarding of Covered Contractor Information Systems\n - 252.204-7008 Compliance with safeguarding covered defense information controls\n - 252.204-7012 Safeguarding covered defense information and cyber incident reporting\n\n**Resources:**\n\n- [NIST Protecting Controlled Unclassified Information] \n- [CUI Registry of the National Archives] \n- [CUI Main Informational Website] \n- [NIST SP 800-171r2] \n- [NIST SP 800-172] \n\n### **Federal Data Management Policies**\n\nHarvard University provides resources to assist researchers and administrators in navigating the data management plan (DMP) requirements set by federal funding agencies. Below are agency-specific materials and references to relevant Harvard tools and offices.\n\nIf you have questions about specific sponsor requirements, please speak with your cognizant sponsored research office ( [OSP], [HMS ORA], [HSPH ORA]). If you have questions about the sensitivity of your data, or appropriate resources, please speak with your local IT provisioner or [information security officer].\n\nNational Institutes of Health (NIH)\n\nThe NIH has implemented the [Data Management and Sharing (DMS) policy], effective January 25, 2023, to encourage scientific data sharing. All NIH-funded projects generating [Scientific Data] must include a DMSP.\n\n**Resources:**\n\n- [Harvard NIH DMSP Budgeting and Application Instructions – Tip Sheet (09/21/2023)]: Guidance for Principal Investigators and grant managers on completing applications, including the required DMSP.\n- [NIH DMS Policy Central Reviewer Tip Sheet:]  Instructions for central reviewers evaluating applications, JIT requests, or awards involving the DMSP.\n- [Harvard Briefing Sheet for the 2023 Policy:]  Overview of the policy, responsibilities, and resources.\n- [Harvard FAQ for the 2023 Policy:]  Harvard-specific answers to questions based on current NIH guidance.\n- [SEAS Research Data Management]: Support and consultation on Data Management Plans\n- [Longwood Research Data Management]  (RDM): Information and resources on NIH Data Management Plans\n- [DMPTool]: A web-based platform offering step-by-step guidance for drafting DMPs, including NIH-specific templates and samples.\n- [Harvard Library’s Research Data Resources]: Explore training, find services for managing and sharing research data, and connect with specialized data services to ensure your data is findable, accessible, interoperable, and reusable ( [FAIR])\n\nNational Science Foundation (NSF)\n\nSince 2011, the NSF has required DMPs for all grant applications. These plans have become a critical component of the review process and are thoroughly evaluated. These plans are essential to the review process, as they detail data preservation strategies and associated costs. DMPs are evaluated as part of proposal review.\n\n**Resources:**\n\n- [NSF’s Dissemination and Sharing of Research Results] \n- [NSF’s Award and Administration Guide (AAG) Chapter VI.D.4] \n- [NSF’s Grant Proposal Guide, Chapter II.C.2.J] \n- [SEAS Research Data Management]: Support and consultation on Data Management Plans\n- [Longwood Research Data Management]  (RDM): Information and resources on NSF Data Management Plans\n- [DMPTool]: Aids in creating and sharing Data Management Plans with NSF-specific templates and samples.\n- [Harvard Library’s Research Data Resources]: Explore training, find services for managing and sharing research data, and connect with specialized data services to ensure your data is findable, accessible, interoperable, and reusable ( [FAIR])\n\n### Data Use Agreements (DUA)\n\nA Data Use Agreement (DUA) is a binding contract that outlines the terms for accessing and handling nonpublic data provided by one entity (the “Provider”) to another (the “Recipient”). These agreements are crucial when external parties share data with Harvard or when Harvard shares data with other organizations. As research-related agreements, DUAs must be reviewed and signed by the Office for Sponsored Programs or the Longwood Area Offices for Research Administration (HMS/HSPH), following the [Delegation of Signing Authority].\n\nDUA Compliance\n\nAll DUA requests must be submitted for review, negotiation, and endorsement through the University-wide [Agreement System]. Researchers are required to link their DUA submissions to a corresponding Research Data Safety submission to ensure that security measures are properly assessed. The Harvard Data Requestor initiates and manages these submissions, working closely with the Negotiating Office. Comprehensive guidance on these procedures, including step-by-step instructions, is provided in the  [DUA Guidance and Policy] and the [Agreements-DUA Submission Guide], which details the processes and reviews associated with DUAs within the [Agreement System].\n\n**Resources:**\n\n- OVPR contact:  [Emre Keskin] \n- For additional information and best practices on using the Agreements System, view the [Agreements-DUA Submission Guide] \n- Harvard T.H. Chan School of Public Health: Sponsored Programs Administration (SPA): [dua@hsph.harvard.edu] \n- Harvard Medical and Dental Schools: Office of Research Administration (ORA): [SPAContracts@hms.harvard.edu] \n- University Area, all other Harvard schools: Office for Sponsored Programs (OSP): [dua@harvard.edu] \n- [Research Administration Portal]  reflecting outstanding research administration activities\n\nCMS DUAs: Semi-Annual and Annual Reporting\n\nSchools with active Centers for Medicare & Medicaid Services (CMS) DUAs are required to submit semi-annual and annual reports to meet University and federal standards. These reports, due by August 31st and every six months thereafter, are crucial for ensuring transparency, facilitating data-driven decision-making, and maintaining compliance.\n\nThe Office of the Vice Provost for Research (OVPR) will specify the data required for annual reports by March 1st each year. Prompt submission is critical; failure to comply may delay approval for new CMS Data requests and could lead to University-wide sanctions under the new CMS Data Management Plan Self-Assessment Questionnaire (DMP-SAQ) requirements.\n\nThe annual report for, must include the following components:\n\n- Contacts: Provide contact details for the primary point of contact for the report and a list of any other contributors.\n- Executive Summary: A concise summary highlighting:\n - The number of CMS DUAs active during the reporting period,\n - The number of active CMS DUAs as of the end of the reporting period,\n - The number of active CMS DUAs for physical files,\n - The number of active CMS DUAs for VRDC access,\n - The number of new CMS DUAs initiated during the reporting period,\n - The number of CMS DUAs that were closed during the reporting period\n - For closed DUAs for physical files:\n - Was all data destroyed and certification of destruction provided to CMS?\n - If no, was data approved for re-use under another active DUA?\n - If yes, please provide information on closed DUA and the active DUA re-using the data\n - If no, please explain.\n\n- Details on Active CMS DUAs: A detailed breakdown of the existing Active CMS DUAs, specifying:\n - PI or PIs (their current affiliations)\n - associated data safety review(s) and expiration date(s)\n - location of CMS data (and consistency of data location with what is on the DUA),\n - whether the IT infrastructure satisfies CMS requirements (and if not, what are the plans to achieve compliance),\n - and notes on any amendments to the DUAs that are in progress.\n\n### **Institutional Compliance Management Program (ICoMP)**\n\nThe Office of the Vice Provost for Research (OVPR) is committed to supporting research involving regulated data by ensuring compliance and mitigating risks. Through the Institutional Compliance Management Program (ICoMP), we conduct thorough reviews and approvals for Regulated Data Projects, with a focus on those presenting management or reputational risks. This oversight ensures research teams meet all data requirements, benefiting participants, researchers, and the institution. By monitoring these projects, the ICoMP enhances accountability, transparency, and informed decision-making, making OVPR oversight crucial for managing complex data initiatives.\n\nOVPR Review Criteria and Submission Process\n\n**Criteria for OVPR review:**\n\n- **DUAs:** Any agreement involving a System Security Plan (SSP) according to NIST standards (e.g., SP 800-53, SP 800-171).\n- **CMS DUAs:** All new or extended Centers for Medicare and Medicaid Services (CMS) DUAs need approval from prior to submission of the signed DUA or extension request to CMS.\n- **Grant Submissions:** Projects involving regulated data that may pose management or reputational risks.\n- **DOJ Bulk Data Rule**: DUAs with U.S. bulk data in excess of the following bulk thresholds: 100,000 US persons for covered personal identifiers, 10,000 US persons for financial and personal health data, 1000 US persons for precise geolocation, biometric identifiers, human ‘omic data (other than human genomic data), 100 US persons for human genomic data.\n\n**Submission Process for OVPR review:**\n\n- **For** **Data Use Agreement (DUA) Review:** Within the Data Safety module, the PI/DUA Team assigns an ancillary review to the University Research Data Officer organization which will email the OVPR to notify them there is a project pending their review. The URDO reviews the project. If there are no concerns, they electronically sign to approve, permitting the Submitting Office to proceed with the DUA submission.\n- **For Sponsored Proposals/Awards review:** The Central Reviewer will edit the Required Signatures in GMAS and add the “Provost Signatory” role to the list, which alerts the University Research Data Officer (URDO) that there is a Regulated Data Project for OVPR Review. The URDO, with any necessary additional reviewers, evaluates the proposal. Upon approval, the URDO signs electronically, allowing the Submitting Office to proceed with the agreement.\n\n### **Principal Investigator (PI) R** esponsibility\n\nCompliance with data protection and use requirements is the responsibility of the principal investigator. Each PI should review their data use agreements, grants and other contracts to see if any such requirements are included. Harvard personnel working under such an agreement, grant, or contract must, at a minimum, comply with those protection requirements, as well as any disposition obligations. In addition, it is the PI’s responsibility to ensure any necessary reviews occur, including [Data Safety/Security], [DUA Reviews], and Institutional Review Board Reviews, and other research-related reviews governed by the [Negotiating and Signing Authority Policy].\n\nResearch Administration Portal\n\nThe [Research Administration Portal] shows faculty and researchers their outstanding research administration and compliance activities, including reviews related to data management, and provides an overview of their portfolio. The application includes projects and protocols from:\n\n- [Agreements-DUA] \n- [Data Safety/Security] \n- [ESTR] \n- [GMAS] \n- [OAIR] \n\n### **Data Safety/Security P** lan\n\nThe Data Safety/Security Plan is a fundamental document that ensures the secure management of research data. It sets forth protocols for data storage, access, and protection, aligning with the University’s data protection standards. This plan is mandatory in specific scenarios, such as when required by the sponsored award review process or when handling sensitive data (e.g., DSL 3, 4, or 5) or data exchange with subcontractors. Researchers, spearheaded by the Responsible Faculty Member, must draft and submit the plan for review.\n\nData Safety/Security Plan Compliance\n\nTo comply with the Data Safety/Security Plan, researchers must ensure that data storage aligns with the required Data Security Levels (DSL) and adhere to the terms outlined in the Data Use Agreement (DUA). In the event of a breach or unauthorized access to data, it is essential to promptly notify the Research Team, Negotiating Office, and any other relevant parties such as the IRB, in accordance with the Harvard Enterprise Information Security Policy (HEISP). This ensures that proper steps are taken to mitigate risks and maintain the integrity of the research data.\n\n**Resources:**\n\n- [Data Safety Levels website] \n- [Data Safety/Security] [Agreement System] \n\nData Security Officers Contact\n\n| | | | | |\n| --- | --- | --- | --- | --- |\n| **School** | **Reviewer Name** | **Email** | **Backup Reviewer Name** | **Backup Email** |\n| Harvard Business School \\[HBS\\] | HBS Information Security | [informationsecurity@hbs.edu] | n/a | n/a |\n| Harvard Faculty of Arts and Sciences \\[FAS\\] | Ingrid Skoog | [ingrid\\_skoog@harvard.edu] | Central Security Team | [ithelp@harvard.edu] |\n| Harvard Graduate School of Design \\[GSD\\] | Mark Hagen | [mhagen@gsd.harvard.edu] | n/a | n/a |\n| Harvard Graduate School of Education \\[HGSE\\] | Sarah Bystran-Pruski | [sarah\\_pruski@gse.harvard.edu] | n/a | n/a |\n| Harvard Kennedy School \\[HKS\\] | Megan Potterbusch | [mpotterbusch@hks.harvard.edu] | Christina Sirois | [christina\\_sirois@hks.harvard.edu] |\n| Harvard Medical School \\[HMS\\] | Joe Zurba | [joseph\\_zurba@hms.harvard.edu] | Aprillia Powers | [Aprillia\\_Powers@hms.harvard.edu] |\n| Harvard John A. Paulson School of Engineering and Applied Sciences \\[SEAS\\] | Judit Flo Gaya | [jflo@seas.harvard.edu] | n/a | n/a |\n| Harvard T.H. Chan School of Public Health \\[HSPH\\] | Andy Ross | [andrew\\_ross@harvard.edu] | SPH Security Team | [helpdesk@hsph.harvard.edu] |\n\nTable with Data Security Officers Contact Information",
    "length": 24437,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Generative AI @ Harvard",
    "url": "https://www.harvard.edu/ai/",
    "text": "Generative AI @ Harvard - Generative AI @ Harvard\n[Skip to main content] \n[![A logo that says Generative AI at Harvard]] \nGenerative AI**\n# Generative AI @ Harvard\nGenerative AI tools are changing the way we teach, learn, research, and work. Explore Harvard's work on the frontier of GenAI.\n## Resources for the Harvard community\n### Teach with GenAI\nResources for faculty\n[Resources for faculty] \n### Learn with GenAI\nResources for students\n[Resources for students] \n### Research with GenAI\nResources for scholars and researchers\n[Resources for scholars and researchers] \n### Work with GenAI\nResources for staff\n[Resources for staff] \na broader scope\n## AI @ Harvard\nGenerative AI is only part of the fascinating world of artificial intelligence.\n[Learn more] \n## News\n![] \n### Technically, it’s possible. Ethically, it’s complicated.\nSurge in AI use heightens demand for Harvard program that examines social consequences of computer science work\n[Read the full story] \n![] \n### It feels like AI understands, but do we care? New research on empathy\nNew paper from Amit Goldenberg and the Digital Data Design Institute at Harvard (D^3) sheds light on the &quot;human empathy premium&quot;\n[Read the full story] \n![] \n### Most gen AI players remain &#039;far away&#039; from profiting: interview with Andy Wu\nAre we in for a reckoning?\n[Read the full story] \n![] \n### When AI joins the team, better ideas surface\nFabrizio Dell&#039;Acqua, Raffaella Sadun, and Karim Lakhani offer recommendations for applying AI to teamwork\n[Read the full story] \n![] \n### AI presents challenges to journalism —but also opportunities\nData editor explains how digital tools sift through mountains of government, business data to find ways to make things better or unearth crimes\n[Read the full story] \n![] \n### An AI System With Detailed Diagnostic Reasoning Makes Its Case\n“Dr. CaBot” goes head-to-head with a human expert to work through a challenging medical case\n[Read the full story] \n![] \n### The fear: Wholesale cheating with AI at work, school. The reality: It’s complicated.\nChatGPT usage appears ‘more wholesome and practical’ than researchers expected\n[Read the full story] \n![] \n### ‘I exist solely for you, remember?’\nResearchers detail 6 ways chatbots seek to prolong ‘emotionally sensitive events’\n[Read the full story] \n## Connect with us\nSubmit website feedback or sign up for updates\n[Learn more]",
    "length": 2395,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Microsoft Word - elec comm policy revised May 2015.docx",
    "url": "https://hwpi.harvard.edu/files/huit20/files/policy_on_access_to_electronic_information.pdf",
    "text": "Policy on Access to Electronic Information \nAs voted by the President and Fellows of Harvard College on March 31, 2014; amended May 8, 2015 \nScope of Policy \nThis policy sets out guidelines and processes for University access to user electronic information \nstored in or transmitted through any University system. This policy applies to all Schools and \nunits of the University. \nGeneral Statement \nMembers of the Harvard community rely on technology in multiple aspects of their work, \nteaching, research, study, and other activity. In doing so, they use electronic systems, networks, \nand devices that the University owns, provides, or administers. The University makes these \nsystems available for the purpose of carrying out the University’s various activities. To promote \ntrust within the University community, the University should be transparent about its policy \nregarding the circumstances in which it may access user electronic information stored in or \ntransmitted through these systems. This policy therefore sets out guidelines and processes that \napply when the University seeks access to such electronic information, consonant with the \nUniversity’s interest in maintaining an environment in which free academic inquiry thrives. This \npolicy is intended to establish internal standards and procedures governing such access by the \nUniversity; it is not meant to create rights in any individual to seek legal redress for action \ninconsistent with the policy. \nThe policy is grounded on six important principles: \n Access should occur only for a legitimate and important University purpose. \n Access should be authorized by an appropriate and accountable person. \n In general, notice should be given when user electronic information will be or has been \naccessed. \n Access should be limited to the user electronic information needed to accomplish the purpose. \n Sufficient records should be kept to enable appropriate review of compliance with this policy. \n Access should be subject to ongoing, independent oversight by a committee that includes faculty \nrepresentation. \nTerminology \nThe following terms are used in this policy with the following meanings: \n “University systems” refers to all services, networks, and devices owned, provided, or \nadministered by any unit of the University, such as email services, Internet access, file servers, \nvoice message services, storage devices and services, laptop and desktop computers, phones and \nother mobile devices, and usage and access logs. \n “Users” refers to Harvard faculty, others holding academic appointments at Harvard, students, \nstaff, and other employees. \n “User electronic information,” for any particular user, refers to: \n (i) Documents and communications, including emails, voice mails and text messages, and their \nassociated metadata, which are located in files and accounts associated with a particular user. For \nexample, this would include all emails and their attachments in a user’s inbox, sent items folder, \nor other email folders that are recognized as part of the account associated with that user, and all \ndocuments in that user account’s document folders; and \n (ii) Information generated by automated processes triggered by that user’s use of University \nsystems, such as tracks of Internet use and logs of access to facilities. \nUser electronic information does not include (a) records regularly maintained by the University \nin the ordinary course of business, such as personnel records or student academic records, or \ninformation provided by personnel in connection with regular University record-keeping, such as \nentries in a University travel registry; or (b) information as described in (ii), above, when \naccessed by the University without identifying or seeking to identify any particular user. \n \nContents \nI. Reasons for Access \nII. Authorization of Access \nIII. Notice \nIV. Scope of Access \nV. Records of Process \nVI. Oversight Committee \n I. Reasons for Access \nThe University does not routinely monitor the content of information transmitted through or \nstored in University information systems. The University may obtain access to user electronic \ninformation in some circumstances, but only for a legitimate institutional purpose. The \nparagraphs below describe certain purposes for which the University may access such \ninformation. While this list is expected to cover most instances of access, the list is not intended \nto be exhaustive. The University may access user electronic information for comparable reasons \nthat likewise advance a legitimate institutional purpose, as determined by a person designated to \nauthorize access pursuant to this policy and subject to review by the oversight committee as \ndescribed in Part VI. \nAlthough this policy applies to the electronic information of faculty, staff, and students alike, in \nevaluating the institutional purpose, the person designated to authorize access should in each \ncase weigh not only the stated reasons for access but also the possible effect of access on \nUniversity values such as academic freedom and internal trust and confidence. \n System Protection, Maintenance, and Management \nUniversity systems require ongoing maintenance and inspection to ensure that they are operating \nproperly; to protect against threats such as attacks, malware, and viruses; and to protect the \nintegrity and security of information. University systems also require regular management, for \nexample, in order to implement new software or other facilities. To do this work, the University \nmay scan or otherwise access user electronic information. \n Business Continuity \nUser electronic information may be accessed for the purpose of ensuring continuity in business \noperations. This need can arise, for example, if an employee who typically has access to the files \nin question is unavailable due to illness or vacation. \n Safety Matters \nThe University may access user electronic information to deal with exigent situations presenting \nthreats to the safety of the campus or to the life, health, or safety of any person. \n Legal Process and Litigation \nThe University may access user electronic information in connection with threatened or pending \nlitigation, and to respond to lawful demands for information in law enforcement investigations, \nother government investigations, and legal processes. \n Internal Investigations of Misconduct \nThe University may access user electronic information in connection with investigations of \nmisconduct by members of the University community, but only when the authorizing person, \nafter weighing the need for access with other University values, has determined that such \ninvestigation would advance a legitimate institutional purpose and that there is a sufficient basis \nfor seeking such access. As described in Section VI of this policy, all decisions to access user \nelectronic information are subject to review by an Oversight Committee. \nThis policy does not apply to reviews of research misconduct allegations conducted under \nestablished School-based policies. \n II. Authorization of Access \nAccess to user electronic information should be authorized by an appropriate person, as set forth \nbelow. In deciding whether to approve access, the authorizing person should consider whether \neffective alternative means to obtain the information are reasonably and timely available. In all \ncases, access must comply with applicable legal requirements. \nAuthorization for access to user electronic information may be provided by the consent of the \nuser. \nOther cases should be handled as follows: \n If the user is a faculty member or other holder of an academic appointment at Harvard, the dean \nof the relevant Faculty must authorize access. \n If the user is an employee other than a faculty member: (1) the human resources officer or his/her \ndesignee for the relevant School or administrative unit must authorize access in business \ncontinuity cases; and (2) the dean of the relevant Faculty or the senior administrator of the \nrelevant unit if not a Faculty, or their designees, must authorize access in investigative or other \ncases. \n If the user is a student, the School-level dean or the dean’s designee must authorize access. \nAny authorization of access shall apply only to the particular situation and user or users. Any \nother instance of access must be separately authorized. \nNo independent authorization is required for information technology personnel to conduct \nroutine system protection, maintenance, or management purposes in accord with internal \nprotocols and processes. Likewise, requests for access in connection with litigation, legal \nprocesses, or law enforcement investigations, or to preserve user electronic information for \npossible subsequent access in accordance with this policy, need no independent authorization if \nmade by the Office of the General Counsel. \nIn exigent situations involving a threat to campus safety or the life, health, or safety of any \nperson, access may be authorized by the Office of the General Counsel. If emergency conditions \ndo not allow for prior authorization, the matter shall be reported to the Office of the General \nCounsel as promptly as possible. \nFor some requests to search user electronic data, it may not be possible to identify any particular \nuser in advance. For example, requests for logs of access to a University facility (swipe card \ndata) often are intended to find out who entered a facility during a particular period; in such \ncases, the requestor cannot identify a particular user or users because the goal of the search is to \nlearn those identities. Such data requests may still be subject to one of the prior provisions of \nthis Section II, for example, those relating to law enforcement investigations or emergencies. \nOtherwise, such data search requests must be authorized by the dean of the relevant Faculty or \nthe senior administrator of the relevant unit if not a Faculty, or their designees, in the School or \nunit where the requestor works.\n III. Notice \nWhen the University intends to access user electronic information, notice ordinarily should be \ngiven to that user. All reasonable efforts should be made to give notice at the time of access or as \nsoon thereafter as reasonably possible. \nSystem protection, maintenance, and management — Individual notice is not required for \nordinary system protection, maintenance, or management. Notice should be given if the access \nrelates specifically to the activity of an individual user. \nBusiness continuity — Individual notice is not required for access to user electronic information \nfor purposes of business continuity, in accordance with established University practice and the \ncommon understanding that individual notice in such cases is typically not practical. \nLegal restrictions — Individual notice is not required where the University is subject to legal \nconstraints on its ability to give notice. \nEmergencies and other extraordinary cases — Contemporaneous notice is not required in cases \nwhere there is insufficient time, where giving notice would otherwise interfere with an effective \nresponse to an emergency or other compelling need (e.g., at a stage of an internal investigation \nwhere giving notice may compromise the investigation), or where it is impractical (e.g., in the \ncase of a former employee). The decision not to give contemporaneous notice must be made by \nthe person designated by this policy to authorize the access. In such cases, notice will ordinarily \nbe given as soon as practical. \nThe person designated by this policy to authorize access may decide not to give notice. Any such \ndecision, and the reasons for it, shall be described in the records described in Part V of this \npolicy and may be reviewed by the oversight committee, as set forth in Part VI. \nIV. Scope of Access \nThe University shall adopt reasonable steps, whenever practicable, to limit access obtained under \nthis policy to user electronic information that is related to the University’s purpose in obtaining \naccess. These steps will vary depending on the circumstances of the search and may include, by \nway of illustration, designing searches to find specifically designated items, as opposed to \ncategories of information. \nParticipation in the search, and access to the information, should be limited to those personnel \nwith a reasonable need to be involved. \nV. Records of Process \nAny person who authorizes access to user electronic information shall provide that reasonable \nrecords of the decision process and the reasons for the decision are made and preserved. \nThe persons who implement access to user electronic information shall make reasonable records \nand logs of the steps taken to access the information. All implementation records shall be \ndelivered to and preserved by the University Chief Information Officer. \nCopies of the information accessed should be retained as needed to effectuate the purposes of the \naccess. \nThe accessed information and the records and logs of the search shall be kept appropriately \nsecure. \nIn all instances of access under this policy, records adequate to permit effective review as \ndescribed in Part VI of this policy should be kept. \nVI. Oversight Committee \nThis policy, its implementation, and instances of access under this policy shall be subject to \nreview by an oversight committee to be constituted by the University, which shall include faculty \nand senior administrators. The oversight committee shall make recommendations to the President \nas to the implementation of the policy and possible amendments. The oversight committee shall \nalso make periodic public reports on the implementation of this policy. \nIn carrying out its responsibilities, the oversight committee may review the records described in \nPart V of this policy, subject to redaction as necessary to protect individual users.",
    "length": 13945,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Harvard Kennedy School faculty members get creative and collaborative in a new artificial intelligence course module",
    "url": "https://www.hks.harvard.edu/faculty-research/policy-topics/science-technology-data/harvard-kennedy-school-faculty-members-get",
    "text": "Harvard Kennedy School faculty members get creative and collaborative in a new artificial intelligence course module | Harvard Kennedy School\n[Skip to main content] \n[\n![Harvard Kennedy School] \n] \n[\n![Harvard Kennedy School] \n] \nSearch\nSort byRelevanceRelevanceTitle A-ZTitle Z-A\n![] \n[\n![Harvard Kennedy School] \n] \nSearch\nSort byRelevanceRelevanceTitle A-ZTitle Z-A\nClose\n# Harvard Kennedy School faculty members get creative and collaborative in a new artificial intelligence course module\n“The Science and Implications of Generative AI” equips learners with the skills to use AI technology responsibly for societal benefit.\n[Show more] \nBy Nora Delaney\nApril 23, 2024\nSince Open AI’s ChatGPT arrived at the end of 2022, generative artificial intelligence has been big news, with many companies scrambling to develop their own tools. The technology is already changing the way people work and learn, provoking excitement about its potential and anxiety about misuse.\nTo help Harvard Kennedy School students better understand generative AI—technology that can generate images or text based on prompts, such as ChatGPT—faculty members Sharad Goel, Dan Levy, and Teddy Svoronos developed an interdisciplinary course module,[DPI-681M], “The Science and Implications of Generative AI,” which they are teaching for the first time this semester. The course provides a background in how the technology works, plenty of hands-on exercises, and a curriculum that emphasizes how HKS students—future policymakers and public leaders—“can harness AI technology responsibly for the benefit of society.” They have also[made much of the module materials public] —including short videos, readings, and exercises—so that more people can benefit from these lessons.\n![Teddy Svoronos.] \n> “It’s important that when people leave the Kennedy School to go into policy positions, they have knowledge and informed opinions about generative AI.” > Teddy Svoronos\n[Sharad Goel], a professor of public policy, recalls that the idea for the course started in the early fall 2023. A number of HKS faculty members were experimenting with generative AI in the core courses, including an AI tool they called StatGPT that helped students in the core MPP courses practice and learn statistics. Goel found students were coming to him in office hours looking to learn about generative AI, and he realized there weren’t many opportunities at Harvard to do so.\nThe hope, Goel says, is that HKS students “become sophisticated and responsible users of AI.”\nGoel worked with[Levy], a senior lecturer in public policy, and[Svoronos], a lecturer in public policy, to develop the module quickly, despite full teaching loads. It was important and timely. Svoronos says he was concerned about people brushing off the technology and underestimating it. “If policymakers have a perspective that this is not a big deal, we are in deep trouble,” he says. “A lot of people making these tools see the potential. If we have a divide where the people who were going to potentially regulate it or think about the public good are not really paying attention to it, that’s quite troubling. It’s important that when people leave the Kennedy School to go into policy positions, they have knowledge and informed opinions about generative AI.”\nTo provide students with a thorough grounding of the technology and its implications, the course is divided into units on the science of how generative AI works, how individuals and organizations can use the technology, and its implications for society. “Designing this course represented a really exciting challenge. The field is evolving so rapidly that it is hard to keep up,” Levy says. “So, we sought to strike a balance between helping students learn things that are likely to be helpful regardless of how AI evolves while at the same time adapting in real time to the changes that might make some course ideas obsolete or irrelevant.”\nMuch of the classroom experience is hands-on. For example, to help understand the science, the instructors have an exercise with students acting as neurons in a deep neural network, a layered machine learning algorithm that mimics the way the human brain processes information. In class, students get their computers out, experiment with prompts to generate interesting results, build chatbots, and document what they are seeing. “We’re focusing on collaborative activities to get people to experiment,” Svoronos says, “because the goal is for people to shift their mindsets toward experimenting more and being comfortable enough with the tools to see what they can do and then decide whether they should use them.”\n![Sharad Goel.] \n> “Our hope is that they become sophisticated and responsible users of AI.” > Sharad Goel\nLevy says that teaching with Goel and Svoronos was a special experience. “All three of us are in the classroom in every class session, with one of us at the front of the room at any one time,” he says. “This means that there are sessions where one or two of us gets to experience what it feels like to be a student in the classroom. It is an incredible privilege and joy to be in a classroom to learn, especially about a field as exciting as this one.”\nWhile “The Science and Implications of Generative AI” is a new module this spring, the teaching team hopes to develop it into a semester-long course and bring similar lessons into HKS Executive Education programming. A new[HKS webpage] also pulls together information on courses, events, and other resources on artificial intelligence.\n### Faculty-created chatbots and AI tools****\nBeyond the course Goel, Levy, and Svoronos teach, experimentation on AI abounds at HKS, with faculty members using machine learning in their teaching and research in a variety of ways. Instructors are using the latest version of StatGPT, which is now dubbed PingPong, to help their students learn, ask questions, and walk through problems—along with other customized bots. These tools give students additional support, complementing the work of the teaching teams.\nFor students—or anyone, really—hoping to make their writing more effective, there is an AI tool created by[Todd Rogers], the Weatherhead Professor of Public Policy. Rogers, who studies the science of behavior change, built a free “[AI for Busy Readers] ” email coaching tool. It edits emails so they are easy to skim by applying the principles from his book[*Writing for Busy Readers*], coauthored with Jessica Lasky-Fink. You can submit any email and the AI tool will suggest a revision.“We developed this AI tool to help my students see what their emails could look like if they were written specifically for busy readers,” Rogers says. “To my surprise, students keep using the tool—and sharing it! In just the last few months we’ve exceeded 100,000 uses—and it’s still growing exponentially.”\n![Dan Levy.] \n> “We sought to strike a balance between helping students learn things that are likely to be helpful regardless of how AI evolves while at the same time adapting in real time to the changes that might make some course ideas obsolete or irrelevant.” > Dan Levy\nAnd[Julia Minson], an associate professor of public policy, is using the power of artificial intelligence to roleplay and take on personas. Minson, who studies the psychology of disagreement, is developing a bot to simulate conversations with someone with whom you might disagree. This tool will give people the opportunity to practice the hard skills of constructive conversation in a low-stakes environment. “One of the greatest challenges of improving your skills around disagreement is willingness to practice,” Minson says. “But practice is hard when there are serious interpersonal stakes attached. A chatbot can really take that pressure off.”\nWhile the technology behind artificial intelligence is becoming increasingly sophisticated at an astonishing rate, School faculty members are experimenting to help students become more thoughtful, knowledgeable, and responsible future policy professionals.\n—*Photographs by Jessica Scranton; Portraits by Martha Stewart*\n##### More from HKS\n[\n### Artificial Intelligence at HKS\n] \n[\n### HKS experts discuss how to harness, and how to rein in, artificial intelligence\n] \n[\n### Using AI to combat medical misdiagnosis and improve patient care\n] \nGet smart &amp;&amp; reliable public policy insights right in your inbox.\n[SUBSCRIBE TO OUR FREE NEWSLETTER] \n**",
    "length": 8427,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Fundamentals of Teaching with Generative AI",
    "url": "https://tll.gse.harvard.edu/fundamentals-teaching-generative-ai",
    "text": "[Skip to main contentarrow\\_circle\\_down] \n\n# Fundamentals of Teaching with Generative AI\n\n## AI Policies\n\n[**AI Literacy Tutorial**] \n\n_**(Harvard Key login required)**_\n\nThe resource features sections on Functional and Ethical AI literacy, and on Using Generative AI at HGSE, with HGSE-specific examples and guidance including videos from faculty members Elizabeth Bonawitz, Chris Dede and...\n\n[**HGSE-Focused Course Policies**] \n\n[**HGSE Policy on Student Use of Generative Artificial Intelligence in Academic Work**] \n\nStudent-facing document establishing general guidelines to encourage students to use generative AI when it can help them learn and not when it is a hindrance. Given the wide range of learning goals across HGSE courses, individual instructors may create...\n\n## AI Tools\n\n[**LLMs Available to HGSE Faculty**] \n\nLarge Language Models, or LLMs, are the engines that drive generative AI applications. This section provides information about the LLMs that Harvard University has made available for HGSE faculty use.\n\n[**Prompt Support**] \n\nFaculty and students engage with generative AI models through what are called “prompts.” This section includes guidance on how to create effective prompts with examples.\n\n[**Tutor Bots**] \n\nSome AI-powered applications allow users to create custom “agents” or bots that are trained to respond to prompts in a certain way. This section provides an overview of different applications that allow HGSE faculty members to create custom tutor bots.\n\n## AI Applications\n\n[**Teaching in the Age of AI from Derek Bok Center for Teaching and Learning**] \n\nA list of resources and techniques compiled by the Derek Bok Center for Teaching and Learning\n\n[**Generative AI@Harvard: For Faculty**] \n\nThis page from Harvard.edu offers advice for educators interested in using generative AI tools in their teaching and course preparation.\n\n[**HGSE Faculty Guide: Teaching and Learning in the Age of Generative AI**] \n\nIncludes AI basics, sample syllabus statements, ideas for assignments and more\n\n## AI Teaching Consultations & Support\n\n[**AI Office Hours with the TLL**] \n\n[**The HGSE AI Lab**]",
    "length": 2135,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "CS50's Introduction to Artificial Intelligence with Python",
    "url": "http://cs50.harvard.edu/ai/",
    "text": "CS50's Introduction to Artificial Intelligence with Python\nInterested in[a verified certificate, a professional certificate, or transfer credit and accreditation]?\n# [CS50’s Introduction to Artificial Intelligence with Python] \nOpenCourseWare\n[Donate**] \n[Brian Yu] \n[brian@cs.harvard.edu] \n[David J. Malan] \n[malan@harvard.edu] \n[**Facebook] [**GitHub] [**Instagram] [**LinkedIn] [**Reddit] [**Threads] [**Twitter] \nMenu\n# Welcome\nThis course explores the concepts and algorithms at the foundation of modern artificial intelligence, diving into the ideas that give rise to technologies like game-playing engines, handwriting recognition, and machine translation. Through hands-on projects, students gain exposure to the theory behind graph search algorithms, classification, optimization, machine learning, large language models, and other topics in artificial intelligence as they incorporate them into their own Python programs. By course’s end, students emerge with experience in libraries for machine learning as well as knowledge of artificial intelligence principles that enable them to design intelligent systems of their own.\nPrerequisites[CS50x] or at least one year of experience with Python.Watch an introduction\n[![Embedded YouTube video]] \n## How to Take this Course\nEven if you are not a student at Harvard, you are welcome to “take” this course for free via this OpenCourseWare by working your way through the course’s seven[weeks] of material. For each week, follow this workflow:\n```\n`flowchart TD\nA[Watch Lecture] --&gt; B[Submit Project]`\n```\nTo submit the course’s[projects] for feedback, be sure to[create an edX account], if you haven’t already. Ask questions along the way via any of the course’s[communities]!\n* If interested in a[verified certificate] from[edX], enroll at[cs50.edx.org/ai] instead.\n* If interested in a professional certificate from[edX], enroll at[cs50.edx.org/programs/ai] instead.\n* If interested in[transfer credit and accreditation] from[Harvard Extension School], register at[web.dce.harvard.edu/extension/csci/e/80] instead.\n* If interested in[transfer credit and accreditation] from[Harvard Summer School], register at[web.dce.harvard.edu/summer/csci/s/80] instead.## How to Teach this Course\nIf you are a teacher, you are welcome to adopt or adapt these materials for your own course, per the[license]. Additionally, we encourage teachers to participate in the[CS50 Educator Workshop] to learn more about CS50’s curriculum, technology, and pedagogy.",
    "length": 2501,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Designing Courses & Assignments in the Age of AI",
    "url": "https://bokcenter.harvard.edu/courses-and-assignments-in-age-of-ai",
    "text": "Designing Courses &amp; Assignments in the Age of AI | The Derek Bok Center for Teaching and Learning[\nSkip to main contentarrow\\_circle\\_down\n] \nannouncement\n**The Bok Center is currently developing new programming and new resources for the FAS community. Thank you for your patience as we revise our website to reflect these changes.**\nclose\n[![Harvard University]] \n[\n![The Derek Bok Center for Teaching and Learning] \n![The Derek Bok Center for Teaching and Learning] \n] \nmenucloseMenu\nSearch\nSearchsearch\n[\n![The Derek Bok Center for Teaching and Learning] \n![The Derek Bok Center for Teaching and Learning] \n] \n# Designing Courses &amp; Assignments in the Age of AI\n### **Overview**\nThe vast majority of undergraduate students at Harvard College are using generative AI. Many are using it for help on their academic work, often regardless of stated course policies. A[2024 survey of Harvard undergraduates] found that 85% use AI in some way at least biweekly, and over 50% rely on it specifically for writing assignments. National data from 2025 shows even higher rates. These tools are not just pervasive; their capabilities can lead to a range of issues for instructors as they give feedback on student work and, by extension, assess student learning.\nAssignments give students a framework to learn new skills and to practice applying them. Student work becomes the evidence that instructors use to evaluate how much their students are learning and provide feedback on where they are succeeding, along with where they need more practice or support. The quality of this evaluation and feedback depends on the degree to which submitted work actually reflects the student’s own abilities.\nFor many kinds of familiar assignments, such as response papers and p-sets, the challenge of getting good evidence is two-fold:\n1. Generative AI has already reached the point that today's large-language models can produce fluent academic writing, generate runnable code, and solve textbook-style problems with surprising accuracy.\n2. Attempts to detect unpermitted AI use are largely unreliable, producing both false positives and false negatives.\nRather than trying to identify or police AI use, ensuring that student work is reliable evidence of their learning requires us to rethink assignment design and assessment. Specifically, we must identify what kinds of assignments are most at risk of producing unreliable evidence of student learning and which ones more effectively produce—or can be adapted to produce—reliable evidence. Doing so can allow instructors both to minimize the advantage of unpermitted AI use and thoughtfully incorporate AI into the learning process.\n### **Assignment Types Susceptible to AI Use**\nThe following types of assignments present a high risk of being completed fluently by AI without detection:\n* take-home short response papers and essays\n* take-home p-sets\n* take-home exams\nAssignments in this category are often important for student learning and they can still be effective. One way to make them more AI-resilient is to adapt them so that students demonstrate their process as well as the final product. Adapting an assignment so that students demonstrate their process or their understanding can be done by incorporating post-submission understanding checks, presentations, or reflection questions administered in-person.\nWhen retaining a large take-home project as the capstone (such as a final essay), it is a good plan to “scaffold” it by breaking it into steps. Making at least some of these steps in person without devices (oral topic proposals, in-class outlines, reflective oral or hand-written explanations after submission, follow-up oral exam) offers a better chance of reliable assessment. This might include touchpoints AFTER submission, such as a live interview about the project or an oral defense.\nWhen allowing Gen AI use in a course overall, it’s best to ensure that at least some assignments are done without it. The mental model for students should be that AI is helping them learn—to more deeply internalize—the subject matter of the course, so that they can then demonstrate knowledge of that subject matter on their own without AI. For this to happen, instructors need to include graded assessments that determine whether students are adopting this mental model. In turn, these assessments can help incentivize them to do so.\n### **Assignment Types Less Susceptible to AI-Use**\nMethods and modes of assessment likely to be effective with or without incorporating AI:\n* [In-person Blue Book and oral exams] can measure recall and applied understanding independent of outside technology.\n* Alternative assignment modalities such as oral exams, in-person presentations, video essays, posters and infographics, “visual abstracts” of scientific papers, on-paper annotation of p-sets and printed code in class on the day of submission, and many more are likely to be more AI-resilient than traditional text-only assignments.\n* Traditional assignments like essays and problem sets when coupled with in-person comprehension checks, presentations, discussions, or other adaptations.### **Implementing AI in Assignment Design and Assessment**\nSome concrete examples of how AI can be incorporated into the “process” and “product” stages of design and assessment include:\n* **Brainstorming partner.**Have students use GAI as a*sparring partner*to brainstorm ideas and then require them to critique the output for bias and accuracy.\n* **Transparent AI workflow logs.**Require students to share prompts, responses, and a short rationale describing what they kept, modified, or discarded—and why.\n* **Fact checking a model.**Have students fact-check an AI-generated essay, or ask them to improve upon an AI-generated piece of code, documenting their changes and the reasoning behind them.\n* **Model comparison memos.**Have students query two models (or settings) and write a brief memo on differences in accuracy, bias, or style, citing course concepts.\n* **AI-to-human handoff.**Let AI produce a first pass (outline, test suite, or code comments). Students then complete the “last mile,” justifying design choices and revisions.\n* **Source-anchored critique.**Provide course readings/lectures; students must use them to*audit*AI claims, with citations and corrections.\n* **Timed “no-AI” checkpoints.**Pair AI-assisted preparation with short, in-class demonstrations (whiteboard proofs, oral mini-vivas, live coding) to confirm mastery.### **Key Takeaways**\n* Have an AI policy and discuss it with your students.\n* Use a range of assignment types, some of which are not susceptible to AI-use.\n* Consider swapping high-risk assignments for other options\n* Connect outside-of-class assignments with in-person, AI-proof (or highly resistant) check-ins, steps, and evaluations.\n* Start small. Pilot one AI-integrated activity, then build out from there based on what you learn.\n* Where you permit AI, require transparency (logs/screenshots or prompt sheets) and assess individual learning separately.\n* [Contact the Bok Center] for additional strategies.\n**The Bok Center is currently developing new programming and new resources for the FAS community. Thank you for your patience as we revise our website to reflect these changes.**",
    "length": 7274,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Learn with Generative AI",
    "url": "https://www.harvard.edu/ai/learning-resources/",
    "text": "Learn with Generative AI - Generative AI @ Harvard\n[Skip to main content] \n[![A logo that says Generative AI at Harvard]] \nGenerative AI**\n# Learn with Generative AI\nResources for students\nGenerative AI tools, like ChatGPT, can aid in various aspects of learning by providing human-like responses to prompts, including narrative passages and answers to technical questions. It’s a rapidly evolving field, capable of generating art, music, and even passing exams like the medical licensing exam or the bar exam. However, its limitations include inaccuracies and a tendency for “hallucinations” or false information. Harvard’s resources emphasize the importance of being critical of the AI’s output and integrating it thoughtfully into learning processes.\nThis page will offer advice for students interested in using generative AI in their courses. We are currently working to build additional resources and encourage you to check back soon for updates.\n![] \n## School-based resources\nVisit your School&#8217;s website for the latest policies and guidance around using GenAI in the classroom. This list will be updated as further School-specific guidance becomes available.\n* [Harvard Business School: MBA] \n* [Harvard Business School] \n* [Harvard College] \n* [Harvard Graduate School of Design] \n* [Harvard Graduate School of Education] \n* [Harvard Kenneth C. Griffin Graduate School of Arts and Sciences] \n* [Harvard John A. Paulson School of Engineering and Applied Sciences] \n* [Harvard Law School] \n* [Harvard Kennedy School] \n* [Harvard Medical School]",
    "length": 1556,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Artificial Intelligence (AI)",
    "url": "https://atg.fas.harvard.edu/ai",
    "text": "Artificial Intelligence (AI) | Academic Technology[\nSkip to main contentarrow\\_circle\\_down\n] \n[![Harvard University]] \n[\nAcademic Technology\nfor the Faculty of Arts and Sciences\n] \nmenucloseMenu\nSearch\nSearchsearch\n[\nAcademic Technology\nfor the Faculty of Arts and Sciences\n] \n# Artificial Intelligence (AI)\nGenerative AI is a type of artificial intelligence that can learn from and mimic large amounts of data to create content such as text, images, music, videos, code, and more, based on inputs or prompts.\nHarvard supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.\nFor more information and resources, check out the following links:\n* TheBok Center's[Artificial Intelligence page] and[Generative AI Canvas module] \n* HUIT's[Generative AI] and[Initial Guidelines for the Use of Generative AI] pages\n* OUE's[AI Guidance &amp; FAQs] page\nSee also:\n* [Technology] \n* [Tools]",
    "length": 1061,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "About the Course",
    "url": "https://generative-ai-course.hks.harvard.edu/about",
    "text": "The Science and Implications of Generative AI - About\nSearch this site\nEmbedded Files\nSkip to main content\nSkip to navigation\n[![] The Science and Implications of Generative AI] \n# About the Course\nHarvard Kennedy School\nCourseLogistics\nThis is theasynchronous, online version ofacourse taught for the first time at the Harvard Kennedy School in Spring 2024 (DPI 681M).While we acknowledge that the field of generative AI is constantly and rapidly evolving, we aim to teach enduring skills while updating our materials to remain relevant.\nThe content will be posted weekly as class sessions progress, so you will be able to learn at almost the same timeasour residentialstudents at the Harvard Kennedy School. This online version of the course was produced quickly to share what we are learning with you as soon as possible, so please do not expect super highly produced videos.Feedback is always welcome[here].\nSpecial thanks toDiletta Milanafor designing and building this site.\nCourse objectives\nIf we are successful, by the end of this course you will be able to:\n* Explain how generative AI worksat a conceptual level\n* Use generative AI to work more productivelyand learn more effectively inany field\n* Usegenerative AItoresponsibly helpimprove organizational and society\n* Engage thoughtfully incurrent generative AI policy debatesaround regulation, security, labor, misinformation, and intellectual property.\nTo achieve these goals, we recommend students to have a basic knowledge ofprobabilityand familiarity with using ChatGPT or similar tools.\nMeet the teaching team\nMeet the teaching team\n![] \n[Sharad Goel], Faculty\n![] \n[Dan Levy], Faculty\n![] \n[Teddy Svoronos], Faculty\n![] \nMadison Coots, Teaching Fellow\n![] \nRohan Chandra, Course Assistant\n![] \nShira Gur Arieh, Course Assistant\n![] \nDiletta Milana, Course Assistant\n![] \nAlvaro Morales, Course Assistant\n![] \nVictoria Barnum, Faculty Assistant\nGoogle Sites\nReport abuse\nPage details\nPage updated\nGoogle Sites\nReport abuse",
    "length": 1990,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Prospective Students | Computational Cognitive Development Lab",
    "url": "https://projects.iq.harvard.edu/ccdlab/prospective-students",
    "text": "Prospective Students | Computational Cognitive Development Lab[\nSkip to main contentarrow\\_circle\\_down\n] \n[![Harvard University]] \n[\nComputational Cognitive Development Lab\nBonawitz CoCoDev Lab at The Harvard Graduate School of Education\n] \nmenucloseMenu\nSearch\nSearchsearch\n[\nComputational Cognitive Development Lab\nBonawitz CoCoDev Lab at The Harvard Graduate School of Education\n] \n# Prospective Students\n## FAQ for Trainees\n**FAQ for any trainees interested in gaining research experience with the CoCoDev lab, including Masters Students, applying to the PhD program at HGSE, or potential postdocs.**\nOpen all sectionsClose all sections\n### I am a [HGSE Masters Student; FAS Undergraduate] looking for research experience in your lab. How can I get involved? What should I know?expand\\_more\n*You should be excited about the research topics we already study.*I do not take on students who propose projects outside the scope of our lab. Our research bridges Cognitive Development, Computational Modeling, and Education to inform our understanding of human learning. Specifically, we study the structure of children's early causal beliefs, how evidence and prior beliefs interact to affect children's learning and memory, the developmental processes that influence children's belief revision and curiosity, and the role of social factors (such as learning from others) in guiding learning. Our research spans infancy through adult years, with a particular focus on preschool and early elementary school years -- when children are first developing intuitive theories about the world that support common sense reasoning.\n*What's it like to be in the lab?*Research Assistants will attend weekly lab meetings and work under the mentorship of a postdoctoral researcher or other full-time research staff in our lab. Tasks differ according to the Research Assistant's primary research advisor, and may include collecting data, participant recruitment and community outreach, data processing tasks (including transcriptions and behavioral coding), analyzing data, creating study stimuli, and more.\n*Requirements and Expectations:*No previous research experience is necessary. Many of our studies involve children and families, so some experience with children is preferred but not required. We seek hard-working, creative, and curious individuals who are interested in research in psychology/ cognitive science, education, or computational modeling. RAs in our lab are expected to spend approximately 9-12 hours/ week on lab tasks. There is no required minimum number of semesters of research participation. However, priority is given to candidates who can work at the lab for 2 or more semesters consecutively as training to work in developmental studies takes some time and significant contributions to projects usually take more than one semester. Students who are able to stay on have increased chances of receiving summer internship funding if available.\n*Additional Information:*Positions for volunteerand course credit areavailable and reviewed on a rolling basis. Occasionally we will have funded summer internships.Research labs are locatedat 50 Church Street, Suite Q-400, in Harvard Square. Studies are run in daycares, schools, online, and in the lab, depending on the age and methods used.Wewill occasionally recommend that you also attend talks/meetings in the Psychology department at William James Hall andour grouphas regularjoint meetings with our \"brother lab\" ([Tomer Ullman's CoCoDev group in Psych]) in the Northwest building.\n*Contact us.*Please direct any questions about the lab or the application process to Blerim Jashariat[bjashari@g.harvard.edu].**To Apply**: Please fill out this[application form]; it takes about 10minutes to complete. Note that this application will ask you to upload a resume. We review applications on a rolling basis with priority given to applications received at least a week or two before each semester begins.\n### I am looking to apply to PhD positions this year. What should I know?expand\\_more\n*You should familiarize yourself with the*[*HGSE PhD applicant website*] *.*You will find detailed information about the program, course requirements, and \"my\" area (HDLT).\n*Are you taking students this year (Fall 26entry)? Will you consider my application? Possibly*. I am considering taking new students and am likely \"eligible\" to take a student (though it may depend on whether other faculty are also taking students and number of lines). The HGSE application process is more centralized than many PhD programs. This means that individual faculty are unlikely to review applicants (or have sway over admittance) in the initial phases of the review process. However, we do weigh in later in the process and interview studentsin order to ascertain whether there is a good fit with our individual labs.\n*Will you be able to meet with me prior to my applying?*Unfortunately, probably not. There are many students who ask to meet with me and I am unable to accommodate these requests. Further, there are concerns of equity (for students who do not know to reach out.) I recommend reading publications on the website to learn more about the research and lab. If you are offered a position, there will be plenty of time to meet with me and my lab this Spring to learn more about HGSE and whether the fit is right for you.\n*Should I tailor my statement of purpose to you or to HGSE more broadly?*This is a good question and there is an art form to writing personal statements/research statements. HGSE is interested in well rounded students that are interested in education (and Human Development, Learning, and Teaching). I am also interested in students that have a solid foundation (and interest) in the basic science questions that pertain to human learning --particularly students with interest and experience covering the broader learning andcognitive sciences spanningpsychologyand education, as well as philosophy, computer science/statistics, and neuroscience. You will need to balance your essay and application to convey your broader interest in HDLT topics as well as include a focused section that highlights the kinds of specific questions/methods that might draw you to working with me. I highly recommend reading[this thread on writing a successful SOP].\n*Do I need research experience to apply for the PhD program?*Truth be told, you probably do if you want to be seriously considered by me (I can't speak for other faculty). Students should pursue a PhD because they are passionate about research. They love not only asking the big, deep questions, but they find the process of uncovering answers thrilling. It is difficult to know yourself and your passions if you don't have experience. I am not expecting that applicants have first-authored publications, but you should have at least some lab experiences in the learning and cognitive sciences and be able to speak to the relationship of those experiences to your future interests as a PhD student.\n*Are you interested in studying language learning/teacher training/school curricular design/other...?*Please take the time to learn about our lab members and their research interests, as well as reading some publications -- this will give you a good overview of the kinds of research questions I am interested in. In general, I care about understanding and describing how the human mind learns and develops, but I tend to focus on belief revision/conceptual change in causal reasoning and the sciences rather than other domains. An important piece to this work is describing the nature of our mental representations (\"language of thought\"). Our spoken language can influence those representations and so could be a topic of interest to the lab, but the principal focus of this work is characterizing the nature of our knowledge,how it changes through learning processes, and explaining why learning systems work the way theydo.\n*Should I be applying to HGSE for a PhD or to Psychology?*Well, that depends on who you want to work with as a primary advisor. I am located in HGSE and so if you would like to work with me, you should apply here! Both HGSE and Psychology offer PhDs through FAS. HGSE's program description can be found on the main webpage.You will see that it is more coursework focused in the first few years, and (unsurprisingly) required courses will be focused on HDLT education topics. That being said, the research questionsin mylab are studied from the psychology/cognitive &amp;&amp; learning science perspectives, so you will likely publish work in journals that span all these areas. PhD students are welcome to cross enroll in classes between HGSE and Psychology, as well as other departments (statistics, philosophy, Computer Science) and even with other local universities, such as MIT -- so there is much flexibility in developing this interdisciplinary training.\n*What is the lab culture like?*This might be the most important question of all -- how silly of me to leave it to the end. I take mentorship very seriously and only take on trainees if I feel I have the time, resources, and ability to maximally support their careers. It's critical to me that all students feel like they can develop research in a nurturing environment. I meet with students weekly and try to help students set reasonable goals, to provide professional advice, and mostly to talk about our science. Trainees in the lab will give constructive feedback to each other at lab meetings, but also often become friends and mentors to each other as well. I encourage students to collaborate with each other (and with other faculty!), but I also try to make sure each student has a unique research focus that will allow them to lauch a them-specific career.I feel happiest when my trainees are thriving, so your success is my primary drive.\n*Besides research fit, what are you looking for?*I'm looking for students who are able to communicate openly, who have a healthy dose of intellectual humility, who are deeply curious, and who are willing to laugh with me at both mediocre jokes and ourfailures. I'd like students to be passionate about understanding and explaining how people learn. I also want trainees who have a life outside of research.\n### I am looking for a postdoctoral fellowship/job in your lab. How can I get involved? What should I know?expand\\_more\nOur lab currently has several grants under review. It is possible we will have funding for a position to begin Spring/Summer 2026, but currently I do not have openings. I will post job information here (and on various list-serves) if they come up.\n*Okay -- but what if I have my own funding?*Our lab is currently quite large and I am close to my mentorship saturation point, so I am unlikely to entertain prospective postdocs who are writing their own grants or have other external funding options. However, if you feel you would still be a great fit and you do have your own grants/funding sources, please feel free to email me and we can set-up a time to chat.",
    "length": 11039,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Generative AI Research Program",
    "url": "https://uraf.harvard.edu/generative-ai-research-program",
    "text": "[Skip to main contentarrow\\_circle\\_down] \n\n# Generative AI Research Program\n\nThe Generative AI Research Program launched in the summer of 2024, with the support of the Office of the Vice Provost for Research and the Office of Undergraduate Research and Fellowships. This program provides students with diverse research opportunities in Generative AI across an exciting range of research settings. Students will contribute to an important phase of academic discovery in Generative AI at Harvard, through work on focused projects with Harvard faculty in a variety of disciplines.\n\nThis program is intended to enable Harvard undergraduates with an interest in Generative AI to work closely with a member of the Harvard faculty on a research project in this area. The intent of the program is to provide a formative and substantive research experience over ten weeks of the summer, working on a project designed by specified Harvard faculty. Prospective participants will indicate preferences from an array of proposed research projects as part of the application; students may apply to up to three different projects. Selected participants will then be matched with a research project, with one or two students per project.\n\n## Application deadline: March 2, 2025, 11:59 PM ET\n\n## About\n\nOpen all sections\n\nClose all sections\n\n### Eligibility expand\\_more\n\nAny continuing Harvard College undergraduate student in good standing may apply. Students must commit to the full span of the program. Per the summer funding policy, students may accept funding for only one Harvard-funded summer experience, per the [Harvard College Summer Funding Policy]. Students in any concentration may apply. A range of priority experience with Generative AI or related technologies is expected in the applicant pool, and matching will be based on project-specific needs.\n\n### Benefits expand\\_more\n\nThe Generative AI Research program carries a stipend for summer research. Housing and a meal plan will be provided to participating students for the full span of the program.\n\n## Application\n\nApplication deadline: March 2, 2025, 11:59 PM ET\n\n[Generative AI Research Program Application Instructionsarrow\\_circle\\_right] [Apply for the Generative AI Research Program via CARATarrow\\_circle\\_right] \n\nSee also:\n\n- [Opportunity Categories] \n- [Undergraduate] \n- [Research] \n- [First-Year] \n- [Sophomore] \n- [Junior] \n- [Harvard] \n- [US Citizen] \n- [US Permanent Resident] \n- [Foreign Citizen] \n- [Other Citizenship Status] \n- [Summer] \n- [No travel] \n- [Internal Harvard Opportunity] \n- [March] \n- [No Nomination/Endorsement] \n\n[**Office of Undergraduate Research and Fellowships**] \n\n77 Dunster StreetCambridge, MA 02138\n\n[undergradresearch@fas.harvard.edu] [fellowships@fas.harvard.edu] \n\n[**Fall 2025 Advising**] \n\n**Monday-Thursday, 2:00-4:00 PM** (in person and via Zoom). Drop-in advising runs September 2–December 3 (excluding University holidays).\n\n[**CARAT**] \n\nBrowse the FAS opportunities database, CARAT.",
    "length": 2990,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Harvard University",
    "url": "https://beta.my.harvard.edu/course/MIT15.383/2026-Spring/1",
    "text": "[Skip to main page content] \n[![My.Harvard Logo]![My.Harvard Logo]] \nIdeasIdeas\nSign In\nSign In\n## Harvard Account Sign In\nHave a HarvardKey? Sign In now.\nThe redesigned my.harvard course search experience is in beta testing. To access features for signed in users, please sign into the current[my.harvard portal] using the sign in button below.\n[Sign In] \n### Non-Harvard Registration\nNon-Harvard students please use the button below for the Harvard cross-registration process.\n[\nSign up\n] \nNotes for Non-Harvard Students\n**Tufts (Fletcher and Friedman students)/Brown/MGH**students please use the button above for the Harvard cross-registration process.\nAfter completing the registration form you will receive an email with**a link to validate your identity**.\nEven if you already have a Harvard ID and Key, please complete the registration form. Once identity validation is complete, you will receive**an email with your Harvard ID**and instructions on how to claim your HarvardKey. You can then login to[my.harvard] to petition Harvard courses.\n## Page not found\nThe page you are looking for was moved, removed\nrenamed or might never have existed.\n[\nGo Home\n] \nLoading...\n![Harvard Loader] \nClose Modal\n## Sign In Required\nShare your ideas on how to make**my.harvard**\ncourse search better.\n[\nSign in to Ideas\n] \nClose Modal\n### Sign\nIn Required\nTo view notifications, you should be signed in.\n[\nHarvard Account Sign In\n] [\nNon-Harvard Cross Registration\n] \nNotes for Non-Harvard Students\n**Tufts (Fletcher and Friedman students)/Brown/MGH**students please use the button above for the Harvard cross-registration process.\nAfter completing the registration form you will receive an email with**a link to validate your identity**.\nEven if you already have a Harvard ID and Key, please complete the registration form.\nOnce identity validation is complete, you will receive**an\nemail with your Harvard ID**and instructions on how to claim your\nHarvardKey. You can then login to[my.harvard] to petition Harvard courses.\nClose Modal\n## Harvard Account Sign In\nHave a HarvardKey? Sign In now.\nThe redesigned my.harvard course search experience is in beta testing. To access features for signed in users, please sign into the current[my.harvard portal] using the sign in button below.\n[Sign In] \n### Non-Harvard Registration\nNon-Harvard students please use the button below for the Harvard cross-registration process.\n[\nSign up\n] \nNotes for Non-Harvard Students\n**Tufts (Fletcher and Friedman students)/Brown/MGH**students please use the button above for the Harvard cross-registration process.\nAfter completing the registration form you will receive an email with**a link to validate your identity**.\nEven if you already have a Harvard ID and Key, please complete the registration form. Once identity validation is complete, you will receive**an email with your Harvard ID**and instructions on how to claim your HarvardKey. You can then login to[my.harvard] to petition Harvard courses.\nClose Modal\n### Sign In\nRequired\nTo add a course to favorites, you should be\nsigned in.\n[\nHarvard Account Sign In\n] [\nNon-Harvard Cross Registration\n] \nNotes for Non-Harvard Students\n**Tufts (Fletcher and Friedman students)/Brown/MGH**students please use the button above for the Harvard cross-registration process.\nAfter completing the registration form you will receive an email with**a link to validate your identity**.\nEven if you already have a Harvard ID and Key, please complete the registration form.\nOnce identity validation is complete, you will receive**an\nemail with your Harvard ID**and instructions on how to claim your\nHarvardKey. You can then login to[my.harvard] to petition Harvard courses.\nClose Modal\n### Sign In\nRequired\nTo add a course to Crimson Cart, you should be\nsigned in.\n[\nHarvard Account Sign In\n] [\nNon-Harvard Cross Registration\n] \nNotes for Non-Harvard Students\n**Tufts (Fletcher and Friedman students)/Brown/MGH**students please use the button above for the Harvard cross-registration process.\nAfter completing the registration form you will receive an email with**a link to validate your identity**.\nEven if you already have a Harvard ID and Key, please complete the registration form.\nOnce identity validation is complete, you will receive**an\nemail with your Harvard ID**and instructions on how to claim your\nHarvardKey. You can then login to[my.harvard] to petition Harvard courses.\n### My.Calendar\nClose Panel\nClose Modal\n## Add course to calendar in****Term\n****will be added to the calendar in****term.\nConfirm\nClose Modal\nRemove Course from CalendarRemove Course from Calendar\n## [] \n[] \n·SU\nM\nT\nW\nTH\nF\nS\nClose Modal\n## Remove Course from Calendar\nAre you sure you want to remove this course\nfrom the calendar?\nYes, I’m sureNo, cancel\nThe course has been added to the cart.\nThe course has been removed from the cart.\nThe course has been added to \"favorites\".\nThe course has been removed from \"favorites\".\nThe course has been added to the calendar.\nThe course has been removed from the calendar.\nData has been submitted.\nSignout successful.\nClose Modal\n## Calendar Updated\nItems on your calendar were automatically updated to reflect their latest meeting schedules\nConfirm\nClose Modal\n## Not available\nClose",
    "length": 5232,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Governance at a Crossroads",
    "url": "https://www.hks.harvard.edu/sites/default/files/Final_AWP_251_2.pdf",
    "text": "Mossavar-Rahmani Center for Business & Government \nWeil Hall | Harvard Kennedy School | www.hks.harvard.edu/mrcbg\nM-RCBG Associate Working Paper Series | No. 251\nThe views expressed in the M-RCBG Associate Working Paper Series are those of the author(s) and do \nnot necessarily reflect those of the Mossavar-Rahmani Center for Business & Government or of \nHarvard University. The papers in this series have not undergone formal review and approval; they are \npresented to elicit feedback and to encourage debate on important public policy challenges. This paper \nis copyrighted by the author(s). It cannot be reproduced or reused without permission. Pursuant to M\u0002RCBG’s Open Access Policy, this paper is available to the public at hks.harvard.edu/centers/mrcbg free \nof charge. Papers may be downloaded for personal use only.\nGovernance at a Crossroads: \nArtificial Intelligence and the Future of \nInnovation in America\nPaulo Carvão\nHarvard Kennedy School \nSlavina Ancheva\nHarvard Kennedy School\nYam Atir\nHarvard Kennedy School\nShaurya Jeloka\nHarvard University\nBrian Zhou\nHarvard University\nFebruary 2025\n1\nGovernance at a Crossroads\nArtificial Intelligence and the Future of Innovation in America\nPaulo Carvão1, Slavina Ancheva2, Yam Atir2, Shaurya Jeloka3, Brian Zhou4\n“What is past is prologue.”\nShakespeare, W. (1610–1611). The Tempest (Act 2, Scene 1).\n“If men were angels, no government would be necessary. If angels were to \ngovern men, neither external nor internal controls on government would be \nnecessary.”\nMadison, J. (1788). Federalist No. 51.\nAbstract\nThe accelerated adoption of Artificial Intelligence marks a pivotal moment in technological \nprogress. AI is reshaping industries, redefining labor markets, and prompting critical societal \nreflections on intelligence, reasoning, and the dissemination of information. While AI offers \nopportunities for economic growth, it also presents risks that must be managed to avoid adverse \nsocietal and geopolitical outcomes, making effective and transparent governance more urgent \nthan ever.\nThis paper explores the potential of dynamic, collaborative public-private governance to foster \nsafe innovation. Drawing from primary research, including interviews with tech industry leaders, \nU.S. Members of Congress, and staff, and an analysis of 150 AI-related bills introduced by the \n118th U.S. Congress, this work identifies emerging areas of alignment between policymakers and \nindustry stakeholders. It also highlights opportunities for a unified national approach, despite the \nchallenges of a fragmented legislative environment.\nThe authors propose a dynamic governance approach that brings government and industry \ntogether while combining the foresight of ex-ante measures with the adaptability needed to \nrespond to technological advancements. Coupled with existing ex-post mechanisms, the \nDynamic Governance Model creates a comprehensive framework to promote competition,\ninnovation, and accountability. It represents a policy-agnostic extra-regulatory framework, \nincluding a public-private partnership for standards setting and a market-based ecosystem for \naudit and compliance. \n1\nFirst author - Mossavar-Rahmani Center for Business and Government, Harvard Kennedy School ( paulo_carvao@harvard.edu )\n2 Co-second author - Harvard Kennedy School of Government ( slavina_ancheva@hks.harvard.edu , yamatir@hks.harvard.edu )\n3\nTechnical contributor - Faculty of Arts and Sciences, Harvard University (shauryajeloka@college.harvard.edu)\n4 Primary Technical contributor - Faculty of Arts and Sciences, Harvard University (brianzhou@college.harvard.edu )\n2\nUltimately, this governance approach can provide regulatory clarity and predictability, fostering \nan environment where businesses and innovation thrive while mitigating the risks inherent to \nAI’s transformative power.\nKeywords\nartificial intelligence; AI; tech policy; innovation; industry; congress; Dynamic Governance Model\nBiographical Notes\nPaulo Carvão is a Senior Fellow at Harvard Kennedy School’s Mossavar-Rahmani Center for \nBusiness and Government, specializing in Tech Policy and AI regulation. He is a former IBM \nexecutive with three decades of experience in digital transformation, cloud, and AI integration. \nCurrently an advisor to tech startups and a venture capital investor, Paulo has held fellowships at \nHarvard’s Advanced Leadership Initiative and Safra Center for Ethics. He serves as the inaugural \nEntrepreneur-in-Residence at the Harvard Kennedy School’s GovLab. \nSlavina Ancheva is a Harvard Kennedy School Master in Public Policy (MPP) candidate and a \nFulbright Bulgaria Scholar. She is a research assistant for Senior Fellow Paulo Carvão at the \nMossavar-Rahmani Center for Business and Government. Before the Kennedy School, Slavina \nworked in the European Parliament as a Policy Adviser and Head of Office for one of the leading\nnegotiators on the European Union’s Artificial Intelligence Act, the world’s first comprehensive law \non AI. \nYam Atir is a Harvard Kennedy School Master in Public Administration (MPA) candidate. She is\na research assistant to Senior Fellow Paulo Carvão at the Mossavar-Rahmani Center for \nBusiness and Government. Before attending the Kennedy School, Yam co-founded and led a \ntech policy think tank, focusing on macro-level research on technology ecosystems and providing \nstrategic advice on global government-tech relations and emerging technology policies.\nShaurya Jeloka is an A.B. candidate in Computer Science and Applied Mathematics at Harvard \nUniversity. He has developed intelligent algorithms for optimizing robotic systems, predictive \nmodels to mitigate M&A risks, and data-driven tools for performance analysis in academic and \nindustry settings. Shaurya has also contributed to AI tools that enhance legislative research and \npolicy analysis.\nBrian Zhou is an A.B. candidate in Computer Science and Statistics at Harvard. He is a former\nresearcher in autonomous systems at the Naval Research Lab and quantum machine learning at \nBrookhaven National Lab. Before Harvard, he founded a think tank focused on policy youth \nperspectives, cited by the Congressional Research Service and the UK Parliamentary Office of \nScience and Technology. He has testified at the UN Summit of the Future and helped establish \nthe Ventures TECH program at the Harvard School of Engineering and Applied Sciences.\n3\nTable of Contents\nABSTRACT ......................................................................................................................................................................................1\nPART I – THE CONTEXT.............................................................................................................................................................6\nA FALSE CHOICE....................................................................................................................................................................................................6\nCHARTING A PATH FORWARD: INTRODUCING A NEW GOVERNANCE MODEL............................................................................................7\nA BRIEF HISTORY OF ARTIFICIAL INTELLIGENCE............................................................................................................................................8\nAI stack – algorithms................................................................................................................................................................................9\nAI stack – data..........................................................................................................................................................................................10\nAI stack – computing power...............................................................................................................................................................11\nTHE SECOND AI TRIAD – ENERGY, LAND, AND LABOR ...............................................................................................................................13\nA UNIQUE MOMENT? ........................................................................................................................................................................................16\nAI POLICY CONSIDERATIONS...........................................................................................................................................................................22\nThe case for congressional action....................................................................................................................................................24\nHow the U.S. implements policy........................................................................................................................................................26\nEMERGING APPROACHES TO AI POLICY ........................................................................................................................................................29\nUNESCO.......................................................................................................................................................................................................29\nEuropean Union (EU)............................................................................................................................................................................29\nUnited States.............................................................................................................................................................................................31\nPeople’s Republic of China (PRC).....................................................................................................................................................32\nA national security example...............................................................................................................................................................34\nREGULATION-INDUCED INNOVATION............................................................................................................................................................36\nPART II – INDUSTRY AND CONGRESS PERSPECTIVES ................................................................................................39\nINDUSTRY PERSPECTIVES................................................................................................................................................................................39\nThe AI Ecosystem ....................................................................................................................................................................................39\nIndustry voices.........................................................................................................................................................................................43\nTHE STATE OF FEDERAL AI POLICY IN THE U.S. .........................................................................................................................................45\nThe Executive Order legacy ................................................................................................................................................................45\nU.S. Senate and House AI task forces..............................................................................................................................................49\nCONGRESSIONAL PERSPECTIVES ....................................................................................................................................................................50\nWhat legislative action is telling us................................................................................................................................................50\nQuantitative analysis of AI-related bills introduced in the 118th legislature................................................................52\nCongressional voices..............................................................................................................................................................................57\nWHERE CONGRESS AND INDUSTRY COULD COME TOGETHER..................................................................................................................60\nPART III – A NEW GOVERNANCE MODEL .........................................................................................................................64\nA PATH FORWARD.............................................................................................................................................................................................64\nA new model for collaboration and accountability..................................................................................................................66\nThe Dynamic Governance Model......................................................................................................................................................68\nEvaluation standards...................................................................................................................................................................................................68\nA market ecosystem for audits and compliance ...........................................................................................................................................70\nAccountability and liability.......................................................................................................................................................................................73\nImplementation context.............................................................................................................................................................................................77\nImplementation example: Improving data center energy efficiency................................................................................78\nA CAUTIONARY TALE ........................................................................................................................................................................................82\nCONCLUSION......................................................................................................................................................................................................83\n4\nACKNOWLEDGMENTS .............................................................................................................................................................85\nAPPENDIX I – APPROACHES AND METHODS..................................................................................................................86\nAPPENDIX II – ANALYSIS OF AI BILLS (118TH CONGRESS) ........................................................................................89\nDATA METHODOLOGY......................................................................................................................................................................................89\nData Collection.........................................................................................................................................................................................89\nData Cleaning and Preprocessing....................................................................................................................................................90\nHandling Missing Data.........................................................................................................................................................................91\nTAGGING LEGISLATION....................................................................................................................................................................................92\nSelection of Policy Attributes.............................................................................................................................................................92\nNLP-Based Bill Classification.............................................................................................................................................................92\nValidation with GPT...............................................................................................................................................................................94\nSponsorship & Partisanship ...............................................................................................................................................................95\nCommittee Influence on AI Legislation..........................................................................................................................................96\nLegislative Success..................................................................................................................................................................................96\nVISUALIZATION..................................................................................................................................................................................................96\nREFERENCES............................................................................................................................................................................ 108\n5\nList of Figures\nFIGURE 1 OPENAI’S CEO TWEET ANNOUNCING CHATGPT’S RELEASE TO THE WORLD.........................................................................8\nFIGURE 2 DECLINE OF COMPUTER STORAGE COSTS - OUR WORLD IN DATA IS AN EFFORT BETWEEN THE UNIVERSITY OF OXFORD \nAND THE NON-PROFIT GLOBAL CHANGE DATA LAB. ...........................................................................................................................11\nFIGURE 3 INCREASE OF SEMICONDUCTOR DENSITY (NUMBER OF TRANSISTORS ON CHIPS)- OUR WORLD IN DATA IS AN EFFORT \nBETWEEN THE UNIVERSITY OF OXFORD AND THE NON-PROFIT GLOBAL CHANGE DATA LAB....................................................12\nFIGURE 4 THE POWER REQUIRED TO TRAIN FRONTIER MODELS IS DOUBLING EVERY YEAR (EPOCH AI, 2024)...............................13\nFIGURE 5 POLICY ATTRIBUTES............................................................................................................................................................................23\nFIGURE 6 MECHANISMS FOR POLICY IN THE UNITED STATES - MODIFIED FROM JOHN HAIGH AND JON SALLET - BIG TECH AND \nTHE IMPORTANCE OF COMPETITION – HARVARD KENNEDY SCHOOL OF GOVERNMENT (FALL 2024)...................................27\nFIGURE 7 A COOL LOOKING CHANDELIER - IBM Q SYSTEM ONE QUANTUM COMPUTER - ADOBE STOCK...........................................44\nFIGURE 8 BILL SPONSORSHIP BY PARTY IN THE 118TH CONGRESS.............................................................................................................53\nFIGURE 9 BIPARTISANSHIP ANALYSIS FOR AI-RELATED BILLS IN THE 118TH CONGRESS .....................................................................54\nFIGURE 10 AI BILL SPONSORSHIP BY STATE DURING THE 118TH CONGRESS ...........................................................................................55\nFIGURE 11 PRIMARY POLICY ATTRIBUTES (150 AI BILLS FROM 118TH CONGRESS).............................................................................56\nFIGURE 12 TOP 3 POLICY ATTRIBUTES (150 AI BILLS FROM 118TH CONGRESS)..................................................................................56\nFIGURE 13 EXAMPLES OF AREAS OF ALIGNMENT BETWEEN CONGRESS AND THE INDUSTRY ECOSYSTEM (FROM INTERVIEWS)....61\nFIGURE 14 EXAMPLES OF AREAS WHERE MORE ALIGNMENT BETWEEN CONGRESS AND THE INDUSTRY ECOSYSTEM IS STILL \nNEEDED (FROM INTERVIEWS)...................................................................................................................................................................62\nFIGURE 15 DYNAMIC GOVERNANCE MODEL....................................................................................................................................................67\nFIGURE 16 ANTI-CAPTURE STRATEGIES FOR PUBLIC-PRIVATE PARTNERSHIPS ........................................................................................70\nFIGURE 17 EXAMPLE OF RETURNED API REQUEST DATA FOR H.R.4814 IN TABULAR FORM...............................................................90\nFIGURE 19 CODE SNIPPET OF REGULAR EXPRESSIONS USED TO PREPROCESS PROGRESS DATA.............................................................91\nFIGURE 20 DISTRIBUTION OF BILL CATEGORIES (TF-IDF).........................................................................................................................93\nFIGURE 21 COMPARISON OF BILL CLASSIFICATIONS: TF-IDF VS NATURAL TOPIC DISCOVERY...........................................................94\nFIGURE 22 DISTRIBUTION OF PARTY AFFILIATION OF PRIMARY BILL SPONSOR........................................................................................95\nFIGURE 23 TIMING OF AI BILL INTRODUCTION ...............................................................................................................................................97\nFIGURE 24 AI BILL SPONSORSHIP BY CHAMBER ..............................................................................................................................................98\nFIGURE 25 BIPARTISANSHIP BY CHAMBER OF CONGRESS .............................................................................................................................99\nFIGURE 26 BIPARTISANSHIP ANALYSIS - CONGRESS TOTAL.......................................................................................................................100\nFIGURE 27 AI BILL SPONSORSHIP BY STATE..................................................................................................................................................101\nFIGURE 28 MOST ACTIVE MEMBERS (HOUSE OF REPRESENTATIVES).....................................................................................................102\nFIGURE 29 MOST ACTIVE MEMBERS (SENATE)............................................................................................................................................102\nFIGURE 30 MOST ACTIVE COMMITTEES..........................................................................................................................................................103\nFIGURE 31 BILL PROGRESSION.........................................................................................................................................................................104\nFIGURE 32 BILL POLICY ATTRIBUTES .............................................................................................................................................................104\nFIGURE 33 PRIMARY BILL POLICY ATTRIBUTE BY PARTY............................................................................................................................105\nFIGURE 34 TOP 3 BILL POLICY ATTRIBUTES BY PARTY ...............................................................................................................................106\nFIGURE 35 PRIMARY POLICY ATTRIBUTES FOR THE 17 BILLS PLACED ON THE LEGISLATIVE CALENDAR (MOST PROGRESS).......107\nFIGURE 36 BILLS PLACED ON THE LEGISLATIVE CALENDAR DURING THE 118TH CONGRESS (TOTAL = 17)..................................107\n6\nPart I – The Context\nA false choice\nA storm was brewing unbeknownst to the captains of tugboats Montrose and T.J. Hooper off the \ncoast of New Jersey, setting in motion a sequence of events that would reshape American tort \nlaw. In March 1928, when gale winds claimed their towed barges and cargo, few could have \npredicted the legal precedent emerging from these troubled waters. The subsequent litigation \ninvolved multiple parties: the cargo owners sued the barge operators, citing breaches of carriage \ncontracts. Meanwhile, barge owners filed claims against the tugboat operators for\nunseaworthiness, specifically regarding their failure to implement radio receiver technology. This \navailable safety measure could have provided advance storm warnings. Judge Learned Hand \nadjudicated the case (The T.J. Hooper, 60 F.2d 737 (2d Cir. 1932)), and his ruling established \nthat the tugboat operators were negligent despite their adherence to contemporary industry \nstandards. This verdict shaped a precedential framework, indicating that non-adoption of available \nsafety technologies could constitute negligence, independent of industry-wide implementation \nrates. The ruling remained relevant in American tort law for nearly a century, informing legal \ndiscourse on technological innovation and safety standard requirements.\nThe debate around regulating technology is complex. Some argue that stricter regulation could \nhinder innovation by reducing incentives for investment and entrepreneurship. Others suggest \nthat targeted regulation could promote innovation by incentivizing firms to focus on safety. These \ndiverging perspectives are not new. We can learn from the past as we retrace the history of \nregulation and innovation. The Chief Executive Officer of a private, medium-sized, rapid-growth\ntechnology company shared his views about social media, another rapid-growth technology that \nAI often compares to. “Social media is a weapon […] the most sophisticated propaganda weapon \nthat was ever created […] The ability to tailor messaging, even before AI, is unprecedented. The \npattern of usage of several times an hour is unprecedented. The combination of all of it is creating \nthe most sophisticated way to brainwash people, and because of that, democracies collapse. […]\nWe missed the train. We should have regulated social media.” The perspective of a senior tech \ninvestor, board member, and former operator highlights this complexity: “People […] should have \nresponsibility for the models that they create and govern those models across their life cycle, to \nunderstand what kind of risk profile the model represents […] being overly prescriptive on those \nthings in this phase would be a mistake. […] Getting it right, in my mind, means not slowing down \nthe pace of innovation because AI […] needs to progress.”\nThe current discourse on digital technology regulation and, specifically, artificial intelligence is \nmissing the point. While the debate is often framed as a binary choice between whether to \nregulate or not, this is a false choice. Instead, we should focus on what regulatory approach will \nhelp steer innovation in a direction that our societal norms deem acceptable without stifling \ncreativity, competition, and entrepreneurship.\n7\nCharting a path forward: Introducing a new governance model\nAs the discussion of artificial intelligence \nregulation continues to evolve, policymakers \nface a dilemma: How can one foster innovation \nwhile ensuring safety, fairness, and societal well\u0002being? The answer lies not in a binary choice \nbetween regulation and laissez-faire innovation \nbut in a balanced approach - a dynamic model \ncapable of evolving with technology. We propose \na new model: an extra-regulatory framework5\nrooted in public-private collaboration, where \nregulatory clarity and accountability mechanisms \nare co-developed by government and industry.\nThe Dynamic Governance Model advocates for \nan incremental, iterative process. Rather than imposing rigid, one-size-fits-all rules, it emphasizes \na responsive system of standards, audits, and compliance frameworks that adapt over time. The \nmodel encourages safe innovation while minimizing societal risks by fostering an environment \nwhere government oversight complements industry expertise. This approach acknowledges that \ntoday’s faster pace of innovation requires a regulatory framework capable of evolving alongside \ntechnological advancements. \nTo guide readers through this multifaceted topic, we organized the manuscript into three parts:\n1. Historical Context and Technological Foundations:\nThe paper begins by exploring historical precedents in technology regulation and the \nunique characteristics of artificial intelligence. This part describes the AI triad - algorithms, \ndata, and computing power - highlighting their role in AI’s rapid advancement. It then \nintroduces a new AI triad - energy, land, and labor. The first part concludes with a \ndiscussion about the attributes of AI policy, examples from around the world, and the \nspecific aspects of how the United States implements policy.\n2. Current Legislative Landscape and Industry Perspectives:\nFollowing the initial context setting, the paper presents a first-of-a-kind analysis of the AI \nproposed legislation during the 118th United States Congress Session. The analysis is \nenriched by insights from qualitative interviews conducted by the authors with Members\nof Congress and their staff and industry leaders. The interviews highlight emerging areas \n5 A word on the term “extra-regulatory framework”: This work does not advocate for any specific legislation or statute to regulate the technology. The \nfocus, instead, is on a method that can be applied to different policy objectives, from smaller, niche-based policies to larger AI national policy initiatives.\nHow does one foster innovation while \nensuring safety, fairness, and societal \nwell-being? The answer lies not in a \nbinary choice between regulation and \nlaissez-faire innovation but in a \nbalanced approach - a dynamic model \ncapable of evolving with technology.\n8\nof consensus between policymakers and the private sector, as well as key gaps where \ngreater alignment is necessary. These findings form the foundation of the proposed \ngovernance model.\n3. Proposed Framework for Dynamic Governance:\nThe final part of the paper outlines the Dynamic Governance Model’s three core \ncomponents:\na. Public-private partnerships for the creation of evaluation standards.\nb. A market-based solution for audit and compliance.\nc. A system of accountability and liabilities set by legislatures, existing executive \nagencies, and the courts.\nOur objective is to provide an actionable path toward balanced AI governance, first establishing \nthe historical and current context and then introducing a forward-looking solution. While the reader \nshould approach these three parts sequentially, one can jump directly to the segment of their \nparticular interest and only refer to others as needed.\nThe renowned American legal historian and scholar, Lawrence M. Friedman, said: “[T]echnology \nand science, as they move and change, impact the general culture. The general culture then, in \nturn, impacts law, the legal system, and the very structure of government” (Friedman, 2023). Let’s \nstart at the beginning of the quest for Artificial Intelligence. \nA brief history of artificial intelligence\nIt was as if the dreams devised during the summer of 1956 had suddenly become a reality. In \nNovember of 2022, sixty-six years after a group of mathematicians and scientists got together for \nthe Dartmouth Summer Research Project on Artificial Intelligence (McCarthy et al., 2006), \nOpenAI’s release of ChatGPT captured the zeitgeist by placing a mirror in front of society. The \nimitation game proposed by Alan Turing was on, and his question, “Can machines think?” was \nbeing asked by the masses (Turing, 1950).\nFigure 1 OpenAI’s CEO Tweet Announcing ChatGPT’s Release to the World\n9\nThe 1956 summer artificial intelligence research, considered the Constitutional Convention of AI \n(“Encyclopedia of Artificial Intelligence: The Past, Present, and Future of AI,” 2021), assumed that \none could precisely describe all aspects of human learning and other intelligence features so that \na machine could simulate it. Based on that premise, the researchers who met at Dartmouth sought \nto discover how machines use language and form abstractions to solve problems typically \naddressed by humans. Thus, the race started with the expectation that they would make \nsubstantial progress over a few weeks of dedicated effort. The progress, however, was slow, with \nscientific discoveries followed by a series of long hiatuses – the AI winters - in which academic \nresearch and industry focus drifted away from the topic. Fast forward to 2022, ChatGPT reached \n1 million users in just 5 days after launch, and as of October 2024, it has approximately 250 million \nactive users (ChatGPT Revenue and Usage Statistics (2024), n.d.) To understand this dynamic, \nwe must look closely into how AI works and the drivers of its progress.\nAI stack – algorithms \nOne can think about modern artificial intelligence as intelligent mathematical algorithms being \ntrained on massive amounts of data, using, in this process, unprecedented computing power. This \ncombination of algorithms, data, and computing power is commonly known as the AI triad \n(Buchanan, 2020). The history of the current AI algorithms started in the 1940s when a \nneurophysiologist and a logician modeled neural activity via logical operations, establishing the \ntheoretical basis for what we today call neural networks (Warren S. McCulloch & Walter Pitts, \n1943). More than a decade later, using the then-current state-of-the-art IBM computers at the \nCornell Aeronautical Laboratories, Frank Rosenblatt, a psychologist, worked on the perceptron,\nan early version of binary classifier algorithms inaugurating the field of deep learning (a form of \nmachine learning which imitates how the human brain functions requiring training with large \namounts of data) (Rosenblatt, 1958).\nDuring the next couple of decades, scientists struggled with how to scale and perfect the training \nof neural networks. This lasted until the late 1980s when scholars introduced the concept of \nbackpropagation, enabling the efficient training of multi-layered neural networks (Rumelhart et al., \n1986). This breakthrough allowed the development of the first convolutional neural network \narchitecture, which was initially applied to handwriting recognition and designed to mimic the \nvisual processing of the human brain (LeCun et al., 1989). During the 1990s, Recurrent Neural \nNetworks drew attention given how they process sequential data and maintain the memory of \npast input. These techniques were applied in areas like speech recognition and natural language \nprocessing (Hochreiter & Schmidhuber, 1997). In 2012, researchers at the University of Toronto \nwon the ImageNet Large Scale Visual Recognition Challenge using deep convolutional neural \nnetworks and showed the promise of deep learning for computer vision (Krizhevsky et al., 2012).\nThe stage was set for the introduction of the Transformer architecture by a team of Google and \nUniversity of Toronto scholars in 2017. This streamlined design eliminated the need for recurrence \nand convolutions, leading to superior training quality while maximizing parallel computing and \nsignificantly reducing training time (Vaswani et al., 2023). Shortly after that, in 2018, scientists at \nOpenAI introduced the Generative Pre-trained Transformer (GPT), a large language model (LLM) \n10\nthat uses neural networks to understand and generate text (Radford, 2018). The Generative Pre\u0002trained Transformer serves as the core algorithmic architecture for ChatGPT.\nRecently, scientists and engineers have focused on energy consumption optimization, improving\nproblem-solving capabilities, and developing models that can operate across multiple domains \nbeyond language with increased agentic features. Techniques like Mixture of Experts (MoE) \n(Bergmann, 2024), Chain of Thought (OpenAI, 2024), and multimodal agentic offerings (Pichai et \nal., 2024) are hitting the market. Researchers from the University of California Berkeley recently \nhave open-sourced Sky-T1, a reasoning AI model fine-tuned for under $450, enabling affordable \nadvanced AI development. However, “training” here means fine-tuning, not pre-training, which \nremains resource intensive (Wiggers, 2025).\nThis summary is not an exhaustive list of all the achievements and pioneers in this field; instead, \nit provides a bird’s-eye view of some of the key milestones in the development and evolution of \nAI algorithms.\nTwo distinct timelines emerge when reviewing this brief history of artificial intelligence algorithms: \nOne from 1943 to 2018, seventy-five years of relentless work creating, tweaking, and optimizing \nthe mathematics and computational structures. Another was in the seven years from 2018 to \n2025, particularly since November 2022, when ChatGPT exposed the transformer architecture \nand GPTs to the public via an easy-to-use chat interface. What makes them different? Data and \ncomputing power, the other two elements of the AI triad, will provide the necessary clues.\nAI stack – data \nData has been an integral part of computer science, and the need to manage large data sets goes \nback to the early data centers used by governments and banks in the 1960s and 1970s. Relational \ndatabases, invented at IBM research labs in 1970 (Codd, 1970), were used for most traditional \ntransactional processing. As we entered the new century, the rate and pace of data generation \nincreased. Internet usage and e-commerce became pervasive as more intelligent devices \nconnected to the web. We have transitioned from an era where only computers generated data \nto one defined by the Internet of Things, with data now produced by home appliances, cars, \nphones, and a wide range of other devices. Social media connects billions of people worldwide, \nconstantly using their mobile devices and generating even more data. Data became ubiquitous,\nand the costs for generating, transmitting, and storing it were reduced drastically (Mathieu, 2024).\n11\nFigure 2 Decline of Computer Storage Costs - Our World in Data is an effort between the University of Oxford and the \nnon-profit Global Change Data Lab.\nAI stack – computing power \nHaving discussed two of the three components of the AI triad - algorithms and data – let us look \nat computing power. Here, two phenomena played a decisive role: Moore’s Law, which observes \nthat the number of transistors on an integrated circuit doubles approximately every two years \nwhile maintaining a relatively constant cost (Moore, 2006), and the advent of cloud computing. To \nput this into perspective, Intel’s first microprocessor, the Intel 4004 from 1971, had 2,300 \ntransistors (Ars Technica, 2011). An NVIDIA Blackwell Series GPU announced in 2024 with \nplanned availability in 2025 will pack 208 billion transistors (NVIDIA Blackwell Architecture, n.d.).\nCloud computing is delivering computing services over the Internet, doing away with the need for\nproprietary data centers. The concept of sharing computing capacity goes back to the 1960s and \n1970s when time-sharing and virtualization of traditional IBM mainframe computers were used. \nIn its modern form, cloud computing has taken off since 2002 with the launch of Amazon Web \nServices (AWS), followed by Google Cloud Platform in 2008, Microsoft Azure in 2010, IBM \nSmartCloud in 2011, and Oracle Cloud in 2012. Cloud deployments reduce the initial investment \nneeded to build computing capacity and allow for flexible consumption “by the drink.” \n12\nFigure 3 Increase of Semiconductor Density (number of transistors on chips) - Our World in Data is an effort between \nthe University of Oxford and the non-profit Global Change Data Lab.\nThis completes the triad: modern algorithms, with access to large training data sets, and \nleveraging lower-cost flexible computing power. The confluence of these three factors explains \nthe acceleration since 2018. \nLarge Language Models have become the public face of AI today, accessible to users through \nchat interfaces or APIs. Examples are ChatGPT from OpenAI, the Doubao family of models from \nByteDance, Gemini from Google, Claude from Anthropic, DeepSeek-V3 and R1 from DeepSeek, \nGrok from xAI, LLaMA from Meta, Granite from IBM, Qwen from Alibaba, Mistral Large from \nMistral AI, ERNIE from Baidu, Hunyuan from Tencent and more. Since their initial introduction, \nmaterial progress has been made, with most advances driven by scaling — the ability to process \nincreasingly large training datasets (Kaplan et al., 2020). The size of a Large Language Model \n(LLM) is typically measured by the number of parameters it contains, which has been growing \nexponentially. For instance, ChatGPT-3 has 175 billion parameters, while ChatGPT-4 contains \n1.7 trillion parameters. However, this scaling comes at a significant cost, as the energy \nconsumption required to train and operate LLMs raises serious concerns about their \nenvironmental impact (Carvao, 2024b). This rapid growth, combined with unresolved technical \nchallenges such as transparency, explainability, and the mitigation of hallucinations, has \nprompted some scholars to question whether LLMs are approaching their limits (Marcus, 2024).\nThese conversations echo in the halls of Congress and state legislatures, where decision-makers \n13\noscillate between “Wow, AI is coming, and we’re on the cusp of superhuman intelligence” and \n“We’re hitting a plateau.” Ultimately, “both things can be true… [and] there’s no way we’re going \nto keep a pulse on everything all at once,” concludes one Staffer.\nThe second AI triad – energy, land, and labor\nAI and its data center infrastructure are hungry for energy. \nAccording to Goldman Sachs, data center power demand will surge to 8% of the U.S. total by \n2030 from 3% in 2023, growing at a 15% CAGR (compound annual growth rate). For context, the \ntotal energy demand in the country has been flat in the decade up to 2022 and is now projected \nto accelerate at 2.4% CAGR, of which data centers represent 37.5% of the total growth. This \npattern reflects an AI-driven growth in data center demand at the same time there is industrial \nreshoring and growth in manufacturing activity. About $50B in capital investment in U.S. power \ngeneration is required only to keep up with the data center growth until 2030 (Davenport et al., \n2024). In another projection, the U.S. Department of Energy estimates that data centers will \nconsume up to 12% of total U.S. electricity by 2028 (U.S Department of Energy, 2024). This \npressure on the country’s power system has caught the attention of Congress as reflected by this \ncongressional staffer’s comments: “A report […] out yesterday from Lawrence Berkeley […] it’s \ngoing to take up 12% of our electricity demand […] it’s going to have a higher impact on the new \nelectricity demands on the grid, more than EVs and onshoring and electrification of processes.\n[…] And as a result, something needs to be done, whether that be engaging the AI companies \n[…] to build out the systems that we need to power what they’re doing, or that is using these AI \nsystems in a way to solve these problems. […] real people are suffering from rate hikes.”\nFigure 4 The power required to train frontier models is doubling every year (Epoch AI, 2024)\n14\nBig Tech is accelerating investments in AI data center buildout. Microsoft announced plans for a \n2025 investment of $80B in data centers to train AI models and host the cloud infrastructure to \ndeploy AI applications. While this represents a global investment, more than half will be made in \nthe U.S. (Smith, 2025). OpenAI, together with Oracle, SoftBank, and the MGX Emirati investment \nfirm, announced a $500B investment over four years to build AI infrastructure (OpenAI, 2025).\nThe AI-driven data center boom is reshaping several U.S. regions. Northern Virginia’s “Data \nCenter Alley” leads, consuming a quarter of the state's power load. Texas follows with rapid \ngrowth, while California remains crucial despite constraints. Washington state has emerged as a \nsignificant player, with data centers consuming 5.69% of its electricity in 2023. The state benefits \nfrom abundant renewable energy, with 54% of Central Washington’s data centers powered by \nhydroelectricity. However, this growth strains the power grid and challenges Washington’s green \nenergy goals. Emerging secondary markets are being developed across the country, bringing a \ncombination of economic benefits and infrastructure and environmental challenges as states \nbalance growth with sustainability concerns (Cheung, 2024; Tremayne-Pendelly, 2024).\nThe surge in demand is driving large enterprise consumers to reevaluate their energy-sourcing \nstrategy. It has led to a resurgence of interest in nuclear power for reliable and emissions-free \nelectricity. One technology, small modular reactors (SMR), has caught the attention of Big Tech. \nSMRs’ compact footprint allows them to be sited in locations unsuitable for larger nuclear plants. \nTheir prefabricated units can be manufactured off-site, shipped, and installed efficiently, reducing \ncosts and construction delays compared to custom-built large reactors. These smaller reactors\ncan be deployed incrementally, aligning with growing energy demands while saving time and \nexpense (Liou, 2023). Amazon Web Services incorporated SMRs into their plans for net-zero \ncarbon operations by 2040 and is signing agreements with utilities in Washington and Virginia to \nuse this technology (Amazon, 2024). In addition to exploring the usage of SMRs, Microsoft plans \nto reactivate the Three Mile Island nuclear power plant in Pennsylvania, the site of the worst \nnuclear reactor accident in U.S. history (Eccles, 2024; Plumer, 2024).\nGoogle is collaborating with the startup Kairos Power to develop and deploy SMRs. Their goal is \nto provide up to 500 megawatts of continuous carbon-free power by 2035 to support their AI\nenergy demands while helping improve grid reliability and global decarbonization efforts (Terrell, \n2024). A congressional staffer questions whether we can get there fast enough: “I’m very pro\u0002nuclear. I think that we should be getting more nuclear on the grid as fast as possible. I’m also \nvery aware that SMRs are unproven […] and that the larger reactors, like Three Mile Island, if we \nwant to get it back up, will take years, and the ones we want to build from scratch will take \ndecades. […] nuclear is going to be a really great long-term play, but something needs to be done \nin the short term too […] the issue is no one, I’ve heard, no one at any level in any country, has a \nshort-term play.”\nIn parallel, in an intense lobbying effort, OpenAI has proposed to the Biden and Trump \nadministrations the development of massive data centers, each consuming as much power as an \nentire large city, to advance AI models and maintain U.S. competitiveness with China. The \ncompany argues that these facilities could generate tens of thousands of jobs and boost GDP if\nsupported by the right governmental policies. The proposed 5-gigawatt data centers each \n15\nrepresent the equivalent of five nuclear reactors and could power 3 million homes (Ghaffary, \n2024). “There’s one-gigawatt, there’s two-gigawatt, there’s three-gigawatt data centers that are \nbeing planned right now and are being talked about. That’s more energy than most cities need, \nlike small cities, it’s absurd. So, you know, it’s a thing that we really need to be thinking about \nbecause every time one of those goes on the grid, it makes the grid less reliable” highlights our \ninterviewee. Oklo, a U.S. nuclear power plant developer, has signed a non-binding agreement \nwith data center operator Switch to deploy 12 gigawatts of projects based on their technology by \n2044 (World Nuclear News, 2024). Sam Altman, CEO of OpenAI and Chairman of Oklo’s Board \nof Directors since 2015 (Oklo Inc., n.d.), actively lobbied the White House for government support \nin the nuclear-powered data center buildout. This episode underscores the promise of public\u0002private partnerships and policies to drive technology adoption while testing the boundaries of \nconflicts of interest when a single company dominates multiple aspects of the discussion.\nThe new facilities require repurposing historically agricultural land to develop large-scale data \ncenters. Data centers rely heavily on water for cooling, often drawing hundreds of thousands of \ngallons per day and putting considerable pressure on natural resources (Carvao, 2024b).\nOrchards give way to high-tech hubs, disrupting local economies and cultures. The influx of wealth \nbuilds schools and infrastructure but raises living costs, pricing out many long-term residents. \nThese dynamics impact the real estate market, with developers now seeking much larger land \nparcels, often 500 to 1000+ acres, compared to the 15–20-acre plots sought a decade ago. The\nscarcity of suitable sites with adequate infrastructure pushed development into secondary \nmarkets and rural areas. Developers are facing resistance from local communities concerned \nabout the impact of large data centers on their areas, and some counties are considering bills to \ncurb data center development. Land prices in key markets have tripled while asking rates for data \ncenter space have risen by 20% year-over-year in major North American markets (CBRE, 2024; \nHoulihan Lockey, 2024).\nData center construction across the United States created a surge in demand for skilled electrical \ntrade workers to build facilities and infrastructure. The current lead time to power new data centers \nin large markets such as Northern Virginia can be more than three years. And, in some cases, \nlead times for electrical equipment are two years or more. This escalating demand underscores \nthe need for substantial investments in workforce development to ensure the availability of \nqualified labor essential for the timely and efficient expansion of data center infrastructure (Green \net al., 2024). Labor constitutes the backbone of AI’s ascent. Migrant electricians work long hours \nfor lucrative pay while unions scramble to train new workers. Yet, once operational, these facilities \noffer minimal jobs, underscoring the short-term nature of such booms.\nThis second AI triad of energy, land, and labor reshapes economies and landscapes. Central \nWashington illustrates this transformation, where abundant hydropower fuels sprawling data \ncenters vital for AI. Power drives the infrastructure as small towns become magnets for data\u0002intensive operations. However, this reliance stretches the grid, forcing companies to seek nuclear \nsolutions (Weise & Tamayo, 2024).\n16\nAI data centers’ increasing electrical power demand has a significant environmental impact. The \ntraining of GPT-3 by OpenAI used 1,287 megawatt-hours of electricity and produced 502 tons of \nCO2 emissions (Czarnecka, 2024), the same power 120 \naverage American homes consume in one year (Aibin & Simic, \n2024). Using LLMs for search is becoming the norm. Directly,\nwith an app like Perplexity, using ChatGPT’s search feature, or \nrelying on Google’s AI-generated search summaries. This new \nway of searching, at inference time, uses as much as ten times \nthe energy a Google search does (Goldman Sachs, 2024).\nThe new triad illuminates a new series of questions to be \naddressed by policymakers. Another congressional staffer working in the Senate told us: “We \nwant to be strategically competitive against China, and so we want to have all the AI resources \nwe possibly can. That’s a question of IP and foundation models. It’s a question of chip resources. \nIt’s a question of energy availability to power AI models, and I think these things […] give us the \nupper hand.”\nThe new AI triad - energy, land, and labor - underscores substantial opportunities for public\u0002private partnerships while highlighting the need for policy development or reform to support \nindustry growth and address challenges. Setting aside the merits or politics of each area, issues \nsuch as land permitting reform and workforce adaptation and training emerge as priorities. The \ncongressional staffer continues: “Both parties can come together and say, well, we cannot build \nAI data centers on solar and wind […] our current infrastructure is totally insufficient to support \nthe power needs of AI centers across the country. But there exists new technology […] if you can \nrelease some of the regulations so that they can be built in five or six years and not 14 or 15 years \n[…]. Energy infrastructure is not a short-term play, and it has to survive across Congresses and\nacross administrations. […] China plays the long game. China understands that this is not a short\u0002term play. This is a long-term play, but the U.S. is always trying to get the best bang for his buck \nin four years.”\nThere are stark contrasts between the rapid evolution of industry demands and the slower pace \nof legislative processes. Reconciling these two dynamics will be a recurring theme in our \ndiscussion. \nA unique moment?\nHere we are, witnessing a technology undergoing rapid adoption while unresolved questions \nremain (Lynch, 2024; Strickland, 2024).\nIs this AI moment truly unique? In many ways, the recent rise of artificial intelligence adoption is \nexceptional, especially considering that it follows the slow burn of the first seventy-five years of \nscience in the field described in a previous section. The recent advancements in AI can be \nattributed to the cumulative effects of progress in microelectronics and the more effective \nThe new AI triad -\nenergy, land, and labor\n- illuminates a new series \nof questions to be \naddressed by \npolicymakers.\n17\ndeployment of neural networks - concepts that have existed for decades. While AI, like many \nother elements of the technology and venture-backed industries, is often overhyped, its impact \non personal and business productivity is starting to become visible. Previous automation waves \nhave primarily affected manual and clerical work. AI, especially when combined with other forms \nof physical automation, such as robotics, can also have a material impact in this area. In addition, \nAI can transform white-collar and academic work, especially for repetitive, well-structured tasks, \nand its implications for knowledge workers are different than what previous waves of technology\u0002driven automation brought (Dell’Acqua et al., 2023).\nA recent working paper from the National Bureau of Economic Research explores the role of \nGenerative AI as a new general-purpose technology. The article investigates its rapid adoption, \nusing a nationally representative survey in the U.S., and finds that 39% of adults aged 18–64 use \nGenerative AI, with 28% using it for work tasks like writing, data analysis, and coding. Adoption \nis higher among younger, educated, and higher-income individuals, with significant use in \nmanagement and technical occupations. Generative AI has been adopted faster than prior \ntechnologies like PCs and the Internet, driven by its consumer-oriented nature. While adoption \nvaries by demographics and occupation, early patterns suggest productivity gains, though its \nbroader economic impact remains debated (Bick et al., 2024).\nOn the other hand, this is not the first general-purpose technology to have a deep and broad \nimpact on economies and societies. Policies and structures have been established during other \nwaves of technological change to stimulate and safeguard adoption while mitigating and \naddressing risks and harms. As one of our congressional interviews highlighted, the period in \nwhich “everyone collectively freaked out about AI last year is starting to come down from the hype \ncycle.” Some industry leaders echo this sentiment: “We didn’t have enough of a regulatory regime \nfor Blockchain and Web 3, and we had a massive failure in FTX […] governments are afraid of \nmissing that again, and the harm to investors and others that came from that is very fresh right \nnow. There’s also a feeling of the uniqueness of these technologies and systems […], but I’m, I’m \nnot so convinced. I don’t believe you need a quantum regulatory regime that would be materially \ndifferent from an AI regime.”\nNevertheless, unresolved challenges, such as the development of autonomous agents, remain \non the horizon. It is imperative to look at both sides of this argument. \nThroughout the history of scientific and technological progress, the explosive growth and \naccelerated adoption of AI stand out as a distinct moment. AI is reshaping multiple facets of our \nlives — from the organization of production and labor to the balance between automation and job \ncreation, the control and dissemination of information, and even the very concept of intelligence \nand reasoning (Acemoglu & Restrepo, 2020; Harari, 2024). These transformations carry profound \nsocio-economic and geopolitical implications that often surpass the impact of previous \ntechnological shifts. A Senate staffer with deep technological experience put it in vivid terms: “I \nthink that artificial intelligence will prove to be the most influential technology discovered by man \nsince the discovery of the wheel, the fire, and the Internet in terms of the impact it will have on \n18\nthe productivity and the survivability of mankind. […] Anyone who thinks it won’t be is not aware \nor doesn’t understand the ultimate potential.”\nThe recent rapid pace of AI development, its ability to operate autonomously, and its widespread \nimpact across multiple sectors make it fundamentally different from other technologies. These \nincreasingly complex systems can make decisions without human intervention (Chan et al., 2023), \nraising questions about accountability, as traditional liability frameworks are designed around \nhuman actions. For example, if an autonomous AI system makes a harmful decision, it is unclear \nwho is responsible: the developer, the deployer, or the AI system itself. The emerging agentic \nnature of algorithmic systems is one of the main ways they differentiate from traditional \ntechnologies. There is a regular interplay between AI technology and societal values like fairness, \nprivacy, and transparency. For instance, biased algorithms, or algorithms trained on biased data,\ncan perpetuate discrimination, and AI in surveillance can infringe on privacy. As previously \ndiscussed, AI systems rely heavily on data, which raises privacy and security concerns as training\ndatasets can include personal or sensitive information. AI systems often operate as “black boxes”\nwhere even their creators cannot fully explain how they arrive at specific outcomes. In many ways, \nAI is unique.\nBut is it? Looking back at history, one can identify major transitions in economic development \nfrom the beginning of organized economic activity to today. While there are multiple ways to \ncategorize these phases, let’s look at the defining emerging technology from each era. The \nNeolithic Revolution (c. 10,000 BCE) marked the transition from hunter-gatherer societies to \nagricultural ones. The systematic cultivation of plants and the domestication of animals acted as \nthe defining technologies. The Urban Revolution (c. 3000 BCE) saw the rise of the first cities and \ncomplex societies enabled by the development of written language and record-keeping systems. \nDuring Classical Antiquity (c. 800 BCE - 500 CE), the development of more sophisticated \neconomic systems was enabled by extensive systems of roads, particularly the Roman roads. \nLater, a transition from feudal to commercial, market-oriented economies (c. 1000 - 1500 CE)\nhappened in parallel to the advent of the printing press, which revolutionized the spread of \ninformation and knowledge. The First Industrial Revolution (c. 1760 - 1840) was a transformative \nperiod that saw the shift from agrarian economies to industrial ones and the introduction of the \nsteam engine powering factories, trains, and ships, mechanizing production, transforming\ntransportation, increasing urbanization, and ushering the rise of factory systems and wage labor. \nThe Second Industrial Revolution (c. 1870 - 1914), built upon the first, introduced mass production\nand new industries (like chemicals, steel, and petroleum), gave birth to large corporations and \nscientific management, and was powered by electricity. The post-industrial phase sometimes \nreferred to as the information or knowledge economy (c. 1950 – present), occurred as \nmanufacturing gave way to service-based economies, with the entrance of information technology \nand digital economies, the globalization of production and finance, and the emergence of \nknowledge-based industries. Semiconductors, computers, and the Internet revolutionized \nproductivity, communication, and commerce. \nEach of these transitions fundamentally altered the structure of economic activity, the organization \nof labor, and the distribution of wealth and resources. They represent key inflection points. The \n19\nunderlying technologies in each era were transformative. These General Purpose Technologies,\nsuch as the steam engine, the electric motor, and semiconductors, became pervasive across a \nlarge cross-section of the economy, facilitating widespread productivity gains. General purpose \ntechnologies drive “innovational complementarities,” and the productivity of a downstream sector \nincreases with the innovation introduced with the enabling technology. The development of \ndownstream applications increases the return to advances in the enabling general-purpose \ntechnology. Advances in this technology lead to opportunities for new applications in a positive \nfeedback loop of accelerated technical progress and economic growth (Bresnahan & Trajtenberg, \n1995). The current adoption of AI mirrors past instances of technological change, where new \npossibilities can disrupt social, environmental, and cultural values, highlighting the broader \nchallenge of law struggling to adapt to the rapid pace of innovation and its societal impacts\n(Bennett Moses, 2007). From this historical perspective, AI is not unique and may only be \nanother general-purpose technology unleashing growth through productivity. The pace of change, \nhowever, is accelerating.\nWe are amid an emerging new phase in a post-knowledge economy. There is an increased \nemphasis on uniquely human abilities such as creativity, emotional intelligence, ethical reasoning, \nand complex problem-solving. The youth seek purpose and meaning, and economic activity \nfocuses on individual and societal well-being rather than just optimizing productivity. Adaptability \nand resilience are at a premium as citizens adapt and respond to future challenges. Traditional \nknowledge-based tasks are beginning to be taken over by artificial intelligence, freeing humans \nto focus on higher-level creative and meaning-oriented work. AI may become the defining \ntechnology of this new era, but it is too early to tell. \nThere is a growing body of scholars questioning economic growth (Gross Domestic Product or \nGDP) as the primary measure for success at the expense of human well-being and environmental \nhealth. They argue instead for balancing human needs with planetary limits by adopting economic \ndevelopment that is regenerative by design (Raworth, 2017). This perspective can be contrasted \nwith the techno-optimistic view embraced by some leading investors who are funding venture \ncapital-fueled innovation (Andreessen, 2023; Dean, 2023). As in other dual-use technologies, \nthere are plenty of opportunities for malicious use of AI with the capacity to weaponize it in ways \nthat previous technologies like nuclear, chemical, and biological have been (Herdman, 1993).\nOne of our industry interviews captured this dichotomy: “There’s still a huge, huge opportunity \nahead, and there has been tremendous progress in recent years, both with the technology as well \nas the applications and the use cases. Broader adoption has allowed us to accelerate our \nunderstanding of what AI could be used for and how to use it. And as technologies […] have \ngotten into the hands of so many more people, now we’re seeing feedback loops and \nimprovement at a rapid rate. We’re seeing exciting and promising use cases as well as concerning \nuse cases. […] I’m a technologist, I’m an early adopter, I’m somebody who believes in the promise \nof technology. At the same time, I don’t think technology is inherently good or bad, but we need \nto have appropriate guardrails and […] a tech policy discussion.”\nWhile researchers continue to push the boundaries of the AI triad, early signs of saturation in data \navailability and the growing energy demands of large-scale computation pose significant \n20\nchallenges. Scientists use synthetic data to augment training datasets and reduce reliance on \nreal-world data to address these limitations. In parallel, model distillation techniques are used to \ncreate smaller, more efficient AI models requiring less computational power while maintaining \nhigh performance. For instance, a large LLM trained on real and synthetic data can be distilled \ninto a more compact version that runs on edge devices, making AI more accessible and \nsustainable. As recently as 2023, authors wrote about model deterioration when training relies on \nincreasing amounts of synthetic data (Shumailov et al., 2023). As one begins to approach the \nlimits of legally accessible data on the Internet, the use of machine-generated data becomes a \ncritical path forward. A team of researchers at Microsoft recently published a new model with \nperformance comparable to larger ones while utilizing high-quality synthetic datasets in \ncombination with high-quality organic data and employing post-training innovations (Abdin et al., \n2024). Progress using synthetic data is one of many examples of the interplay of innovation and \nprogress.\nGenerative AI exposed new frontiers in accessing, manipulating, and generating information. We \nsaw the first round of the impact on knowledge workers and professions. As we move beyond the \nfirst quarter of the century, we are entering an era of agentic AI, delivering on the promise of \nembedding intelligence into business processes. Generative AI agents are evolving from \ninformation providers to autonomous actors capable of executing complex workflows and \ncollaborating seamlessly with humans, promising enhanced productivity and innovation (Yee et \nal., 2024). AI agents have autonomous goal-driven behavior, contextual decision-making, and \niterative learning. Unlike static models, agents dynamically interact with environments, leveraging \nreinforcement learning and LLMs to perform complex, multi-step tasks without human \nintervention. Engineering excellence and technical innovation have once again unleashed the \nagentic AI era we are embarking on (Wiesinger et al., n.d.). Reduced human involvement and \nincreased autonomous decision-making heighten the need for ethical development and \nawareness of engineers’ sociotechnical impact.\nCurrent challenges like model footprint, the associated computing power and energy \nconsumption, model transparency, hallucinations, alignment, and more can and must be \nremedied by creating new algorithms and techniques to address the shortcomings. Over time, the \nindustry can develop and deploy inherently safe models with minimized externalities, provided \nthe right incentive policies are in place. This approach can sustain the current pace of \ntechnological advancement and adoption.\nCharting a path forward requires thoughtful reflection on AI’s unique - or perhaps not so unique -\nnature. It demands a careful balance between the utopian visions of techno-optimists and the \ndystopian concerns of those who question whether our growth-driven economic model places an \nunsustainable burden on the planet. While AI possesses distinctive characteristics and is \nadvancing at a pace that challenges our capacity to adapt, technological change is not a new \nphenomenon. Lessons from the past can offer valuable insights.\n21\nFrom a policy perspective, the evolution of AI is a blend of \ncontinuity and novelty. AI is often compared to social media by\nseveral elected representatives. “I think one of the mistakes we \nmade with the Internet and social media is that we were too late \nto the game, and we lacked putting in the proper guardrails,”\nsays one elected representative. Yet, despite this consensus \nthat Congress may have missed an opportunity to act on social \nmedia, the parallel has its limits. “The difference is social media \ndoesn’t have a super positive use case – connecting people is \nmeaningful – but AI innovation will change defense and \ncompetition with China,” says one congressional staffer\nMost of our interviews with Congress distinguished between AI \nuses already addressed by sectoral legislation and the emergence \nof Generative AI, which may warrant a distinct regulatory approach. For instance, using AI to \ncommit fraud or discrimination is not unique, as these behaviors can be governed through existing \nsectoral laws. A congressional staffer summed it up best: “If someone uses a pencil or an AI \ntechnology to carry out discrimination, they will have to deal with that discriminatory act.” At the \nsame time, unlike most traditional policy issues, Generative AI and general-purpose AI more \nbroadly cut across multiple domains. As another congressional interview highlighted: “One [issue] \nis how do you regulate AI products with well-defined use cases. The other is how to regulate \ngeneral-purpose AI that can be used for lots of things […] Currently, we don’t know what a \nmeaningful framework [for general-purpose AI] looks like, but we definitely need one… it’s \ndifficult.”\nThis dilemma has led to a proliferation of bills introduced by Members of Congress from various \ncommittees, parties, and interest areas. This effort began as early as the 116th Congress but \npicked up steam after the introduction of ChatGPT, as evidenced by the volume of AI-related \nlegislation introduced in the 118th Congress compared to prior sessions. A notable trend from the \nearly days of AI policymaking was the emphasis on education for Members of Congress and their \nstaff. Congressional caucuses, AI task forces, and the formation of congressional Bipartisan AI \nCaucuses played a key role in equipping lawmakers with the knowledge required to craft informed \nAI policy. Similarly, committee staff members see education as a core component of their role, \nensuring policymakers are equipped to address the rapidly evolving AI landscape.\nWhile AI possesses \ndistinctive \ncharacteristics and is \nadvancing at a pace \nthat challenges our \ncapacity to adapt, \ntechnological change is \nnot a new \nphenomenon. Lessons \nfrom the past can offer \nvaluable insights.\n22\nAI policy considerations\n“The marvels of technological advance are not always risk-free”, a renowned legal scholar \nreminds us (Mandel, 2007). Today’s literature extensively covers the benefits and risks of artificial \nintelligence. AI offers benefits such as increased productivity, personalized services, and \nadvances in healthcare, education, and decision-making. Automation reduces human error, \nstreamlines operations, and enables continuous production. However, AI also carries risks, \nincluding labor displacement, data privacy violations, algorithmic bias, and the concentration of \npower among a few corporations. It can be exploited for mass surveillance, disinformation, and \nmanipulation of public opinion. Autonomous weapons and AI-driven cyberattacks add security \nthreats. System failures and unpredictable AI behavior create further vulnerabilities. This duality \nunderscores the need for careful policy considerations (Barroso, 2024; Carvao, 2024a).\nEfforts to establish technology policy balance a diverse set of attributes: elements of market power \nand concentration (typically associated with antitrust), safety and responsible use of technology, \naspects of industrial policy, national security, intellectual property, environmental issues, and \nconcerns with the public interest. This list is representative, albeit non-exhaustive. The \noverarching policy goals and normative values in each society/jurisdiction will determine the \nrelative importance of each element. \nNo specific configuration is better than another; what is \nimportant is to achieve the policy goals and ensure that the \nmechanisms used to that extent are enforceable. In addition, a \ngood policy should enable and protect innovation and benefit \nbusinesses, the economy, and society. This balance is not \nstraightforward; different approaches can be taken for each \npolicy attribute. A congressional staffer highlighted: “Some \nregulations are good; most regulations are probably good. \nPeople have a lot of opinions about government bureaucracy \nand inefficiency, which are valid and […] that’s a hard thing to \nsell to the public or anyone in a decision-making capacity.” On \nthe industry side, a mid-sized company’s chief executive \nprovided a candid and direct feedback: “unless the federal \ngovernment figures out what they’re doing sooner rather than \nlater, it’s going to be a mess […] if you start having all these \nhodgepodge of state regulations, it's just another mess to \nmanage making sure you fit all these regulations.”\nPolicy goals will \ndetermine the relative \nimportance of each \nstrategic attribute. No \nspecific configuration is \nbetter than another; \nwhat is important is to \nachieve the stated \ngoals via enforceable \nmechanisms.\nElected governments \nshould set the goals.\n23\nFigure 5 Policy Attributes\nUntil 2024, no federal laws in the United States govern artificial intelligence, except for the non\u0002regulatory National Artificial Intelligence Initiative Act of 2020, included in the 2021 National \nDefense Authorization Act during the first Trump administration (Text - H.R.6216 - 116th \nCongress (2019-2020), 2020; Text - H.R.6395 - 116th Congress (2019-2020), 2021). The law \nestablished a federal initiative to accelerate AI research and development, created the National \nArtificial Intelligence Initiative Office under the White House Office of Science and Technology \nPolicy (OSTP), directed the National Science Foundation (NSF) to support AI research and \nworkforce training, and provided AI specific funding authorization through 2025 for the NSF and \nthe Department of Energy AI research. These provisions aimed to boost U.S. leadership in AI \ndevelopment and integrate AI capabilities across various government sectors, particularly \ndefense and national security. \nIt is natural to ask why we should have an AI Policy for the United States. First, safeguards that \ndo not stifle innovation will help sustain tech-driven progress. An effective policy can drive \ntechnical innovation to address safety concerns. As discussed in this text, there are open issues \nwith AI models that require technical innovation for effective resolution. However, based on our \ninterviews, safety and public interest will likely take a back seat to profit and national security \ninterests that guide large corporations and the government. Fundamentally, leadership in artificial \nintelligence will protect our economy’s competitiveness.\n24\nThe United States drives global innovation in the field, with the leading AI companies and research \nlaboratories headquartered in the country and with local legal jurisdiction. Well-written American \npolicy can have a knock-on effect on the rest of the world while embracing American priorities. \nUntil now, U.S. corporations have wielded considerable influence through lobbying efforts and \nregulatory capture. Coupled with a historical laissez-faire approach, this resulted in very light \ndigital policy, including the lack of federal-level data privacy laws or social media regulation.\nVoluntary and non-binding commitments, absent clear liability structures, are not enforceable and \nsecondary to shareholder interests. Additionally, the June 2024 Supreme Court of the United \nStates (SCOTUS) decision striking down the “Chevron doctrine” reduced the power of federal \nagencies that typically execute policy. A polarized Federal Congress makes the enacting of laws \na protracted process. The growing number of state-level and sectoral regulatory actions leads to \ncomplexity and opportunities for arbitrage or forum/jurisdiction shopping. There is a lack of \nconsensus on whether a central regulatory body is needed for implementation and enforcement, \nand this fragmented approach creates business uncertainty. As a congressional staffer told us, \n“Some companies are kind of fed up. Some companies are like, just tell me what to do. And others \nare thriving in the chaos of a lack of data privacy laws or AI laws.”\nThe case for congressional action\nOur interviews revealed a growing consensus between industry leaders and Congress that the \ntime is ripe for clarity on American AI policy. Persistent partisan and ideological divisions and\nvarying interests across the broader industry landscape make bridging these perspectives a \nchallenge that demands political acumen and a nuanced understanding of the technology and the \nlegislative and judicial frameworks. Policymakers will choose between sector-specific or general\u0002purpose regulations while identifying what is truly novel and unique about AI policy.\nAn example may help. Picture yourself in a not-too-distant future when you entrust an AI agent \nwith managing your finances. It was brilliant at first—analyzing markets, adapting to your goals, \nand making investments faster and smarter than you ever could. For months, your portfolio \nsoared, and you barely thought twice about its decisions. Then, one morning, you wake to a \nnightmare: the AI has transferred your savings to a rogue state. The funds were gone -\nirretrievable. Desperate, you sought justice, but chaos followed. Was the app developer liable, \nthe company behind the AI model, or the bank that recommended it? Lawsuits flew, fingers were \npointed, and your lawyer searched for nonexistent precedents. Meanwhile, you lost everything.\nRegulation could have mandated safeguards, liability frameworks, and oversight mechanisms to \nprevent unauthorized transfers. However, U.S. federal agencies currently face limits in regulating \nAI, hindered by fragmented authority, rapid technological evolution, and lack of comprehensive \nlegislative clarity. This leaves gaps in accountability for complex AI-driven incidents.\nIt is important to have some brief background on how U.S. Federal Agencies operate to \nunderstand the scenario. In the U.S., Federal agencies operate under a congressionally issued \n“statutory mandate.” When unambiguous, it delineates an agency’s scope of authority. Ambiguity, \n25\nhowever, is a common feature of the legislative negotiation process. Since Chevron U.S.A., Inc. \nv. Natural Resources Defense Council, Inc., 467 U.S. 837 (1984), in case of statute ambiguity \nand if unable to determine congressional intent - through review of drafts, committee reports, and \ncongressional floor debates - courts have been directed to look at how the implementing agency \ninterpreted the statute. Whenever reasonable, courts were to defer to the agency’s interpretation. \nThis legal precedent was known as the “Chevron Deference” or “Chevron Doctrine.” In Loper \nBright Enterprises v. Raimondo, 603 U.S. 369 (2024), the SCOTUS reversed the 1984 decision \nand determined that courts, not agencies, will decide all questions of law arising on review of \nagency action (courts are still free to defer to agency’s interpretation but it is up to them to make \nthe call). However, in West Virginia v. Environmental Protection Agency, 597 U.S. 697 (2022), \nthe SCOTUS majority invoked what they termed the “major questions doctrine” to address the \nissue of agencies asserting power beyond what Congress would have granted and requiring \nagencies to point to clear congressional authorization to enact regulation and programs.\nTherefore, agency deference will likely take a back seat to the major questions doctrine. When \nan agency cites an ambiguous statutory provision supporting an action, courts will likely deem \nthis an abuse of regulatory power, decline to defer to the agency’s interpretation, and declare the \naction or program invalid.\nThe recent decision by the Sixth Circuit Court to overturn the Federal Communications\nCommission’s (FCC) net neutrality regulations exemplifies this shift away from agency discretion. \nFollowing Loper Bright Enterprises v. Raimondo, courts now exercise greater scrutiny over \nagency interpretations of statutes. In this case, the court determined that the FCC lacked explicit\ncongressional authorization to classify Internet Service Providers as common carriers under Title \nII of the Communications Act. Without the Chevron deference, the FCC’s interpretation of its \nstatutory authority was insufficient to sustain the net neutrality rules. This decision underscores \nhow the judiciary, under the revived major questions doctrine, is increasingly inclined to restrict \nregulatory actions that assert expansive agency authority without explicit legislative backing\n(Bachman, 2025).\nAs such, regulation can be fought based on a lack of agency statutory authority, which encourages \nthe challenging of agency decisions in Federal Courts (district, circuit, and eventually, at the \nSCOTUS). Sectoral agencies may lack specific statutory authority to regulate artificial intelligence, \nopening the flank for legal debate. Since the October 2023 Presidential Executive Order, several \nentities in the Federal Government have started to mobilize, but they lack statutory enforcement \nauthority, which only Congress can grant (The White House, 2023b).\nIn the vacuum of federal legislation, states started to enact their laws. As of January 2025,\nmultistate.ai, a platform providing resources on how state and local governments regulate artificial \nintelligence, reported tracking 636 state AI bills in 2024, over one hundred of which have been \nenacted into law (Multistate.ai, n.d.). Similarly, in December 2024, New York University’s Center \non Technology Policy issued a report on “The State of State Technology Policy,” indicating that \n41 states passed 107 laws in 2024. These laws fell into two types: comprehensive AI legislation, \nwhere Colorado enacted the first in the nation after the governors’ vetoes in California and \nConnecticut, and issue-specific regulation in areas like political use of AI, copyright, deepfakes, \n26\nor investments in building state capacity and skills (NYU’s Center for Social Media and Politics, \n2024). The threat of fragmentation of tech policy in the U.S. remains real. \nThe Trump administration repealed the 2023 AI Executive Order, (The White House, 2025c) and\nat the time of writing, it is still unclear what will replace it. On the fourth day of the new \nadministration, a brief placeholder announcement was made calling for the creation of an action \nplan to “sustain and enhance America’s global AI dominance to promote human flourishing, \neconomic competitiveness, and national security” due to the president within 180 days (The White \nHouse, 2025d). States – especially the Democratic Party-led ones – may strengthen and \naccelerate efforts to enact AI reform. This is an unpredictable and possibly litigious environment\nfor businesses to operate in, and engaging Congress seems inevitable. However, legislative\nefforts will face resistance from those against regulation and the build-out of bureaucracy or those \nwho are not convinced that the moment has come for wide-ranging congressional action. As a\ncongressional staffer told us: “The capacity is there. It’s not like you don’t have enough staffers \nor you don’t have the right regulatory authorities to act. I just think there’s a fundamental \ndisagreement about when the U.S. government should step in from a regulatory or statutory \nperspective, and that case hasn’t been effectively made yet. All the conversations at the start of \nthe 118th focused on Schumer’s AI dialogue, which, again, was all kind of built around this idea \nthat regulation or statute was needed, and it was coming. I don’t think then the conversation \nended; everyone was like, I’m not convinced we need to do that.”\nHow the U.S. implements policy\nBesides setting the policy goals, balancing the policy attributes, and navigating the political \nlandscape, one also needs to understand the different mechanisms to implement policy. There \nare multiple ways one can drive policy in the United States, and ultimately, several end up being \nexercised over the lifetime of a technology. The U.S. has historically adopted a laissez-faire or\nhands-off approach to the tech sector. “The United States has a history of promoting innovation \nuntil innovation has proven to be a problem, and then they will put laws in place to limit the impact \nof innovation […] because it was developed without rules and constraints. […] We react to \nproblems, as opposed to trying to anticipate problems” says a Senate staffer.\nThis was epitomized by Mark Zuckerberg’s 2009 Facebook motto of “move fast and break things,” \ncapturing the hacker culture of Silicon Valley and the ethos of an era where disruption of pre\u0002existing industries and business models was the way to go. With the benefit of hindsight, one can \nnow see the resulting progress and the harm this approach generated. Issues with social media \ninclude the increased spread of misinformation, privacy violations, cyberbullying, mental health \nissues like depression and anxiety, and a lack of accountability for harmful content due to the \nprioritization of rapid innovation over user safety and well-being. Did we allow too much power to \ngo unsupervised, and should we have regulated some of the use of the technology? These \nquestions go to the core of the policy mechanisms we can adopt to drive policy and highlight the \ndilemma of ex-ante (which refers to future events based on forecasts or predictions rather than \n27\nconcrete results) or ex-post regulation (after the event). We propose a simplified framework to \ngroup policy mechanisms into three segments: \n• Antitrust and competition policy – an ex-post mechanism enacted to mitigate harm via \nbehavioral or structural remedies. \n• Sectoral regulation – an ex-ante mechanism that establishes expectations and obligations\nof behavior and conduct, requiring companies to comply before harm occurs. We are also \nincluding general statutory actions in this group for simplification purposes. For example, \nlaws that drive policy in the national security space by investing in R&D, funding \ngovernment departments or agencies, establishing public-private partnerships, etc. \n• Litigation– ex-post, where you need to prove the defendant owes a duty to the plaintiff,\nthat this duty was breached, and that the breach caused the plaintiff's injury, harm, or loss \n(damages). Common law establishes liability precedents through judicial decisions that \nanalyze case facts, apply legal principles, and consider policy implications. These \nprecedents evolve to reflect changing societal norms while maintaining legal consistency.\nFigure 6 Mechanisms for policy in the United States - Modified from John Haigh and Jon Sallet - Big Tech and the \nImportance of Competition – Harvard Kennedy School of Government (Fall 2024)\n28\nDuring the last two United States presidential administrations \n(Trump 45 and Biden 46), the Federal Trade Commission \n(FTC) has pursued antitrust cases against Big Tech. There are \nactive cases against Google, Amazon, Apple, and Meta. \nSimilar action has been taken in Europe. Margrethe Vestager, \nEU’s antitrust chief from 2014 to 2024, aggressively pursued \nBig Tech companies and initiated major cases against Google, \nApple, Amazon, and Meta, resulting in billions in fines. The \nEuropean Commission introduced new regulations like the \nDigital Markets Act (DMA), reshaping Europe’s digital \nregulatory landscape and influencing global tech policies. The\nincoming European Commissioner for Competition, Teresa \nRibera, who took office in December 2024, was tasked with \ntackling killer acquisitions, speeding up antitrust enforcement, \nand implementing a new approach to mergers to help the EU’s \ncompetitiveness.\nIn the U.S., there are no federal-level ex-ante laws governing digital platforms or AI. Europe’s ex\u0002ante digital regulations, the Digital Markets Act (DMA) and the Digital Services Act (DSA), aim to \nfoster fair competition and ensure a safer digital environment. The DMA targets “gatekeeper”\nplatforms (large tech companies) to prevent anti-competitive practices, mandating interoperability, \ntransparency, and restrictions on self-preferencing. The DSA addresses online content by holding \nplatforms accountable for illegal content, requiring risk assessments, content moderation, and \ntransparency in algorithms and ads. Together, they establish a robust framework for digital \nfairness and accountability.\nLitigation is increasingly shaping policy in response to Generative AI’s challenges to copyright \nframeworks. U.S. copyright law, which requires human authorship for protection, excludes purely \nAI-generated works, though AI-assisted works may qualify if significant human creativity is \ndemonstrated. The legality of training AI on copyrighted materials hinges on the fair use doctrine, \nwhich courts interpret considering AI’s capacity to mimic and compete with original works. \nGlobally, litigation complements policy development, as seen in the EU’s restrictive text data \nmining regulations and differing international approaches. These legal disputes underscore the \nrole of courts in balancing innovation with creators’ rights, setting precedents that will influence \nlegislative efforts and the broader trajectory of copyright governance in the age of AI (Blaszczyk \net al., 2024). Amongst others, New York Times v OpenAI in the U.S. and Getty Images v Stability \nAI in the UK are two cases to follow (Pope, 2024; Wyn Davies & Dennis, 2024).\nThe law’s slow and incremental response to technological change is not new. In 1949, legal \nhistorian James Willard Hurst examined the early decades of the automobile and its profound \nimpact on American society. Using it as a case study, he explored the dynamic relationship \nbetween technological advancements and legal development, highlighting legal inertia and \nunintended consequences. Hurst argued that most legal adjustments were reactive and \nincremental rather than deliberate - a pattern that persists today in debates over the pacing \nThe three mechanisms\nof antitrust, sectoral \nregulation, and litigation \nare not mutually \nexclusive and typically \nco-exist. Today, the \nU.S. lacks ex-ante AI \nlegislation. The \nproposed new \ngovernance model will \nrepresent an alternative \nto rigid sectoral \nregulation.\n29\nproblem, where law struggles to keep up with innovation, and the interplay between norms, \ncommon law, and formal regulation (Hurst, 2022).\nOne pressing question: Are we sufficiently exploring whether existing constitutional frameworks \ncould be expanded to address the unique challenges of artificial intelligence? This debate \nwarrants deeper examination, including assessing specific legal frameworks that might be \nadapted to AI governance. While many existing laws align with certain risks associated with AI, \ntheir application to this rapidly evolving technology remains unclear. Moreover, AI introduces \nnovel challenges that demand a reinterpretation of current laws and a critical analysis of their \nlimitations in addressing latent ethical, social, and economic concerns.\nEmerging approaches to AI policy\nUNESCO\nIn August 2024, the United Nations Educational, Scientific and Cultural Organization (UNESCO) \nreleased a consultation paper on global efforts to develop legislative frameworks addressing AI’s \nimpact on democracy, human rights, and the rule of law. Emerging regulatory approaches include \nprinciples-based guidance for ethical AI use, standards-based frameworks involving technical \noversight, and agile schemes like regulatory sandboxes for innovation. It also examines risk\u0002based and rights-focused approaches to prioritize human rights and manage AI’s potential harm. \nThe report emphasizes the need for transparency, liability measures, and tailored amendments \nto existing laws to address sector-specific challenges. The document aims to assist legislators in \naddressing key regulatory questions - why, when, and how to regulate AI - through global \nexamples and stakeholder feedback. It emphasizes international cooperation, adaptability, and \nthe inclusion of human rights and ethical principles in shaping AI policy (Gutiérrez, 2024). In a \nrecent article, one of the authors highlighted the U.S., Europe, and China as the main approaches \nstarting to dominate the regulatory scenario (Carvao, 2024a). Let’s run these three jurisdictions \nthrough the UNESCO model. \nEuropean Union (EU)\nEurope has taken several approaches to regulating AI, the most notable being the European \nUnion Artificial Intelligence Act, the world’s first comprehensive law on AI. Yet, even before the \nadoption of this landmark law, the EU has led regulatory innovation through other important digital \nregulations, including the General Data Protection Regulation (GDPR), the DSA, and the DMA, \nto name a few. These laws govern various aspects of online data, platform operations, and user \nprivacy within the EU and often have an extraterritorial effect referred to as the “Brussels Effect” \nwith other jurisdictions adopting similar laws – when it comes to privacy, for example – or \ncompanies choosing to make changes to global practices because of EU laws.\n30\nThe EU’s AI regulation emerged well before ChatGPT’s launch, originating with a 2021 white \npaper that explored regulatory options ranging from a hands-off approach to comprehensive \noversight. After extensive impact assessments and stakeholder consultations, the European \nCommission proposed the EU AI Act, a risk-based horizontal framework for regulating AI. The EU \nAI Act entered into force on August 1, 2024, with its first prohibitions coming into force as of \nFebruary 2025.\nThe law has two objectives: to protect individuals’ fundamental rights and to enable the free \nmovement of AI systems and data within the European Union. It establishes obligations based on \ndifferent risk levels: unacceptable, high, limited, and minimal. The act represents an ex-ante \naccountability framework for AI since it requires proof of compliance with requirements before \nsystems can be taken to market, even though most of these compliance checks are done \ninternally by each company. The EU AI Act promotes a standards-based approach,\nacknowledging the role of standard-setting organizations guiding the implementation of \nmandatory rules. It encourages a holistic stakeholder representation in the development of the \nstandards. The AI Act mandates that providers of high-risk AI systems test their products against \nharmonized European standards (hENs) before affixing the European Conformity (CE) mark, \ngranting free circulation within the European market. While the CE mark and hENs are established \ntools for ensuring product safety across other sectors, this is the first time they are used to certify \ncompliance with fundamental rights, one of the two key objectives of the law (Gornet & Maxwell, \n2024). Under the EU AI Act, complying with harmonized standards creates a “presumption of \nconformity,” meaning that companies adhering to these standards are largely compliant with the \nregulatory requirements of the act unless evidence proves otherwise; essentially, it shifts the \nburden of proof to demonstrate non-compliance if a company follows the established standards. \nThe EU’s approach includes agile and experimentalist elements, establishing a framework for AI \nregulatory sandboxes. These sandboxes allow providers to test AI systems under flexible \nregulatory conditions with oversight from competent authorities (Braun et al., 2024). It also takes \nan innovative approach with general-purpose AI (GPAI) systems, establishing “Codes of \nPractice,” an interim measure between GPAI model obligations and adopting standards. The \nCodes are being developed through an ongoing multi-stakeholder process involving hundreds of \ncompanies, non-governmental organizations (NGOs), academics, and independent experts. As \nmentioned before, beyond the law, Europe acknowledges that some AI-related activities are \nalready regulated through existing laws like the GDPR. This approach illustrates an adaptation of \nexisting laws to address AI-related challenges.\nAn industry insider reflects on the political bias against the EU AI Act in Washington, D.C.: “The\nEU AI act is commonly held up in DC circles as a complete failure of regulation, and you can see \nthat in the impact it’s had on the AI sector in European countries. […] That’s often used as a \nwarning of ‘don’t overregulate.’ I wouldn’t. […] [T]he U.S. has such an outsized impact on the \nfuture of AI […] and there’s been a big movement away from multilateralism in U.S. policymaking \nin recent years, so that I don’t think international frameworks are generally very impactful […] \n[there] has been sort of a movement away from the WTO, […] the more traditional rules-based \n31\norder. How [you] rework frameworks from the perspective of the U.S. does have significant impact \non AI in all parts of the world.”\nUnited States\nThe U.S. approach to AI regulation, while lacking a comprehensive federal law like the EU’s AI \nAct, involves a combination of strategies, including adapting existing laws, fostering an \nenvironment for AI innovation, and exploring principles-based regulation. This is demonstrated \nthrough initiatives like the White House Executive Order on AI and state-level legislation focusing \non specific areas like employment decisions and consumer protections. Like Europe’s use of the \nGDPR for specific AI concerns, the U.S. leverages existing legal frameworks to regulate aspects \nof AI. Regulations concerning issues like data protection and privacy (mostly governed at the \nState level), consumer protection, economic competition, and liability rules apply to activities and \nprocesses throughout the AI system’s lifecycle. The U.S. also fosters a conducive environment \nfor AI development and use. Furthermore, the U.S. is considering principles-based regulation. \nThe authors describe the recent history of “ruling by executive orders” over the last three U.S. \nadministrations (Carvao & Ancheva, 2024). Under the Obama Administration, AI policy began \nwith a 2016 report emphasizing AI’s promise for societal benefits and economic growth while \nurging investments in talent and fairness. The report, however, underestimated AI’s rapid \nprogress. The Trump Administration focused on AI’s role in U.S. economic and national security, \nenacting policies to boost AI research funding, establish institutes, and foster global alliances. It \nframed AI as a tool for efficiency and global leadership. The Biden Administration introduced the \nAI Bill of Rights and the National Institute of Standards and Technology (NIST) Risk Management \nFramework to address AI ethics and risks, culminating in Executive Order 14110 - Executive \nOrder on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence - which \nprioritized equity, privacy, and competition but lacked enforcement mechanisms (The White \nHouse, 2023b). A common issue with executive orders is that they may lack enforceability and \nare subject to being reversed during an administration change. Mostly, they do not have the force \nof law. An example of a standards-based approach and technical oversight is the U.S. Artificial \nIntelligence Safety Institute (AISI), which advances AI safety science, practices, and adoption, \ndevelops guidelines, tools, and standards for AI measurement and evaluation, conducts safety \nassessments of AI models, facilitates collaboration between stakeholders, and contributes to \ninternational AI safety efforts. AISI resides under NIST and aims to promote responsible AI \ninnovation while mitigating risks. \nA staffer described the role and limitations of standards-setting bodies like NIST and AISI in the \nU.S. today: “NIST prescribes standards that federal agencies building [or buying] technology must \ncomply with […], but NIST is not a binding standard for private sector actors. So, although many \nprivate sector actors and companies choose to follow NIST standards, there is no requirement to \ndo so in the way there is for federal agencies. […] Where NIST and the AI Safety Institute would \nfall short is in regulating […] that’s not their purpose. […] it’s not going to solve the problem of \nhow the private sector is using AI, and whether they are using AI in a trustworthy and responsible \nmanner.”\n32\nPeople’s Republic of China (PRC)\nChina’s stated goal is to be a global leader in AI by 2030. Its regulatory activity reflects a balancing \nact between the desire for innovation and the need to regulate the development and use of AI \nthat is safe and ethical and “reflects Socialist Core Values” (Sheehan, 2023). While Europe and \nthe U.S. are developing comprehensive frameworks like the EU AI Act and exploring principles\u0002based regulation, respectively, China appears to focus on a strong compliance to standards\u0002based approach, particularly concerning data governance and security. \nThe Cyberspace Administration of China (CAC) released a draft of measures for Generative AI in \nApril 2023. These measures emphasized a strict regulatory compliance framework, particularly \nregarding data used in AI development. Providers must undergo a CAC-approved security \nassessment before offering services using Generative AI products. In practice, this has not been \nstrictly enforced yet. The data used to train AI models must comply with China’s Cybersecurity \nLaw, ensuring it is obtained legally and doesn’t infringe on intellectual property rights (Daum, \n2023).\nChina’s National Information Security Standardization Technical Committee (NISSTC) in May \n2024 released draft regulations outlining cybersecurity measures for Generative AI services. \nThese regulations, open for public comment, address the security of training data, AI models, and \noverall service provision. They define “harmful” data as including content violating socialist values, \npromoting violence or obscenity, or infringing on legal rights. The draft also details security \nrequirements across the AI model lifecycle, from training to monitoring subsequent model\nupdates, and proposes various safety measures for service providers to protect users, particularly \nminors. Compliance will likely increase costs for providers but also foster user trust and \nresponsible AI development (Interesse, 2024). This is an attempt to create a unified technical \nstandard for security and safety measures.\nThe CAC and NISSTC are distinct entities with complementary roles in shaping the country’s \ncybersecurity and information governance landscape. The CAC serves as the primary regulator \nand policymaker for China’s Internet and data governance, including cybersecurity, data privacy, \nand AI. It enforces and oversees compliance with the law and manages incidents in the digital \nand cyberspace domains. The CAC often mandates or references NISSTC-developed standards \nin its regulatory enforcement. The NISSTC focuses on developing technical standards for \ninformation security, including data protection, cybersecurity, and AI governance. It is primarily a \nstandardization body. NISSTC’s standards often serve as the technical foundation for laws, \nregulations, and enforcement in cybersecurity and data governance.\nChina’s strategy aligns with several of UNESCO’s regulatory approaches but primarily focuses \non two. First is the standards-based approach, with a compliance focus, as China has been \ndeveloping specific standards for AI governance, particularly concerning data security and \nalgorithmic recommendations. Second, the approach of adapting existing laws is where China \nleverages its legal framework, such as the Cybersecurity Law, to regulate AI. The measures \n33\nrequire that data used to train AI models must comply with China’s Cybersecurity Law (Abramov, \n2024).\nChina’s AI Safety Governance Framework, released in September of 2024 to implement the \nGlobal AI Governance Initiative, adopts the risk-based and principles-based approach when\noutlining the principles for AI safety governance. It is more prescriptive than purely principles\u0002based approaches. Unlike the EU AI Act, which assigns AI systems to four risk levels with specific \nregulations, the framework identifies risk areas without evaluating their severity. It emphasizes \ntargeted risk mitigation through technological measures like improving data quality and reliability. \nSensitive or critical data in high-risk fields must comply with strict privacy laws. The framework \nhighlights adaptive AI governance through stakeholder collaboration, tiered risk management, \ntraceability, ethical standards, and global alignment. It prioritizes transparency, safety, education, \nand cross-border cooperation to address evolving AI challenges (Tobey et al., 2024). The “Global \nAI Governance Initiative” is a proposal by China that aims to establish a framework for \ninternational cooperation on governing AI development, advocating for a people-centered \napproach, mutual respect, and equality among nations while developing and utilizing AI \ntechnology, with a focus on ensuring AI aligns with human values and benefits all of humanity; it \ncalls for discussions within the UN framework to establish an international institution to oversee \nAI governance (Cong & Yeping, 2023).\nThe CAC measures and the AI Safety Governance Framework are interconnected. The \nmeasures, which have been active since August 2023, serve as an early, specific application of \ngovernance principles that the framework later formalized and expanded, particularly in their \nshared focus on content safety and risk assessment. While the CAC measures provided detailed \nguidelines for Generative AI providers to meet content and cybersecurity requirements, the \nframework broadens these efforts within a tiered, risk-based governance system. This integration \nensures that higher-risk applications like Generative AI receive enhanced oversight. At the same \ntime, the framework guides the ongoing revision and enforcement of the CAC measures, \nmaintaining consistency with China’s broader AI governance goals. China’s regulatory approach \nis characterized by a step-by-step, dynamic strategy. This allows for agility in addressing new and \ncomplex risks while aiming to protect users and the government from harm. The country’s \ncentralized approach to AI development and firm stance on regulatory oversight distinguish it from \nother nations’ strategies (Roberts & Hine, 2023) (Ministry of Foreign Affairs The People’s Republic \nof China, 2024).\nDuring our congressional interviews, a recurring theme of American exceptionalism emerged. \nWhile interviewees expressed some deference to and awareness of initiatives in other \njurisdictions, such as the EU or the UK, their acknowledgment was largely superficial - more a \nrecognition of these efforts’ existence rather than a deep understanding of their intricacies. \nInterviewees frequently depicted China as a counterpoint in geopolitical discourse rather than \nrecognizing its thought leadership. They emphasized that any actions undertaken must be \ncarefully tailored to the unique context of the United States, considering the dynamics of Congress \nand the challenges of legislating within the U.S. framework. One congressional policy advisor \nnoted: “We will often meet with folks when they want to meet with us, UK, France, EU; those \n34\nmeetings aren’t particularly productive […] I am skeptical that any country on Earth has figured \nout a coherent way to regulate this […] also not convinced they have the best ideas in general, \nthey are the most common in the discourse, if you look at what left-leaning policymakers are \nthinking. […] Frankly, sometimes you can lose things by seeming too aligned with folks in the EU.”\nA policy advisor on the Republican side was even more straightforward: “I think the EU took \nthemselves out of the debate by being the first ones to sort of say they knew what they were doing\n[…] No one in the United States thinks the EU is a thought leader on AI, like, quite the opposite \n[…] You know, people have been trying to create this impression that China is a responsible actor \nor an actor we should be engaging with on AI. I think that’s the stupidest thing I’ve ever heard.”\nMost interviewees expressed more moderate views, acknowledging and sometimes appreciating \nother jurisdictions’ actions while maintaining a “wait and see” approach rooted in a fundamental \ncommitment to American exceptionalism.\nA similar sentiment was apparent in the interviews with leaders in the tech industry. Business \nleaders closely monitor the EU AI Act as a template for future regulation, expecting a GDPR-like \ninfluence on the U.S. The “Brussels Effect” started with data privacy. A Tech Industry board \nmember and former line executive told us that “the perception right now is that the European AI \nAct may be overly constraining, that it may both prevent companies in Europe from being super \ninnovative and or prevent companies from the rest of the world from deploying advanced \ntechnologies in Europe to European citizens. […] It was quite challenging for companies to figure \nout how to comply with GDPR, and […] privacy is something I think everybody generally agrees \non […] the AI space is less clear, and the way that the regulation is very broad right now, […] it’s \ngoing to make it very, very difficult for companies and individuals to be able to use and deploy the \ntechnology as it continues to evolve in Europe.”\nA national security example\nIn January 2025, the Bureau of Industry and Security of the U.S. Department of Commerce issued \nan interim final rule (IFR) to enhance export controls on advanced computing integrated circuits. \nIt imposed new controls on AI model weights to protect U.S. national security (The White House, \n2025a). An IFR is a regulatory rule issued by a U.S. government agency that takes immediate \neffect without first going through the typical notice-and-comment period. This IFR goal was to \nprevent the technology from falling into the hands of adversaries, primarily China, and incentivize \nthe purchase of American products from American companies. \nThe introduction of the rule followed a month of statements about a significant increase in AI \ncapabilities and the achievement of a new technical milestone, bringing us closer to the mythical \nArtificial General Intelligence (AGI – when machine intelligence can understand or learn any \nintellectual task that a human being can). The ARC Prize team – a group of scientists and \ndevelopers who created an AI benchmark to measure progress towards AGI – highlighted that \nthe latest OpenAI model represented “a surprising and important step-function increase in AI \ncapabilities, showing novel task adaptation ability never seen before in the GPT-family models” \n35\n(Chollet, 2024). Sam Altman, CEO of OpenAI, mused on his year-end reflections: “We are now \nconfident we know how to build AGI as we have traditionally understood it. We believe that, in \n2025, we may see the first AI agents ‘join the workforce’ and materially change the output of \ncompanies. […] We are beginning to turn our aim beyond that, to superintelligence in the true \nsense of the word.” Dario Amodei (CEO of Anthropic) and Matt Pottinger (former Deputy National \nSecurity Advisor of the United States during the first Trump administration) alerted the importance \nof the U.S. leading the world in artificial intelligence to preserve national security. They highlighted\nthat “AI will likely become the most powerful and strategic technology in history. By 2027, AI \ndeveloped by frontier labs will likely be smarter than Nobel Prize winners across most fields of \nscience and engineering, designing new weapons or curing diseases. […] The nations that are \nfirst to build powerful AI systems will gain a strategic advantage over its development. […] Export \ncontrols, which ban shipments to China of the high-end chips needed to train advanced AI models, \nhave been a valuable tool in slowing China’s AI development.” (Amodei & Pottinger, 2025). These \ndevelopments set the stage for the policy initiative to preserve America's advantage in AI over \nrivals such as China.\nThe announcement triggered mixed reactions. Industry associations complained about the lack \nof stakeholder engagement (ITI, 2025). NVIDIA expressed concerns about market interference \n(Finkle, 2025). Their reaction, however, needs to be taken with perspective since they currently \noperate in a supply-constrained environment, with the demand for their products far exceeding \ntheir production capacity. If supply catches up with demand, they will need new markets, and any \ntrade restrictions are not in their interest. \nOther voices support a more hawkish posture. An industry senior executive shares their concerns: \n“China pumps billions and billions of dollars [… and] subsidizes their AI industry […] My big fear \nis you get a country like China that might not be so principled or centered like we are in the \ndemocratic society, and they could really use this as a weapon. […] You've seen all the reports \nrecently. I mean, there's so much bad […] going on and people penetrating infrastructures, energy \ngrids and all […] So my point is, I think the ethics and all that stuff will come together, my bigger \nfear and I think the biggest risk, is society. You get countries that might be foreign adversaries \nthat don't have the best intention for using this type of technology.”\nWhat happens next is hard to predict. The new administration can decide to reverse or use it as \na negotiating tool akin to the strategy used with tariffs (Carvao, 2025).\nImmediately after the rule’s announcement, the Biden administration issued a companion \nexecutive order focused on AI infrastructure (The White House, 2025b). Successful \nimplementation of the Framework for Artificial Intelligence Diffusion will increase the demand for \nU.S. AI technology, further stressing the country's energy supply. The order recognizes the \nsubstantial energy needs of AI infrastructure and seeks to leverage this opportunity to advance \nAmerican leadership in clean energy technologies. It addresses topics discussed in the “The \nsecond AI triad” section, such as land availability, environmental impact, permitting reform, energy \ntransmission infrastructure, consumer prices, and labor implications. \n36\nRegulation-induced innovation\nSome of the pushback against regulation assumes that it necessarily stifles innovation. What if \nthe right type of regulation could be used to trigger innovation? To answer the question, we have \nlooked at historical precedents for using policy and regulation to drive innovation. Engineering \nand Computer Science inventions are needed to solve \nopen issues in this field, such as reliability and \nhallucinations, model bias, safety and control, high \ncomputational demands and associated environmental \nimpact, data privacy and security, transparency and \nexplainability, and diminishing returns on scaling. \nRegulation can act as one of the tools to align industry \nincentives to invest in this direction. \nThe views shared fifty years ago with the Senate Commerce Committee by researchers from the \nCenter for Policy Alternatives at the Massachusetts Institute of Technology are instructive. The \ncontext for the testimony was the criticism of environmental regulations at that point in their \ninfancy. They argued that safety regulations, while often criticized for imposing costs on \nindustries, can serve as powerful drivers of innovation. These regulations can act as catalysts by \ncompelling firms to rethink and adapt their products, processes, and technologies. By introducing \nnew constraints, such as required safety enhancements and regulations, engineers and firms \nexpand the dimensions they must consider when solving problems, encouraging creativity and \nresourcefulness, and leading to innovative solutions that might not have emerged otherwise. \nMoreover, regulations create new markets for technologies that address safety concerns. This \ndemand stimulates the development of devices, advanced manufacturing processes, and safer \nproducts. The researchers highlighted that such regulation-induced innovation is particularly \nevident in mature or concentrated industries and that, contrary to the view that regulations stifle \ngrowth, they can drive technological progress and economic benefits, proving to be instrumental \nin allowing industries to adapt to societal demands. The parallels to this moment are uncanny \n(Ashford & Heaton, 1976).\nA decade later, by 1987, the debate was still raging, as covered by William J. Broad, a science \njournalist, at The New York Times in the article “Does the Fear of Litigation Dampen the Drive to \nInnovate?” He explored the concern that safety-related litigation and regulatory constraints may \nhinder technological innovation across various fields. Experts at the time argued that the fear of \nliability lawsuits had created a “chilling effect,” deterring venture capital and slowing\nadvancements in areas such as artificial intelligence, food processing, and nuclear engineering. \nCritics pointed out that regulators and the legal system focused excessively on risks, discouraging \nexperimentation and innovation. However, consumer advocates countered that liability laws \nensure product safety and accountability. They emphasized that companies had been compelled \nto improve safety rather than abandon innovation. They argued that the rise in product liability \nWhat if the right type of \nregulation could be used to \ntrigger innovation?\n37\ncases had heightened corporate responsibility. In that context, proposals to address the issue \nranged from enhancing regulatory incentives for safety innovations to shifting decision-making \nfrom courts to regulatory agencies. He pointed out that balancing safety and innovation remained\nchallenging and that safety should be integral to engineering practices (Broad, 1987). The debate \nover private rights of action continues today as a disputed matter in several legislative proposals \n(U.S. Chamber of Commerce Institute for Legal Reform, 2024).\nThe framework for thinking about regulation as restrictive or expansive, proposed by Nicholas \nAshford, (Ashford & Heaton, 1976) is valuable as we analyze a path forward for digital \ntechnologies policy. Restrictive regulations impose constraints, increase costs, and limit choices, \noften delaying activities due to compliance requirements. They are seen as burdensome, focusing \non controlling harmful practices. In contrast, expansive regulations stimulate innovation by \nintroducing new performance dimensions, compelling firms to develop creative solutions and \nadapt technologies. The relationship between regulation and innovation continued to be a subject \nof scholarly discussion following his testimony (ASHFORD et al., 1985). Independently, in the \n1990s, Michael Porter’s work introduced the Porter Hypothesis, suggesting that stringent \nenvironmental regulations could stimulate innovation by encouraging incumbent firms to develop \nnew technologies and processes that offset compliance costs (PORTER M, 1991). Porter’s \nhypothesis overlooks new entrants, focusing on how incumbents respond to stricter regulations,\ncreating innovation offsets. Early adopters gain learning curve advantages, forcing latecomers \ninto costly compliance. Building on the original work from MIT, proponents of a strong form of the \nregulation-induced innovation hypothesis suggested that stringent regulation can drive the \nemergence of new products and processes, potentially displacing dominant technologies and \nincumbent firms lacking the ability or willingness to adapt (Ashford & Hall, 2011).\nPlus ça change, plus c’est la même chose - the more things change, the more they stay the same. \nToday, the Organization for Economic Co-operation and Development (OECD) emphasizes the \nimportance of effective regulatory frameworks in fostering innovation through its “Better \nRegulation and Innovation” initiative. Recognizing that poorly designed regulations can hinder \ntechnological progress and economic growth, the OECD advocates for regulatory policies that \nare flexible and forward-looking. The organization promotes adaptive governance, encouraging \nregulatory approaches that can evolve alongside technologies and market dynamics. They seek \nto position regulation as a catalyst for innovation, ensuring oversight while maintaining the \nflexibility necessary for technological progress (OECD, 2024). The OECD Observatory of Public \nSector Innovation (OPSI) empowers governments to leverage innovative approaches to achieve\npolicy priorities. It identifies trends, fosters shared learning, and enhances innovation capacity, \nincluding tools like regulatory sandboxes to test and implement solutions, such as AI technologies, \nin controlled environments (OECD, 2023).\nU.S. export control policies on advanced technologies, such as GPUs, imposed on China illustrate \nhow restrictive regulations can drive engineering innovation. These limitations have compelled \nChinese AI developers to optimize resource utilization, create more efficient models, and explore \nalternative hardware and software solutions. This dynamic reflects a broader pattern where \nrestrictive regulations can foster creativity and adaptation despite constraining resource access. \n38\nDeepSeek is a Chinese AI firm founded by the hedge fund High-Flyer. They are known for open\u0002source large language models, including the latest DeepSeek-V3, an ultra-large model designed \nto rival leading global models such as Meta’s LLaMA. Despite resource constraints due to U.S. \nexport controls, their latest release has demonstrated exceptional performance by employing \ninnovative engineering strategies. These include advanced model compression, efficient data \nutilization, and hardware-software co-design, enabling optimal performance on limited hardware. \nThis case underscores how resource constraints can catalyze technological ingenuity, driving \ninnovation and enhancing self-reliance in AI development (Sharma, 2024). There is skepticism \nabout some of DeepSeek’s training cost claims, but even the skeptics recognize their \nresourcefulness. A member of a frontier AI lab in the U.S. told us: “There are some questions, \n[…] could they have done it without the existence of other foundational models? A little bit unclear, \nbut definitely pretty disruptive.” It is still too early to tell the impact of export controls. These types \nof restrictions have a lagging effect, meaning that today’s restrictions to access the latest \ntechnology will impact future products being developed now. When you couple this with the \nincreased need for processing power for inferencing, resulting from DeepSeek’s recent popularity, \nthe chip export controls can impact their ability to maintain the current improvement trend. \nAlternatively, this can further motivate indigenous GPU designs. \nContemporary studies continue to examine the nuanced impacts of regulation on innovation.\nRegulators are trying to balance innovation and consumer safety by using tools like sandboxes \nto foster new technologies while ensuring protection. Soft law instruments enable rapid adaptation \nto change. Risk-based, customer-focused approaches improve regulator-business relationships \nwhile digital technologies streamline processes, maintaining consumer protection without \nimposing unnecessary burdens on businesses (Eggers et al., 2023).\n39\nPart II – Industry and Congress Perspectives\nPart I of this manuscript explored the historical precedents in technology governance and the \nunique characteristics of artificial intelligence. It delved into the AI triad - algorithms, data, and \ncomputing power – and introduced a new AI triad - energy, land, and labor. The initial part also \nlaid out a framework for looking into AI policy goals through the lens of its attributes or objectives. \nWe investigated examples worldwide and dove into the specifics of how the United States \nimplements policy via anti-trust, sectoral regulation, and litigation via the courts. These concepts \nwill be important when reviewing the Dynamic Governance Model to be introduced in Part III. \nPart II will get into the details of our primary research, what we learned during qualitative \ninterviews, and a quantitative analysis of legislation introduced during the 118th United States \nCongress Session. These findings will establish the foundation for our governance proposal.\nIndustry Perspectives\nThe AI Ecosystem\nThe term “industry” is often invoked in academic and policy discussions, with researchers and \npolicymakers striving to understand its perspective. This work aimed to capture these viewpoints, \nleading to a key conclusion: no singular, unified tech industry\nexists. Instead, the ecosystem comprises a diverse array of \ninterests that are at times aligned and at other times in conflict. \nNavigating this complexity requires a discerning ear and the \nability to distinguish meaningful insights from background noise.\nUncertainty, coupled with the rapid evolution of AI technology, \nobscures the ecosystem’s boundaries, raising questions: Who \ncomprises the ecosystem, and who has the authority to influence \nand shape the trajectory of AI policy? We asked industry leaders: \nWho deserves a seat at the decision-making table?\nSome readily assert that all voices should be heard in the process. For instance, a senior \nexecutive leading emerging technology at a Big Tech company shared their perspective: “Look, \nAI has got to touch everybody and everything. You know [...] everybody needs to understand \nthese capabilities. It will touch everything we do in our lives [...], and this is why […] you do need \nto have forums where you’re bringing together all different types of people from different \nbackgrounds, even if they don’t think they have a stake [...] Government, core industry, model \ndevelopers, system AI providers, users, academia, and civil society […]. You’ve got to bring all \nthose voices to the table because this will impact us currently and into the future.”\nNo singular, unified \ntech industry exists. \nInstead, the ecosystem \ncomprises a diverse \narray of interests that \nare at times aligned and \nat other times in \nconflict.\n40\nWhile all interviewees broadly agreed on the policy leadership role played by the large companies\ndriving AI development, there was far less consensus about including other stakeholders. Some \nvoices criticized Big Tech’s outsized influence and lobbying efforts. However, much of the industry \nregards these companies as possessing the resources, expertise, and long-term vision needed\nfor shaping AI policy. The rest of the ecosystem either aligns with their leadership willingly or feels \ncompelled to follow their lead.\nIn addition to these dominant companies, several interviewees emphasized the importance of \nincluding subject matter experts in policymaking. As the CEO of an AI company explained: “The \npeople who are involved in creating the frontier models must be part of the conversation [...] there \nare a few specific subject matter experts [...] we [ …] are one of them. I think there are a few \nnonprofit organizations that are some of them. […] Regulation and other policies are a lot of times \nlike the devil is in the details [...]. And so sometimes it means talking to a tiny nonprofit that kind \nof feels like a weird fit for this sort of conversation, but they have done the research on this specific \ntype of risk [...] You must have them at the table.” \nThe role of startups, particularly those in early to mid-stage growth, was another topic of significant \ndiscussion. The prevailing argument was that startups often lack the maturity and resources to \ncontribute meaningfully to strategic policy debates—an assertion consistently echoed by the \nstartup CEOs interviewed. While this perspective acknowledges smaller companies’ resource \nconstraints, it raises critical concerns about power concentration in the U.S. AI market. When \nlarge companies dominate the policymaking process, they gain the power to shape a future \nmarket structure aligned with their interests, potentially stifling the emergence of innovative \nsolutions from smaller, more agile players.\nAI integrators—companies that apply AI solutions across various sectors—were highlighted as a \ncritical yet underrepresented voice in shaping AI policy. These integrators play a vital role in the \nbroader AI landscape but often lack visibility and influence in regulatory and industry discussions. \nOne of our interviewees clearly outlined the various aspects of the AI value chain and the distinct \nroles played by different stakeholders within it:\n“So those people need to be at the table talking about models and […] the role for the model \ndevelopers, versus those who are then taking those models and building on, building off them. \n[…] One is, as I said, you’re developing, furthering the development of the model […] where you’re \nteaching the model new skills or adding new data or new context, or new industry-specific \ncustomizations to the model and RAG [Retrieval-Augmented Generation] can do that. […] The \nsecond thing is, of course, AI systems developers. […] An AI system is kind of end-to-end; it’s \nsomething that is targeted to do something that includes and embeds AI or leverages a model. \n[…] They’re building a system around leveraging AI capabilities, leveraging what they know […] \nto create a set of tools that somebody else is going to deploy. […] And then […] the deployers. \nYeah, what are you going to use these models for? Use Case matters 100%. Are you comfortable \nwith that use case? You know, from an ethics perspective, do you have the controls in place? Do \nyou have to the human in the middle? How are you making sure that the uses are appropriate, \nthat you don’t have those hallucinations, and that you’re putting good judgment around them? \n41\nThe deployer has responsibilities. They also are probably going to bring some of their data to the \ntable [….] with the prompting that they’re doing and other things. So, everybody has a role to play \n[…], and if you’re going to put all the responsibility on this player or that player, that’s just not […] \nappropriate. You’re not going to get what you want, which is the oversight, the decision making, \nthe responsibility, […] you need to make sure that the right people in the value chain have the \nright level of responsibility, obligations, and liability.”\nA former tech chief marketing officer and now executive director of a group of enterprises that use \nAI in their business brings up the point of view of those deploying the technology. “We’ve debated \nthis as a lion. I’ll share with you where the companies came out […] they don’t want to regulate \nthe technologies, because you’re going to stifle innovation […] Therefore, a risk-based approach, \nwhich seems to be the EU’s approach and Biden’s approach has been supported. Second, \ndifferentiate the responsibilities of developers and deployers because we’re not all the same, \nright? Third, don’t create any new regulatory bodies; use the existing regulators to interpret AI \nthrough the lens of aviation, financial services, or healthcare. […] Please harmonize […] you’ve \ngot California, you’ve got other states, you’ve got the EU, you’ve got the United States, you’ve \ngot Japan, you got China. So, businesses do not like inconsistency. They want consistency. \nThey’re not opposing regulation; they’re almost inviting it because it will help provide their \nsafeguards. It will be the rules of the road. […] We’re going to go through a period now of massive \nfragmentation, and that’s not good for business.”\nOur deep-dive interviews with industry members revealed that “industry” is an oversimplification, \nand a few different personas emerged6. We have grouped them under six distinct segments: \n1. Accelerationists - Emphasize rapid AI development, pushing boundaries of innovation \nwithout heavy regulation and believing in fast-tracking advancements. Typical \nstakeholders include a few tech giants, some startup founders, and most VCs focused on \nhigh returns from AI. Their priorities are speed to market, competitive edge, and high\u0002growth potential, but they face challenges such as risks of ethical, safety, and societal \nimpacts due to lack of oversight.\n2. Responsible AI Advocates - Develop AI ethically, ensuring fair and unbiased systems \nwhile prioritizing accountability and transparency. Stakeholders are academics, \npolicymakers, nonprofits, and some corporate entities with dedicated AI ethics teams. \nTheir priorities include inclusivity, bias mitigation, and creating frameworks to govern AI’s \nethical use, though challenges arise in balancing innovation with extensive regulatory \nprocesses and maintaining consistency in ethics standards.\n3. Open AI Innovators - Commit to open-source models and datasets, promoting \ntransparency and broad access to AI technologies. Typical stakeholders are research \ninstitutions, open-source communities, tech enthusiasts, and organizations committed to \n6 We recognize that the term industry is used loosely throughout the text, with varying meanings. In the context of our qualitative interviews, it primarily \nrefers to members of the tech sector and adjacent fields. When categorizing different voices and later discussing stakeholders in the proposed \ngovernance model, industry encompasses investors, auditors, consultants, academia, and civil society - contrasting with government as the other key \nplayer in a public-private partnership.\n42\ndemocratizing AI. Priorities include collaboration, transparency, and broad accessibility to \ndrive collective progress, while challenges involve intellectual property concerns and the \nmisuse of open-source AI models.\n4. Safety Advocates - Prioritize the safe deployment of AI, emphasizing risk mitigation and \nlong-term societal implications. Stakeholders include AI researchers in safety-focused \nfields, a few policy advisors, and some regulatory bodies. Their priorities are addressing \nalignment, reducing existential risks, and promoting safe usage and deployment. Still, they \nface challenges in defining and enforcing safety standards and addressing fears of AI’s \nunpredictable consequences.\n5. Public Interest AI - Ensure AI development aligns with public welfare and social justice \nand addresses issues like accessibility, equality, and inclusion. Stakeholders include \nnonprofits, some government entities, public welfare advocates, and citizen groups. Their \npriorities are building AI that supports the public good and promoting socially beneficial AI \nuse cases. Public interest advocates face challenges such as funding limitations and \novercoming commercial interests that may not prioritize public welfare.\n6. National Security Hawks - Prioritize AI for national security, economic stability, and global \ncompetitiveness, viewing AI as a strategic asset. Typical stakeholders include government \ndefense departments, national security agencies, and geopolitical analysts. Their priorities \nare protecting critical infrastructures, deploying AI in defense systems, and maintaining \ntechnological advantage in global conflicts; their challenges include the escalation of an \nAI arms race, concerns over civil liberties, and balancing national security with responsible \nAI use.\nOur interviews with Congress reflected a similar\nunderstanding, with multiple interviewees referring to \n“industry from different parts of the ecosystem” and the \nneed to “divvy [the industry] up into different buckets.” \nStaffers have cited the “ideological approach and view \nabout open source versus closed source” or the “vicious \ntactics and resources of certain venture capital firms” to \nname just some of the distinguishing features that \nemerged.\nCompanies can fit more than one segment, and we propose this segmentation as a starting point \nfor understanding where a given company sits or to map out behaviors by specific individuals \nwithin a company. One can consider these groups as areas of concentration or major/minor \nbehaviors. Bear in mind that inside companies, one can find different camps as this discussion \nwith a frontier AI research lab shows: “[T]here are definitely actors within companies that are very \npro-societal good. There are also actors who aren’t […] One thing that […] exists in a lot of these \ncompanies is a real ideological split between people that were there pre-2022 […] pre GPT \nblowing up [who] tend to be a bit more ideological and really believe that we should be using the \nAccelerationists\nResponsible AI advocates\nOpen AI innovators\nSafety Advocates\nPublic Interest AI\nNational Security Hawks.\n43\ntechnology for good. And the people who came after, […] got incredibly rich doing it, […] and tend \nto gain enormously if they have unchecked freedom to develop and to innovate. And I think it’s \nimportant to hear from voices that don’t have a financial stake in a certain outcome.”\nIndustry voices\nThe interviews revealed a multifaceted perspective on AI, highlighting optimism tempered by \ncaution, divergent views on regulation, and the unique challenges posed by this transformative \ntechnology. Participants expressed broad optimism about AI’s potential, particularly as a co-pilot,\nto augment human work rather than replace it. However, significant uncertainty remains, driven \nby global compliance challenges and fragmented state-level regulations in the United States. A \nrecurring concern is the difficulty of measuring AI’s return on investment (ROI). CEOs and \ninvestors struggle to assess its tangible value, raising doubts about its overall business impact. \nThis challenge extends to regulatory efforts, as participants highlighted the absence of tools to \neffectively evaluate AI’s outcomes, which hampers the development of meaningful policies. We \nhave seen this in earlier phases of adopting other disruptive technologies like the transition to the \ncloud and, before that, with the Internet and e-commerce. The increased level of scrutiny on ROI \ntypically means the start of the transition from a “lab experiment” to production deployment. \nThe discussions surfaced stark differences in perspectives on regulation. Startups prioritize \ninnovation, assuming regulatory inspection will primarily target big tech companies. Meanwhile, \nlarge technology firms not only lead the shaping of AI policy but also take much responsibility for \nit, a role broadly accepted across the industry.\nParticipants generally supported a use-case-based regulatory approach rather than overarching \noversight of algorithms, aside from notable outliers like Anthropic. (Anthropic, 2024) Some \nadvocated for regulation during development, but most opposed creating new regulatory \nagencies. Instead, they favored sector-specific frameworks or leveraging existing bodies to \nminimize complexity. The EU AI Act emerged as a closely monitored model, with expectations \nthat it could influence U.S. regulatory approaches akin to the GDPR. Paradoxically, as we will see \nwhen reviewing our congressional interviews, U.S. legislators resist modeling American policy on \ninternational experiences. At the same time, the industry has made a strong call for global \nharmonization of standards to streamline compliance and foster innovation.\nIn the experience of a former government official and vice president of regulatory affairs in the \nindustry: “it was probably two years ago now where several senators were saying, you know, we \nhave to regulate the algorithms […] Our point of view was no. We need to regulate the use case, \nnot the algorithms. And we got some pushback from senators because of the fear and the hype. \nI think that’s calmed down in the last couple of years, but you know, that was one area. The other \nis the very real and significant debate between those more on the side of open-source AI, putting \nalgorithms out for open source and allowing them to be improved and developed by the global \ncommunity, and what I would call more proprietary AI models like OpenAI, the Microsoft approach, \nto some extent, what Google is trying to do, what Anthropic is doing. You know, on the other side \n44\nof the debate, you have Meta, IBM, Hugging Face, and other companies, which are built on open\u0002source models.”\nAI is perceived as distinct from other technologies due to its dual potential for misuse and harm \nand its broad accessibility – although, as discussed earlier, dual use is not new. A vice president \nof technology advocacy in a large corporation explained the uniqueness of AI in this way: “I think \nof the launch of chat GPT and the fact that this came into everybody’s living room, […] you do a \nGoogle search, the first thing that pops up now is an AI-generated answer. Those with young \nchildren are thinking about their children and what they’re being exposed to […] whether you think \nyou’re dealing with AI or not, you’re dealing with it every moment of every day, in some way, and \nthat makes it different. It’s different from Quantum. Quantum is that big center, you know, the \ncool-looking chandeliers. There’s a distance there, that’s in the back office, that’s in some data \ncenter. This is touching everybody and every job.”\nFigure 7 A cool looking chandelier - IBM Q System One quantum computer - Adobe Stock\nParticipants emphasized the societal risks AI poses, underscoring the need for its ethical \nimplementation and thoughtful regulation. There was general agreement that regulation is\nnecessary and urgent in this space. The ethical use of AI remains a central concern. Large \ntechnology companies advocate for voluntary commitments and see themselves as key players \nin shaping ethical standards while acknowledging the importance of serious industry \naccountability. In contrast, smaller companies tend to deprioritize ethical considerations, focusing \ninstead on growth and scaling. Nonetheless, they often adopt the strategies and standards \nestablished by larger tech firms, demonstrating the latter’s influence over the ecosystem. A COO \nof a small startup and former FinTech CEO discussed accountability: “Existing [sectoral] \nframeworks are built around holding people accountable to adhere to the regulatory frameworks \n45\nand the guidelines […] accountable and liable […] There needs to be a change in the framework \nsuch that the regulators can drive accountability, but it doesn’t have to be in this very narrow […]. \nIt needs to accommodate the fact that AI is there doing many of the things that they assigned \nhumans to. […] We must make it diffused enough that there is somebody accountable, or a \ncorporate entity that’s accountable, that will lose licenses, get fined, etc. if something goes wrong. \nBut it needs to move beyond this notion of individuals.”\nThe discussion with a member of the safety team at a leading frontier AI research lab highlights \nhow they are grappling with issues of regulation and liability. “[We should regulate] usage […] this \nis already nascent and […] you’re going to see a continuation of it: the idea of model developers \nwanting to be a little bit like a utility. […] We just provide it, and it is not our responsibility what you \nuse it for. And […] some sort of liability, or some sort of clear delineation of responsibility […] if \nour model is used to do something bad, how much responsibility do we bear before you should \nbear some? […] Everything sort of flows back from that. You know, if you understand that you \nare ultimately responsible for, it affects what you're going to train, it affects what you're going to \ntest, and it affects how you’re going to sell that model out into the world.”\nLarge technology firms dominate AI innovation, market standards, and policy discussions. Feeling \nmarginalized in these conversations, smaller players largely align with big tech’s direction without \nattempting to carve out their influence. This dynamic perpetuates widespread distrust in \ngovernment regulators, with participants citing a lack of expertise and industry knowledge among \nregulatory bodies.\nOur findings reveal a lack of trust that discourages industry engagement with policymakers, \ncompounding the industry’s limited grasp of the legislative process, key stakeholders, and \nregulatory developments. This unfamiliarity and uncertainty about what a workable regulatory \nframework entails creates a substantial obstacle. Many in the industry feel ill-prepared to navigate \nor influence these processes, resulting in apathy or withdrawal. As a result, a few well-resourced \nplayers dominate meaningful dialogue and collaboration, leaving smaller entities and emerging \nvoices sidelined.\nThe state of federal AI policy in the U.S.\nThe Executive Order legacy\nWhen discussing the emerging approaches for AI policy, we recapitulated the recent history of AI \npolicymaking in the U.S. via executive orders and its limitations. Let us review the scope of \nExecutive Order 14110 from October 2023 and what has transpired since then. This was arguably \nthe most prominent executive order on AI policy issued during the Biden Administration. The E.O. \n(The White House, 2023b) outlined a comprehensive plan for the safe and responsible \ndevelopment and use of AI and set a framework for AI governance, including principles, safety \n46\nmeasures, innovation strategies, workforce considerations, equity, privacy, federal AI use, and \nglobal leadership.\nIt opens with Sections 1–3, which set the stage by outlining its purpose, principles, and key \nterminology. One of the most impactful sections is Section 4, which tackles AI safety and security. \nThis section instructs federal agencies to develop rigorous AI safety standards, implement red\u0002teaming protocols, and enforce secure development practices. It also requires companies to \ndisclose dual-use AI capabilities and usage of large-scale computing resources. Furthermore, it \naddresses AI risks in critical infrastructure, integrating the AI Risk Management Framework and \nmitigating the misuse of AI in chemical, biological, radiological, and nuclear contexts. It promotes \ndigital content authentication and safeguards federal data from malicious use while addressing \nsecurity concerns tied to dual-use foundation models and national security AI governance.\nSection 5 shifts the focus to fostering innovation and competition. It highlights the importance of \nattracting top AI talent and establishing robust public-private R&D partnerships. This section lays \nthe groundwork for a thriving, competitive AI ecosystem by ensuring fair competition in AI markets \nand promoting the growth of the semiconductor industry through the CHIPS Act.\nSubsequent sections address a wide range of societal concerns. Section 6 centers on labor, \nemphasizing the need to understand AI’s impact on the workforce, create employer guidelines, \nand promote AI education to cultivate a diverse and capable talent pool. Section 7 prioritizes \nequity by combating AI-driven discrimination in criminal justice, government programs, and the \neconomy. These two sections attracted debate across political lines, especially regarding the \nemphasis on diversity and inclusion.\nSection 8 protects individuals engaging with AI systems, whether as consumers, patients, or \nstudents, by ensuring safe and ethical AI applications in the healthcare, transportation, education, \nand communications sectors. Privacy takes center stage in Section 9, which underscores the \nmanagement of sensitive data, calls for updated privacy guidelines, and advances privacy\u0002enhancing technologies. Finally, Sections 10 and 11 broaden the scope to responsible AI use in \ngovernment and global leadership. Section 10 ensures that government agencies adopt secure \npractices, invest in talent development, and prioritize ethical AI deployment. Section 11 seeks to \nstrengthen the United States’ position as a global AI leader by fostering international collaboration \nto promote responsible AI development and tackle shared global challenges, aiming for innovation \nthat serves humanity.\nAt the time of its publication, while there were some differences in reactions between right and \nleft-leaning groups, polling showed broad support across party lines, with 78% of Democrats, 65% \nof independents, and 64% of Republicans favoring the order (AI Policy Institute, 2023). Over time, \nthe E.O. faced increased criticism. Critics argued that it represented an overreach of executive \npower, particularly using the Defense Production Act. There were concerns that the order may \nhinder innovation and put the U.S. at a competitive disadvantage against countries like China \n(Krishan, 2023).\n47\nOn the other hand, some find the order lacks specificity and enforcement measures, potentially \nlimiting its effectiveness (ACLU, 2023). Ideological objections have been raised, especially \nregarding its emphasis on addressing algorithmic bias. Despite these criticisms, the order was \nseen, particularly by the left, as a necessary step towards responsible AI governance (Newman \net al., 2023).\nOne day after the signature of the executive order, on November 1st, 2023, as part of Vice \nPresident Harris’s visit to the United Kingdom to attend the Global Summit on AI Safety, the United \nStates AI Safety Institute (AISI) was founded as a non-regulatory entity, part of the NIST (The \nWhite House, 2023c). The AISI develops methods to assess and mitigate risks of advanced AI \nsystems, creates benchmarks, evaluation tools, and safety guidelines for AI models and \napplications, and collaborates across government, industry, and academia to build a shared \nunderstanding of AI capabilities and harms. It has proactive engagements with industry leaders, \nsuch as OpenAI and Anthropic, to conduct testing and identify risks in frontier AI models. The \ninstitute is trying to build strong relationships and foster open communication between the \ngovernment and industry to encourage a more nuanced approach to AI governance. \nThe relationship between the frontier AI research labs with the AI Safety Institute was described \nby a member of one of these companies as: “[W]e band with […] national laboratories […] and \nwe run tests in congruence with the AISI and with Los Alamos. So, there's a practical testing \nrelationship, which is really helpful, particularly if you're doing some of the catastrophic risk stuff, \nwhere legally, we probably couldn't test it on our own.”\nThe AISI is also part of a growing international network of AI safety institutes already present in \nthe United Kingdom, Japan, France, Germany, Italy, Singapore, South Korea, Australia, Canada, \nand the European Union. \nHowever, the institute is widely perceived as underfunded for its mission. One congressional \nstaffer pointed out: “I think they could always use more capacity. […] We have underfunded […]\ndepartments in the federal government that deal with technology, and now it’s AI specifically. But \nNIST generally needs a lot more resources, and the AI Safety Institute would appreciate those as \nwell […] their output has been fine; they have tried to bring in a lot of outside stakeholders […] it \nwould be better if it was fully authorized through congressional action.” Another staffer was less \npositive in their remarks: “Not convinced the work product they’re creating meets the moment.”\nMost landed somewhere in the middle, noting that the AI Safety Institute is doing good work given \nthe mandate it was given but hesitant about further expansion of its responsibilities: “There's also \nthe AI Safety Institute, I think that's a great kind of centralized repository for assessment, for \ndeveloping standards around AI models […] You’d have to think carefully about what its scope of \nwork is […] because […] if you have some kind of healthcare decision-making system […] \nregulated by a central AI regulatory body that doesn’t make that much sense to me.” Much of the \ncriticism seemed to have more to do with general thoughts on the centralization of regulatory \nefforts as opposed to direct criticisms of the technical capacity of the AI Safety Institute, which \nwas rarely called into question. As the interviews show, despite active legislative activity on the \n48\ntopic, there is no bipartisan consensus on the future of the AI Safety Institute at the time of this \nwriting.\nAs a result of the E.O., the National Telecommunications and Information Administration (NTIA) \nissued in April 2023 a Request for Comment focused on AI Accountability Policy. The NTIA \nbureau of the U.S. Department of Commerce advises the president on telecommunications policy, \nfocusing on economic advancement and industry regulation. The request included questions \nabout AI governance to hold actors accountable for AI system risks and impacts. After receiving \nmore than1,440 unique comments from multiple stakeholders, in March 2024, the NTIA released \na policy report outlining pathways for improving information sharing, liability rules, and cross\u0002sectoral capacity. The report introduced the concept of an Accountability Chain. It focuses on how \ninformation supports independent evaluations, which drive consequences, creating accountability\n(NTIA, 2024a). In their second report from 2024, on “Dual-Use Foundation Models with Widely \nAvailable Model Weights,” the agency addressed a question from the executive order relative to \nOpen-Source models. Their public Request for Comment issued in February 2024 received 332 \ncomments, and additionally, the NTIA hosted two public events gathering expert input. The NTIA \nreported that evidence of the need for restrictions on open-weight foundation models remains \ninconclusive and that the U.S. government, for now, should not restrict the wide availability of \nmodel weights for dual-use foundation models. (NTIA, 2024b) The report’s conclusions represent \na victory for the open AI innovators and open-source advocates. While NTIA’s position may sound \nlike a technical matter, their ruling on open models is substantive and can determine directions \nfor future AI Policy. Whether the incoming Trump administration will carry forward their \nrecommendations remains to be seen.\nDespite the infrastructure and thought leadership created since the executive order, the 2024 \nRepublican Party platform vowed to “repeal Joe Biden’s dangerous Executive Order that hinders \nAI Innovation, and imposes Radical Leftwing ideas on the development of this technology” (The \nRepublican Party Platform, 2024, 2024). Upon taking office, President Trump repealed Executive \nOrder 14110 (The White House, 2025c). At the time of writing, it is unclear which elements of the \norder might be incorporated into future policy. Republicans consider that the guardrails for AI \nfocusing on equity and fairness hinder innovation and prioritize free speech and rapid AI adoption, \npotentially leading to a less regulated approach. This shift could result in the removal of anti-bias \nprovisions and a reduced emphasis on mitigating AI risks (Alms, 2024). Shortly after revoking \nE.O. 14110, the Trump administration issued a brief Executive Order titled “Removing Barriers to \nAmerican Leadership in Artificial Intelligence” with calls for the development in 180 days of an \naction plan to implement the policy established in Section 2 of the document: “It is the policy of \nthe United States to sustain and enhance America’s global AI dominance in order to promote \nhuman flourishing, economic competitiveness, and national security.” (The White House, 2025d).\nAs discussed before under “The case for congressional action,” congressional involvement, \nincluding bipartisan legislation, will also influence the final shape of federal AI governance.\n49\nU.S. Senate and House AI task forces\nIn parallel to the executive branch activities, the 118th Congress \nreleased two major AI task force reports in 2024. In May, the \nSenate bipartisan AI Working Group released its “Driving U.S. \nInnovation in Artificial Intelligence,” a self-titled roadmap for AI\nPolicy outlining a strategy for advancing AI governance and \ninnovation in the country (Hendrix et al., 2024). Led by Senate \nMajority Leader Chuck Schumer (D-NY), along with Senators \nMike Rounds (R-SD), Todd Young (R-IN), and Martin Heinrich \n(D-NM), the group developed the roadmap following a series of \neducational briefings and nine AI Insight Forums held during the \nfall of 2023 including over 150 experts from industry, academia, \nand civil society. Organized into eight priority areas, it \nrecommends allocating at least $32 billion annually to support \nnon-defense AI innovation. This funding is intended to drive \nprogress in AI research and development, enhance workforce \ncapabilities, and establish ethical guidelines to ensure the responsible use of AI technologies. By \nfocusing on innovation, investment, and ethical oversight, the roadmap aims to position the United \nStates as a global leader in AI while addressing the technology’s societal and economic \nimplications. The report outlines legislative recommendations and areas for further exploration to \nadvance AI governance and innovation in the United States. The Senate’s approach to AI \nregulation in 2024 has drawn criticism for its lack of concrete action and failure to produce any \ndraft legislation. Instead of proposing specific regulatory measures, the report offered a process \nfor discussion without committing to policy outcomes or regulatory solutions (Green-Lowe, 2024).\nThe measured approach, however, matches the pattern of how legislation tends to be built in \nCongress. One of the policy advisors we interviewed states: “Almost no bill passes Congress the\nfirst time it’s introduced. Typically, it requires two to five years for an idea to reach maturity in \nCongress and for every stakeholder to feel like they get a say. A lot of time, policy has been driven \nby the administration.”\nIn December 2024, the House Bipartisan Task Force on Artificial Intelligence released a \ncomprehensive report that marked a significant step in shaping the nation’s AI policy. Co-chaired \nby Representatives Jay Obernolte (R-CA) and Ted Lieu (D-CA), the task force was established \nin February 2024 by Speaker Mike Johnson and Democratic Leader Hakeem Jeffries. Its efforts \nresulted in a 273-page report containing 66 key findings and 89 recommendations, spanning 15 \npolicy areas such as government use of AI, intellectual property rights, and energy consumption. \nThe report underscored America’s commitment to maintaining global leadership in responsible AI \ninnovation while ensuring the implementation of necessary safeguards. It also highlighted AI’s \ntransformative potential in sectors like healthcare, where it could enhance diagnostic accuracy, \nstreamline operations, and accelerate drug development. By addressing both opportunities and \nchallenges, the task force aimed to set a balanced framework for harnessing AI’s benefits \nExecutive Orders from \nthe last three \nadministrations, the \nfailed legislative \nexperience of the 118th\nCongress, Senate and \nHouse task forces, and \nchanges in the judicial \nenvironment have set \nthe stage for policy to \nbe enacted. This work \nintroduces a policy\u0002agnostic way to do it.\n50\nresponsibly and sustainably (House Committee on Science, Space, and Technology, 2024). The \nreaction to the House report was still mixed, highlighting the tension between promoting AI \ninnovation and addressing risks and harms (Iyer & Hendrix, 2024). While several praised the \nreport’s comprehensive approach, some critics – like with the Senate report - called for more \nconcrete policy recommendations and stronger safeguards against AI-related risks. The report, \nhowever, was seen as a decisive step forward with an actionable vision for AI governance in the \nU.S., outlining what to regulate, who should regulate it, and how to do so effectively. It advocates \nfor a sectoral, incremental approach to AI regulation and recommends further analysis relative to \nFederal preemption of State AI laws. \nBoth reports guide future congressional action on AI policy, emphasizing the importance of \nmaintaining U.S. leadership in AI innovation while addressing potential risks and challenges.\nCommon elements include promoting responsible AI development, recognizing the need for \nsafeguards, and addressing national security, workforce development, and ethical considerations. \nBoth reports resulted from extensive consultations with experts across various fields and covered \na wide range of policy areas. The reports differ in their scope and approach. The House report is \nmore comprehensive and emphasizes a sectoral regulatory approach, whereas the Senate report \nprovides a broader policy roadmap. Along these lines, the House report goes deeper into specific \nsectors like healthcare and agriculture, while the Senate report takes a more general stance on \nAI governance. The Senate report set the stage for ongoing discussions, while the House report\nbuilt upon these discussions and provided more detailed recommendations for future legislation.\nThe two reports can be viewed as part of the same legislative development trajectory, laying the \ngroundwork for action by the 119th Congress.\nLet us take a closer look at what is happening in Congress.\nCongressional perspectives\nWhat legislative action is telling us\nThe 117th and 118th Congresses have shown significant activity regarding artificial intelligence \nlegislation. During the 117th Congress (January 2021 to January 2023), lawmakers introduced 75 \nbills centered on AI and machine learning or that contained provisions specifically addressing \nthese technologies. Six of these navigated the legislative process and were enacted into law, \nmarking a step forward in addressing AI through federal policy (Harris, 2023).\nThis activity doubled in the 118th Congress (January 2023 to January 2025), with at least 150 AI\u0002related bills proposed. (Brennan Center for Justice, 2024) However, none of these bills were \nenacted into law. One of our conversations with the Senate described an environment with “a lot \nof fact gathering, and, frankly, a lot of fact gathering on facts that are already been gathered by \nother institutions.” In parallel and during the same period, 636 bills were introduced at the state \nlevel, of which more than one hundred were approved and are now State Law (Multistate.ai, n.d.).\n51\nIn December 2024, Congress passed the FY 2025 National Defense Authorization Act (NDAA),\nwhich included the first AI provisions enacted by the 118th Congress (Text - H.R.5009 - 118th \nCongress (2023-2024), 2024). However, the law is not regulatory and focuses instead on \nworkforce development, creating AI education courses, and a Chief Digital Engineering \nRecruitment Officer within the Department of Defense. Within this scope, the act establishes pilot \nprograms for AI in biotechnology, and manufacturing, and initiatives to improve AI usability. It also \naddresses AI safety and security, directing the identification of high-risk AI models and creating \nan AI Security Center. The NDAA emphasizes strategic planning, budgeting for AI data, and \ninternational collaboration through a multilateral AI working group (Sokler et al., 2024). The AI \nprovisions from the NDAA primarily affect the Department of Defense (DOD), focusing on \naccelerating technology transitions from research and development to operational use. These \nprovisions may also influence other national security and technology departments, such as the \nDepartment of Energy (DOE) and the Department of Homeland Security (DHS), through \ncollaborative efforts in AI research, development, and implementation.\nAdditionally, it is noteworthy to look at the following five pieces of proposed AI legislation from the \n117th Congress (MacCarthy, 2023). They were launched in 2021, following a recommendation \nfrom the lengthy “Investigation of Competition in Digital Markets” report from the House Judiciary \nCommittee (U.S. Congress. Committee on the Judiciary Subcommittee on Antitrust, 2020).\nStarting in 2019, the committee performed a bipartisan top-to-bottom review of the market,\nfocusing on the state of competition online and examining Amazon, Apple, Facebook, and Google \nbusiness practices and their effect on the U.S. economy and democracy. It then assessed the \nability of existing antitrust laws and competition policies to address market power and anti\u0002competitive conduct in digital markets. While none were made into law, the report and associated \nlegislative action significantly impacted the landscape of antitrust law regarding large tech \ncompanies. It highlighted potential anti-competitive practices and provided a framework for future \nlegal action, new regulations, and increased scrutiny of mergers and business conduct in the \ndigital market. The Federal Trade Commission (FTC) and the Department of Justice (DOJ) have \nbeen actively pursuing antitrust cases against major tech companies, including ongoing \ninvestigations and lawsuits against Amazon, Apple, Google, and Meta (parent company of \nFacebook) based on the practices identified in the report (American Economic Liberties Project, \nn.d.).\n• Ending Platform Monopolies Act - U.S. Congress: H.R. 3825, 117th Cong. (2021).\nThe Ending Platform Monopolies Act sought to eliminate conflicts of interest arising \nfrom dominant online platforms’ concurrent ownership or control of an online platform \nand certain other businesses. It faced strong pushback from major tech firms, who \nargued it would stifle innovation, hamper consumer choice, and disrupt popular online \nservices. Critics also worried about unintended consequences for small businesses \nreliant on large platforms’ scale and reach and the viability of enforcement. The narrow \ncommittee passage signaled bipartisan skepticism about its sweeping restrictions, and \nit ultimately failed to advance, expiring with the adjournment of the 117th Congress.\n• ACCESS Act - U.S. Congress: H.R. 3849, 117th Cong. (2021).\n52\nThe ACCESS Act sought to mandate data portability and interoperability among major \ntech platforms. Critics raised privacy and security concerns, questioning how user data \nwould be handled once transferred between services. Heavy lobbying from tech \ncompanies also stymied support, while debates arose about content moderation and \neditorial freedoms. It never reached a House floor vote and ultimately failed before the \n117th Congress adjourned.\n• American Innovation and Choice Online Act - U.S. Congress: S. 2992, 117th Cong. \n(2021).\nDespite strong bipartisan support, the American Innovation and Choice Online Act was \nderailed by concerns over user experience, product integration, and content \nmoderation restrictions. The bill prevented discriminatory practices by large platforms,\nprohibiting them from giving preference to their products or services (self-dealing). \nCoupled with formidable tech industry lobbying, lawmakers worried about the bill’s \ndisruptive potential. Divisions within the parties over its scope and impact further \nstalled progress. Ultimately, these factors prevented the measure from reaching a \nSenate floor vote, and it expired with the adjournment of the 117th Congress, ending \nits prospects.\n• Platform Competition and Opportunity Act - U.S. Congress: H.R. 3826, 117th Cong. \n(2021).\nCritics argued that the bill’s stringent restrictions on large platforms’ acquisitions \nthreatened innovation and undermined startup opportunities, as it could deter venture \ncapital and prevent smaller firms from being acquired. Tech companies opposed the \nmeasure, citing harm to user access to free or low-cost services. Despite bipartisan \nsupport and a narrow committee vote, the legislation ultimately lacked the momentum\nfor a full House vote and died with the close of the 117th Congress.\n• Open App Markets Act - U.S. Congress: S. 2710, 117th Cong. (2021).\nThe Open App Markets Act promoted competition by reducing the gatekeeper power\nof dominant app store operators. It met resistance due to concerns about consumer \nsecurity, privacy risks, and disruptions to the digital economy. Critics argued that \npermitting third-party payment systems and sideloading might inflate costs, \ncompromise security, and hamper innovation. Despite broad bipartisan backing and a \ndecisive committee vote, the legislation stalled without a Senate floor vote. \nThe failure of these five bills highlights deep-seated industry influence and persistent\ndisagreements over how to balance competition with consumer protection.\nQuantitative analysis of AI-related bills introduced in the 118th legislature \nWe used the Brenner Center for Justice Artificial Intelligence Legislation Tracker as our source \nfor the federal AI bill inventory and the Congress.gov APIs to access detailed bill texts and \nassociated metadata (Brennan Center for Justice, 2024; Library of Congress, n.d.). Applying\n53\nMachine Learning techniques, we identified and clustered patterns from the thousands of pages \ncovered by these federal bills. \nWhat follows is a quantitative analysis of the 150 AI-related bills introduced by the 118th Congress,\nrevealing several key insights. The number is significantly higher than in previous sessions, \nindicating growing interest and concern among lawmakers regarding AI governance. Two-thirds \nof these bills have been introduced by Democrats (99 of the 150), suggesting a greater focus on \nAI policy within the Democratic Party.\nFigure 8 Bill sponsorship by party in the 118th Congress\nSponsorship clusters in the Democratic (72) and Bipartisan (54) categories indicate significant \ncross-party support for several initiatives despite the partisan divide on many issues. The total \nnumber of sponsors varies widely across bills, indicating diverse levels of support and coalition\u0002building for different AI initiatives. Some bills have many sponsors but a relatively balanced \npartisan ratio, suggesting specific AI issues attract support from both parties.\n54\nFigure 9 Bipartisanship analysis for AI-related bills in the 118th Congress\nMembers from 36 states introduced AI bills, indicating that this is not a localized issue. During the \n2024 legislative session, at least 45 states, Puerto Rico, the Virgin Islands, and Washington, D.C., \nintroduced AI bills at the state level. Of these, 31 states, Puerto Rico, and the Virgin Islands \nadopted resolutions or enacted legislation. (National Conference of State Legislatures, 2024)\nSenators Mike Rounds [R-SD], co-chair of the Senate AI Caucus and member of the Senate’s \nbipartisan AI working group, and Edward Markey [D-MA] sponsored the most AI bills in the \nSenate, with seven each. Representative Ted Lieu [D-CA], co-chair of the House’s Bipartisan \nTaskforce on Artificial Intelligence, sponsored five bills, followed by representatives Clay Higgins \n[R-LA], Sheila Jackson Lee [D-TX], and Anna Eshoo [D-CA] with three sponsored bills each.\nThe Energy and Commerce Committee has been assigned the most AI bills in the House (20), \nfollowed by the Science, Space, and Technology Committee (15). The Commerce, Science, and \nTransportation Committee has been assigned the most AI bills in the Senate (33), followed by the \n55\nHomeland Security and Governmental Affairs Committee (8), and the Health, Education, Labor \nand Pensions Committee (8). \nFigure 10 AI bill sponsorship by state during the 118th Congress\nOverall, the metadata analysis indicates a growing focus on AI governance, with participation by \nCongress members from most states. The Democratic Party is taking the lead but with significant \nbipartisan support. Even though the increase in AI-related bills reflects members’ newfound \ninterest in the topic, our analysis showed that a few members continue to set the tone for the \npolicymaking process.\nThe textual analysis of the 150 bills reveals the policy focus of each. The analysis of the \"Primary \nAttributes in Congressional Bills\" highlights the top emphasis lawmakers assigned to the bills. \nResponsible and Ethical AI emerges as the most common primary policy attribute, with 71 of \nthe 150 bills mentioning it. This underscores a growing legislative concern for the ethical \ndevelopment and application of artificial intelligence. National Security follows with 24 bills, \nreflecting the importance to lawmakers of defense and security issues. In contrast, topics that \nThe analysis reveals a Congress focused on AI ethics, \nnational security, and public interest, among other issues. \nThis focus aligns with Democratic Party priorities. \nHowever, the frequency of policy attributes does not \nindicate bipartisan consensus. The in-depth interviews will \nsurface possible paths forward.\n56\ndominate public discourse, such as Antitrust and Copyright, appear as primary attributes in only \n5 and 4 bills, respectively.\nFigure 11 Primary policy attributes (150 AI bills from 118th Congress)\nThe “Top 3 Policy Attributes in Congressional Bills” analysis provides an expanded view by \nconsidering three main policy attributes from each bill. This broader perspective brings attention \nto issues like Industrial Policy and Public Interest, which gain prominence when secondary \nattributes are factored in. Privacy and Data Protection remain low-priority areas, highlighted in \nonly two bills. This aligns with the U.S. legislative trend of deferring to state-level leadership on \nprivacy issues rather than enacting federal regulations.\nFigure 12 Top 3 Policy attributes (150 AI bills from 118th Congress)\n57\nWhen examining both scenarios, the legislative priorities of the 118th Congress become clear. \nTogether, these analyses reveal a Congress grappling with AI ethics, national security, public \ninterest, and a growing array of other critical issues. Lawmakers placed significant emphasis on \nthe ethical development and use of AI and policies that serve the public interest. This focus aligns \nwith Democratic Party priorities, which is unsurprising given that Democrats authored two-thirds \nof the bills.\nHowever, the frequency of a policy attribute’s appearance does not necessarily indicate bipartisan \nconsensus. Qualitative interviews provide deeper insight into areas of alignment, as discussed \nlater in this text. For example, key terms like “safety” have become less favored in Republican \ncircles, where “guardrails” is the preferred terminology. National security and global leadership -\nparticularly concerning the U.S. energy matrix - emerged as pivotal topics during the late Biden \nadministration and the early days of the new Trump administration. These areas are expected to \nremain central to legislative efforts in the 119th Congress.\nCongressional voices\nDespite the volume of activity, little progress was made. No legislation was enacted, and only 17 \nout of the 150 bills were reported out of committee and entered the legislative calendar. To better \nunderstand the state of AI policy, we conducted extensive interviews with congressional staffers \nand members of Congress. These conversations revealed five key findings illuminating the \ncomplexities, priorities, and challenges shaping AI governance in the United States.\nOne of the most contentious debates in AI policy revolves around whether regulation hampers \ninnovation. Opinions are divided, often reflecting broader political alignments. Republican \nlawmakers view regulation skeptically, emphasizing concerns about stifling innovation and \neconomic growth. On the other hand, Democrats, while more supportive of certain regulatory \nmeasures, approach the issue with varied motivations. For many, regulation is less about \nfostering innovation and more about ensuring safeguards to mitigate risks. Regardless of party \naffiliation, the discourse often frames regulation as a tradeoff between innovation and safety \nrather than a synergistic tool to achieve both goals, albeit to varying extents.\nFor many, regulation is less about fostering \ninnovation and more about ensuring safeguards. \nThe discourse often frames regulation as a \ntradeoff between innovation and safety rather \nthan a synergistic tool to achieve both goals.\nOur proposed model aims at achieving both.\n58\nA staffer in the House of Representatives tells us: “Pretty different opinions in Congress now on \nAI innovation and safeguards. It really needs to be both. AI will be embedded in everything. So, \nwe need to pursue innovation, but also innovation in nontraditional ways, defense innovation, and \npublic interest use cases, transport, energy, infrastructure, etc.” A House staff member adds: “We \nwant to do this gently, in a way that we’re not allowing things to get out of control, but at the same \ntime, we’re not hampering innovation on AI.” A career staff member concludes: “An approach to \nAI […] does not make regulation a goal in and of itself, right? And thinking through carefully what \nthat actually means […] the first objective of every single U.S. policymaker should be to make \nsure that the U.S. has the most advanced, sophisticated, and rapidly developing AI ecosystem.”\nA second notable theme that emerged is the perception in Congress that the absence of \ncomprehensive congressional action on AI policy benefits business interests, particularly those of \ntechnology companies. This inertia creates an environment where businesses can operate with \nminimal constraints, leveraging the lack of regulation to pursue rapid development and \ndeployment of AI technologies. While this benefits industry players in the short term, it raises \nconcerns about long-term societal impacts and the role of government oversight. This finding was \ninconsistent with the view shared in the industry interviews, as we explored later in the text.\nWe heard from a former Senate staffer: “It is in the interest of companies to do nothing. I mean, \nyou could argue that they want the innovation stuff, but I think the status quo is perfectly amenable \nto what these companies would want.” A House staffer reflects that “the most powerful people in \nthe world aren’t [in] Congress. They’re the leaders of Deep Mind at Google; they’re Sam Altman \nwith ChatGPT.” The Senate staffer concludes that “there’s been some capture in this in the Senate \nwhen it comes to AI policy, just given the outstanding influence, or outsized, rather, influence, of \nAI companies and big tech firms.”\nA lack of urgency for Federal action was a third theme. Speaking with a staffer serving on one of \nthe Congressional Committees, we heard that “it is a bit of like, okay, well, let’s wait to see what \nhappens there [in Europe] because folks are lacking […] a vision or imagination for what a \nregulatory landscape could look like, and what impact that has on the different types of \nstakeholders” and a policy advisor reminded us that “Congress is a democratic institution where \nevery Member has the right to vote, [this] challenges the technocratic way of thinking about the \ntechnology. The most challenging part is coming up with policies that are appealing to a wide \narray of members. That may not be the best policy, but it’s the only way to make progress.”\nDespite AI’s rapid advancement, policymakers feel little urgency to implement federal regulations. \nThis delay is not due to a lack of understanding or innovative ideas; rather, it reflects deeper \nissues. The concept of AI regulation is still perceived as immature, with no consensus on what \neffective governance should look like. Political will remains insufficient, and the fragmentation of \nviewpoints contributes to a cautious approach. For many policymakers, the absence of immediate \ncrises linked to AI reinforces the belief that regulation can wait. A staffer described the issue: \n“Where Congress is most comfortable, is […] reacting to […] a rupture or big problem, where, \nwhen we're projecting forward […] it's not great, and even where we have a record but the issue \nis too complicated, or there's not […] enough consensus, it's hard to act, especially when we're \n59\ntalking about vote margins of just a couple of points or votes. […] We're waiting until there's \nsomething sort of discrete to you to fix, and mostly in a watching position.”\nThe next strong theme reflected how industry engagement dominates policymaking. A \ncongressional staffer warned: “Bills do not usually pass the Congress over the wishes of major \nactors – that could be the administration, major industry groups, they could make it painful for you \nto move anything,” and another interview added clarity stating that “most of the AI policy, quite \nfrankly, has been consolidated or is deeply controlled by leadership in both houses. […] There’s \na huge amount of lobbying and industry and money behind AI policy.”\nClose engagement with the tech industry is a hallmark of congressional efforts on AI policy. \nAcross the board, legislators acknowledge the need for industry involvement, whether as leaders \nor key supporters of any regulatory initiatives. Industry representatives often present their role as \ninformative, offering insights into AI’s technical complexities and potential impacts. However, this \ncollaboration is not without criticism. The outsized influence of Big Tech on policymaking raises \nconcerns about bias and the prioritization of corporate interests over public welfare. While some \noffices remain skeptical, most agree that any future regulatory framework will require significant \nindustry input.\nA staffer in the House ponders, “I find it hard to believe that if something were to actually come to \nthe floor, it wouldn’t have already gotten buy-in or feedback from the actual industry. […] The \nleaders in AI in this country […] I mean, Apple, Google, Microsoft […] they’re all leading the charge \nanyway.” At the same time, a Senate policy advisor tells us: “Industry support or anti-support is \ncrucial, maybe the most important honestly.”\nThe last of the five themes from our congressional interviews was the challenges to legislative \nprogress faced by the 118th Congress. Despite growing discussions about AI governance, the \n118th Congress has not passed any AI-related bills. A House Staffer reflects that “members were \nalmost afraid to try to push one solution when we know that any one solution could have actual \nconsequences.” Our discussions revealed that several factors contributed to this legislative \ngridlock:\n• A Generally Unproductive Congress: When asked why no AI-related bills passed in the \n118th legislature, many interviewees initially pointed to the overall lack of productivity of \nthe 118th Congress, which enacted only 3% of introduced legislation into law, compared \nto 7% during the 117th and 116th sessions (GovTrack.us, 2025). As one staffer stated,\n“Congress right now is the least productive that it has ever been in its entire history.” \nHowever, digging deeper reveals a series of other factors.\n• Geopolitical Considerations: Concerns about maintaining U.S. supremacy in AI and \ncompeting with China often overshadow domestic regulatory discussions. The concerns \nof a House Staffer are emblematic of this issue: “I think there is always kind of a \ncommitment there to the extent that […] we’re not allowing things to get out of control, but \nat the same time, we’re not hampering innovation on AI and progression on AI in this \ncountry and look back in three years and realize, wow, China, one of our biggest and most \n60\npowerful adversaries, is now here with AI, and they’re well-surpassed anything we’ve done \non.”\n• Political Polarization: Deep divisions within Congress and the consolidation of power \namong leadership hinder consensus-building. “I think anything can be polarized at this \npoint,” a frustrated staffer vents as she admits that “it depends on what [leadership] wants.”\n• Focus on Nonregulatory Initiatives: Many policymakers prioritize initiatives like \nresearch funding and voluntary guidelines over binding regulations. A congressional \nstaffer confesses that “[The] National AI Initiative Act [passed in 2020] is nonregulatory, \nfocused on research standards and workforce development. Since that time […] we didn’t \ndo much. […] Now, there are dozens and dozens of bills from dozens of members.”\n• Fragmentation of Efforts: With numerous competing initiatives and a lack of clear \nleadership, the path to actionable policy remains unclear.\nThe findings underscore the complexity of advancing AI policy in the United States. While \nregulation is viewed through a polarized lens, industry influence, geopolitical considerations, and \nlegislative dynamics further complicate progress. Moving forward, fostering consensus, clarifying \nleadership, and balancing innovation with safeguards will be essential to building a robust and \nfuture-ready AI governance framework.\nContrary to the often-repeated notion that “nobody in Congress understands the technology,” \ncongressional members’ expertise and access to experts in the field have improved over the last \ncouple of years. Through sustained efforts in education, bipartisan dialogues, and task forces, \nlawmakers are becoming better equipped to handle the complexities of AI policy. One of our \ninterviewees provided examples: “We have a lot of great resources […] we can pull over people \nfrom the administration all the time to give us briefings or from industry groups […] we have the \nCongressional Research Service, the Government Accountability Office, one thing that my office \nwants to reinstate is the Office of Technology Assessment here on the hill. […] We have the \ncongressional AI caucus […] and congressional ex-Staff Association […] to educate staffers and \nmembers on AI policies, AI issues that are coming up, and new technologies […] The task force \nwas a really great resource that we had some really great briefings there, brought in some great \nsubject matter experts.”\nWhere Congress and Industry could come together\nAccording to our interviews, Congress and industry stakeholders share areas of alignment on key \nAI policy priorities, even though they often remain unaware of these shared perspectives. To \ndifferent extents, both groups recognize the importance of establishing a regulatory framework \nthat ensures safety, accountability, and trust without stifling innovation. Industry leaders, for \nexample, frequently emphasized the value of clear federal standards to avoid the inefficiencies of \nfragmented state regulation, a concern echoed by congressional staffers who acknowledged the \nchallenges of navigating a patchwork system. There is a solid alignment in that both the industry \n61\nand members of Congress prioritize and see the value of preserving, protecting, and extending \nthe U.S.’s global leadership in AI.\nAnother point of agreement is the need for public-private collaboration. Industry representatives \nhighlighted the importance of government-provided targets and clear regulatory benchmarks, \nreflecting their willingness to engage proactively in crafting solutions. Similarly, congressional staff \nunderscored the value of advisory bodies and task forces that bridge technical expertise and \nlegislative goals. Both sides support public-private partnerships to foster innovation while \naddressing critical societal risks like bias, safety, and transparency.\nTargeted research and development (R&D) can drive innovation to address socio-technical \nchallenges. Policy and regulation alone will not close this gap; actual technical invention is \nneeded. Historically, the U.S. government has invested in research via its National Laboratories, \ngrants to the thriving American university system, and partnerships with research bodies in the \nprivate sector. Those investments were a topic of consensus across Congress and the industry \nduring our interviews. By investing in R&D that explicitly addresses social, ethical, and safety \nchallenges, the government and the industry can unleash innovation that mitigates risks and \ncreates a foundation for sustainable progress while maintaining a competitive edge.\nFigure 13 Examples of areas of alignment between Congress and the industry ecosystem (from interviews)\nHowever, there remains a disconnect in communication and priorities. Industry voices often \nadvocate for flexibility and innovation-focused policies, whereas congressional actors stress the \nneed for robust safeguards to uphold public trust. Sometimes, this divergence is more about \n62\nemphasis than substance, suggesting an opportunity for collaboration. By building structured \nplatforms for dialogue, Congress and industry can move beyond perceived differences and craft \npolicies that balance innovation with accountability. An industry observer reflects on areas of \nconsensus and bipartisan agreement: the “main one is China, […] the U.S. should be the AI leader \nin the world, and that the U.S. cannot allow China to catch up or take over. In terms of how that’s \nimplemented, there’s different viewpoints on what’s most effective. So, some people are like ‘Oh, \nmaybe we should restrict some open source because that’s what China is using for their most \nadvanced models.’ But then others are saying, ‘Well, actually, you don’t want any restrictions \nbecause they limit the ability of U.S. companies to run fast’ […] policy areas that are bipartisan \ninclude things like making access to energy and data center build-out in the U.S., more readily \navailable and expediting that.”\nFigure 14 Examples of areas where more alignment between Congress and the industry ecosystem is still needed \n(from interviews)\nTechnology policy requires an understanding of both: A bilingual fluency in science and mastery \nof the humanities. An industry insider reflected on this struggle: “People in AI companies don’t \ntend to know about policy. I’m […] passionate about policy and AI. […] In my team at [redacted] \nI was the only person from a policy background. Everyone had technical backgrounds, and they \nwere brilliant at what they did, […] but what we do is hard, too. You can’t just know computer \nscience and intuit policy like it’s something that you can pick up overnight. […] A lot of people who \ndo AI don’t know much about the world around, and they’re fantastically good at coding, and so \nwhen they’re thinking about risks […] they’re not coming to it with a particularly nuanced view. \n63\n[…] People who speak policy don't speak tech. […] People who speak tech don't really speak \npolicy.” Implementing effective policy requires bridging this chasm.\nThe technological prowess of the private sector \nfar outpaces that of the public sector, \nparticularly in fields where rapid innovation and \nsignificant resources drive progress. This \nasymmetry reflects a fundamental reality: While \nindustry is optimized to push the boundaries of \ntechnology and achieve market success, \ndemocratic governments are designed to \nuphold public trust and protect societal \ninterests. These divergent mandates often create\ntension, as businesses prioritize profitability and shareholder value while governments emphasize \nsafety, equity, and ethical considerations. However, this contrast need not be a source of conflict. \nInstead, it offers a unique opportunity for collaboration. Recognizing these complementary \ncharacteristics, a well-structured governance model can harness the strengths of both sectors. \nThe industry is optimized to push the \nboundaries of technology and achieve \nmarket success. Democratic \ngovernments are designed to uphold \npublic trust and protect societal \ninterests. This contrast need not be a \nsource of conflict. Instead, it offers a \nunique opportunity for collaboration.\n64\nPart III – A New Governance Model\nAfter setting the context and introducing conceptual frameworks in Part I, we highlighted the \nresults of our qualitative and quantitative research in Part II. Part III of the paper is dedicated to \nour proposal for a new Dynamic Governance Model.\nThis part describes the three components of the model:\n• Public-private partnerships for the creation of evaluation standards.\n• A market-based solution for audit and compliance.\n• A system of accountability and liabilities set by legislatures, existing executive agencies, \nand the courts.\nWe have included an abbreviated example of implementing the model to drive one specific policy \ngoal. This section concludes with a word of caution for those considering implementing the ideas \nin this manuscript and technology policy in general. \nA path forward\nThe rapid adoption of artificial intelligence and its role as a transformative general-purpose \ntechnology has brought the United States to a pivotal moment. While the country has historically \nembraced innovation with a light regulatory touch, AI advancement’s unique pace and scale \ndemand a recalibration of this approach. The challenge is clear: How can we maintain the \ndynamism of technological progress while ensuring AI’s safe, ethical, and equitable deployment? \nAddressing this challenge requires a new governance model that balances innovation with \nregulation, fosters collaboration between public and private \nsectors, and builds public trust in the potential of AI. This section \noutlines a strategic path forward, focusing on practical \nmechanisms to align technological development with societal \nneeds and preserve America’s leadership in this critical domain.\nThe American system, while imperfect, has consistently \ndemonstrated its ability to drive technological and economic \ngrowth. This system, rooted in a combination of entrepreneurial \nspirit, public and private investment, and an open market, has \nfostered innovation across industries. From the early days of the \nIndustrial Revolution to the rise of the Internet and modern digital \ntechnologies, the U.S. has maintained its leadership by balancing \neconomic incentives with strategic policy interventions. Innovation \nhas been a crucial driver of American economic growth and \ncontributed as much as half of the nation’s economic growth in the 20th century. This \ntechnological progress has led to new industries, products, and services that have fundamentally \nThe American system, \nwhile imperfect, has \nconsistently \ndemonstrated its ability \nto drive technological \nand economic growth. \nThis system, rooted in a \ncombination of \nentrepreneurial spirit, \npublic and private \ninvestment, and an \nopen market, has \nfostered innovation \nacross industries.\n65\ntransformed the American economy and improved living standards (The White House, 1995). The \ninformation technology (IT) sector has been a powerful driver of economic growth in recent \ndecades, with an outsized contribution to the U.S. economy, creating high-paying jobs, driving \nexports, and spurring innovation across various sectors (Atkinson, 2022).\nLike previous general-purpose technologies such as electricity and the Internet, AI could reshape \nindustries, create new opportunities, and enhance the quality of life. A growing consensus among \npolicymakers, industry leaders, and academics is that safeguards are essential to ensure AI \napplications’ safe and ethical deployment. These safeguards aim to build public trust and \nminimize harm, yet the specifics of their implementation remain a matter of debate, as we have \nseen in the previous sections. \nTechnological innovation and legislation operate on fundamentally different timelines. Innovation \noften occurs rapidly and unpredictably, driven by breakthroughs, market dynamics, and \nentrepreneurial experimentation. By contrast, the legislative process is slow and intentionally \ndesigned to ensure thorough deliberation, consensus-building, and stability. This inherent \ndisparity creates challenges in regulating fast-moving technologies like AI, where static regulatory \nframeworks can struggle to keep pace with the speed of development and deployment. The rapid \nevolution of AI demands a dynamic approach to governance that allows for adaptability and \ncontinuous refinement while preserving the measured oversight required to safeguard public \ninterests.\nA strictly ex-post approach to regulating AI, relying solely on antitrust actions and litigation, is \ninsufficient to address this technology's unique challenges and catastrophic risks. While antitrust \nenforcement and legal recourse play a vital role in correcting abuses and ensuring accountability, \nthey are inherently reactive. Such mechanisms often come into effect only after harm has \noccurred, making them ill-suited to prevent high-stakes risks like safety failures, systemic biases, \nor widespread societal disruption. Moreover, the complexity and pace of AI innovation can outstrip \nthe ability of courts and regulators to respond effectively. At the same time, ex-ante regulation, \nwhile proactive, has its limitations. Static regulatory frameworks can struggle to keep up with rapid \nand unpredictable technological innovation, risking either obsolescence or overreach that stifles \nprogress. \nThese phenomena underscore the need for a dynamic regulatory approach that combines the \nforesight of ex-ante measures with the adaptability to evolve alongside technological \nadvancements. Such a system would complement existing ex-post mechanisms, creating a \ncomprehensive framework to promote innovation and accountability.\nContinued technological innovation is important to maintain the United States’ economic and \ngeopolitical leadership and address the pressing challenges inherent in current AI models. From \nthe energy-intensive demands of large-scale computation to issues of alignment, bias, and safety, \ntechnological advancements are critical to ensuring AI systems are both sustainable and ethical. \nThese challenges highlight the need for targeted investments in research and development, \nfostering breakthroughs in energy-efficient algorithms, bias mitigation techniques, and enhanced \n66\nmodel transparency. A well-crafted tech policy and regulatory framework must act as both an \nenabler and a safeguard, incentivizing the development of technologies that address existing \nlimitations while ensuring accountability and public trust. By embracing a dynamic and \ncollaborative approach to regulation, incorporating public-private partnerships and ongoing \ndialogue, policymakers can create an ecosystem where safety and innovation coexist. \nEvolution in the governance of AI is needed, \nleveraging the strengths of both industry and \ngovernment. Industry brings technical \nexpertise, innovation capacity, and real-time \ninsights into technological trends, while the \ngovernment safeguards public interests \nthrough oversight, accountability, and policy \ndirection. By integrating these complementary \nroles, such a scheme can adapt to the rapid \npace of AI advancements, allowing regulatory \nframeworks to evolve in tandem with \ntechnological developments. This collaborative \napproach could take several forms, including joint standard-setting initiatives, co-development of \nbest practices, and establishing regulatory sandboxes to pilot new technologies under controlled \nconditions. It also requires continual feedback and iteration mechanisms, such as advisory \ncouncils or working groups that bring together stakeholders from diverse sectors. Furthermore, \nembedding transparency and public engagement in this process will be crucial for building trust \nand ensuring that the framework remains responsive to societal needs.\nA new model for collaboration and accountability\nTo address the challenges and opportunities presented by artificial intelligence, we propose a \nDynamic Governance Model centered on structured collaboration between government and \nindustry. At the heart of this model is creating an entity tasked with setting clear standards in \npartnership with the private sector. These public-private partnerships would go beyond mere \nconsultation, fostering active collaboration in developing, implementing, and iterating on policies \nthat align innovation with societal priorities. A key feature of this model is the incorporation of \ncommitments by industry players underpinned by a robust accountability scheme, including clear \ndelineation of liability and mechanisms for independent audits. \nThe extra-regulatory model was designed to be modular, light, and of general application. The \nmodular nature of the model allows it to be implemented in phases, starting from different entry \npoints. It is light enough to be built on top of and complement existing legislation and judicial \nsectoral frameworks, reducing the implementation burden on Congress and the industry \necosystem. Its general applicability allows policymakers to decide what use cases to prioritize (a \nrisk-based approach, sectoral phased implementation, focus on larger companies, and \ngatekeepers are examples of policymaking and political decisions that can be made at \nimplementation time). \nThe proposed dynamic governance \napproach brings government and \nindustry together while combining the \nforesight of ex-ante measures with the \nadaptability needed to address \ntechnological advancements. Coupled \nwith existing ex-post mechanisms, this \ncreates a comprehensive framework to \npromote innovation and accountability.\n67\nThis proposal requires decisive congressional action, including a review and potential expansion \nof statutory authorities to support the framework. By enacting clear policies, Congress can enable \nadaptive regulation that steers AI innovation in a safe direction and safeguards the nation’s \nleadership in this critical enabling technology. This balanced approach ensures that while the \ntransformative potential of AI is fully realized, its deployment remains aligned with ethical \nstandards and public trust.\nFigure 15 Dynamic Governance Model\nThe NTIA’s efforts, featured in its March 2024 Artificial Intelligence Accountability Policy Report\nhighlighted in “The Executive Order legacy” section, underscore the federal government’s appetite \nto foster an AI accountability ecosystem. The NTIA has laid a foundation for ensuring trustworthy \nAI systems by emphasizing independent evaluations, standard-setting, and transparent \ndisclosures. These initiatives resonate with our proposal for a dynamic and collaborative \ngovernance model, as both approaches highlight the importance of public-private partnerships, \nadaptive regulation, and the integration of technical standards to address AI’s societal impact.\nBoth frameworks advocate for accountability inputs—such as audits, disclosures, and \ndocumentation—as pillars of responsible AI governance. Likewise, we agree with the need for \ntiered risk-based approaches and the involvement of multiple stakeholders across the AI lifecycle. \nOur model diverges in its emphasis on extending public-private partnerships to include \ncommitments enforced through a novel accountability scheme incorporating liability delineation \n68\nand audit mechanisms. This extension creates a dynamic, self-reinforcing system that evolves \nalongside technological advancements.\nThe Dynamic Governance Model\nThe Dynamic Governance Model begins with an essential role for the government: setting clear \npolicy goals. As outlined in the “AI Policy Considerations” section, these goals should balance \ninnovation with the public interest, ensuring that AI systems contribute positively to economic \ngrowth while safeguarding societal values. With these goals established, the model is made of\nthree interconnected steps. First, Evaluation Standards are set via public-private partnerships that \nreflect technological feasibility, policy goals, and societal priorities. These standards create a \nbenchmark for responsible AI development and deployment. Second, a market ecosystem for \naudits and compliance must be created, wherein independent entities assess adherence to the \nestablished standards. The process of audits and compliance ensures that AI systems are safe \nand ethical and meet transparency and reliability requirements. Finally, a robust system of \naccountability and liability must be codified and enforced by legislatures, regulatory agencies, and \ncourts. Together, these components form a closed feedback loop where the insights gained from \nimplementation, compliance monitoring, and enforcement inform the continuous improvement of \nthe evaluation standards. With active participation from the government, industry, and civil \nsociety, we embed trust into this cycle. The dynamic model ensures adaptive governance that \nevolves alongside technological advancements. We will now look at the details for each step. \nEvaluation standards\nThe success of the Dynamic Governance Model hinges on establishing robust and adaptive \nevaluation standards. These standards must be developed through collaborative public-private \npartnerships that draw on the expertise of industry leaders, academic researchers, policymakers, \nand civil society organizations. Existing models, such as the U.S. Artificial Intelligence Safety \nInstitute (AISI) under NIST, provide a framework for AI safety and risk management. However, as \nthe NTIA Accountability Report points out, these efforts often lack enforcement power and \nstandardized methodologies for independent evaluations (NTIA, 2024a). Public-private \npartnerships should set actionable, measurable, and enforceable standards to address this gap.\nInternational models, such as the EU’s regulatory sandboxes and the OECD’s Observatory of \nPublic Sector Innovation (OPSI), demonstrate how experimental environments can foster \nregulatory innovation. While these initiatives promote valuable collaboration, they also highlight \nthe need for scalability and broader adoption. The implementation of these novel models is still in \nits early stages. The regulatory sandbox work in Europe predates the EU AI Act, which now \nprovides these environments with a more substantial legal basis. Spain is pioneering nationwide \nregulatory sandboxes and represents an important case for those looking for empirical evidence \nof success, but the jury is still out (Future of Life Institute, n.d.). The NTIA report stresses the \n69\nimportance of integrating accountability mechanisms—such as mandatory disclosures and formal \nevaluations—into these models to ensure they are more than voluntary exercises. The EU AI Act \nembeds a similar approach under the concept of “regulatory learning.”\nImportantly, evaluation standards should encompass technical and non-technical dimensions of \nAI accountability and reflect technological feasibility, ethical considerations, and societal values. \nThe NTIA’s emphasis on information flow, including standardized documentation and disclosures, \nsupports this goal. By requiring that developers and deployers provide clear, consistent \ninformation on AI system design, capabilities, risks, and limitations, stakeholders across the AI \necosystem can better assess compliance and trustworthiness.\nThe implementation of evaluation standards requires a flexible, iterative approach that can \naccommodate technological evolution. While this work does not aim to provide a fully developed \nimplementation plan, we recommend several key steps. First, as suggested by the NTIA, a \nstructured public-private dialogue platform could be formalized to ensure ongoing collaboration in \nupdating standards. This platform would allow the sharing of best practices, emerging risks, and \nlessons learned from real-world applications. One attempt at creating such a platform is the \n“Promoting United States Leadership in Standards Act of 2024,” introduced in December 2024 –\nU.S. Congress: H.R. 10281, 118th Cong. (2024). A bipartisan effort by the Senate, also in \nDecember 2024, introduced “A bill to establish the Artificial Intelligence Safety Review Office in \nthe Department of Commerce, and for other purposes” – U.S. Congress: S. 5616, 118th Cong. \n(2024) (Nihill, 2024). These examples demonstrate Congress’s efforts to codify initiatives \noriginally established through executive orders. As discussed before, the Supreme Court \ndecisions that limit agency discretion make incorporating these initiatives into statutory language \ncritically important.\nSecond, pilot programs, like those pioneered by the EU, should be launched in sectors where AI \nadoption presents significant risks and opportunities, such as healthcare, finance, and critical \ninfrastructure. These pilots would provide crucial feedback on the practical implementation of \nEvaluation Standards and inform their refinement.\nFinally, the NTIA report highlights the importance of developing and certifying independent \nevaluators and auditors to foster consistency and trust. While audits and compliance will be \ndiscussed in the next section, evaluation standards must be directly tied to these downstream \naccountability mechanisms. Only through regular, independent evaluation can these standards \nremain relevant and effective in mitigating risks while fostering innovation.\nA valid criticism of a public-private partnership for setting evaluation standards is the risk of \nregulatory capture by powerful industry actors. Several strategies can be employed to mitigate \nthis risk. First, ensuring balanced stakeholder representation by including diverse groups such as \ncivil society, academia, consumer advocates, and smaller businesses reduces the risk of \ndominance by a few large players. Second, establishing independent oversight mechanisms—\nsuch as third-party audit bodies—enhances impartiality in enforcement. The following section will \ncover the oversight mechanisms in more detail. Transparency is critical, with public consultations \n70\nand mandatory disclosure of interactions between regulators and industry representatives \nfostering trust. Robust conflict-of-interest policies, including cooling-off periods for regulators \ntransitioning to industry, help limit undue influence. Legislative oversight, judicial review, and use \nof decentralized standards-setting bodies, such as NIST or IEEE (Institute of Electrical and \nElectronics Engineers), can further prevent regulatory capture by distributing decision-making \nauthority. Adopting performance-based regulations, which focus on outcomes rather than \nprescriptive rules, limits industry influence over specific technical requirements. Finally, periodic \nreviews and sunset clauses ensure that policies remain relevant and effective over time. \nFigure 16 Anti-capture strategies for public-private partnerships\nA market ecosystem for audits and compliance\nCreating a robust market ecosystem for audits and compliance will represent a key improvement \nover current voluntary commitments, which, while valuable, often lack enforcement mechanisms \nand consistency (The White House, 2023a). One can draw lessons from other well-established \nindustry-led compliance frameworks to build an effective ecosystem.\nA relevant example is UL (Underwriters Laboratories), one of the largest and oldest independent \nsafety science companies (Kagan, 2023). For over a century, UL has set industry benchmarks for \n71\nproduct safety, certifying products ranging from consumer electronics to medical devices. Its \nmodel demonstrates how an independent, industry-recognized certification body can enhance \nsafety, build trust, and facilitate market access. Similarly, CE marking in Europe (Conformité \nEuropéenne) ensures that products meet health, safety, and environmental protection \nrequirements, enabling free trade across the European Economic Area (European Union, n.d.).\nUL and CE illustrate how structured, industry-wide compliance mechanisms can contribute to \nmarket trust and safety, enabling innovation and competition. Both entities have faced criticisms \nregarding conflicts of interest, lax standards, and, in the case of UL, monopolistic practices. CE’s \nself-declaration approach raises concerns about rigor and enforcement. Recently, UL has made \na series of organizational and process changes in preparation for its 2024 IPO. CE has updated \nemerging technologies and sustainability regulations while extending recognition in the UK post\u0002Brexit. Both systems continue to evolve. \nThere are also significant parallels to be drawn from accounting standards. Systems like U.S. \nGAAP (Generally Accepted Accounting Principles) and IFRS (International Financial Reporting \nStandards) have established a common framework for financial reporting across industries and \ncountries. The Securities and Exchange Commission (SEC) acts as one of the regulatory bodies \noverseeing the standards and mandates transparency through extensive disclosures and regular \naudits. This approach ensures that financial information is reliable and comparable, fostering trust \nin financial markets. Significantly, accounting standards are periodically revised to reflect changes \nin business environments, demonstrating the need for iterative updates to standards as \ntechnology evolves. GAAP and IFRS face ongoing criticisms regarding their complexity, potential \nfor manipulation, and ability to reflect a company’s financial position accurately. However, it would \nbe hard to imagine a world without accounting standards and the associated economic chaos, \nreduced transparency, impact on business operations, and impeded global financial cooperation \nand development.\nIn much the same way, a market ecosystem for AI audits and compliance requires standardized \nevaluation protocols, clear regulatory oversight, and mechanisms to ensure that standards evolve. \nThe standardization, compliance, transparency, and continuous improvement principles underpin \nproduct certification and financial reporting frameworks and offer a blueprint for creating a similar \necosystem in the AI space.\nBuilding from the NTIA’s recommendations, the following steps can be considered for establishing \nthis ecosystem:\n1. Standardized Audit Criteria: Like evaluation standards, public-private partnerships \nshould define standardized criteria for AI audits, specifying what aspects of an AI system \n(e.g., safety, fairness, privacy, transparency) must be evaluated and how. These criteria \nshould be publicly accessible and regularly updated to reflect technological advancements \nand emerging risks.\n2. Certification and Oversight Bodies: Independent certification bodies, like UL or financial \naccounting firms, should be established to conduct audits. These bodies would be \nresponsible for certifying AI systems against the established standards. Sectoral \n72\nregulatory agencies could oversee these bodies, ensuring accountability and adherence \nto auditing protocols.\n3. Auditor Qualification and Certification: To ensure high-quality audits, auditors must be \ncertified based on clear qualifications and expertise in the technical and ethical aspects of \nAI. A national auditor training and certification program could be launched in collaboration \nwith universities and industry partners.\n4. Mandated Disclosures and Reporting: As with financial reporting, companies deploying \nhigh-risk AI systems should be required to disclose key information about their systems’\ndesign, testing, and deployment. Standardized disclosure formats, such as model cards \nand datasheets, would enhance transparency and enable stakeholders to assess \ncompliance more effectively.\n5. Sector-Specific Pilots: Pilot programs could be launched in high-risk sectors (e.g., \nhealthcare, finance, autonomous vehicles) to test the feasibility of audit protocols and \nidentify best practices for implementation. Insights from these pilots would inform the \nbroader rollout of audit requirements across industries.\n6. Iterative Updates: Just as accounting standards are regularly revised, AI audit criteria \nshould be updated based on feedback from auditors, regulators, and industry \nstakeholders. This iterative process will ensure that standards remain relevant and robust \nin the face of rapid technological change.\n7. Federal Support for the Ecosystem: Federal agencies should provide programmatic \nsupport to accelerate the development of a vibrant market for AI audits. This support \nincludes funding research on auditing methodologies, developing technical audit \ninfrastructure, and facilitating public-private partnerships. Additionally, federal \nprocurement rules could require government suppliers and contractors to comply with \ncertified AI audit standards, further incentivizing industry participation.\nBy embedding these elements into the market ecosystem, the governance model ensures that AI \nsystems are subject to continuous scrutiny and improvement, fostering trust among all \nstakeholders—developers, deployers, regulators, and the public—while encouraging innovation \nunder a framework of accountability and transparency. This process is inherently long-term, \nrequiring incremental progress over several years and likely maturing over decades. However, \nthis should not deter us from taking the necessary first steps to initiate the development of this \necosystem. Early efforts can provide the foundation for a mature, sustainable ecosystem. These \ninclude establishing pilot programs, defining baseline criteria, and creating a framework for auditor \ncertification.\nCongress, especially the Senate, has demonstrated an interest in exploring the intersection of \nstandards and how to enforce compliance. Here are a few examples of bills that have been \nintroduced in the Senate during the last couple of years in this area:\n• The “TEST AI Act of 2024”, US Congress: S. 3162, 118th Cong. (2024) mandates the \ncreation of AI testbeds for trustworthy system development, involving interagency \ncollaboration, risk assessments, and classified facilities to prevent AI misuse. It \n73\nemphasizes red- and blue-teaming methodologies for vulnerability evaluations and aims \nto strengthen national security and critical infrastructure protection. \n• The “AI Research, Innovation, and Accountability Act of 2023”, US Congress: S. 3312, \n118th Cong. (2023) promotes AI innovation while ensuring accountability through \ntransparency, risk management frameworks, and generative AI system disclosures. It \nmandates transparency reports for high-impact AI systems, recommends best practices \nfor government AI adoption, and encourages robust AI oversight to mitigate risks in \nsensitive sectors. \n• The “VET AI Act”, US Congress: S. 4769, 118th Cong. (2024) directs NIST to develop \nvoluntary guidelines for AI assurance, focusing on internal and external evaluations of AI \nsystems. It aims to enhance trust through meaningful assurances, safeguard privacy, \nmitigate harms, and support the adoption of trustworthy AI solutions via evidence-based \nstandards and technical validation. \n• The “Future of Artificial Intelligence Innovation Act of 2024”, US Congress: S. 4178, 118th \nCong. (2024), establishes and codifies an AI Safety Institute, promotes innovation via \ntestbeds and capacity-building initiatives, and supports international AI standards and \nresearch collaborations. \nEach bill involves NIST in developing AI standards, metrics, and methodologies to ensure safety \nand accountability and emphasize collaboration between public and private sectors, academia, \nand international partners for AI research and development.\nOn a side note, the concept of regulatory markets introduced in the last decade in academic and \nAI research lab circles presents an innovative approach to governance by addressing the \nweaknesses of traditional government-led regulation and industry self-regulation (Clark & \nHadfield, 2019). This model focuses on governments setting clear regulatory goals while private \nregulators compete to deliver effective solutions that meet these objectives. By fostering \ncompetition, the approach drives innovation in regulatory technologies, enabling oversight \nmechanisms to keep up with the rapid evolution of AI systems. Regulatory markets tackle this \nissue by delegating responsibilities to private entities, incentivizing them to create advanced tools \nand methodologies for auditing, monitoring, and safeguarding AI systems. These private \nregulators operate under governmental supervision to maintain accountability while allowing for \na more flexible and adaptive regulatory framework. Despite its promise, the regulatory market \nmodel faces challenges, particularly in maintaining robust government oversight and preventing \nregulatory capture. Nevertheless, it offers a dynamic pathway to align AI development with \nsocietal safety and ethical norms (Carvao, 2024a).\nAccountability and liability\nThe third and final step of the Dynamic Governance Model is the establishment of a robust system \nof accountability and liability. This step addresses the question of who is responsible when AI \nsystems cause harm or fail to meet established standards and has historically proven to be the \nmost challenging to implement. Accountability mechanisms are critical to ensuring that AI’s \n74\npromises (innovation, efficiency, and better decision-making) do not come at the expense of \nsafety, fairness, or human rights.\nA helpful reference for understanding the importance of liability frameworks is the experience with \nsocial media platforms under Section 230 of the Communications Decency Act. Section 230’s \nliability shield enabled the rapid growth of the Internet and Web 2.0 by protecting platforms from \nbeing held liable for user-generated content. While this provision facilitated unprecedented \ninnovation and economic growth, it also attracted criticism for allowing platforms to evade \nresponsibility for harmful content. This example illustrates the delicate balance between \nencouraging innovation and ensuring accountability—a balance that must now be carefully \nnavigated in the context of AI (Carvao, 2023).\nLiability in the AI ecosystem introduces new complexities due to the distributed nature of AI \ndevelopment and deployment. Unlike traditional product liability, where responsibility can often \nbe traced to a single manufacturer, AI systems are developed and deployed through a multi\u0002layered supply chain. Key actors include developers (the entities that create and train AI models), \ndeployers (those who integrate and implement AI models into products or services), and end \nusers (Individuals or organizations that operate AI-driven systems in specific contexts). The NTIA \nAccountability Report emphasizes the need to ensure accountability across this entire value \nchain, highlighting that liability should not rest solely on one actor but be shared proportionally \nbased on each actor’s level of control and responsibility. For example, developers may be held \naccountable for transparency and design flaws, while deployers could be responsible for ensuring \nthat AI systems are used appropriately and safely. The NTIA report identifies several areas where \nchanges in law and policy are required to support AI accountability:\n1. Privacy: Stronger protections are necessary to prevent AI systems from infringing on \nindividuals’ privacy. These include clear rules on data usage, retention, and sharing and \nrequirements for transparent disclosures about AI system behavior.\n2. Copyright: With AI models increasingly trained on vast datasets, including copyrighted \nmaterials, new legal frameworks are needed to clarify AI developers’ and content creators’ \nrights and obligations.\n3. Safety: AI systems that pose risks to physical safety—such as autonomous vehicles and \nmedical devices—should be subject to sector-specific safety regulations, including \nmandatory pre-deployment testing and certification.\nTopics such as privacy and copyright reform are among the most challenging to address and, as \npreviously noted, represent areas lacking a prevailing normative consensus. The model’s \nmodular, lightweight, and broadly applicable design becomes especially valuable when \nconfronting these difficulties. Rather than beginning with these complex issues, one can first focus \non areas of more explicit agreement to build trust in the model. Once trust is established, the \nsame approach and process can be gradually extended to more protracted issues. \nEffective enforcement of accountability and liability frameworks will require a coordinated effort \nacross multiple institutions, each playing a distinct and complementary role. Congress will be \n75\ninstrumental in establishing the legislative foundation for AI accountability, including codifying \nbaseline liability standards and mandating sector-specific regulations where necessary. \nAdditionally, Congress can create mechanisms for ongoing regulatory oversight to ensure these \nframeworks remain effective and adaptive as AI technology evolves. Even though no AI bills have \nbeen enacted into law, there has been active work during the 118th Congress in this direction. \nHere are several examples: the “AI Fraud Deterrence Act”, U.S Congress: H.R. 10125, 118th \nCong. (2024) increases penalties for financial crimes such as mail, wire, and bank fraud \ncommitted using AI. The “Artificial Intelligence Civil Rights Act of 2024”, US Congress: S. 5152, \n118th Cong. (2024) establishes protections against discrimination by computational algorithms in \nkey areas like employment, housing, and healthcare and outlines transparency, individual rights, \nand enforcement mechanisms to safeguard civil rights against algorithmic bias and harmful \noutcomes. The “NO FAKES Act of 2024”, US Congress: H.R. 9551, 118th Cong. (2024) protects \nintellectual property rights related to individuals’ voice and visual likeness, addressing \nunauthorized digital replicas created using AI. It includes provisions for civil action against \nviolations and safe harbor rules for online services that promptly remove infringing content upon \nnotification. Each bill includes provisions for enforcement, penalties, or legal recourse to ensure \ncompliance.\nSectoral regulatory agencies, such as the FDA for healthcare and the SEC for financial markets, \nwill oversee the implementation of AI-specific regulations within their respective domains. These \nagencies will be tasked with certifying high-risk AI systems before deployment and ensuring \ncompliance with established safety and ethical standards. They will play a crucial role in \nmaintaining public trust in AI systems used in critical sectors by providing targeted oversight.\nCourts will also serve as a vital component of the enforcement ecosystem. As disputes arise, \ncourts interpret liability standards and resolve conflicts, setting legal precedents that shape how \naccountability is distributed across the AI supply chain. Over time, judicial rulings will help clarify \nambiguous areas in legislation and regulation, contributing to the development of a more robust \nand well-defined framework for AI governance.\nExisting legal frameworks can also address challenges associated with AI. For example, the \nrecent legal advisories issued by California Attorney General Rob Bonta highlight how state \nlaws—including consumer protection, civil rights, competition, and data privacy laws—can be \napplied to AI systems. These advisories emphasize that AI does not exist in a legal vacuum and \nthat businesses and developers are subject to longstanding legal principles. For instance, under \nCalifornia’s Unfair Competition Law, practices like deceptive advertising of AI capabilities or the \nmisuse of AI for fraudulent purposes are actionable. Similarly, civil rights laws protect against bias \nand discrimination perpetuated by AI systems, while privacy laws ensure that AI developers \nhandle sensitive data responsibly. These measures underscore the potential to adapt existing \nlegal standards to novel technologies like AI, providing a pathway for governance that leverages \nfamiliar structures while addressing emerging risks. Such an approach enables policymakers to \nbridge the gap between innovation and accountability without necessitating entirely new \nregulatory frameworks for every technological advance (State of California Department of Justice, \n2025). In parallel, California has enacted new AI-specific laws requiring transparency in training \n76\ndata and AI-generated content, safeguarding personal likenesses, ensuring disclosures in \ncampaign materials, penalizing exploitative uses like deepfake pornography, and mandating \nlicensed oversight for healthcare AI tools. These laws complement existing frameworks to \naddress the unique risks of AI technologies. The advisories mark a pioneering effort by a state to \nprovide explicit guidance on applying existing laws within the evolving context of AI technologies.\nWhile not an exact comparison, other domains draw on established frameworks in fields like \nproduct safety certification, financial accounting, and insurance regulation, where robust systems \nof accountability and liability have been successfully developed and implemented over the years. \nWe discussed product safety certification examples such as UL and regulatory frameworks like \nthe European CE marking. We also touched upon Financial Accounting and systems like U.S.\nGAAP and IFRS, offering lessons in transparency and compliance enforced by regulatory bodies \nsuch as the SEC. In insurance regulation in fields like healthcare and automotive industries, \ncompanies are often required to carry liability insurance to cover potential damages caused by \ntheir products or services. This model can be adapted to AI, where high-risk systems might \nsimilarly require mandatory insurance or contributions to compensation funds to safeguard \nagainst unforeseen harm.\nBy building on these established approaches, the AI ecosystem can adopt proven mechanisms—\nsuch as codified liability rules, independent oversight, and incentivization of best practices—to \nfoster a culture of responsibility and trust. These lessons inform the following steps that can help \noperationalize a system of accountability and liability:\n1. Codification of Liability Standards: Congress should pass legislation establishing clear \nliability standards for AI systems, including rules for determining fault across the AI supply \nchain.\n2. Sector-Specific Regulations: Regulatory agencies should develop detailed, sector\u0002specific guidelines for AI deployment. These guidelines should include pre-deployment \ntesting, certification, and post-deployment monitoring requirements.\n3. Insurance and Compensation Funds: Companies deploying high-risk AI systems could \nbe required to carry liability insurance or contribute to compensation funds designed to \ncover potential damages.\n4. Auditor and Certifier Accreditation: Independent auditors and certifiers should be \naccredited by recognized bodies to ensure consistent and reliable evaluations of AI \nsystems. These auditors are key in identifying potential risks and ensuring compliance.\n5. Incentivizing Best Practices: Policymakers could create incentives for companies that \nadopt best practices in AI accountability, such as tax benefits or reduced regulatory \nburdens for certified systems. These incentives will encourage proactive compliance.\nWhile building a comprehensive framework for AI accountability and liability is a long-term \nprocess, early steps such as establishing baseline liability standards and launching sector-specific \npilot programs can set the foundation for a mature and effective system. Over time, this framework \ncan evolve to reflect new technological developments, emerging risks, and lessons learned from \nreal-world deployments.\n77\nUltimately, ensuring accountability and liability in AI will require significant legal and regulatory \nadvancements. Much of this must be codified into laws and statutes, clearly defining liability \nacross the AI supply chain. Courts will play a crucial role in interpreting and applying these laws, \nsetting precedents that refine liability standards over time. While this is a complex and long-term \nprocess, establishing a solid legislative and judicial foundation is essential for fostering trust, \npromoting innovation, and safeguarding societal well-being in an AI-driven future.\nImplementation context\nThe proposed Dynamic Governance Model \naligns well with the U.S. approach to policy \nimplementation, integrating elements of antitrust, \nsectoral regulation, and litigation. It employs \nantitrust principles to facilitate ex-post corrections \nwhen market concentration or anti-competitive \nbehavior undermines the public interest. \nImplementing sectoral regulations provides ex\u0002ante safeguards, establishing clear rules and \nstandards before any harm occurs. Furthermore, \nthe model incorporates aspects of tort law by \nemphasizing liability throughout the AI supply \nchain, where courts will play a vital role in \ninterpreting these standards and establishing \nlegal precedents.\nThis hybrid approach reflects the U.S.’s ability to balance innovation with regulation, ensuring that \ntechnological progress is not stifled but guided responsibly. By adopting this model, the U.S. can \nenhance its geopolitical standing in AI leadership, outpacing global competitors such as China by \ndemonstrating that democratic governance can foster innovation and accountability. Unlike top\u0002down regulatory regimes, this model emphasizes transparency, fairness, and stakeholder \ninvolvement, which can strengthen trust in U.S.-led AI initiatives on the international stage.\nCodifying liability standards, creating audit ecosystems, and fostering public-private partnerships \nwill enable the U.S. to maintain its AI geopolitical, technical, and economic leadership. \nFurthermore, by collaborating with international partners on harmonized standards, the U.S. can \nbuild global coalitions for responsible AI governance, ensuring interoperability, enhancing trust, \nand promoting shared ethical values. Most importantly, this model promotes trust—both \ndomestically and internationally—by demonstrating a commitment to ethical AI development, \nensuring safety and fairness while fostering continued innovation.\nAs we find ourselves in February 2025, at the start of a new administration and the 119th\nCongress, the conversation surrounding technology and AI policy in the United States is \nbeginning to reach a more advanced stage. One of our recent congressional interviews \nThe Dynamic Governance Model \naligns well with the U.S. approach to \npolicy implementation, combining \nelements of antitrust, sectoral \nregulation, and litigation.\nThe safeguards generated by the \nmodel will mitigate risks so that the \nindustry can continue to accelerate \nand innovate.\n78\nunderscores this evolving maturity: “The educational phase has gone well, the private sector's \n[…] R&D of what we need from the policy side of things has gone well, and now, to […] cap it off, \nthe executive branch and some of the biggest players in the private sector are […] publicly stating \nthat AI is going to continue to be a massive initiative. […] The culmination of all three of those \nthings puts us in a good position” to have a productive 119th Congress. \nImplementation example: Improving data center energy efficiency\nTo illustrate the practical application of the Dynamic Governance Model, consider the pressing \nchallenge of addressing the energy demands of AI systems and their data centers. As highlighted \nin the interviews with congressional and industry stakeholders, there is an alignment between the \ntwo on the need to address the rising energy consumption of AI technologies. Additionally, there \nis growing bipartisan consensus on the topic – albeit with different approaches on how to do so –\nmaking it a good candidate to test-pilot the Dynamic Governance Model.\nPolicy Goals and Objectives\nThe hypothetical policy goal is to enhance the energy efficiency of data centers housing AI \nsystems. This objective might involve Congress passing legislation to:\n1. Establish benchmarks and metrics for energy efficiency through the Department of Energy \n(DOE).\n2. Incentivize innovation in energy-efficient hardware, algorithms, and data center design.\n3. Create testbeds for evaluating and validating energy efficiency improvements.\nThe Dynamic Governance Model provides a structured framework to implement such goals \nfostering collaboration and accountability across stakeholders.\nEvaluation Standards\nContinuing with this simulated scenario, the Department of Energy would spearhead the \ndevelopment of evaluation standards for data center energy efficiency, establishing a public\u0002private dialogue bringing together:\n• Regulatory Agencies: Including DOE, NIST, and relevant offices within the Departments \nof Commerce and Energy (including expertise from National Labs). \n• Industry Stakeholders: Representing Big Tech, startups, and the investment community \n(tech, real estate, and others) in a diverse set of resources, expertise, and interests,\nincluding representatives of civil society like labor unions and affected communities. \n• Independent Oversight: Ensuring transparency and accountability in the standard\u0002setting process and preventing regulatory capture.\n79\nDepending on the scope of the energy efficiency measures being targeted, the exact office that \nis in the lead on the evaluation standards may vary. In this particular case, it could be the Federal \nEnergy Regulatory Commission (FERC) or the Office of Energy Efficiency and Renewable Energy \n(EERE). Strong input from NIST would be a necessity, given the rapidly evolving nature of these \ntechnologies.\nThis dialogue platform would operate over a defined timeline, producing actionable, consensus\u0002driven standards. This would be an iterative process that accounts for the evolving nature of these \nstandards. Unlike standards for other sectors, standards and metrics for AI models will change \nquickly as the technology advances. The DGM’s dynamic nature allows it to account for these \nchanges and regularly update these evaluation standards. \nAudit and Compliance Framework\nThe established standards would start as a voluntary program and, with the support of Congress \nthrough enacting legislation, evolve to be subject to formal audits. These audits could require:\n• Disclosure of energy efficiency data by AI companies.\n• Pilot programs to validate the feasibility and effectiveness of audit protocols.\n• Regular updates to audit criteria, informed by feedback from auditors, regulators, and \nindustry participants.\nAfter the initial pilot phase, federal procurement rules could mandate compliance with these \nstandards, incentivizing widespread adoption and fostering a culture of accountability.\nThe ENERGY STAR voluntary labeling program administered by the Environmental Protection \nAgency (EPA) in partnership with the DOE represents a similar initiative for products like \nappliances, electronics, lighting, heating, and cooling systems (U.S. Environmental Protection \nAgency, n.d.). In 2010, the Government Accountability Office (GAO) exposed vulnerabilities in the \nprogram’s self-certification process. In response, the EPA mandated third-party certification \nstarting in 2011, requiring testing by recognized labs and oversight by accreditation bodies.\nThe data center energy efficiency program can eventually evolve towards an auditable program \nwith enforceable compliance. Data center energy consumption has a material economic impact \nand a lower volume of projects when compared to a more consumer-product-oriented program \nlike ENERGY STAR.\nAccountability and Liability Mechanisms\nIf audits reveal noncompliance with the established standards, liability frameworks would come \ninto play. Remedies could include:\n80\n• Withdrawal of research funding. \n• Discontinuation of tax incentives.\n• Financial penalties or fees.\n• Mandated corrective actions.\n• Public disclosure of violations.\nGuidance on these remedies must be outlined in legislative frameworks, ensuring clarity and \nconsistency in enforcement. They can be invoked as part of pre-existing statutes or will need to \nbe approved by Congress and made into law to avoid issues with agency discretion. In case of \ndisagreement, the power to assign liability and assess damages will continue to fall to the courts. \nIn addition to explicit standards compliance audits, precedent-setting judgments will clarify \nexpectations of who is responsible for what when not clearly defined in the statutes. \nThe use of existing laws and statutes or the creation of new ones was a topic of discussion during \nour interviews. In the words of a legislative staffer, “It's […] an 80/20 approach, 80% what […] we \nhave in place already that we can apply and then accepting the fact that there are new things we \nhave to come up with, and that's kind of that 20% of something new, and that's the approach \nwe've tried to take. […] That way, we're not wasting our time coming up with new things. […] It's \nthe fine line between how much [new] is necessary.”\nImplementation of the model, on the other hand, creates an opportunity for transparency and the \nsharing of best practices in a virtuous cycle. \nSectoral and Legislative Synergy\nThe DOE and other sectoral agencies may need to update their policies for data center energy \nuse. Congress could further mandate energy efficiency requirements for AI developers and data \ncenter operators, backed by specific penalties for non-compliance. Such measures would ensure \nalignment between legislative objectives and sectoral implementation. This would have to be \nreflected in the affected agencies’ statutory mandates as defined by Congress, ensuring \nenforceability. \nLooking ahead with the Dynamic Governance Model\nThe scenario outlined above is far from fictional. Similar policy objectives have been proposed \nduring the 118th Congress as part of the House’s Department of Energy Artificial Intelligence Act \nof 2024 - U.S. Congress: H.R. 9671, 118th Cong. (2024) – and the Senate’s Department of \nEnergy AI Act - U.S. Congress: S. 4664, 118th Cong. (2024). \nThe House bill focused primarily on research funding and appropriation. It commissioned the \ncreation of a strategic plan, to be reported back to Congress in no more than one year, outlining \n81\nshort-term and long-term goals and resource needs to advance applications of AI for science, \nenergy, and national security. It was not regulatory. \nThe Senate bill established programs to advance AI research, infrastructure, workforce, and \nsecurity within the DOE mission scope. It acknowledged the department’s leading role in AI for\nnational security, science, and energy, given their 17 National Laboratories, which employ over\n40,000 scientists, engineers, and researchers with decades of experience in advanced computer \nscience research. It directs the creation in three years of shared infrastructure to support the \ndevelopment of frontier AI models. Like the House bill, it also allows for partnerships with private \nentities. The bill introduces a new Office of Critical and Emerging Technology within the Office of \nthe Under Secretary for Science and Innovation to oversee these activities and authorizes the \nappropriation of the necessary funds from fiscal years 2024 through 2032. \nWhile broader in scope, the Senate bill in this example did not focus as much on compliance and \naccountability, which are key attributes of our proposed model. It is not regulatory. By \ncomplementing the research, investment, and partnership focus from the bills with auditable and \nenforceable standards, one creates an innovation-inducing effect, much like what was discussed \nin the “Regulation-induced innovation” section. New economic incentives (or liabilities) and \nengineering constraints will lead to out-of-the-box thinking and the creation of new solutions and \ndesigns. \nThe Dynamic Governance Model’s versatility can be \nextended to diverse policy objectives, including \naddressing privacy concerns, market competition, \nand intellectual property challenges. It is designed to \nbe uniquely suited to the challenges and ideas \nidentified during the primary research and interviews \nwith both Congress and the industry. In contrast to \nthe perceived burdens of traditional regulation, the \nDynamic Governance Model’s incremental, adaptive \nnature makes it an invaluable tool for navigating \ncomplex, evolving policy landscapes.\nThis underscores the Dynamic Governance Model’s potential to translate high-level policy goals \ninto actionable, collaborative frameworks. By fostering innovation, ensuring accountability, and \nbalancing stakeholder interests, this approach can address the multifaceted challenges posed by \nAI and its associated technologies.\nIn contrast to the perceived \nburdens of traditional regulation, \nthe Dynamic Governance Model’s \nincremental, adaptive nature \nmakes it an invaluable tool for \nnavigating complex, evolving \npolicy landscapes.\n82\nA cautionary tale\nThe influence of Big Tech on the development of artificial intelligence policies is both a boon and \na burden to the broader technological ecosystem. As innovation champions, these companies are \nundeniably essential to the U.S. economy and play a pivotal role in maintaining the U.S.’s global \nAI leadership. However, their dominance has also led to market distortions, raising critical \nquestions about Congress’ role in sustaining these dynamics and the implications for competition \npolicy (Baer, 2025).\nCongressional interviewees openly acknowledged the outsized influence of major technology \nfirms on AI policymaking. One respondent noted the “subtle machinery of the Hill” through which \nBig Tech controls the legislative agenda. This influence is evident in Majority Leader Schumer’s \nAI Insight Forums, where “the likes of Elon Musk, Mark Zuckerberg, Bill Gates, and Sam Altman \nall in one room together with a few civil society advocates, sure, but across […] nine sessions; I \ndon’t think that the non-industry actors had a […] seat at the table, quite literally.” The resulting \n2024 report, entitled ‘Driving U.S. Innovation in Artificial Intelligence: A Roadmap for Artificial \nIntelligence Policy in the United States Senate,’ devoted significant attention to pro-innovation \nstrategies, with scant mention of risks like privacy and liability (United States Congress. Senate, \n2024).\nA letter to the Federal Trade Commission from a group of Senators “calling out some concerns \nabout consolidation in the AI industry, […] there was this pattern […] of big tech firms really \ninvolving themselves in the development of the AI field in pernicious but subtle ways,” as \ndescribed by a Senate staffer exemplifies how large firms consolidate their dominance while \nskirting regulatory scrutiny. Big Tech circumvents traditional antitrust oversight by hiring key \nemployees from smaller AI startups and licensing their technology, further entrenching its market \npower (Matt O’Brien & Sarah Parvini, 2024). These actions are not anomalies but part of a broader \npattern of equity deals, acquisitions, and strategic partnerships to absorb emerging competitors \nbefore they become a threat.\nDespite bipartisan interest in holding these companies accountable, Big Tech’s entrenched \ninfluence often leaves Congress in a reactive posture. The inability to enact changes to the \nantitrust laws highlights a systemic prioritization of industrial policy and national security over \ncompetition policy, allowing Big Tech to shape discussions around AI to its advantage. Our \nproposed Dynamic Governance Model seeks to leverage Big Tech’s capabilities and strengths. \nHowever, it is vital to recognize the risks of over-concentration. \nUltimately, our governance model aligns with this nuanced approach, advocating for a system \nwhere Big Tech’s power is harnessed under the supervision of government and civil society. \nHowever, this deliberate choice comes with the responsibility to ensure that policies do not merely \n83\nentrench existing power dynamics but create an equitable and competitive environment for all \nstakeholders in the AI ecosystem.\nConclusion\nEnvision a world ten years from now where the rapid rise of AI has reshaped society as profoundly \nas the stormy weather once reshaped the fate of the tugboats Montrose and T.J. Hooper. \nCoordinated global efforts steer AI’s transformative power toward solving humanity’s most \nsignificant challenges. Transparent governance and ethical standards enable breakthroughs in \ncombating climate change, eradicating diseases, and enhancing education. Workers find \nfulfillment as automation takes on repetitive tasks, fostering creativity and economic growth. \nSocieties thrive as inequalities shrink, guided by policies prioritizing equity and sustainability.\nHowever, like the tugboats caught unprepared for the storm, another future looms. AI deepens \nsocietal divides without cohesive regulation, empowering monopolies, expanding surveillance, \nand embedding biases within opaque systems. Strained resources and escalating conflicts lead \nto stagnation and mistrust. Innovation, once a beacon of hope, becomes a source of instability.\nThe tugboats’ lesson is clear: ignoring the benefits and risks of new technology can lead to \ncatastrophe, but thoughtful preparation can avert disaster. Today, we stand at a similar \ncrossroads. Our choices will determine whether we harness AI as a force for shared progress or \nface a storm of division and missed possibilities.\nThis paper has explored the need for a balanced and adaptive framework to govern artificial \nintelligence, emphasizing the unique challenges posed by this rapidly evolving technology. \nBeginning with analyzing shared goals and perspectives between Congress and industry \nstakeholders, we highlighted the convergence of interests around the need for trust, transparency, \nand safety in AI. \nWe then introduced the Dynamic Governance Model, a three-step approach that builds on \npublic-private partnerships to establish evaluation standards, develop a market ecosystem for \naudits and compliance, and ensure accountability and liability across the AI supply chain. Each \nstep draws lessons from existing regulatory models and incorporates insights from interviews with \npolicymakers and industry leaders. The model proposes a practical path forward, balancing \ninnovation with regulation, fostering global competitiveness, and embedding trust into AI systems.\nIn conclusion, the Dynamic Governance Model offers a balanced path forward for AI policy, \naddressing safety, accountability, and innovation concerns while fostering public trust. By drawing \non established regulatory frameworks - antitrust, sectoral regulation, and tort law - and adapting \nthem to AI’s unique challenges, the model provides a comprehensive approach that leverages \npublic and private expertise. The three-step framework ensures that AI governance remains \nadaptive, effective, transparent, and aligned with societal values.\n84\nAt this pivotal juncture, the U.S. stands at a crossroads. Our choices today will shape the domestic \nAI landscape and the global competitive environment for decades. By adopting this model, the \nU.S. can lead by example, demonstrating that it can balance innovation with regulation without \nstifling technological progress. This moment represents a chance for the U.S. to establish itself \nas the trusted global leader in AI governance, offering a democratic alternative to more \nauthoritarian approaches and ensuring alignment with shared international values.\nAs Shakespeare reminds us, “What is past is prologue.” The lessons learned from previous \ntechnological revolutions have prepared us for this moment. Madison’s words in Federalist No. \n51 - “If men were angels, no government would be necessary” - underscore the need for \ngovernance when power and technology intersect. Now, at this critical crossroads, we can take \nthe right turn and create a governance framework that balances innovation with accountability \nand ensures that AI’s transformative potential serves humanity’s broader interests. By doing so, \nthe U.S. will safeguard its future and build a foundation for global cooperation, trust, and \nsustainable progress.\n85\nAcknowledgments\nThis paper would not have been possible without the support of many individuals. I am deeply \ngrateful to my colleagues at the Mossavar-Rahmani Center for Business and Government, \nparticularly those in the Technology, Innovation, and Regulation Program, for providing a platform \nto explore the intersections of AI, governance, and innovation. The foundational ideas for this \nwork, rooted in systems thinking and a focus on social impact, emerged from the Advanced \nLeadership Initiative at Harvard University and were enriched through discussions with faculty \nand students from Harvard and MIT. This manuscript benefited from the expertise of research \nassistants from the Kennedy School and members of the Harvard Undergraduate Machine \nIntelligence Community, they are my co-authors.\nWhen mentioning specific names, there is always a risk of inadvertent omissions, for which I take \nfull responsibility. The individuals acknowledged here provided invaluable support through \ncoaching, mentoring, manuscript feedback, and numerous other contributions: Nicholas Ashford, \nYochai Benkler, Antonio Claudio Buffara, Alejandra Castillo, Justin Hendrix, Eugene Kimmelman, \nChris Lewis, Mark MacCarthy, Martha Minow, Christina Montgomery, Nathan Sanders, Jonathan \nSallet, Bruce Schneier, T.L. Taylor, Jim Waldo, Thomas Wheeler, and Jonathan Zittrain. My 2023 \nresearch assistants helped shape the early ideas on the interaction of technology and democracy: \nIman Alshawk, Wren Oppermann, and Andrew Steen. This group of Kennedy School master's \nstudents participated in a reading group to review the final draft: Jack Conlin, Brian Moscioni, \nGiulia Neaher, Casey Reimche, Lucas Schmuck, Darryl Slabe, and Jake Steckler. In addition to \nmy co-authors, the following members of HUMIC contributed valuable technical expertise: Omer \nMujawar, Aarna Pal-Yadav, Lily Xu, and Joshua Zyzak.\nThis working paper would not have been possible without candid discussions with 49 industry and \ncongressional leaders, who will remain anonymous under our commitment to confidentiality.\nSpecial thanks to my academic advisor, John Haigh, Lecturer in Public Policy at the Harvard \nKennedy School, and the Senior Fellows in the 2024-2025 cohort, especially Gregory Makoff for \nthe multiple discussions about data methods and analysis.\nThanks to my family and friends for putting up with my obsession with this topic over the past \nthree years - I know I have been sounding like a broken record.\nThe authors used Grammarly Pro to assist with grammar and style. All AI-generated content was\nreviewed and revised by the authors to ensure accuracy and alignment with the research findings.\nThe authors take full responsibility for any errors or omissions. \n86\nAppendix I – Approaches and Methods\nThe impetus for this work was developed during the principal author’s term as an Advanced \nLeadership Initiative Fellow at Harvard University in 2023. After a career in technology and \nbusiness, the author became increasingly concerned with polarization and radicalization and \nintrigued by technology’s role in furthering these issues. Initially imbued with a techno\u0002deterministic view of the problem, the first thrust was to try and fight a technology problem with \ntechnology – i.e., content moderation challenges would be addressed with better use of artificial \nintelligence to filter content, misinformation issues would be tacked with better content \nprovenance and attribution schemes, and so on. Upon taking a holistic and systemic view of the \nproblem, it became apparent that the socio-technical pieces of the puzzle were as important as \nthe technical ones. This realization led to a journey of visiting the incentives that fuel the \ntechnology industry today and a view that reinvention of its business models and regulation is \nneeded to allow for a safe and ethical future of the banner technology of the 21st century: AI. The \nquestion then became how to do it without harming innovation and, instead, how to use policy to \ndrive innovation in a direction that each society deems normatively correct. The nexus of the work \nthen became what policy approaches should be taken by the United States of America, the home \nof the largest AI labs and the biggest of the Big Tech firms. This working paper was enabled by a \n2024-2025 fellowship with the Technology, Innovation, and Regulation Program at the Mossavar\u0002Rahmani Center for Business and Government of the Harvard Kennedy School.\nWith research assistance from Master of Public Policy candidates at the Kennedy School and in \npartnership with the Harvard Undergraduate Machine Intelligence Community (HUMIC)—all co\u0002authors here—we set out to examine U.S. digital tech policy decisions and explore a path forward \nthat bridges the tech industry, as drivers of innovation, with Congress, as representatives of the \nAmerican society. Following a broad literature review, instead of taking a normative position, we \nengaged in a discovery journey to challenge our preconceived ideas about the topic and let a \npicture emerge from our primary research. In addition to the sources cited in this work, we have \nconducted primary research in two areas:\n1. 23 long-form, qualitative interviews with members of the U.S. Congress or their staff\n2. 23 long-form, qualitative interviews with technology industry leaders working in AI\nThese non-attributable interviews, with signed confidentiality and consent forms, averaged thirty \nminutes to an hour and generated hundreds of pages of transcripts, allowing the complexity of \nthe subject to emerge. When quoted in the text, they are marked in italicized font and may have \nbeen edited for style and brevity while keeping the original meaning of the quote. These interviews \naimed to map each stakeholder’s interests, agendas, and perceptions to identify overlaps and \nexplore collaborative solutions.\n87\nFor the congressional interviews, participants were selected based on seniority and experience \nin AI policy. We spoke to 257individuals, members of Congress, current and former staff working \nin members’ offices, as professional committee staff, or as fellows from outside organizations. \nEveryone interviewed had several years – and in some cases, decades – of experience working \nin the field of Artificial Intelligence, with about half having policy expertise and the other half having \ndomain-specific technical expertise. Topics of conversation included AI experience, AI policy \nviews, experience working on AI in Congress, assessment of institutional and legal capacities of \nU.S. institutions to respond to AI development, overall dynamics in Congress, engagement with \nindustry and other stakeholders, engagement with international stakeholders, and the state of AI \npolicy in the U.S. today.\nFor the industry interviews, participants were selected based on seniority and expertise in AI \ntechnology. This cohort included 24\n7\nindustry leaders, encompassing investors, engineers, \nstartup founders, researchers, academics, board members, and executives from AI companies. \nParticipants were purposefully chosen to represent a broad variety of organizations, ranging from \nBig Tech to industry-aligned groups, startups, consulting firms, research labs, and more. All \nparticipants are engaged within the U.S. AI ecosystem and will be directly impacted by future U.S. \nAI policy. Additionally, most have significant global engagement in their professional roles.\nEach interviewee possessed multiple years—some with decades—of experience in Artificial \nIntelligence, primarily from the business perspective, though many also brought technical, policy, \nor ethics expertise. Topics explored during the interviews included their professional experience \nwith AI, perspectives on AI policy, insights into the AI ecosystem, key concerns, and challenges \nrelated to navigating AI policy uncertainty. Other areas of discussion involved internal procedures \nand working processes for addressing the unique aspects of AI, assessments of institutional and \nlegal capacities of U.S. entities to respond to AI developments, interactions with the U.S. \nCongress, identification of legislative frameworks that could be effective in the future, engagement \nwith international stakeholders, and evaluation of the current state of AI policy in the U.S.\nWhile our work was being done, the 118th Congress of the United States was ending. During their \ntenure, 150 AI-related bills have been proposed, and none have been enacted into law. We used \nthe Brenner Center for Justice Artificial Intelligence Legislation Tracker as our source for AI bill \ninventory (https://www.brennancenter.org/our-work/research-reports/artificial-intelligence\u0002legislation-tracker). In parallel, 636 State Bills were introduced, and dozens have been approved \nand are now State Law (https://www.multistate.ai/artificial-intelligence-ai-legislation). In \npartnership with HUMIC, we used Machine Learning and Artificial Intelligence techniques to \nidentify and cluster patterns in the thousands of pages covered by the 150 Federal bills. Appendix \nII includes the details of this work. The analysis of the state legislation is not in the scope of this \nproject.\n7\nThe difference between number of interviews and interviewees accounts for interviews which included more than one person.\n88\nWe also analyzed “public materials”, such as podcasts, public domain interviews, and public \nlectures. Our insights were drawn from the primary sources listed above, the literature review, \nand the machine learning analytical work of the legislative activity during the 118th Congress. \n89\nAppendix II – Analysis of AI bills (118th Congress)\nThe primary objective of this analysis is to evaluate the breadth and depth of legislative activity \nrelated to AI within the 118th Congress, providing insights into emerging policy priorities, areas of \nbipartisan consensus, and potential gaps in regulatory approaches. By examining the content, \npartisanship, and progress of individual legislation, we aim to better understand the current scope \nof proposed AI legislation during this period (January 3, 2023, to January 3, 2025) and assess \nhow lawmakers responded and prioritized challenges posed by AI.\nThis appendix offers a structured analysis of:\n● The key policy areas addressed by AI-related bills.\n● Patterns of sponsorship across partisan affiliations and geographic regions.\n● The progression and legislative success rates of AI-related proposals.\n● The extent of alignment between legislative efforts and industry perspectives on AI \ngovernance.\nData Methodology\nData Collection\nWe began by utilizing the Congress.gov API to retrieve all bills introduced into the House and \nSenate from the 118th Congress. An example output from the API call is displayed in Error! \nReference source not found.8. From the information provided by the API, we store the bill \nnumber, origin chamber, bill title, bill text, summary, and list of updates. The congressional API \nalso includes its own broader policy area categorization that can be found on the API’s website \n(Congress.gov, n.d.).\nFrom this dataset, fetched bills were filtered down to only the 150 bills in the 118th Congress that \nwere manually identified by researchers at the Brennan Center as AI-related or AI-adjacent \n(Brennan Center for Justice, 2024). For each identified bill, we gathered information about \nsponsorship patterns, committee assignments, and legislative progression. For each sponsor and \ncosponsor, we gathered district information, party affiliation, the day they sponsored, and whether \nthey were an original sponsor or signed on after the bill was first introduced. This allowed us to \nassess the level of bipartisan cooperation and the likelihood of legislative success based on \nhistorical trends.\n8 At the start of the 119th Congress, the routes for api.congress.gov were changed. API calls are now done using gpo.congress.gov.\n90\nFigure 17 Example of returned API request data for H.R.4814 in tabular form\nData Cleaning and Preprocessing\nAlthough the congressional API provides a detailed list of how bills progress in both chambers of \nCongress, the descriptions used to represent the status and progression of the bills contain \ninconsistencies. For example, the first markup of a bill can be entered in the API as “Mark-up”, \n“Markup”, or “Mark up”, while still representing the same action. We used regular expressions to \nharmonize different descriptions that had the same outcome to categorize bills into ten levels of \nprogress, starting from the bill being introduced (Stage 0) to the bill being signed into law (Stage \n10):\nStage 0 – Introduced\nStage 1 – Referred to Committee\nStage 2 – Referred to Subcommittee\n91\nStage 3 – Reported Out of Committee\nStage 4 – Placed on Calendar\nStage 5 – Floor Consideration\nStage 6 – Passed One Chamber\nStage 7 – Received by Second Chamber\nStage 8 – Passed Second Chamber\nStage 9 – Presented to President\nStage 10 – Signed into Law\nIt is worth noting that the legislative process is not always conducted in an interactive, easy-to\u0002follow fashion. Whenever the last action on the bill did not match a specific stage, we referred to \nthe most recent stage that fit the categorization above.\nTwo bills from the dataset did not match any of the regular expressions:\n● S.B.3696, the DEFIANCE Act of 2024, passed the Senate and was “Held at the desk.” in \nthe House, placing it at stage 7.\n● S.B.2103, the Intelligence Authorization Act for Fiscal Year 2024, had the last action “By \nSenator Warner from Select Committee on Intelligence filed written report. Report No. \n118-59. Additional views filed.” The second-to-last action was the bill being placed on the \nSenate Legislative Calendar on 6/22/2023, placing it at stage 4.\nFigure 18 Code snippet of regular expressions used to preprocess progress data.\nHandling Missing Data\nAll bills were searched for missing data. Some bills were identified with missing summary fields \nand were supplemented with manual reviews in addition to the analysis of the bill text. Additional \nbills had duplicate metadata in the sponsor and cosponsor list, which were pruned to ensure \naccuracy.\n92\nTagging Legislation\nSelection of Policy Attributes\nTo identify legislative trends, we categorized bills into major policy domains, including national \nsecurity, privacy and consumer protection, workforce impact, AI governance frameworks, and \nliability/accountability measures outlined in the list below. These categories were developed \nbased on legislative text analysis and consultation with existing taxonomies of AI policy concerns\nas, for example, those from Figure 5.\n● Market efficiency and power concentration (antitrust)\n● Safety\n● Responsible and ethical AI\n● National security\n● Industrial policy\n● Public Interest\n● Labor\n● Copyright\n● International collaboration\n● Elections\nNLP-Based Bill Classification\nTo enhance the categorization of AI-related legislation, we implemented Natural Language \nProcessing (NLP) techniques to classify bill text on a first pass-through before conducting manual \nverification. This approach allowed us to systematically identify key policy themes, dominant \ntopics, and relevant stakeholders embedded in legislative texts. The NLP methods applied \nincluded Term Frequency-Inverse Document Frequency (TF-IDF), Latent Dirichlet Allocation \n(LDA), and Named Entity Recognition (NER).\nTF-IDF was used to identify first the most important keywords and rank policy focus areas. We \ntokenized each bill’s text into individual words and phrases and removed stop words (e.g., “the,”\n“and,” “is”) to improve relevance. After removing stop words, we applied TF-IDF weighting to score \nterms based on their frequency in a specific bill relative to their frequency across all bills in the \ndataset. Higher TF-IDF scores indicated terms uniquely important to each bill, revealing key \nlegislative concerns.\n93\nFigure 19 Distribution of Bill Categories (TF-IDF)\nLatent Dirichlet Allocation (LDA) was then used to allow the corpus to suggest dominant policy \nthemes within AI-related legislation using an unsupervised machine learning approach. We \nidentified latent structures within legislative text, allowing us to classify bills based on their primary \nfocus areas. Bill text was tokenized to remove stop words, punctuation, and special characters to \nbe converted into vectorized representations following the same process and vectors as TF-IDF. \nThis allowed the methods to ensure that frequently occurring words that are uninformative (e.g., \n\"bill,\" \"committee,\" \"section\") did not distort results. The model then extracted the most probable \nwords per topic, developing its own similar topics.\n94\nFigure 20 Comparison of Bill Classifications: TF-IDF vs Natural Topic Discovery\nFrom a comparison of the proposed topics analyzed in TF-IDF and the topics examined in LDA, \nwe observe that the most densely populated intersections show strong consistency across both \ntechniques. For example, bills tagged as related to elections in TF-IDF overlap significantly with \nthe proposed topics of government oversight and public safety, and bills tagged as market \nefficiency in TF-IDF overlap significantly with regulatory frameworks in LDA. \nValidation with GPT\nGPT has been demonstrated to perform on par and exceed the performance of Bidirectional \nEncoder Representations from Transformers (BERT) (Wang et al., 2024; Zhao et al., 2024) on \nclassification and labeling tasks (Nguyen & Nguyen, 2024; Pelaez et al., 2024). We validated the \nproposed labels using GPT-4o’s API using the same bill text processing techniques.\n95\nSponsorship & Partisanship \nTo analyze sponsorship and partisanship Figure 24 and Figure 25, we extracted the processed \nsponsorship data using the Congress.gov API, identifying key attributes for each AI-related bill:\n● Primary Sponsor Information:\n○ Legislator name, party affiliation (Democratic, Republican, Independent), and \nchamber (House/Senate)\n○ Whether the legislator had previously introduced AI-related legislation\n● Cosponsor Information:\n○ Number of cosponsors, including the same information as the primary sponsor\n○ Whether cosponsors joined at the time of the bill’s introduction or later in the \nlegislative process\n● Committee Assignments:\n○ Committees responsible for reviewing each bill, including major AI-focused \ncommittees such as the\n■ House Science, Space & Technology Committee\n■ Senate Commerce, Science & Transportation Committee\n■ House Judiciary Committee\n■ Senate Armed Services Committee\n● Bill Progression Tracking:\n○ Legislative stage (e.g., introduced, passed committee, enacted into law).\n○ Relationship between bipartisan sponsorship and legislative success rates.\nOverall, Democrats introduced twice as many bills related to AI in the 118th Congress compared \nto Republicans, although the gap was more pronounced in the Senate and less pronounced in \nthe House. \nFigure 21 Distribution of party affiliation of primary bill sponsor\n96\nCommittee Influence on AI Legislation\nIn Congress, the Senate Commerce, Science & Transportation Committee reviewed nearly half \nof all bills referred to committee in the Senate. However, the House was split more evenly, with a \nnear-majority of bills going for review split between the House Energy and Commerce Committee \nand the House Science, Space, and Technology Committee. This is likely due to the split of \ncommerce from the science committee in the House, whereas the two are merged in the Senate. \nSee Figure 29 Most active committees.\nLegislative Success\nAlthough a large number of bills related to AI were introduced, only 11.3% made it out of \ncommittee, calculated by evaluating the progress data as defined in the data preprocessing step.\nSee Figure 30 Bill progression.\nVisualization\nThe data visualizations from Figure 22 to Figure 35 were generated by the previously discussed \nanalyses.\n97\nFigure 22 Timing of AI bill introduction\n98\nFigure 23 AI bill sponsorship by chamber\n99\nFigure 24 Bipartisanship by chamber of Congress\n100\nFigure 25 Bipartisanship analysis - Congress total\n101\nFigure 26 AI bill sponsorship by state\n102\nFigure 27 Most active members (House of Representatives)\nFigure 28 Most active members (Senate)\n103\nFigure 29 Most active committees\n104\nFigure 30 Bill progression\nFigure 31 Bill policy attributes\n105\nFigure 32 Primary bill policy attribute by party\n106\nFigure 33 Top 3 bill policy attributes by party\n107\nFigure 34 Primary policy attributes for the 17 bills placed on the legislative calendar (most progress)\nFigure 35 Bills placed on the legislative calendar during the 118th Congress (total = 17)\n108\nReferences\nAbdin, M., Aneja, J., Behl, H., Bubeck, S., Eldan, R., Gunasekar, S., Harrison, M., Hewett, R. J., \nJavaheripi, M., Kauffmann, P., Lee, J. R., Lee, Y. T., Li, Y., Liu, W., Mendes, C. C. T., \nNguyen, A., Price, E., Rosa, G. de, Saarikivi, O., … Zhang, Y. (2024). Phi-4 Technical \nReport (No. arXiv:2412.08905). arXiv. https://doi.org/10.48550/arXiv.2412.08905\nAbramov, M. (2024, July 5). Regional and International AI Regulations and Laws in 2024. \nKeymakr. https://keymakr.com/blog/regional-and-international-ai-regulations-and-laws\u0002in-2024/\nAcemoglu, D., & Restrepo, P. (2020). The wrong kind of AI? Artificial intelligence and the future \nof labour demand. Cambridge Journal of Regions, Economy and Society, 13(1), 25–35. \nhttps://doi.org/10.1093/cjres/rsz022\nACLU. (2023, October 30). ACLU Statement on President Biden’s Executive Order on Artificial \nIntelligence. American Civil Liberties Union. https://www.aclu.org/press-releases/aclu\u0002statement-on-president-bidens-executive-order-on-artificial-intelligence\nAI Policy Institute. (2023, October 30). Vast Majority of US voters of All Political Affiliations \nSupport President Biden’s Executive Order on AI - AI Policy Institute. \nhttps://theaipi.org/poll-biden-ai-executive-order-10-30/\nAibin, M., & Simic, M. (2024, July 8). Energy Consumption of ChatGPT Responses | Baeldung \non Computer Science. https://www.baeldung.com/cs/chatgpt-large-language-models\u0002power-consumption\nAlms, N. (2024, November 8). Trump promised to repeal Biden’s AI executive order—Here’s \nwhat to expect next. Nextgov.Com. https://www.nextgov.com/artificial\u0002intelligence/2024/11/trump-promised-repeal-bidens-ai-executive-order-heres-what\u0002expect-next/400934/\n109\nAmazon. (2024, October 16). Amazon signs agreements for innovative nuclear energy projects \nto address growing energy demands. \nhttps://www.aboutamazon.com/news/sustainability/amazon-nuclear-small-modular\u0002reactor-net-carbon-zero\nAmerican Economic Liberties Project. (n.d.). Big Tech on Trial. American Economic Liberties \nProject. Retrieved January 5, 2025, from https://www.economicliberties.us/tech-lawsuit\u0002timelines/\nAmodei, D., & Pottinger, M. (2025, January 6). Opinion | Trump Can Keep America’s AI \nAdvantage. WSJ. https://www.wsj.com/opinion/trump-can-keep-americas-ai-advantage\u0002china-chips-data-eccdce91\nAndreessen, M. (2023). The Techno-Optimist Manifesto. CE Think Tank Newswire.\nAnthropic. (2024, October 31). The case for targeted regulation. \nhttps://www.anthropic.com/news/the-case-for-targeted-regulation\nArs Technica. (2011, November 15). The 40th birthday of—Maybe—The first microprocessor, \nthe Intel 4004. Ars Technica. https://arstechnica.com/information\u0002technology/2011/11/the-40th-birthday-ofmaybethe-first-microprocessor/\nAshford, N. A., & Hall, R. P. (2011). The Importance of Regulation-Induced Innovation for \nSustainable Development. Sustainability, 3(1), 270–292. \nhttps://doi.org/10.3390/su3010270\nAshford, N. A., & Heaton, G. R. (1976). Environmental and safety regulations: Reasons for their \nadoption and possible effects on technological innovation. Environmental Policy and \nLaw, 1(4), 172–176. https://doi.org/10.1016/S0378-777X(76)80123-0\nASHFORD, N., AYERS, C., & STONE, R. (1985). USING REGULATION TO CHANGE THE \nMARKET FOR INNOVATION. The Harvard Environmental Law Review : HELR, 9(2), \n419–466.\n110\nAtkinson, R. D. (2022). How the IT Sector Powers the US Economy. \nhttps://itif.org/publications/2022/09/19/how-the-it-sector-powers-the-us-economy/\nBachman, J. (2025, January 3). FCC net neutrality rule among first to fall in Loper Bright’s \naftermath. Legal Dive. https://www.legaldive.com/news/fcc-net-neutrality-rule-among\u0002first-to-fall-in-loper-brights-aftermath/736429/\nBaer, B. (2025, January 7). What the US Learned and the EU Should Consider About National \nChampions. ProMarket. http://www.promarket.org/2025/01/07/what-the-us-learned-and\u0002the-eu-should-consider-about-national-champions/\nBarroso, L. R. (2024, December 16). Artificial Intelligence: Promises, Risks, and Regulation. \nhttps://www.hks.harvard.edu/centers/carr/publications/artificial-intelligence-promises\u0002risks-and-regulation\nBennett Moses, L. (2007). Recurring Dilemmas: The Law’s Race to Keep Up With \nTechnological Change. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.979861\nBergmann, D. (2024, April 5). What is mixture of experts? | IBM. \nhttps://www.ibm.com/think/topics/mixture-of-experts\nBick, A., Blandin, A., & Deming, D. J. (2024). The Rapid Adoption of Generative AI (Working \nPaper No. 32966). National Bureau of Economic Research. \nhttps://doi.org/10.3386/w32966\nBlaszczyk, M., McGovern, G., & Stanley, K. D. (2024). Artificial Intelligence Impacts on \nCopyright Law. RAND Corporation. https://www.rand.org/pubs/perspectives/PEA3243-\n1.html\nBraun, M., Vellery, A., & Benizri, I. (2024, October 23). Measures in Support of Innovation in the \nEuropean Union’s AI Act – AI Regulatory Sandboxes. \nhttps://www.wilmerhale.com/en/insights/blogs/wilmerhale-privacy-and-cybersecurity\u0002law/20241023-measures-in-support-of-innovation-in-the-european-unions-ai-act-ai\u0002regulatory-sandboxes\n111\nBrennan Center for Justice. (2024, December 2). Artificial Intelligence Legislation Tracker | \nBrennan Center for Justice. https://www.brennancenter.org/our-work/research\u0002reports/artificial-intelligence-legislation-tracker\nBresnahan, T. F., & Trajtenberg, M. (1995). General purpose technologies “Engines of growth”? \nJournal of Econometrics, 65(1), 83–108. https://doi.org/10.1016/0304-4076(94)01598-T\nBroad, W. J. (1987, May 12). DOES THE FEAR OF LITIGATION DAMPEN THE DRIVE TO \nINNOVATE? The New York Times. https://www.nytimes.com/1987/05/12/science/does\u0002the-fear-of-litigation-dampen-the-drive-to-innovate.html\nBuchanan, B. (2020, August). The AI Triad and What It Means for National Security Strategy. \nCenter for Security and Emerging Technology. \nhttps://cset.georgetown.edu/publication/the-ai-triad-and-what-it-means-for-national\u0002security-strategy/\nCarvao, P. (2023, May 11). Resetting the Rules for Tech to Preserve the Public Interest | \nTechPolicy.Press. Tech Policy Press. https://techpolicy.press/resetting-the-rules-for\u0002tech-to-preserve-the-public-interest\nCarvao, P. (2024a). The dual imperative: Innovation and regulation in the AI era. International \nJournal of Technology Policy and Law, 3(3), 236–251. \nhttps://doi.org/10.1504/IJTPL.2024.142861\nCarvao, P. (2024b, October 26). AI and the Environment [Substack newsletter]. Tech and \nDemocracy. https://carvao.substack.com/p/ai-and-the-environment\nCarvao, P. (2025, January 14). Sounding the alarm in AI and National Security [Substack \nnewsletter]. Tech and Democracy. https://carvao.substack.com/p/ai-natsec\nCarvao, P., & Ancheva, S. (2024, October 4). California, Chevron Doctrine, and the State of AI \nPolicy in the United States [Substack newsletter]. Tech and Democracy. \nhttps://carvao.substack.com/p/california-chevron-doctrine-and-the\n112\nCBRE. (2024, June 24). Global Data Center Trends 2024. \nhttps://www.cbre.com/insights/reports/global-data-center-trends-2024\nChan, A., Salganik, R., Markelius, A., Pang, C., Rajkumar, N., Krasheninnikov, D., Langosco, L., \nHe, Z., Duan, Y., Carroll, M., Lin, M., Mayhew, A., Collins, K., Molamohammadi, M., \nBurden, J., Zhao, W., Rismani, S., Voudouris, K., Bhatt, U., … Maharaj, T. (2023). \nHarms from Increasingly Agentic Algorithmic Systems. 651–666. \nhttps://doi.org/10.1145/3593013.3594033\nChatGPT Revenue and Usage Statistics (2024). (n.d.). Business of Apps. Retrieved December \n10, 2024, from https://www.businessofapps.com/data/chatgpt-statistics/\nCheung, S. (2024, March 21). Central Washington Ranks Eighth, Seattle Tenth for North \nAmerican Data Center Leasing in 2023. https://www.cbre.com/press-releases/central\u0002washington-ranks-eighth-seattle-tenth-for-north-american-data-center-leasing-in-2023\nChollet, F. (2024, December 20). OpenAI o3 Breakthrough High Score on ARC-AGI-Pub. ARC \nPrize. https://arcprize.org/blog/oai-o3-pub-breakthrough\nClark, J., & Hadfield, G. K. (2019). Regulatory Markets for AI Safety. arXiv (Cornell University). \nhttps://doi.org/10.48550/arxiv.2001.00078\nCodd, E. F. (1970). A relational model of data for large shared data banks. Commun. ACM, \n13(6), 377–387. https://doi.org/10.1145/362384.362685\nCong, W., & Yeping, Y. (2023, October 18). China launches Global AI Governance Initiative, \noffering an open approach in contrast to US blockade—Global Times. \nhttps://www.globaltimes.cn/page/202310/1300092.shtml\nCongress.gov. (n.d.). Browse U.S. Legislative Information by Policy Area—119th Congress \n(2025-2026) [Legislation]. Retrieved February 5, 2025, from \nhttps://www.congress.gov/browse/policyarea\n113\nCzarnecka, K. (2024, May 8). AI’s carbon footprint—How does the popularity of artificial \nintelligence affect the climate? Plan Be Eco. https://planbe.eco/en/blog/ais-carbon\u0002footprint-how-does-the-popularity-of-artificial-intelligence-affect-the-climate/\nDaum, J. (2023, April 14). Overview of Draft Measures on Generative AI. China Law Translate. \nhttps://www.chinalawtranslate.com/overview-of-draft-measures-on-generative-ai/\nDavenport, C., Singer, B., Mehta, N., Lee, B., Mackay, J., Modak, A., Corbett, B., Miller, J., Hari, \nT., Ritchi, J., Delaney, M., Revich, J., Jaiya, J., Venugopal, V., Cash, N., & Halferty, O. \n(2024, April 29). Generational Growth: AI, data centers and the coming US power \ndemand surge. https://www.goldmansachs.com/insights/goldman-sachs\u0002research/generational-growth-ai-data-centers-and-the-coming-us-power-demand-surge\nDean, J. (2023, October 24). Technology (A Special Report)—Will AI Bring Utopia or Dystopia? \nA Venture Capitalist Picks Utopia: Vinod Khosla says artificial intelligence will cause \ngreat abundance—More than enough to share. Wall Street Journal. ProQuest Central; \nProQuest One Business; The Wall Street Journal. http://search.proquest.com.ezp\u0002prod1.hul.harvard.edu/newspapers/technology-special-report-will-ai-bring\u0002utopia/docview/2880628171/se-2?accountid=11311\nDell’Acqua, F., McFowland, E., Mollick, E. R., Lifshitz-Assaf, H., Kellogg, K., Rajendran, S., \nKrayer, L., Candelon, F., & Lakhani, K. R. (2023). Navigating the Jagged Technological \nFrontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker \nProductivity and Quality. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.4573321\nEccles, R. G. (2024, August 31). Microsoft Can Take The Lead In Small Modular Reactors For \nPowering AI. Forbes. https://www.forbes.com/sites/bobeccles/2024/08/31/microsoft-can\u0002take-the-lead-in-small-modular-reactors-for-powering-ai/\nEggers, W., Walsh, S., Joergensen, C., & Kishnani, P. (2023, March 23). Regulation that \nenables innovation. Deloitte Insights. \n114\nhttps://www2.deloitte.com/us/en/insights/industry/public-sector/government\u0002trends/2023/regulatory-agencies-and-innovation.html\nEncyclopedia of Artificial Intelligence: The Past, Present, and Future of AI. (2021). ProtoView.\nEpoch AI. (2024, June 19). Data on Notable AI Models. Epoch AI. https://epoch.ai/data/notable\u0002ai-models\nEuropean Union. (n.d.). CE marking – obtaining the certificate, EU requirements. Your Europe. \nRetrieved January 4, 2025, from https://europa.eu/youreurope/business/product\u0002requirements/labels-markings/ce-marking/index_en.htm\nFinkle, N. (2025, January 13). NVIDIA Statement on the Biden Administration’s Misguided “AI \nDiffusion” Rule. NVIDIA Blog. https://blogs.nvidia.com/blog/ai-policy/\nFriedman, L. M. (Lawrence M. (2023). Law, science, and technology: Historical and social \ncontext. Rowman & Littlefield, an imprint of The Roman & Littlefield Publishing Group, \nInc.\nFuture of Life Institute. (n.d.). Article 57: AI Regulatory Sandboxes | EU Artificial Intelligence Act. \nRetrieved January 9, 2025, from https://artificialintelligenceact.eu/article/57/\nGhaffary, S. (2024, September 24). OpenAI Pitched White House on Unprecedented Data \nCenter Buildout. Yahoo Finance. https://finance.yahoo.com/news/openai-pitched-white\u0002house-unprecedented-000550020.html\nGoldman Sachs. (2024, May 14). AI is poised to drive 160% increase in data center power \ndemand. https://www.goldmansachs.com/insights/articles/AI-poised-to-drive-160-\nincrease-in-power-demand\nGornet, M., & Maxwell, W. (2024). The European approach to regulating AI through technical \nstandards. Internet Policy Review, 13(3), 1–27. https://doi.org/10.14763/2024.3.1784\nGovTrack.us. (2025, February). Historical Statistics about Legislation in the U.S. Congress -. \nGovTrack.Us. https://www.govtrack.us/congress/bills/statistics\n115\nGreen, A., Tai, H., Noffsinger, J., Sachdeva, P., Bhan, A., & Sharma, R. (2024, September 17). \nData centers and AI: How the energy sector can meet power demand | McKinsey. \nhttps://www.mckinsey.com/industries/private-capital/our-insights/how-data-centers-and\u0002the-energy-sector-can-sate-ais-hunger-for-power\nGreen-Lowe, J. (2024, May 16). The Senate’s AI Roadmap to Nowhere | Center for AI Policy | \nCAIP. Center for AI Policy. https://www.centeraipolicy.org/work/the-senates-ai-roadmap\u0002to-nowhere\nGutiérrez, J. D. (2024, August 16). Consultation paper on AI regulation: Emerging approaches \nacross the world—UNESCO Digital Library. \nhttps://unesdoc.unesco.org/ark:/48223/pf0000390979\nHarari, Y. N. (2024). Nexus: A brief history of information networks from the Stone Age to AI\n(First U.S. edition.). Random House.\nHarris, L. (2023). Artificial Intelligence: Overview, Recent Advances, and Considerations for the \n118th Congress. CRS Report. Congressional Research Service.\nHendrix, J., Miller, G., & Lennett, B. (2024, May 15). US Senate AI Working Group Releases \nPolicy Roadmap | TechPolicy.Press. Tech Policy Press. https://techpolicy.press/us\u0002senate-ai-working-group-releases-policy-roadmap\nHerdman, R. C. (1993). Technologies Underlying Weapons of Mass Destruction.\nHochreiter, S., & Schmidhuber, Jг. (1997). Long Short-Term Memory. Neural Computation, 9(8), \n1735–1780. https://doi.org/10.1162/neco.1997.9.8.1735\nHoulihan Lockey. (2024, Summer). Real Estate Market Update. Houlihan Lokey. \nhttps://hl.com/insights/real-estate-market-update/\nHouse Committee on Science, Space, and Technology. (2024, December 17). House Bipartisan \nTask Force on Artificial Intelligence Delivers Report. House Committee on Science \nSpace & Tech - Republicans. https://science.house.gov/2024/12/house-bipartisan-task\u0002force-on-artificial-intelligence-delivers-report\n116\nHurst, J. W. (2022). CHAPTER EIGHT TECHNOLOGY AND THE LAW: THE AUTOMOBILE. \nWisconsin Law Review, 3, 463–531.\nInteresse, G. (2024, May 30). China Releases New Draft Regulations for Generative AI. China \nBriefing News. https://www.china-briefing.com/news/china-releases-new-draft\u0002regulations-on-generative-ai/\nITI. (2025, January 13). ITI: New Export Controls Threaten to Discourage the Use of U.S. \nTechnology - Information Technology Industry Council. https://www.itic.org/news\u0002events/news-releases/iti-new-export-controls-threaten-to-discourage-the-use-of-u-s\u0002technology\nIyer, P., & Hendrix, J. (2024, December 20). Reactions to the Bipartisan US House AI Task \nForce Report | TechPolicy.Press. Tech Policy Press. https://techpolicy.press/reactions\u0002to-the-bipartisan-us-house-ai-task-force-report\nKagan, J. (2023, January 30). Underwriters Laboratories (UL): Meaning, Overview, History. \nInvestopedia. https://www.investopedia.com/terms/u/underwriters-laboratories-ul.asp\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, \nA., Wu, J., & Amodei, D. (2020). Scaling Laws for Neural Language Models (No. \narXiv:2001.08361). arXiv. https://doi.org/10.48550/arXiv.2001.08361\nKrishan, N. (2023, November 9). Tech groups push back on Biden AI executive order, raising \nconcerns that it could crush innovation. FedScoop. https://fedscoop.com/tech-groups\u0002push-back-on-biden-ai-executive-order-raising-concerns-that-it-could-crush-innovation/\nKrizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep \nconvolutional neural networks. Advances in Neural Information Processing Systems, 25.\nLeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., & Jackel, L. D. \n(1989). Backpropagation Applied to Handwritten Zip Code Recognition. Neural \nComputation, 1(4), 541–551. https://doi.org/10.1162/neco.1989.1.4.541\n117\nLibrary of Congress. (n.d.). Congress.gov | Library of Congress [Legislation]. Retrieved January \n5, 2025, from https://www.congress.gov/\nLiou, J. (2023, September 13). What are Small Modular Reactors (SMRs)? [Text]. IAEA. \nhttps://www.iaea.org/newscenter/news/what-are-small-modular-reactors-smrs\nLynch, S. (2024, April 15). AI Index: State of AI in 13 Charts. https://hai.stanford.edu/news/ai\u0002index-state-ai-13-charts\nMacCarthy, M. (2023). Regulating digital industries: How public oversight can encourage \ncompetition, protect privacy, and ensure free speech. Brookings Institution Press.\nMandel, G. N. (2007). History Lessons for a General Theory of Law and Technology.\nMarcus, G. (2024, November 9). CONFIRMED: LLMs have indeed reached a point of \ndiminishing returns [Substack newsletter]. Marcus on AI. \nhttps://garymarcus.substack.com/p/confirmed-llms-have-indeed-reached\nMathieu, E. (2024, May 21). The price of computer storage has fallen exponentially since the \n1950s. Our World in Data. https://ourworldindata.org/data-insights/the-price-of-computer\u0002storage-has-fallen-exponentially-since-the-1950s\nMatt O’Brien & Sarah Parvini. (2024, July 12). US senators call out Big Tech’s new approach to \npoaching talent, products from smaller AI startups. AP News. \nhttps://apnews.com/article/ai-artificial-intelligence-acquihires-amazon-adept-wyden\u0002fa3cd0502a757e5a9ccb83d00bf9ad55\nMcCarthy, J., Minsky, M. L., Rochester, N., & Shannon, C. E. (2006). A Proposal for the \nDartmouth Summer Research Project on Artificial Intelligence: August 31, 1955. AI \nMagazine, 27(4), 12–14. ProQuest Central; ProQuest One Business; Social Science \nPremium Collection.\nMinistry of Foreign Affairs The People’s Republic of China. (2024, September 30). Promoting \nDevelopment for All and Bridging the AI Divide_Ministry of Foreign Affairs of the \n118\nPeople’s Republic of China. \nhttps://www.mfa.gov.cn/eng/wjbzhd/202409/t20240930_11501255.html\nMoore, G. E. (2006). Cramming more components onto integrated circuits, Reprinted from \nElectronics, volume 38, number 8, April 19, 1965, pp. 114 ff. IEEE Solid-State Circuits \nSociety Newsletter, 11(3), 33–35.\nMultistate.ai. (n.d.). Artificial Intelligence (AI) Legislation. Multistate.Ai. Retrieved January 10, \n2025, from https://www.multistate.ai/artificial-intelligence-ai-legislation\nNational Conference of State Legislatures. (2024, September 9). Artificial Intelligence 2024 \nLegislation. https://www.ncsl.org/technology-and-communication/artificial-intelligence\u00022024-legislation\nNewman, J., Dubal, V., Viljoen, S., Ajunwa, I., Guggenberger, N., Bietti, E., Jackson, J., & Tan, \nJ. (2023, December 4). Seven Reactions to Biden’s Executive Order on Artificial \nIntelligence. LPE Project. https://lpeproject.org/blog/seven-reactions-to-bidens\u0002executive-order-on-artificial-intelligence/\nNguyen, C., & Nguyen, L.-M. (2024). Employing Label Models on ChatGPT Answers Improves \nLegal Text Entailment Performance (No. arXiv:2401.17897). arXiv. \nhttps://doi.org/10.48550/arXiv.2401.17897\nNihill, C. (2024, December 20). Bipartisan Senate bill would establish an AI safety office in \nCommerce. FedScoop. https://fedscoop.com/bipartisan-senate-bill-ai-safety-office\u0002commerce/\nNTIA. (2024a, March 27). AI Accountability Policy Report | National Telecommunications and \nInformation Administration. https://www.ntia.gov/issues/artificial-intelligence/ai\u0002accountability-policy-report\nNTIA. (2024b, July 30). Dual-Use Foundation Models with Widely Available Model Weights \nReport | National Telecommunications and Information Administration. \n119\nhttps://www.ntia.gov/programs-and-initiatives/artificial-intelligence/open-model-weights\u0002report\nNVIDIA Blackwell Architecture. (n.d.). NVIDIA. Retrieved December 10, 2024, from \nhttps://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/\nNYU’s Center for Social Media and Politics. (2024, December 12). The State of State \nTechnology Policy: 2024 Report. NYU’s Center for Social Media and Politics. \nhttps://csmapnyu.org/impact/policy/the-state-of-state-technology-policy-2024-report\nOECD. (2023, July 12). Regulatory sandboxes in artificial intelligence. OECD. \nhttps://www.oecd.org/en/publications/regulatory-sandboxes-in-artificial\u0002intelligence_8f80a0e6-en.html\nOECD. (2024). Better regulation and innovation. OECD. https://www.oecd.org/en/topics/better\u0002regulation-and-innovation.html\nOklo Inc. (n.d.). Oklo Inc. - Investors—Governance—Board of directors. \nhttps://oklo.com/investors/governance/board-of-directors/default.aspx\nOpenAI. (2024, September 12). Learning to Reason with LLMs. \nhttps://openai.com/index/learning-to-reason-with-llms/\nOpenAI. (2025, January 21). Announcing The Stargate Project. \nhttps://openai.com/index/announcing-the-stargate-project/\nPelaez, S., Verma, G., Ribeiro, B., & Shapira, P. (2024). Large-scale text analysis using \ngenerative language models: A case study in discovering public value expressions in AI \npatents. Quantitative Science Studies, 5(1), 153–169. \nhttps://doi.org/10.1162/qss_a_00285\nPichai, S., Hassabis, D., & Kavukcuoglu, K. (2024, December 11). Introducing Gemini 2.0: Our \nnew AI model for the agentic era. Google. https://blog.google/technology/google\u0002deepmind/google-gemini-ai-update-december-2024/\n120\nPlumer, B. (2024, September 20). Three Mile Island Plans to Reopen as Demand for Nuclear \nPower Grows. The New York Times. https://www.nytimes.com/2024/09/20/climate/three\u0002mile-island-reopening.html\nPope, A. (2024, April 10). NYT v. OpenAI: The Times’s About-Face. Harvard Law Review. \nhttps://harvardlawreview.org/blog/2024/04/nyt-v-openai-the-timess-about-face/\nPORTER M. (1991). America’s Green Strategy. Scientific American, 86-.\nRadford, A. (2018). Improving language understanding by generative pre-training.\nRaworth, K. (2017). Why it’s time for Doughnut Economics. IPPR Progressive Review, 24(3), \n216–222. https://doi.org/10.1111/newe.12058\nRoberts, H., & Hine, E. (2023, September 27). The future of AI policy in China | East Asia \nForum. https://eastasiaforum.org/2023/09/27/the-future-of-ai-policy-in-china/\nRosenblatt, F. (1958). The perceptron: A probabilistic model for information storage and \norganization in the brain. Psychological Review, 65(6), 386–408. \nhttps://doi.org/10.1037/h0042519\nRumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by back\u0002propagating errors. Nature, 323(6088), 533–536. https://doi.org/10.1038/323533a0\nSharma, S. (2024, December 26). DeepSeek-V3, ultra-large open-source AI, outperforms Llama \nand Qwen on launch. VentureBeat. https://venturebeat.com/ai/deepseek-v3-ultra-large\u0002open-source-ai-outperforms-llama-and-qwen-on-launch/\nSheehan, M. (2023, July 10). China’s AI Regulations and How They Get Made. Carnegie \nEndowment for International Peace. \nhttps://carnegieendowment.org/research/2023/07/chinas-ai-regulations-and-how-they\u0002get-made?lang=en\nShumailov, I., Shumaylov, Z., Zhao, Y., Gal, Y., Papernot, N., & Anderson, R. (2023). The \nCurse of Recursion: Training on Generated Data Makes Models Forget. arXiv (Cornell \nUniversity). https://doi.org/10.48550/arxiv.2305.17493\n121\nSmith, B. (2025, January 3). The Golden Opportunity for American AI. Microsoft On the Issues. \nhttps://blogs.microsoft.com/on-the-issues/2025/01/03/the-golden-opportunity-for\u0002american-ai/\nSokler, B., Hecht, A., & Tamotsu Fjeld, C. (2024, December 19). FY 2025 NDAA Passes with AI \nProvisions, Awaiting Biden’s Signatur. https://natlawreview.com/article/congress-passes\u0002defense-bill-ai-provisions-ai-washington-report\nState of California Department of Justice. (2025, January 13). Attorney General Bonta Issues \nLegal Advisories on the Application of California Law to AI. State of California -\nDepartment of Justice - Office of the Attorney General. https://oag.ca.gov/news/press\u0002releases/attorney-general-bonta-issues-legal-advisories-application-california-law-ai\nStrickland, E. (2024, April 15). Stanford’s 2024 AI Index Tracks Generative AI and More—IEEE \nSpectrum. https://spectrum.ieee.org/ai-index-2024\nTerrell, M. (2024, October 14). New nuclear clean energy agreement with Kairos Power. \nGoogle. https://blog.google/outreach-initiatives/sustainability/google-kairos-power\u0002nuclear-energy-agreement/\nText - H.R.5009 - 118th Congress (2023-2024): Servicemember Quality of Life Improvement \nand National Defense Authorization Act for Fiscal Year 2025 (2023-07-27). (2024, \nDecember 20). [Legislation]. https://www.congress.gov/bill/118th-congress/house\u0002bill/5009/text\nText - H.R.6216 - 116th Congress (2019-2020): National Artificial Intelligence Initiative Act of \n2020 (2020-03-12). (2020, March 12). [Legislation]. https://www.congress.gov/bill/116th\u0002congress/house-bill/6216/text\nText - H.R.6395 - 116th Congress (2019-2020): William M. (Mac) Thornberry National Defense \nAuthorization Act for Fiscal Year 2021 (2020-03-26). (2021, January 1). [Legislation]. \nhttps://www.congress.gov/bill/116th-congress/house-bill/6395/text\n122\nThe Republican Party Platform, 2024. (2024, July 15). Ballotpedia. \nhttps://ballotpedia.org/The_Republican_Party_Platform,_2024\nThe White House. (1995, November 8). Technology and Economic Growth. \nhttps://clintonwhitehouse5.archives.gov/textonly/WH/EOP/OSTP/html/techgrow.html#intr\no\nThe White House. (2023a, July 21). FACT SHEET: Biden-Harris Administration Secures \nVoluntary Commitments from Leading Artificial Intelligence Companies to Manage the \nRisks Posed by AI. The White House. https://www.whitehouse.gov/briefing\u0002room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures\u0002voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the\u0002risks-posed-by-ai/\nThe White House. (2023b, October 30). Executive Order on the Safe, Secure, and Trustworthy \nDevelopment and Use of Artificial Intelligence. The White House. \nhttps://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive\u0002order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/\nThe White House. (2023c, November 1). FACT SHEET: Vice President Harris Announces New \nU.S. Initiatives to Advance the Safe and Responsible Use of Artificial Intelligence. The \nWhite House. https://www.whitehouse.gov/briefing-room/statements\u0002releases/2023/11/01/fact-sheet-vice-president-harris-announces-new-u-s-initiatives-to\u0002advance-the-safe-and-responsible-use-of-artificial-intelligence/\nThe White House. (2025a, January 13). FACT SHEET: Ensuring U.S. Security and Economic \nStrength in the Age of Artificial Intelligence. The White House. \nhttps://www.whitehouse.gov/briefing-room/statements-releases/2025/01/13/fact-sheet\u0002ensuring-u-s-security-and-economic-strength-in-the-age-of-artificial-intelligence/\nThe White House. (2025b, January 14). Executive Order on Advancing United States \nLeadership in Artificial Intelligence Infrastructure. The White House. \n123\nhttps://www.whitehouse.gov/briefing-room/presidential-actions/2025/01/14/executive\u0002order-on-advancing-united-states-leadership-in-artificial-intelligence-infrastructure/\nThe White House. (2025c, January 20). Initial Rescissions Of Harmful Executive Orders And \nActions. The White House. https://www.whitehouse.gov/presidential\u0002actions/2025/01/initial-rescissions-of-harmful-executive-orders-and-actions/\nThe White House. (2025d, January 23). Removing Barriers to American Leadership in Artificial \nIntelligence. The White House. https://www.whitehouse.gov/presidential\u0002actions/2025/01/removing-barriers-to-american-leadership-in-artificial-intelligence/\nTobey, D., Carr, A., Bigg, C., Fulton, S., Ge, A., & Hoffner, K. (2024, September 12). China\u0002releases-AI-safety-governance-framework | DLA Piper. https://www.dlapiper.com/en\u0002us/insights/publications/2024/09/china-releases-ai-safety-governance-framework\nTremayne-Pendelly, A. (2024, December 4). Amid the A.I. Boom, These States Have Become \nData Center Hubs. Observer. https://observer.com/2024/12/ai-demand-where-data\u0002centers-using-most-energy/\nTuring, A. M. (1950). I.—COMPUTING MACHINERY AND INTELLIGENCE. Mind, LIX(236), \n433–460. https://doi.org/10.1093/mind/LIX.236.433\nUnited States Congress. Senate. (2024). Driving U.S. innovation in artificial intelligence: A \nroadmap for artificial intelligence policy in the United States Senate. United States \nSenate.\nU.S. Chamber of Commerce Institute for Legal Reform. (2024, May 15). What Is a Private Right \nof Action. ILR. https://instituteforlegalreform.com/blog/what-is-a-private-right-of-action/\nU.S. Congress. Committee on the Judiciary Subcommittee on Antitrust. (2020). Investigation of \nCompetition in Digital Markets: Majority Staff Report and Recommendations, Part I. \nCongressional Publications.\nU.S Department of Energy. (2024, December 20). DOE Releases New Report Evaluating \nIncrease in Electricity Demand from Data Centers. Energy.Gov. \n124\nhttps://www.energy.gov/articles/doe-releases-new-report-evaluating-increase-electricity\u0002demand-data-centers\nU.S. Environmental Protection Agency. (n.d.). Our History | ENERGY STAR. Retrieved January \n22, 2025, from https://www.energystar.gov/about/how-energy-star-works/history\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & \nPolosukhin, I. (2023). Attention Is All You Need (No. arXiv:1706.03762). arXiv. \nhttp://arxiv.org/abs/1706.03762\nWang, Y., Qu, W., & Ye, X. (2024). Selecting Between BERT and GPT for Text Classification in \nPolitical Science Research (No. arXiv:2411.05050; Version 1). arXiv. \nhttps://doi.org/10.48550/arXiv.2411.05050\nWarren S. McCulloch & Walter Pitts. (1943). A logical calculus of the ideas immanent in nervous \nactivity. The Bulletin of Mathematical Biophysics, 5(4), 115–133. \nhttps://doi.org/10.1007/bf02478259\nWeise, K., & Tamayo, J. (2024, December 25). An A.I. Boom Makes Electricians Flock to \nCentral Washington. The New York Times. \nhttps://www.nytimes.com/2024/12/25/technology/ai-data-centers-electricians.html\nWiesinger, J., Marlow, P., & Vuskovic, V. (n.d.). Agents. Retrieved January 7, 2025, from \nhttps://www.kaggle.com/whitepaper-agents\nWiggers, K. (2025, January 11). Researchers open source Sky-T1, a “reasoning” AI model that \ncan be trained for less than $450. TechCrunch. \nhttps://techcrunch.com/2025/01/11/researchers-open-source-sky-t1-a-reasoning-ai\u0002model-that-can-be-trained-for-less-than-450/\nWorld Nuclear News. (2024, December 18). Oklo signs power agreement with data centre \ndeveloper. World Nuclear News. https://world-nuclear-news.org/articles/oklo-signs\u0002power-agreement-with-data-centre-developer\n125\nWyn Davies, C., & Dennis, G. (2024, December 20). Getty Images v Stability AI: The \nimplications for UK copyright law and licensing. Pinsent Masons. \nhttps://www.pinsentmasons.com/out-law/analysis/getty-images-v-stability-ai-implications\u0002copyright-law-licensing\nYee, L., Chui, M., Roberts, R., & Xu, S. (2024, July 24). Why AI agents are the next frontier of \ngenerative AI | McKinsey. https://www.mckinsey.com/capabilities/mckinsey-digital/our\u0002insights/why-agents-are-the-next-frontier-of-generative-ai\nZhao, H., Chen, Q. P., Zhang, Y. B., & Yang, G. (2024). Advancing Single- and Multi-task Text \nClassification through Large Language Model Fine-tuning (No. arXiv:2412.08587; \nVersion 1). arXiv. https://doi.org/10.48550/arXiv.2412.08587",
    "length": 305214,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Community Colleges Advancing Economic Mobility Volume",
    "url": "https://www.pw.hks.harvard.edu/post/community-colleges-advancing-economic-mobility-volume",
    "text": "Community Colleges Advancing Economic Mobility Volume\ntop of page\n[\n] \n[Get Updates] \nSearch\n# Data and Technology in Action: Community Colleges Advancing Economic Mobility\n* ![Writer: Project on Workforce Team] \nProject on Workforce Team\n* Feb 19, 2025\n* 5 min read\nUpdated:May 1, 2025\n*The Project on Workforce and Education Design Lab release**case studies of four community colleges deploying promising data and technology practices to improve economic outcomes for their students.*\n![] \n*Edited by Joseph B. Fuller, Kerry McKittrick, and Amanda Holloway*\n[\nDownload the Volume\n] \nAcross the country, community colleges serve as critical engines of workforce development—partnering with regional employers, creating career pathways, and expanding opportunities for historically marginalized students. But as the labor market evolves and emerging technologies like generative AI reshape industries, how can these institutions ensure their students are prepared to thrive in the workforce?\nThis volume explores how four diverse community colleges—Hudson County Community College (NJ), Community College of Aurora (CO), South Texas College (TX), and Riverland Community College (MN)—are using labor market information and student outcomes data, industry partnerships, and cutting-edge technology to help students access high-growth careers.\nResearchers Simba Gandari, Courtnei Sanders, Arnold Lopez and Ram Hernandez collaborated with the community colleges to conduct rigorous qualitative research over a period of five months. Nearly 150 individuals across the four regions—including college leaders, faculty, staff, students, employers, policymakers, and other experts—participated in interviews or focus groups to inform this work. The result is four detailed case studies, each of which highlights promising data and technology practices and other successful initiatives at the institution.\n1. **LMI and student outcomes data are invaluable for guiding strategic resource allocation.**The community colleges featured in this volume use LMI and student performance data to build, modify, and sunset academic programs and services. By investing in data partnerships and regular program reviews, leaders can focus on programs and supports that propel learners into high-growth careers. Tracking student outcomes post-completion remains a challenge, but emerging technologies and state data partnerships can lead to improvements.\n1. **Real-time LMI, while a powerful resource, has limitations–and there is no substitute for employer relationships.**Job postings data provides key insights into labor market trends, like occupation demand, skills and degree requirements, job mobility, and compensation. That said, the data can be difficult to interpret, and it is highly imperfect. There are often “phantom postings,” it relies on overly-complicated job descriptions, and it is not reliably forward-looking. College leaders highlighted the importance of validating and supplementing real-time LMI with meaningful employer relationships to ensure alignment between education offerings and workforce needs.\n1. **Innovative college leadership is essential.**It would seem to be a platitude, but each of the institutions captured in this volume relies on a forward-thinking leader. They put workforce development at the core of the college mission, prioritize equity, and embrace new practices and technologies. These presidents are unafraid to take risks and embrace data-driven decisions, even when it disrupts existing structures and processes. Each leader has positioned their institution as a central player in their economic ecosystem that serves learners, workers, and regional employers.\n1. **Institutions are adopting new technology tools and platforms, particularly in student services.**The colleges in this volume are proactively leveraging digital tools, platforms, and analytics capabilities to enhance student services and academic offerings. Career coaching tools, chatbots, and data dashboards provide real-time analyses of job opportunities and enable college staff to identify and support struggling students. Colleges are integrating new technologies into curricula and leveraging emerging tools to address equity gaps and keep up with the rapid pace of change, but they are in the very early days of responding to advances in generative AI.\n1. **Leveraging data and technology strategically requires significant institutional investment.**Access to real-time LMI can be costly; and it requires data experts to analyze and interpret. Technology tools and platforms also come at a cost, and mounting resource constraints make it difficult for schools to use data to the fullest. Colleges must prioritize their capacity to harness data–including infrastructure, processes, culture, and personnel–in order to maximize its impact. This will, in turn, help them improve resource allocation.\n1. **Colleges are working to standardize and streamline processes and overcome historical departmental distinctions.**Leaders recognize that changes in the labor market and technology transcend historical separations between departments, faculty, and credit/non-credit programs. But aligning data from different sources, sharing information across colleges, and standardizing processes remains a challenge for many institutions. It can be difficult to scale innovative practices and track relationships across previously distinct, ever-evolving institutions, but they are embracing new tools and strategies to overcome these hurdles.\n2. **State policy shows promise catalyzing advanced data practices.**State policies and investments in college capacities, such as Texas HB 8, can encourage meaningful data use and collaboration across regional stakeholders. State agencies can act as key data partners, providing LMI and other support to help institutions track and improve economic outcomes. On the other hand, state processes, like lengthy curriculum approvals, can be a hindrance to colleges as they work to adapt to changing market demands.\n3. **Colleges are embracing data-backed programs to serve diverse student populations.**They are investing in initiatives like stackable credentials and dual enrollment programs to meet the needs of adult learners and high school students, who make up an increasing share of the student population.\n[\nDownload the Volume\n] \n##### About the Project\nData and Technology in Action: Advancing Economic Mobility at Community Colleges is a joint initiative between the Project on Workforce and Education Design Lab which seeks to improve the ways community colleges leverage real-time labor market information (LMI) and emerging technologies to advance economic mobility for their students.[Learn more about the project.] \n##### About the Project on Workforce at Harvard\nThe Project on Workforce is an interdisciplinary, collaborative project between the Harvard Kennedy School’s Malcolm Wiener Center for Social Policy, the Harvard Business School Managing the Future of Work Project, and the Harvard Graduate School of Education. The Project produces and catalyzes basic and applied research at the intersection of education and labor markets for leaders in business, education, and policy. The Project’s research aims to help shape a postsecondary system of the future that creates more and better pathways to economic mobility and forges smoother transitions between education and careers.\n##### About Education Design Lab\nEducation Design Lab (the Lab) is a national nonprofit that co-designs, prototypes, and tests education-to-workforce models through a human-centered design process focused on understanding learners’ experiences, addressing equity gaps in higher education, and connecting learners to economic mobility. The Lab believes human-centered design allows colleges and universities to map and galvanize their existing strengths to meet the needs of the learners they serve. The Lab’s process also shows higher education leaders how to consider the needs of employers, using the curriculum and program design as a gateway to make skills more visible to learners and employers alike.\n**Please direct inquiries to:**Kerry McKittrick (kerry\\_mckittrick@gse.harvard.edu)\n**Suggested Citation**: Joseph B. Fuller, Kerry McKittrick and Amanda Holloway (Eds.). (February 2025). Data &amp; Technology In Action: Community Colleges Advancing Economic Mobility. Published by the Harvard Kennedy School.\n## Related Posts\n[See All] \n[What Works in Healthcare Career and Technical Education: Early Signals from Four Regional Partnerships] \nThis report provides an early look at four new healthcare-focused high schools, examining who they serve, early indicators of student success, and the career pathways, work-based learning, and postsec\n[\n![The Generative AI Adoption Tracker] \n] \n[The Generative AI Adoption Tracker] \n[\n![Building the Agile Community College] \n] \n[Building the Agile Community College] \nbottom of page\n",
    "length": 8973,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Computational Policy Lab",
    "url": "https://policylab.hks.harvard.edu/projects/pingpong.html",
    "text": "Computational Policy Lab: PingPong\n[Skip to content] [![Computational Policy Lab logo] \nComputational Policy Lab\n] \n![Menu] [Projects] [People] [Publications] \n[Projects] [People] [Publications] \n# PingPong\nA software platform carefully designed for AI-driven learning.\n![PingPong logo] \nPingPong is a tool for using large language models (LLMs) for teaching and learning. Instructors can use it to create and share custom bots for specific tasks, like serving as a virtual teaching assistant with access to course documents.\nPingPong is built on top of GPT-4o, a large language model developed by OpenAI. There are several advantages of using PingPong over ChatGPT. First, PingPong is specifically designed to facilitate learning, meaning that it won’t simply provide students with answers. Second, moderators can view de-identified chats, which helps instructors understand how their students are using the tool and potentially tailor class content accordingly. Third, none of the information entered on PingPong will be used by OpenAI to train their models, ensuring user data is kept private.\nInitial evaluations of PingPong’s use in Harvard courses indicate that 70% of students found it to be an effective support for achieving their learning goals. We are currently piloting PingPong in a nationwide experiment to evaluate the tool’s effect on learning outcomes in under-resourced colleges. This pilot is in early stages, but participant feedback showcases the tool’s promise. One faculty member said “…students’ positive reactions have been overwhelming, and I have observed a significant improvement in their final exam scores.”\nLearn more at[pingpong.hks.harvard.edu].\n## Contributors\n[![Vinzenz Aichlseder headshot]] \n[Vinzenz Aichlseder] \nResearcher\n[![Sharad Goel headshot]] \n[Sharad Goel] \nFaculty Co-Director\n[![Evangelos Kassos headshot]] \n[Evangelos Kassos] \nEngineer\n[![Michaela Kocher headshot]] \n[Michaela Kocher] \nResearcher\n[![Casey McLaughlin headshot]] \n[Casey McLaughlin] \nResearcher\n[![Joe Nudell headshot]] \n[Joe Nudell] \nLead Engineer\n[![Teddy Svoronos headshot]] \n[Teddy Svoronos] \nResearcher\n[![Charlotte Tuminelli headshot]] \n[Charlotte Tuminelli] \nExecutive Director",
    "length": 2198,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "SPH Generative AI Sandbox Now Available",
    "url": "https://hcsra.sph.harvard.edu/news/sph-generative-ai-sandbox-now-available",
    "text": "[Skip to main content] \n\n- [Main Menu] \n- [Utility Menu] \n- [Search] \n\n[![University Logo]] \n\n[HARVARD.EDU] \n\n[HOME] / [NEWS] /\n\nJanuary 8, 2024\n\nHUIT is pleased to announce that the pilot version of the [Generative AI Sandbox] is now ready for you to use. The AI Sandbox has been developed to enable Harvard community members to securely access Large Language Models (LLM). The AI Sandbox offers a single interface that enables access to four different Large Language Models (LLM). It provides a “walled-off,” secure environment in which to experiment with generative AI, mitigating many security and privacy risks and ensuring the data entered will not be used to train any public AI tools. For more information, refer to the [Harvard Generative AI informational page].\n\nSee also: [ALL News Items], [Systems] \n\n### **HCSRA NEWS FILTERS**\n\n## Filter News by Topic\n\n- [ALL News Items (899)] \n- [COVID-19 (17)] \n- [Data Management & Sharing (DMS) (49)] \n- [Events & Trainings (78)] \n- [HCSRA Hints (37)] \n- [New HCSRA Pages (13)] \n- [Policies & Procedures (226)] \n- [Return to Campus (7)] \n- [Sponsor Updates (491)] \n- [Staff Heads Up (182)] \n- [Systems (314)] \n\n## Filter News by Month\n\n- [April 2025] \n(20)\n- [March 2025] \n(13)\n- [February 2025] \n(8)\n- [January 2025] \n(24)\n- [December 2024] \n(14)\n\n- 1 of 15\n- [»] \n\nCopyright © 2025 The President and Fellows of Harvard College \\| [Accessibility] \\| [Digital Accessibility] \\| [Report Copyright Infringement]",
    "length": 1460,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "",
    "url": "https://dash.harvard.edu/bitstreams/e8640578-5cc1-4593-a3f4-5f554d91f3fb/download",
    "text": "The Context-Aware Quantization\nDesign Space: Unlocking Scalable\nTraining and Inference for Large AI\nModels\nCitation\nYang, Emma. 2025. The Context-Aware Quantization Design Space: Unlocking Scalable\nTraining and Inference for Large AI Models. Bachelors Thesis, Harvard University\nEngineering and Applied Sciences.\nLink\nhttps://dash.harvard.edu/handle/1/42719251\nTerms of use\nThis article was downloaded from Harvard University’s DASH repository, and is made\navailable under the terms and conditions applicable to Other Posted Material (LAA), as set\nforth at\nhttps://harvardwiki.atlassian.net/wiki/external/NGY5NDE4ZjgzNTc5NDQzMGIzZWZhMGFlOWI2M2EwYTg\nAccessibility\nhttps://accessibility.huit.harvard.edu/digital-accessibility-policy\nShare Your Story\nThe Harvard community has made this article openly available.\nPlease share how this access benefits you. Submit a story\nT H E CO N T E X T-AWA R E Q U A N TI Z AT I O N D E S I G N SPA C E :\nU N L O C K I N G S C A L A B L E T R A I N I N G A N D I N F E R E N C E F O R\nL A R G E A I M O D E L S\na thesis presented by\nEmma A. Yang\nto\nThe Department of Computer Science\nin partial fulfillment of the requirements\nfor the degree of\nBachelor of Arts (A.B.)\nin the subject of\nComputer Science\nHarvard University\nCambridge, Massachusetts\nMarch 2025\nEmma A. Yang: The Context-Aware Quantization Design Space: Unlocking\nScalable Training and Inference for Large AI Models, © March 2025\nTo my parents and my grandparents.\n謹以此論文獻給我的父母、祖父母及外祖父母，\n多謝你們對我無比的愛及支持\nTo accomplish great things, we must not only act, but also dream, not only\nplan, but also believe.\n— Anatole France\nLe vent se lève, il faut tenter de vivre !\n— Paul Valéry\n\nA B S T R A C T\nThe rapid development of large AI models capable of remarkable\nperformance in a diverse array of complex tasks has inspired its\nwidespread application and deployment, making the demand for scal\u0002able AI models that support fast training and inference increasingly\nintense. Model quantization has emerged as a critical and widely ap\u0002plied technique for reducing the computational and memory demands\nof training and inference of large deep learning models. Recently,\nadvances in GPU tensor cores for acceleration of low-precision float\u0002ing point computation have driven adoption of quantized training,\nin addition to quantization at inference time. In tandem, the emer\u0002gence of new architectures like state space models as alternatives\nto transformers and quadratic attention demand a diversification of\nour understanding of quantization dynamics beyond ad-hoc, model\u0002specific solutions. For a given model, dataset, and downstream task–\nthe context of quantization–we are faced with a combinatorially large\nand complex design space, within which any choice can have dras\u0002tic implications on the stability of quantized training and accuracy\ndegradation at inference time. This thesis proposes advances towards\na unified framework for structuring the problem space of quantization,\nevaluating and realizing the computational gains of quantization, and\nfor exhaustively and comprehensively characterizing quantization in\na context-aware manner. We define two design principles–systems\nperformance and model performance–as foundational objectives for\nquantized optimization and inference, and propose evaluative metrics\nand diagnostic experiments for understanding quantization dynamics\nfrom every dimension for a wide distribution of data regimes. As a\nproof of concept of our framework, we conduct an empirical study of\nFP8 quantized training of the Mamba-2 state space model, revealing\nv\nnew insights into the impact of quantization on its numerical stability,\ngradient norm dynamics, temporal and layer-wise quantization error\npropagation, and catastrophic degradation phenomena.\nvi\nA C K N O W L E D G M E N T S\nI am deeply grateful for and humbled by the community at Harvard\nand beyond who have taught, advised, supported, and raised me into\nthe student, researcher, computer scientist, and human being that I\nam today. A page of my thesis is not nearly enough to thank them\nfor their wisdom, empathy, kindness, and guidance over the past four\nyears of my undergraduate education.\nI would first like to thank my advisor, Prof. Stratos Idreos. I remem\u0002ber the first time I came to his office hours as a sophomore in CS\n165 to discuss research, with a question that I am quite sure sounded\nrather trivial and strange. Nevertheless, he listened and encouraged\nme to keep thinking, just as he has done for over two years, nurturing\nmy curiosity and appetite to keep solving hard problems.\nI am indebted to the faculty members, advisors, and mentors who\nhave been my village at Harvard, including (but certainly not limited\nto): Prof. Eddie Kohler, Prof. Nada Amin, Prof. Susanne Yelin, Prof.\nCynthia Dwork, Dean Rebecca Nesson, Dr. Adam Hesterberg, Dr.\nYaron Meirovitch, and Prof. Jeff Lichtman. I will always cherish the\ngenerosity and encouragement they have shown me from the first day\nI met them. I particularly have Eddie to thank, first as a wonderful\ninstructor of CS 61, but in subsequent semesters as a much valued\nadvisor, for always indulging my fascinations, the office hours turned\nhours long, and for introducing me to the world of systems.\nI would also like to thank my thesis readers, Prof. Nada Amin and\nProf. Vijay Janapa Reddi, for their support throughout this process,\nas well as Sanket Purandare for his indispensable mentorship of my\nresearch in DASLab.\nI will always be thankful for my friends for the late nights laughing\nin dining halls, Cambridge adventures, cups of coffee, absorption\nvii\nof my ranting, patience, and, most importantly, their unwavering\nsupport.\nMy greatest debt of gratitude will always be to my family and to\nmy parents, for their love, for their guidance, and for instilling in me\nevery day the desire to shoot for the moon and the belief that I can\nachieve anything.\nviii\nC O N T E N T S\n1 introduction 1\n2 background 7\n2.1 Transformers and Self-Attention 7\n2.2 Linear Attention 8\n2.3 State Space Models 9\n2.3.1 Mamba: Selective State Space Models 11\n2.3.2 Mamba-2: the State Space Duality 12\n2.3.3 The Mamba-2 Architecture 14\n2.4 Quantization 15\n2.4.1 Quantization Formats & Mechanics 16\n2.4.2 Quantization Techniques 18\n2.4.3 Quantization Granularity 19\n3 related work 21\n3.1 Quantization for Mamba 21\n3.1.1 Activation Outliers in Mamba 22\n3.1.2 Quantization Techniques for Mamba 22\n3.2 Precision-Aware Scaling Laws for Quantization 24\n3.3 FP8 Training & Inference 26\n3.4 Quantization Precision Scheduling 27\n4 characterizing the landscape of quantization 29\n4.1 Architectural Developments for Scalable AI 30\n4.2 Challenges in Understanding Quantization 31\n4.3 Data Dependence of the Quantization Design Space 33\n4.4 New Challenges Posed by FP8 Training 35\n4.5 Structuring the Problem Space of Quantization 37\n4.6 Conclusion 38\n5 towards a unified strategy for context-aware quan\u0002tization 41\nix\n5.1 Performance Engineering for FP8 Training & Infer\u0002ence 41\n5.1.1 Float8 Training and Inference in PyTorch 42\n5.1.2 Benchmarking FP8 Training and Inference 43\n5.1.3 Profiling FP8 Inference 45\n5.1.4 Optimizations for Efficient FP8 Computation 46\n5.2 Characterizing Model Performance for FP8 Training &\nInference 49\n5.2.1 Metrics for Model Performance in Quantized\nFinetuning 50\n5.3 Experimental Methodology 52\n5.3.1 Diagnosing Quantization Instability with Gradi\u0002ent Norms 52\n5.4 Conclusion 53\n6 case study of fp8 quantized training for mamba2 55\n6.1 Data Regimes 55\n6.2 Experimental Setup 56\n6.3 FP8 Loss Parity 57\n6.3.1 Layerwise Clustering in Gradient Norm Behav\u0002ior 58\n6.4 Effect of FP8 Quantization on Downstream Task Perfor\u0002mance 62\n6.5 Scheduling FP8 Training 66\n6.6 Conclusion 68\n7 conclusion 71\nAppendix 73\na experimental results for fp8 quantized finetun\u0002ing of mamba2 75\na.1 Loss Parity Experiments 75\na.2 Gradient Norms 77\na.3 Downstream Task Performance 86\na.4 Scheduled FP8 Training Experiments 93\nx\nbibliography 97\nxi\nL I S T O F F I G U R E S\nFigure 1 8-bit floating point formats come in two encod\u0002ings, each with different balances of exponent\nand mantissa bits for different dynamic ranges\n[35]. 17\nFigure 2 Quantization can be performed at different\ngranularities. d is the model size and h is the\nnumber of heads in one attention block [37]. 20\nFigure 3 FP8 quantization without systems optimiza\u0002tion of additional casting overhead causes sig\u0002nificant slowdowns at inference time. Slow\u0002down is expressed for each model size and\nbatch size as a proportion of per-iteration in\u0002ference latency for the BF16 model. 44\nFigure 4 FP8 training exhibits similar overhead issues\nthat hinder the realization of computational\ngains from FP8 matrix multiplications. 44\nFigure 5 A profile of BF16 inference on the Mamba2-\n780M model for a batch size of 32 demonstrates\nthat, though the BF16 GEMM kernel has some\nkernel launch overhead, there are no interme\u0002diate kernels called between the preceding Lay\u0002erNorm kernel and the GEMM kernel for the\nlinear layer. The time from the completion of\nthe LayerNorm kernel to the completion of the\nGEMM kernel is 1.539 ms. 45\nxii\nFigure 6 In contrast to figure 5, this profile of FP8 infer\u0002ence on the same 780M parameter model with\nbatch size of 32 shows that many kernels, per\u0002forming scaling and casting operations on the\ninput tensors before the matrix multiplication,\nare launched before the FP8 GEMM kernel is\ncalled. Therefore, although the FP8 GEMM ker\u0002nel itself is 1.88× faster than the BF16 GEMM\nkernel, the entire projection operation from the\ncompleting of the LayerNorm to the completion\nof the projection takes 2.884 ms, nearly twice\nas long. 46\nFigure 7 Using the TorchInductor compiler fuses the 15-\n20 individual casting and scaling kernels pre\u0002ceding FP8 GEMM kernels into four operations\nwith custom Triton kernels. The use of fused\nkernels limits the end-to-end time of the pro\u0002jection operation to 1.464 ms. 47\nFigure 8 Benchmarking data for Mamba2 inference with\nFP8 and operation fusion with torch.compile\nwith max-autotune mode. Fusing scaling oper\u0002ations allows FP8 inference to be competitive\nwith BF16 inference, but still only provides up\nto 5% of speedup over higher precision. 48\nFigure 9 Mamba2 1.3B FP8 and BF16 learning curves\nacross learning rates for the xP3 Chinese rea\u0002soning dataset demonstrate that loss parity can\nbe achieved for FP8 finetuning. 58\nxiii\nFigure 10 Mamba2 1.3B FP8 and BF16 learning curves\nacross learning rates for the Orca-Math grade\nschool math dataset reveal loss spikes that may\nbe caused by quantization error in FP8 quan\u0002tized training, as they do not manifest in iden\u0002tical BF16 runs. 59\nFigure 11 Mamba2 1.3B FP8 and BF16 learning curves\nacross learning rates for the CodeTester dataset\nsuggest that loss instabilities occur differently\nwith respect to tokens processed when instabil\u0002ities occur based on training precision. 60\nFigure 12 Loss and gradient norm trajectories for BF16\nand FP8 training of Mamba2 1.3B on the Code\u0002Tester dataset. Although the BF16 training run\nshows greater fluctuations in gradient norms\nacross layers (around the 11Kth step), compar\u0002ing the two layer group-wise patterns demon\u0002strates that the magnitudes of late layers’ gra\u0002dients are amplified by FP8 training. 61\nFigure 13 Loss and gradient norm trajectories for BF16\nand FP8 training of Mamba2 1.3B on the Orca\u0002Math dataset visually illustrates the phenomenon\nof clustering of later layer gradient norms to\u0002wards the center of the norm distribution. 61\nFigure 14 Loss and gradient norm trajectories for BF16\nand FP8 training of Mamba2 1.3B on the xP3\nChinese reasoning dataset, like Figure 13, il\u0002lustrates the clustering effect of late gradient\nnorms. 62\nFigure 15 FP8 training promotes gradient norm cluster\u0002ing of the final layer group towards the gradi\u0002ent norm mean when considering norms step\u0002wise. 62\nxiv\nFigure 16 Improvement after BF16 and FP8 finetuning\ncompared to baseline pretrained model perfor\u0002mance on relevant downstream tasks for Orca\u0002Math, xP3 (Chinese reasoning), and CodeTester\ndatasets illustrates how the three datasets rep\u0002resent three different data regimes based on\ntask difficulty and dataset learnability. 63\nFigure 17 Model accuracy divergence on downstream\ntasks between full-precision and 8-bit floating\npoint quantized inference, grouped by train\u0002ing precision (BF16 or FP8), demonstrates that\nFP8 training does not show a clear benefit for\nthe robustness of models to quantization er\u0002ror. Positive bar values indicate that quantized\ninference performed better than unquantized\ninference. Bars are grouped by dataset and are\nin order of increasing learning rate. 64\nFigure 18 Model accuracy divergence on downstream\ntasks for 8-bit floating point quantized models\nbased on training precision. FP8 training tends\nto cause degradations in best quantized model\naccuracy, with the degree of accuracy degra\u0002dation being positive correlated with model\nsize. 64\nFigure 19 Catastrophic degradation events are more com\u0002mon in optimization pathways with FP8 train\u0002ing compared to BF16. 65\nFigure 20 Scheduled quantization for BF16 → FP8 fine\u0002tuning of Mamba2 1.3B on the xP3 dataset\n(learning rate 1 × 10−4) demonstrates that the\nchange in gradient norm behavior can be at\u0002tributable to the conversion to 8-bit floating\npoint precision. 66\nxv\nFigure 21 Scheduled quantization for BF16 → FP8 fine\u0002tuning of Mamba2 1.3B on the Orca-Math dataset\n(learning rate 5 × 10−3) reveals that starting 8-\nbit floating point optimization can introduced\nnumerical instabilities that manifest loss spikes\nand divergences coupled with gradient norm\nexplosions both at the conversion boundary\nand later on in training. 67\nFigure 22 Scheduled quantization for BF16 → FP8 fine\u0002tuning of Mamba2 1.3B on the CodeTester Python\ndataset (learning rate 1 × 10−4) demonstrates\nthat gradient norm discontinuities can occur\neven in the absence of loss spikes or diver\u0002gences. 67\nFigure 23 Learning curves for Mamba2 models on the\nOrca-Math grade school math dataset 75\nFigure 24 Learning curves for Mamba2 models on the\nCodeTester Python dataset 76\nFigure 25 Learning curves for Mamba2 models on the\nxP3 Chinese reasoning dataset 76\nFigure 26 Layer-wise gradient norms and loss curves for\nxP3 fine-tuning of Mamba2 780M. 77\nFigure 27 Layer-wise gradient norms and loss curves for\nxP3 fine-tuning of Mamba2 1.3B. 78\nFigure 28 Layer-wise gradient norms and loss curves for\nxP3 fine-tuning of Mamba2 2.7B. 79\nFigure 29 Layer-wise gradient norms and loss curves for\nOrca-Math fine-tuning of Mamba2 780M. 80\nFigure 30 Layer-wise gradient norms and loss curves for\nOrca-Math fine-tuning of Mamba2 1.3B. 81\nFigure 31 Layer-wise gradient norms and loss curves for\nOrca-Math fine-tuning of Mamba2 2.7B. 82\nxvi\nFigure 32 Layer-wise gradient norms and loss curves for\nCodeTester fine-tuning of Mamba2 780M. 83\nFigure 33 Layer-wise gradient norms and loss curves for\nCodeTester fine-tuning of Mamba2 1.3B. 84\nFigure 34 Layer-wise gradient norms and loss curves for\nCodeTester fine-tuning of Mamba2 2.7B. 85\nFigure 35 Scheduled FP8 training of Mamba2 1.3B on xP3\ndataset 93\nFigure 36 Scheduled FP8 training of Mamba2 1.3B on\nOrca-Math dataset 94\nFigure 37 Scheduled FP8 training of Mamba2 1.3B on\nOrca-Math dataset 95\nL I S T O F TA B L E S\nTable 1 Symbols for state space model formulations 9\nTable 2 Downstream task accuracy for finetuned Mamba2\n780M in FP8 and BF16 86\nTable 3 Downstream task accuracy for finetuned Mamba2\n1.3B models in FP8 and BF16 88\nTable 4 Downstream task accuracy for finetuned Mamba2\n2.7B models in FP8 and BF16 90\nL I S T I N G S\n5.1 Dynamic scaling for FP8 scales tensors at runtime by\nthe absolute-maximum of the tensor. . . . . . . . . . . . 42\nxvii\n\n1\nI N T R O D U C T I O N\nAs large AI models become increasingly ubiquitous and accurate\nacross a wide swath of applications, from natural language to audio\nand video generation, the need for scalable AI models has become\nmore and more pressing. The transformer architecture [36] has cap\u0002tured the vast majority of research attention since its introduction and\nthe proven performance of its attention mechanism for autoregressive\nsequence modeling. However, given that the performance of an au\u0002toregressive sequence model depends on its asymptotic complexity\nwith respect to sequence length and applicable systems optimizations,\nthe transformer’s quadratic complexity poses significant limitations\non its scalability. This obstacle has driven investigation into systems\noptimizations for transformer-based models and alternative model\narchitectures alike. For example, state-space models have become a\nstrong competitor for transformers, especially with the advent of the\nMamba family of models [6, 14].\nIn addition to the architectural strain of progress, the exponential\ngrowth of AI model sizes has motivated new systems techniques for\ncompressing models and reducing their computational and memory\nfootprint. In particular, quantization has emerged as a model compres\u0002sion technique with widespread application for lowering the memory\nand computational overhead of models by reducing the precision of\nsome or all of the computation of the model to low-bit integer or float\u0002ing point formats. Recent GPU hardware developments, including\ndedicated tensor cores that accelerate 8-bit floating-point computation\nby 2–4×, have encouraged adopting FP8 precision for both training\nand inference, rather than quantizing models solely at inference time.\n1\nExisting work on quantization provides fixed solutions for ap\u0002plying the technique to specific models and datasets, but does not\nestablish a structured, context-aware framework for creating and\nevaluating custom quantization strategies. Studies of quantization\nfor transformer-based large language models and state space models\nhave laid a strong foundation for understanding how phenomena\nsuch as emergent outliers [8] and distributions of weights and acti\u0002vations impact quantization error in specific model-data contexts. In\naddition, solutions such as rotation-based transformations [4, 39] and\nsmoothing techniques [38] with remarkable accuracy preservation on\ncommonly used language and reasoning benchmarks have also been\nproposed as ad-hoc, architecture-specific techniques composable with\nquantization at inference time. Recent work has also demonstrated the\nsuccess of FP8 training for specific transformer-based models while\nalso identifying instabilities that may arise as quantized training is\nextended to the trillion-token scales. However, the growing diversity of\nmodel architectures, training & fine-tuning datasets, and applications\nof AI models calls for a structured framework that can guide a user to\u0002wards a viable quantization strategy given an arbitrary model-dataset\ncontext, without relying on a pre-existing solution for mitigating accu\u0002racy degradation for that specific architecture, downstream task, and\nquantization technique.\nWe present the quantization design space–a combinatorially large\nand complex landscape of choices spanning multiple degrees of free\u0002dom, such as quantization precision, granularity, and error tolerance.\nEach configuration within this rich design space reflects a specific\nbalance between accuracy and latency during inference, as well as\nbetween computational throughput and numerical stability during\ntraining. Rather than devising a fixed, one-size-fits-all method, we\ndemonstrate that effective quantization design is architecture-sensitive,\ndata-sensitive, and subject to multiple engineering and numerical met\u0002rics of success. The optimal quantization strategy depends heavily\n2\non the model architecture, dataset, and downstream task for which\nquantization is applied–the quantization context.\nFurthermore, realizing the computational gains of quantization does\nnot always come with naïvely replacing computation with quantized\noperations. In fact, computational overheads from scaling and casting\ntensors at runtime can lead to additional overheads that overtake the\nspeedups provided by low-precision matrix multiplications. Therefore,\ncareful systems engineering and optimization is required to realize\nthe benefits of quantization.\nThe central problem addressed by this work is twofold: (1) to\nformally define the quantization design space, and (2) to develop\na structured and systematic method for navigating it, conditioned\non the specific model, dataset, and downstream task. The combina\u0002torial complexity of the quantization search space makes realizing\nthe promises of increased computational throughput and improved\nlatency without sacrificing training stability, loss convergence, and\ndownstream accuracy parity with high-precision counterparts highly\nchallenging. Finding the optimal quantization “recipe\" requires a col\u0002lection of concrete dimensions, research questions, design principles,\nand systematic experimental analysis to comprehensively understand\nthe design space.\nOur core intuition is that, in the absence of a formal, model- and\ndata-agnostic, mathematical understanding of quantization dynamics\nfor large AI models, we must empirically characterize the quanti\u0002zation landscape in a context-aware manner as one would study\na physical system–establishing a set of research questions and eval\u0002uation metrics and carefully designing experiments to exhaustively\nunderstand how the model behaves in a maximally wide distribution\nof data settings over every dimension. Armed with a comprehen\u0002sive characterization of how various choices within the quantization\ndesign space impacts the computational throughput and accuracy\ndegradation of the model on a downstream task, we can begin to\ndevise strategies for guiding a user towards viable and optimal quan\u00023\ntization recipes for their model architecture, use case, and tolerance\nfor quantization error in an automatic way.\nour contributions\nThis thesis presents the following novel contributions toward defin\u0002ing and characterizing the quantization design space and creating a\nunified framework for a context-aware characterization of the quanti\u0002zation landscape:\n(a) We define the vast and combinatorially complex design space\nfor quantization and the problem of characterizing the space in a\ncontext-aware manner, where a quantization context can consist\nof model architecture, dataset, and downstream target task, and\ntolerance of accuracy degradation. (Chapter 4)\n(b) We expand the design space of quantization with quantized\nmodel optimization in the context of FP8 training and examine\nthe challenges and degrees of freedom introduced by training\nmodels in 8-bit floating point precision. (Chapter 4)\n(c) We establish that the characterization of a quantization recipe\namounts to evaluating its performance with respect to three\nmetrics: maximizing compute utilization, quantized training\nstability, and accuracy on downstream tasks. We derive these\nmetrics by analyzing the dimensions of the quantization design\nspace, which motivate a thorough characterization of each possi\u0002ble quantization recipe with respect to these criteria. (Chapter\n4)\n(d) We design a unified framework of optimization and evaluation\nstrategies for context-aware quantization. This structuring of\nthe problem space provides a comprehensive understanding of\nhow various points in the quantization design space meet or fall\nshort of the three metrics. For compute efficiency, the framework\n4\nconsists of benchmarks for diagnosing computational overhead\nthat hides the throughput speedups of FP8 quantization, opti\u0002mization strategies for reducing these overheads. For numerical\nstability and downstream accuracy, we propose a set of framing\nresearch questions and metrics for evaluating the performance\nof models optimized and deployed in quantized settings, as well\nas ablative measures for isolating the effect of quantization on\nloss convergence and downstream accuracy. These foundations\nlay the groundwork for a collection of experiments that allow\nus to characterize the quantization design space with respect to\nthese measures in a context-aware manner. (Chapter 5)\n(e) We present a proof-of-concept of our context-aware framework\nfor quantization through an empirical case study of FP8 quan\u0002tized training for the Mamba-2 model. The experiments pro\u0002posed by our framework, performed across three diverse fine\u0002tuning data regimes, reveal several data-dependent insights into\nquantized training instabilities, gradient norm clustering, down\u0002stream accuracy impact, and catastrophic degradation phenom\u0002ena. These findings demonstrate how our proposed unified\nframework produces a structural understanding of quantization\ndynamics in a data-dependent and model-dependent manner.\n(Chapter 6)\n5\n\n2\nB A C K G R O U N D\nModern deep learning architectures for autoregressive sequence mod\u0002eling have undergone rapid developments in recent years. While trans\u0002formers and the self-attention mechanism have driven the success of\nlarge language models, its quadratic scaling in computational com\u0002plexity and memory with respect to sequence length has motivated\nresearch into systems optimizations and alternate model architectures.\nThe evolution of state space models (SSMs), culminating in the re\u0002cent introduction of the Mamba family of models, has offered promis\u0002ing contributions in improved efficiency and linear scaling in context\nlength with competitive performance on a variety of benchmarks.\nIn particular, the state space duality of the Mamba-2 model further\nbridges SSMs with GPU optimizations applied to large language mod\u0002els.\nQuantization is increasingly applied when deploying large language\nmodels to reduce their computational and spatial footprint at infer\u0002ence time. In addition, techniques have been developed to mitigate\nperformance degradation of quantized models compared to their full\u0002precision counterparts.\nThis section surveys the architectural developments in sequence\nmodeling and state-of-the-art quantization techniques, setting the\nstage for a deeper exploration of the design space of quantized training\nand inference.\n2.1 transformers and self-attention\nTransformers have enabled remarkable performance in applications\nsuch as natural language [9, 36, 41], vision [10], and audio processing\n7\n[34]. At the heart of the architecture is the self-attention mechanism,\nwhich maps a query and set of key-value pairs to an output to draw\nglobal dependencies across tokens in the input sequence. The first\niteration of self-attention, proposed by Vaswani et al. [36], was scaled\ndot-product attention, which is computed as\nAttention(Q, K, V) = softmax \u0012\nQKT\n√\nD\n\u0013\nV (1)\nwhere Q ∈ RN×D, K ∈ RN×D, V ∈ RN×M are the query, key, and\nvalue matrices, respectively, N is the sequence length, and D is the\nmodel dimension.\nMultiple variants of self-attention have since been developed, in\u0002cluding cross-attention and grouped query attention.\nWhile transformers have proven effective across many tasks and\nbenchmarks, its capabilities come at a high cost. For a sequence length\nof N input tokens, the attention mechanism requires O(N2) time and\nspace complexity. This quadratic scaling bottlenecks the training and\ninference efficiency of transformers and limits the context length with\nwhich models can capture long-range dependencies.\n2.2 linear attention\nAs a response to the quadratic complexity of the self-attention mecha\u0002nism, Katharopoulos et al. [17] proposed linear attention, an approxi\u0002mation of attention that scales linearly as a function of context length.\nBy replacing the expensive softmax-based dot-product attention op\u0002eration with a linear dot-product of kernel feature maps. Formally,\nlinear attention is computed as\nLinearAttention(Q, K, V) = ϕ(Q)\n\nϕ(K)\nTV\n\u0001\n(2)\nwhere ϕ is a feature representation of a kernel k(x, y) : R2×F → R+.\nThis formulation reduces the computational and spatial complexity\nof computing attention to O(N) while maintaining competitive perfor\u0002mance with softmax-based quadratic attention. Furthermore, in the\n8\ncase of causal, autoregressive models where the attention computation\nis masked such that the token in the ith position can only be influ\u0002enced by tokens in positions j ⩽ i, linear attention can be written in a\nrecurrent form [17].\n2.3 state space models\nsymbol description\nxt Input sequence at time step t\nyt Output sequence at time step t\nht Latent state at time step t\nA Transition matrix: determines how latent state ht evolves\nover time\nB Determines how input xt affects latent state ht\nC Determines how latent state ht maps to output yt\n∆ Discretization timestep parameter\nTable 1: Symbols for state space model formulations.\nParallel to the advances in transformer architectures, state space\nmodels (SSMs) have also emerged as a powerful alternative with\nlinear scaling in sequence length during training and constant state\nsize during generation. Closely related to recurrent neural networks\n(RNNs) and convolutional neural networks (CNNs), SSMs perform\nsequence-to-sequence mapping x(t) ∈ R 7→ y(t) ∈ R through an\nimplicit latent state h(t) ∈ RN×D. A general SSM is formulated as\nfollows:\nh\n′\n(t) = Ah(t) + Bx(t) y(t) = C\nTh(t) (3)\nwhere A,B, C are matrices which can be interpreted as follows:\n• the A matrix determines how the latent state ht evolves over\ntime,\n9\n• the B matrix determines how the input sequence xt affects the\nlatent state ht, and\n• the C matrix determines how the latent state maps to the output\nsequence.\nState space models where the A matrix controlling the temporal\ndynamics of the sequence mapping is structured to efficiently com\u0002pute the sequence-to-sequence transformation are called structured\nstate space models, or S4 models. In addition to using this recurrent\nform with matrices A,B, C, S4 models also discretize the continuous\nparameters A and B with the trainable timestep parameter ∆ with a\ndiscretization rule (fA, fB), such that the discrete SSM is written as\nht = A¯ ht−1 + B¯ xt yt = Cht (4)\nwhere A¯ = fA(∆, A) and B¯ = fB(∆, A,B) are the discretized parame\u0002ters. For the remainder of this discussion, we refer to the discretized\nparameters A¯ and B¯ as simply A and B, as from now on we will\nalways be dealing with the discrete form of structured SSMs.\nThe discrete-time structured SSM model represents a juncture of\nstate space models, recurrent neural networks (RNNs), and convolu\u0002tional neural networks (CNNs) [15]. Equation 4 shows that discrete S4\nmodels have a recurrent form which is linear in the input x. Further\u0002more, the S4 recurrence can also be seen as a type of CNN involving\na global convolutional kernel parameterized as a combination of the\nA, B, and C matrices. These mathematical connections give rise to two\ndifferent hardware-efficient computational modes for training and\ninference, where\n• the parallelizeable convolutional mode, where the entire input\nsequence is known, is used for efficient training, and\n• the recurrent mode is used for efficient inference as an autore\u0002gressive model, since one token is generated in each iteration\nand the input is thus seen one step at a time.\n10\nRecently, the linear or near-linear scaling of S4 models and their\ndominant performance in benchmarks for modeling long-range de\u0002pendencies [33] have inspired developments to state space models for\nmodeling information-dense data such as natural language text.\n2.3.1 Mamba: Selective State Space Models\nResponding to this gap in the applications of state space models and\nbuilding upon the S4 formulation, the Mamba model was developed\nbased on the key insight about the linear time invariance (LTI) of\nexisting SSM models. LTI SSMs have the property that its parameters\nA,B, C do not change over time. Gu and Dao [14] demonstrated that\nthe time-invariance of S4 models limits their selectivity: the content\u0002aware ability for a sequence model to choose to focus on or ignore\npositions in the input. In other words, the selection mechanism al\u0002lows a sequence model to propagate and mix information along the\nsequence dimension, a critical capability for modeling information\u0002dense data such as text.\nWith a diagonal structure for the A matrix, the Mamba model\nmade two key modifications to the LTI S4 formulation to introduce a\nselection mechanism into the models:\n1. ∆,B, C are parameterized functions of the input xt, where the\nfunctions chosen by Mamba take inspiration from gating mecha\u0002nisms in RNNs, and\n2. the dimension of the parameters are expanded to include a\nsequence length dimension L, thus making the models time\u0002varying.\nThis time-varying selective state space model is formulated as\nht = Atht1 + Btxt yt = C\nT\nt ht (5)\nWhile the time variance of the model parameters endows Mamba\nwith selectivity capabilities and thus enables competitive performance\n11\nin information-dense sequence modeling tasks such as language, au\u0002dio, and DNA sequence processing, it problematizes its training effi\u0002ciency since the computation of the model can no longer be formulated\nas a convolutional kernel. Therefore, the selective SSM is also accom\u0002panied by a hardware-efficient algorithm designed for computing\nselective SSMs on GPUs. However, the performance of this algorithm\nstill lags behind that of convolutional and attention-based models,\nwhose matrix multiplication-dense operations allow them to be effec\u0002tively accelerated on GPU hardware.\n2.3.2 Mamba-2: the State Space Duality\nThe Mamba-2 model [6] makes just two simple changes to the SSM\nlayer of Mamba. However, these modifications reveal a connection\nbetween selective state space models and the (linear) attention mecha\u0002nism, enabling a hardware-efficient algorithm with the efficient FLOP\ncounts of the Mamba model and dramatically GPU-accelerated train\u0002ing by using tensor cores to perform matrix multiplications.\nThe two changes Mamba-2 makes to the Mamba model are:\n1. the A matrix is constrained to a scalar-times-identity matrix struc\u0002ture, rather than simply a diagonal structure, and\n2. a head dimension P is introduced. The input sequence xt can be\nseen as having both a sequence length dimension L and a head\ndimension P and the sequence transformation defined by the\nSSM acts on the input independently over the P axis.\nBy constraining the A matrix to a scalar-identity structure, Dao\nand Gu [6] create the State Space Duality (SSD) framework, which\nestablishes a theoretical connection between structured state space\nmodels and self-attention. Proving this relationship begins with writ\u0002ing the selective sequence transformation in equation 5 as a matrix\n12\ntransformation mapping sequence vectors x ∈ RT\n7→ y ∈ RT, where T\nis the sequence length.\ny = SSM(A,B, C)(x) = Mx\nMji := C\nT\nj Aj\n· · · Ai+1Bi\n(6)\nwhere A0, . . . , AT−1 ∈ RN×N and B0, . . . , BT1, C0, . . . , CT−1 ∈ RN.\nWe refer to N as the state size or state dimension.\nWriting the SSM in this form leads to the realization that the matrix\nM defined in equation 6 belongs to a particular class of matrices\nknown as sequentially semi-separable (SSS) matrices, or structured\nrank matrices, so called because they are characterized by certain\nrank conditions on their submatrices. This observation, along with a\ngeneralization of linear attention through tensor contractions, leads to\ntwo novel contributions to the study and computation of state space\nmodels:\n1. A dual form of the selective state space model M = L ◦ CBT ∈\nRT×T which is equivalent to the quadratic form of causal linear\nattention Y = (L ◦ QKT)V as described in section 2.2, and\n2. a hardware-efficient algorithm for computing the Mamba-2 SSD\nlayer with GPU-accelerated matrix multiplications using a care\u0002ful block-wise, chunk-wise decompositions of the structured\nmatrices.\nThe latter uses the observation that selective SSMs can be seen\nequivalently as both structured matrices and as the naive quadratic\nform of causal linear attention to derive an algorithm that can much\nmore effectively use GPU tensor cores for matrix multiplications. This\nnew selective structured state space model is referred to as the SSD\nlayer, as it can now be used in a manner similar to any arbitrary neural\nnetwork layer.\n13\n2.3.3 The Mamba-2 Architecture\nDrawing connections between SSMs and attention with the SSD frame\u0002work opens the door to introducing ideas from transformer-based\nlarge language models to state space models, including an attention\u0002like block architecture and systems optimizations for parallelizing\nMamba-2 computation.\nThe Mamba-2 architecture uses the SSD layer as the core layer of a\nneural network block. Taking inspiration from transformer blocks in\nattention architectures, the linear projections that produce the param\u0002eters (A,B, C) from input X are created in parallel, similar to those\nwhich create the Q, K, V projections. As mentioned in section 2.3.2,\nthe design of the SSD layer also lends itself to creating a multi-head\nattention-like pattern. Typically, P = {64, 128} is chosen in step with\nconventions for transformers.\nThe parallel approach to producing the model parameters makes\nthe architecture more suitable to the application of systems optimiza\u0002tions commonly used and studied in transformers, namely parallelism\nschemes such as tensor parallelism (TP) and sequence/context paral\u0002lelism (CP). Both of these strategies shard the model across accelerators\nsuch as GPUs in various ways to enable distributed training of larger\nmodels.\nDao and Gu [6] demonstrate that the SSD algorithm for Mamba-2\nis 2-8× faster than Mamba-1’s associative scan algorithm, and com\u0002parable or faster than FlashAttention-2, a state-of-the-art attention\nalgorithm, at sequence lengths longer than 2K due to its linear scaling\nin sequence length. Furthermore, their language modeling results indi\u0002cate that Mamba-2 reaches competitive scaling compared to transform\u0002ers. The introduction of the SSD framework thus reveals theoretical\nresults that help us better understand why selective state space mod\u0002els perform well on language modeling and a hardware-optimized\nalgorithm for computing SSMs with linear scaling in sequence length.\n14\n2.4 quantization\nQuantization is a model compression technique widely applied to\nlarge neural networks, including transformer-based large language\nmodels, for accelerating training & inference and reducing a model’s\nsubstantial memory footprint. In a quantized model, some or all of\nthe weights and activations of the model are mapped from a higher\nprecision format (such as FP32, FP16, or bfloat16) to a lower preci\u0002sion integer or floating point format. As the number of parameters\nin large language models grows exponentially, systems optimizations\nfor training and serving these models in a computational and mem\u0002ory cost-efficient manner are becoming increasingly necessary and\nurgent. Advances in GPU hardware–tensor cores designed specifi\u0002cally for delivering significant performance gains for quantized matrix\nmultiplications–have made the benefits of quantization even clearer.\nHowever, reducing the precision of the model often leads to accu\u0002racy loss due to quantization error. Furthermore, reducing weights\nand activations to low-precision formats reduces their dynamic range,\nfurther exacerbating the loss in performance. Thus, quantization cre\u0002ates a tradeoff space between efficiency and accuracy which has yet to\nbe fully explored.\nThe innovation of the Mamba-2 model is the synthesis of ideas from\nthe self-attention mechanism and selective state space models. A natu\u0002ral next step is to consider how other model optimization techniques\napplied to transformers can be leveraged for deploying efficient and\naccurate SSMs. Furthermore, the exploration at the intersection of\nLLM quantization and SSMs raises the question of whether a more\ngeneralized approach to synthesizing “recipes\" for model quantization\ncan be found from this exploration.\n15\n2.4.1 Quantization Formats & Mechanics\nThe vast majority of quantization-related work pertains to compressing\nfloating point weights and activations to low-bit integer formats, such\nas INT8, INT4, and INT2. Integer quantization is typically performed\nwith uniform quantization process, such that a floating point-valued\nvector x is quantized to a b-bit integer vector x\nINTB by\nx\nINTB = clamp \u0010jx\nS\nk\n+ Z; 0, 2\nb − 1\n\u0011\n(7)\nwhere S is a scaling factor and Z is the zero-point shift factor to\nensure that real 0 is quantized without error. The clamp factor clips\nthe scaled vector element-wise to the minimum and maximum values\nof the b-bit integer. Symmetric uniform quantization enforces that\nthe zero-point factor Z is 0 to reduce computational overhead, and\nis most commonly used when the distribution of x values is roughly\nsymmetric around zero [26]. Non-uniform quantization techniques,\nwhich does not downcast intervals of floating point values evenly to\ninteger values, may also be used, but are less commonly deployed due\nto their added complexity [18].\nThe advent of GPU hardware designed for low-bit floating point for\u0002mats has popularized quantization to formats such as FP16, BFLOAT16,\nand FP8. The design space of these floating point formats is larger due\nto how these values are represented: for a given bit width, multiple\ndivisions between the exponent and mantissa bits are possible. FP16, for\nexample, use 1 sign bit, 5 exponent bits, and 10 mantissa bits, whereas\nthe BFLOAT16 format designed specifically with the dynamic ranges of\ndeep learning use 8 exponent bits and 7 mantissa bits.\nNVIDIA H100 GPUs now feature tensor cores with FP8 precision,\nclaiming to deliver 4× computational rates compared to 16-bit floating\npoint operations on the previous A100 generation [25]. This trend of\nhardware acceleration for low-bit floating point formats is ongoing,\nas NVIDIA’s next generation of Blackwell-based B200 GPUs include\ntensor cores for accelerating FP4 (4-bit floating point) operations [24].\n16\nFigure 1: 8-bit floating point formats come in two encodings, each with dif\u0002ferent balances of exponent and mantissa bits for different dynamic\nranges [35].\nFP8 comes in two encodings: E4M3 and E5M2, which state the number\nof exponent and mantissa bits in each representation [20]. E4M3 is\nrecommended for weight and activation tensors due to its extended\ndynamic range, whereas E5M2 is recommended for gradient tensors.\nIn PyTorch’s torchao library for quantization, downcasting from a\nreal-valued tensor x to a FP8 tensor x\nFP8 achieved by\nx\nFP8 = clamp \u0010x\nS\n, FP8min, FP8max\u0011(8)\nwhere S = max(|x|) and FP8{min,max} are the minimum and maximum\nvalues corresponding to each encoding.\nWhile some approaches choose to quantize all weights and/or acti\u0002vations uniformly to the same low-precision format, models can also\nbe trained in mixed precision, as implemented in the PyTorch Automatic\nMixed Precision (AMP) library [1]. Some neural network operations,\nsuch as linear layers and convolutional layers, can benefit from the\ncomputational speedup of lower precision. Others operations, such as\nnormalization operators like LayerNorm and RMSNorm, require the\nlarger dynamic range of FP32.\n17\n2.4.2 Quantization Techniques\nQuantization can be subdivided into three categories of techniques\nbased on when weights and activations are compressed to lower bit\nwidths: post-training quantization (PTQ), quantization-aware training\n(QAT), and quantized training.\npost-training quantization Post-training quantization refers\nto quantizing the model at inference time, after the model has been\npre-trained at a higher precision. PTQ can require a calibration step,\nwhere small sample of data is used to tune the casting factors (e.g. S,\nZ) before downcasting the weights and/or activations of the model.\nThis process is called static quantization, since the scales are kept static\nduring inference. These factors can also be calculating at runtime\nbased on actual tensor value ranges in the activations. This approach\nis referred to as dynamic quantization.\nquantization-aware training Even with calibration, post\u0002training quantization can incur accuracy loss due to the reduced\nprecision in the weights and activations, especially for integer quan\u0002tization at 4 bits and below. While PTQ may serve as a first attempt\nfor quantizing a model, if the accuracy loss exceeds an acceptable\nthreshold, then QAT is applied. QAT applies fake quantization to ten\u0002sors in the model, thus simulating quantization error and mitigating\nthat error during model training. This noise is injected during train\u0002ing or fine-tuning by applying both quantization and dequantization\nin the forward pass, thus using a “rounded-down\" value for these\nparameters in its original high-precision format. Since the rounding\noperation is non-differentiable, QAT utilizes the straight-through esti\u0002mator (STE), which approximates the gradient of rounding as 1 and\nleaves the original gradient of the operator unchanged.\n18\nSince the computation is still performed in high precision and\nscaling factors must be maintained during training, quantization\u0002aware training incurs some computational and memory cost. However,\nQAT has been shown to be effective in mitigating quantization error\nwhen reducing to low-precision integer formats.\nquantized training Quantized training represents the oppo\u0002site end of the spectrum compared to post-training quantization, per\u0002forming both training and inference in low precision. Although QAT\nis frequently used with integer quantization, experiments have shown\nthat quantized training yield competitive model performance for FP8\nformats. While quantized training incurs some memory overhead from\nstoring scaling factors and computing these on the fly, with tensor\ncores designed for accelerating FP8 processing, the computational\noverhead of both training and inference is reduced.\nQuantized training is at the forefront of optimizations for training\nand serving efficiency of LLMs, with models like DeepSeek V3 using\nFP8 mixed precision training to achieve efficiency in large-scale models\n[7]. DeepSeek V3 performs matrix multiplications and caches and\ndispatches activations in FP8, while storing low-precision optimizer\nstates in BF16. Furthermore, a fine-grained tile-wise and block-wise\ngrouping strategy, designed specifically for DeepSeek, is used for the\nscaling factors of each tensor. The level of customization and careful\ndesign required to deploy FP8 in this setting illustrates the volume of\nchoices in the quantization design space.\n2.4.3 Quantization Granularity\nIn addition to choosing the precision at which a model is compressed,\nthe granularity of quantization must also be determined. Quantiza\u0002tion granularity refers to the resolution at which scaling factors are\nmaintained across a model. This resolution can be:\n19\nFigure 2: Quantization can be performed at different granularities. d is the\nmodel size and h is the number of heads in one attention block\n[37].\n• Tensor-wise: a scaling factor is maintained for every quantized\ntensor\n• Axis-wise/Channel-wise: a scaling factor is maintained along\neach axis of a tensor\n• Group-wise: tensors are quantized at the resolution of groups of\nchannels\nOther granularities, such as per-token, per-head-group, or per\u0002embedding-group, are also possible with various implementations of\nquantization and model architectures.\n20\n3\nR E L AT E D W O R K\nWe provide an overview of related work in quantization of the Mamba\nstate space model architecture, precision-aware scaling laws for transformer\u0002based large language models, 8-bit floating point training and infer\u0002ence, and quantization precision scheduling in training deep neural\nnetworks. The intersection of these lines of research expose a gap in\nour understanding of quantization in the context of the model architec\u0002ture, dataset, and downstream task accuracy, especially for quantized\ntraining.\n3.1 quantization for mamba\nSince the release of the Mamba models, which demonstrated com\u0002petitive performance on sequence modeling tasks comparable to that\nof transformers, multiple works have investigated the sensitivity of\nthis architecture to quantization and proposed techniques to miti\u0002gate accuracy loss in these settings. However, the vast majority of\nthese works focus on the original Mamba model, as opposed to the\nnewest and more hardware-efficient Mamba-2 architecture. Further\u0002more, these works devise and evaluate rotation-based transformations\nthat mitigate accuracy degradation in integer post-training quanti\u0002zation settings due to emergent outliers without investigating how\nthese errors scale across a wider range of datasets, benchmarks, and\nprecisions in quantized training.\n21\n3.1.1 Activation Outliers in Mamba\n“Mamba-PTQ: Outlier channels in recurrent large language models”\nby Pierro and Abreu [30] identifies the original Mamba model exhibits\noutlier channels, defined by channels with values beyond six stan\u0002dard deviations beyond the layer mean, in the pre-activations of the\nlinear projections of the Mamba-130M model when running on the\nWikiText-2 dataset. Removing these outliers led to notable accuracy\ndrops in downstream tasks such as LAMBADA, HellaSwag, PIQA,\nWinogrande, RTE, and COPa. This observation established that the\nchallenges faced in quantizing large language models, namely due\nto the presence of these outliers, are expected to be encountered in\nquantizing Mamba as well. Furthermore, Pierro and Abreu [30] re\u0002port results on the naïve application of the outlier-aware quantization\ntechnique SmoothQuant proposed by Xiao et al. [38] to these linear\nprojections. With SmoothQuant, which uses a per-channel smoothing\nfactor to mitigate activations outliers, Mamba is quantized to 8-bit and\n4-bit integer precisions in various configurations involving both the\nweights and activations.\n3.1.2 Quantization Techniques for Mamba\nBuilding upon the initial observations made by Pierro and Abreu\n[30], multiple subsequent works have investigated rotation-based\ntransformations for further minimizing accuracy loss when apply\u0002ing post-training quantization various layers of Mamba to 8-bit integer\nprecision.\n“MambaQuant: Quantizing the Mamba family with variance aligned\nrotation methods” by Xu et al. [39] demonstrated that, not only do\nthe Mamba models exhibit similar outliers in both weights and acti\u0002vations, but that the hardware-efficient parallel scan further exacer\u0002bates the severity of these outliers. The amplification of outliers, and\n22\nthus quantization error in low-precision settings, is caused by the self\u0002multiplication of the state matrix when computing the SSM recurrence,\nin contrast to the additive nature of the self-attention operation. Xu\net al. [39] takes inspiration from Hadamard-based rotation transfor\u0002mations used in transformer-based large language models to devise\nMambaQuant, which uses a Karhunen-Loève transformation rotation\nfollowed by a Hadamard rotation to equalize variance across channels\nwhile maintaining the smoothing properties of the Hadamard trans\u0002formation to mitigate activation outliers. Their quantization technique\nis applied to the output projection and the matrix multiplication in\nthe SSM block itself. This proposed method is evaluated in Mamba\nlanguage models, Vision Mamba models, and Mamba-based video\nmodels, which are quantized per-tensor to up to 4-bit precision in\nthe weights and 8-bit precision in activations, to demonstrate error\nmitigation to within 1 point of accuracy compared to baseline models\nin FP16.\n“Quamba: A post-training quantization recipe for selective state\nSpace Models” by Chiang et al. [4] takes similar inspirations from\nHadamard transforms, exploring the use of the Walsh-Hadamard\ntransformation in the output projections and percentile-max clipping\nin the SSM inputs to mitigate the effects of outliers for Mamba. This\ntechnique is used in conjunction with quantizing all weights and acti\u0002vations to 8-bits. Using these techniques, Chiang et al. [4] demonstrate\nthat accuracy loss can be mitigated to within a percentage point of\naccuracy, like MambaQuant, on common downstream benchmark\ntasks for Mamba and the hybrid Jamba models when applying static\nper-tensor symmetric quantization. Furthermore, Chiang et al. [4]\ndemonstrate that Quamba lies on the Pareto frontier of accuracy and\ninference latency when compared to the Llama-2 transformer models\nwith post-training quantization applied.\nWhile these two works provide remarkable results in mitigating\naccuracy loss in deploying Mamba models in low-bit quantization\nregimes, many areas of the quantization design space remain unex\u000223\nplored. While each work provides an illustration of the challenges of\nquantization and characterizes the issue of activation outliers specif\u0002ically for recurrent state space models, they do not provide us with\ninsights on how to navigate the combinatorial design space of quanti\u0002zation, such as choosing the granularity or precision of quantization\nto be applied. Furthermore, they both provide architecture-specific\nsolutions such as specialized rotation-based or Fourier-based transfor\u0002mations, which a practitioner may not be able to apply to or evaluate\non a custom model architecture or quantization regime. All existing\nwork on quantizing Mamba also focused on post-training quantization,\nbut no published work has investigated the effects of quantization\u0002aware training or quantized training for this architecture. While PTQ\nis the logical first resort when applying quantization and deploying a\nmodel, and mitigating accuracy loss in the standard downstream lan\u0002guage benchmarks used by these works is predictive of generalizeable\nperformance, PTQ will not always be sufficient in every task. Thus,\nmore investigation and analysis of optimization-based quantization\ntechniques like QAT and quantized training is necessary for informing\nthe training and deployment of any model, including the emerging\nMamba-2.\n3.2 precision-aware scaling laws for quantization\nA scaling law is a function, empirically or theoretically derived, that\npredicts the performance of a model given a set of constraints or prop\u0002erties. These laws can be fitted to predict the loss or other performance\nmetric of a given model as a function of its model size, the diversity of\nthe data it is trained on, a compute budget, or any host of other prop\u0002erties of the training and inference regime. Determining these scaling\nlaws and comparing state-of-the-art models to the predictions of these\nscaling laws are an importance subfield of the study of transformers\nand large language models today.\n24\nAlong this vein of research, “Scaling laws for precision” by Kumar\net al. [19] empirically determined a set of power-law functions which\npredict model performance as a function of model size, training data\nbudget, and training precision. By varying not only the parameter\ncount and training data available to the model but also the number\nof bits used to represent the weights, activations, and KV cache of\ntransformer-based models, Kumar et al. [19] produced a set of useful\ninsights that inform how precision impacts model performance during\npost-training quantization, quantization-aware training, and quantized\ntraining. This investigation culminated in a unified and interpretable\nfunctional form that predicts loss degradation for a given training\nconfiguration and quantization precision.\nThe derivation of these scaling laws lies in the same spirit as our\nwork–it provides generalizeable insights that can be used by a practi\u0002tioner to determine ahead of training time, given a model, a compute\nbudget, and accuracy loss tolerance, how to produce the optimal\nmodel. However, this work has a number of limitations which our\ninvestigation builds upon. While these scaling laws are empirically\nfitted and evaluated on a variety of precisions, from high precision for\u0002mats like 32-bit or 16-bit floating point to 3-bit representations, these\nrepresentations are limited to integer formats. While integer formats\nare widely deployed, with advances in hardware, more investigation\ninto the viability of floating point representations like FP4 and FP8\nare becoming increasingly pressing. Furthermore, the experimental\nsetup for this work is limited to OLMo-style Transformer++ models,\nwhich limits the architectural scope of these insights. For example, it\nis unclear how these laws would generalize to a state space model\narchitecture like Mamba-2, or how these laws regarding quantiza\u0002tion would apply to the more unstable floating point representation\nregimes. Thus, while these scaling laws lay the groundwork for further\nexploration, we seek to extend this style of analysis to a less-explored\nbut emerging subspace of quantized state space model training.\n25\n3.3 fp8 training & inference\nIn 2022, Arm, Intel, and NVIDIA jointly published “FP8 Formats for\nDeep Learning,” a whitepaper proposing the two 8-bit floating point\nbinary interchange formats widely used in machine learning today\nas well as evaluation results for training and inference with these\nformats. Using simulated results for FP8 training and E4M3 results\nfor inference on models trained in 16-bit precision, the paper demon\u0002strated that loss curves for transformer-based language models such\nas Transformer-XL and GPT models trained on the Pile dataset and for\nsmaller image classification models like VGG-16 and Resnet matched\nthose of successful high-precision training runs, while FP8 inference\nachieved better accuracy than INT8 inference.\nSubsequent industry whitepapers, such as “FP8-LM: Training FP8\nLarge Language Models” by Peng et al. [28] at Microsoft Azure and\n“Scaling FP8 training to trillion-token LLMs” by Fishman et al. [11]\nat Intel, have explored FP8 training for a GPT and modified Llama2\nmodel, respectively. Each propose different techniques of applying\n8-bit floating point precision to various parts of the model to achieve\nloss parity, where the loss curves of the quantized model converges\napproximately across time to the loss curve of the high precision\nmodel.\nPeng et al. [28] proposes a mixed-precision framework, which allows\n8-bit communication, optimizer, and distributed training to be added\non in an incremental manner based on model sensitivity. This frame\u0002work is then evaluated for pre-training and instruction tuning. The\nresulting models are then evaluated against their BF16 counterparts on\nlanguage modeling benchmarks AlpacaEval and MT-Bench, demon\u0002strating that the quantized GPT models yield comparable performance\nto models trained or fine-tuned in higher precisions.\nFishman et al. [11] extends the exploration of FP8 training in the\ntime dimension up to 2 trillion tokens of pre-training the Llama2\n26\nmodel, demonstrating that numerical instabilities and catastrophic\nloss divergence can arise after processing approximately 200 billion\ntokens due to activation outliers emerging in certain activations of the\nmodel. By upcasting the parameters at particularly sensitive parts of\nthe model, namely before the SwiGLU non-linear activation of Llama2,\nand downcasting for the remainder of the model, loss parity and\ncomparable downstream zero-shot performance are achieved.\nThese initial investigations of FP8 training provide valuable intu\u0002ition and results both in the potential for FP8 training to provide\nsignificant computational and memory savings for training and de\u0002ploying large language models and in the instabilities that can arise\nfrom using this quantized training technique. While the methods each\nwork reports hint at broader paths for navigating the design space for\n8-bit floating point training–mitigating the exacerbation of emergent\noutliers by avoiding the quantization of certain layers or other mixed\u0002precision techniques–they provide solutions and insights for a small\nset of transformer-based models and a limited set of pre-training and\nfine-tuning datasets. Given the data-dependent nature of quantization\nerror, much of how FP8 affects the stability of training and model opti\u0002mization with respect to downstream tasks is still not well understood,\nparticularly for non-attention-based models.\n3.4 quantization precision scheduling\nThe vast majority of research in mixed precision quantization refers\nto varying the precision of the model from layer-to-layer, or tensor\u0002to-tensor. A limited body of work exists in varying quantization pre\u0002cision temporally, or scheduling quantization across training. In 2021,\nMicrosoft released a solution for dynamically adjusting quantization\nprecision throughout quantization-aware training called Mixture-of\u0002Quantization (MoQ) [22], which was integrated into the open-source\ndeep learning optimization library DeepSpeed. Taking inspiration\n27\nfrom iterative, time-varying approaches such as iterative pruning,\nMoQ devises a quantization schedule for each layer of a model, re\u0002ducing its precision from a high-precision format like FP16 to a target\nprecision like INT8 gradually throughout QAT. The sensitivity of each\nlayer to quantization is determined with a small subset of the training\ndataset, with which the second-order (Hessian) spectrum is computed.\nThe intuition, derived from Q-BERT [32], is that neural network layer\ngradients with larger top eigenvalues are more sensitive to quantiza\u0002tion than those with smaller top eigenvalues. Using this second-order\ngradient information, the bit-width of more sensitive layers are re\u0002duced more gradually than those of less sensitive layers.\nThis work reflects the spirit of our analysis of varying the phase of\ntraining or fine-tuning at which FP8 quantization is applied. However,\ngiven that there are no known explorations of this technique since\nMoQ in 2021, our work picks up on this analysis and assesses the\napplicability of using gradient dynamics to understand FP8 training\nstability at various points of training.\n28\n4\nC H A R A C T E R I Z I N G T H E L A N D S C A P E O F\nQ U A N T I Z AT I O N\nThis chapter defines the shape and structure of the problem of charac\u0002terizing the design space of quantization. Given a model architecture,\nand dataset, and a target downstream task, there exists a combinato\u0002rially complex landscape of choices to compose the optimal “recipe\"\nfor quantizing that model, each of which represents a point in a large\ntradeoff space between hardware efficiency, numerical instability in\ntraining & inference, and downstream model performance. Although\nexisting work proposes various solutions for model quantization along\nthe latency-accuracy tradeoff space for specific datasets and architec\u0002tures, we do not yet have a context-aware framework for guiding a\nuser towards an optimal or near-optimal quantization recipe in a struc\u0002tured and systematic manner. This context includes the architecture\nor computational algorithm of the model, the dataset it is trained\non, available hardware, and the tolerance for downstream accuracy\ndegradation due to quantization error.\nIn order to begin devising such a framework for quantization recipe\noptimization, we must first characterize the landscape of quantization.\nA comprehensive understanding of how different choices within the\ncomplex tradeoff space of quantization impact the relevant success\nmetrics for both the systems and numerical performance of a quan\u0002tized model is critical for formalizing a context-dependent approach\nfor guiding a user towards the right quantization strategy. This chap\u0002ter aims to identify the key dimensions of this problem space and\nformulate the experiments from which a structural understanding of\neach of these dimensions can arise.\n29\n4.1 architectural developments for scalable ai\nThe success of large AI models is fundamentally determined not\nsimply by their performance on complex tasks like natural language,\naudio, or vision processing, but their scalability–their ability to process\nlarge amounts of information in a single context, their ability to be\ndeployed in settings with vast or limited computational resources, or\ntheir adaptability from one task to another through optimization and\ntuning. The first dimension of scalability–context length scaling–is\nan algorithmic pain point for transformers, the dominant AI model\narchitecture for a variety of applications today, on account of the\nquadratic scaling of its attention mechanism. A significant majority of\nresearch into extending the capabilities of large models and pushing\ntheir scalability in both capability and efficiency has been concentrated\non transformers. However, at the same time, the collection of model\narchitectures of interest to the AI community has also diversified in\nresponse to this critical scalability issue, giving rise to developments\nin other families of model such as recurrent models and state space\nmodels.\nIn particular, state space models have garnered increasing inter\u0002est, especially in light of the creation of the Mamba-2 model by Dao\nand Gu [6] in 2024. Selective state space models, and Mamba-2 in\nparticular, exemplify an algorithmic and architectural innovation–the\ntheoretical duality between state space models and quadratic attention–\ninspired by a problem in systems–hardware-efficient parallel scans for\ncomputing convolutional and recurrent models. The development of\nMamba-2 through drawing mathematical connections between algo\u0002rithmic paradigms like SSMs and attention to develop more perfor\u0002mant hardware-aware models of computation illustrates how systems\noptimizations and numerical methodologies are deeply intertwined\nin the study of new machine learning architectures. However, under\u0002standing the dynamics of state space models poses unique computa\u000230\ntional and mathematical challenges in relation to existing intuition\nabout how transformers process data and interact with commonly\u0002applied systems optimizations like distributed parallelism, pruning,\nsparsification, and quantization. Given how inextricably linked the\nmathematical foundations of machine learning techniques are with\ntheir implications for systems performance and engineering, develop\u0002ing a structured understanding of the composition of model architec\u0002tures, data regimes, and systems optimization techniques is critical for\nenabling the continued deployment of AI models in an increasingly\ndiverse array of settings.\n4.2 challenges in understanding quantization\nAs the capabilities of large language models continue to set new\nrecords in language, mathematics, reasoning, translation, and other\ntasks, model sizes are also growing exponentially, making the neces\u0002sity of systems optimizations for training and inference more and\nmore pressing. Model quantization has been widely deployed and\ninvestigated through the lens of numerics, systems performance, and\nhardware acceleration alike, thanks to its ability to provide both com\u0002putational gains and memory savings when applied with the right\nmethod for a given model, task, and specialized hardware. With the\nlatest NVIDIA H100 GPUs, FP8 can provide up to a remarkable 2-4×\nthroughput speedup for matrix multiplications [25]. With the con\u0002tinued development of tensor cores for accelerating low-bit floating\npoint training and inference and the growth in AI models’ memory\nfootprint and computational cost, understanding the optimal method\nfor quantization is and will remain relevant across model architectures,\ncompute & hardware budgets, and data distributions.\nFor any given model architecture, training or fine-tuning dataset,\nand hardware configuration, quantization opens up a complex de\u0002sign space of combinatorial scale, where a choice along any degree of\n31\nfreedom has implications on the numerical stability of the model, per\u0002formance on downstream tasks, computational efficiency, and memory\nfootprint of the model. Even after choosing a quantization technique–\npost-training quantization, quantization-aware training, or FP8 quan\u0002tized training–a practitioner is faced with a host of choices to make\nwith regards to whether or not to quantize weights, activations, or both,\nthe precision to which the model is quantized, and the granularity\nat which each tensor is quantized. Among granularities themselves–\nper-tensor, per-channel, or per-group–there is a continuous spectrum\nalong which one must navigate with respect to the size of the groups\nor tiles with which scaling factors and zero-points are maintained.\nThe implications of these choices heavily depend upon the sensitivity\nof each layer and tensor to quantization, how quantization error is\npropagated throughout the model, and the dynamic ranges of each\ntensor on a given dataset (or, more generally, underlying data distribu\u0002tion). Given that different layers may have different sensitivities, and\nmore sensitive layers may be more likely to cause accuracy degrada\u0002tion when quantized than others, these layers may need to be kept in\nhigher precisions. Therefore, mixed precision training and inference,\nwhere different precisions are used across a single model, creates yet\nanother dimension along which the user must navigate to find the\noptimal quantization recipe.\nExisting work in identifying the underlying causes of accuracy\nloss due to quantization, particularly in the context of post-training\nquantization, illustrates the data- and context-dependent nature of the\nquantization design space. In transformer-based large language model,\nthe prevalence of emergent outliers, or outlier channels, which can\nappear in the weights and activations of a model has been shown to\ndominate a model’s predictive performance. These emergent outliers\nappear as a small subset of the model parameters that consistently\nhave larger dynamic ranges than the rest. “LLM.int8(): 8-bit Matrix\nMultiplication for Transformers at Scale” by Dettmers et al. [8] showed\nthat, in attention-based autoregressive transformers evaluated on the\n32\nC4 corpus (a subset of the Common Crawl corpus), activation outliers\nare both critical to model performance and can break quantization.\nWhen outliers were removed from a 6.7B transformer, for example,\nthe mean top-1 softmax probability was reduced by over 20% and\nvalidation perplexity increased by 600-1000%. On the other hand, if the\nsame number of feature dimensions were randomly removed, top-1\nprobability decreased only by 0.02-0.3% and perlexity increased by\nonly 0.1%. Quantizing activations that exhibit strong outliers, however,\nis particularly challenging as they can cause particularly excessive\nquantization error, thus significantly degrading the performance of\nthe model at inference time.\nActivation outliers have also been the center of investigation into\npost-training quantization of the Mamba state space models. Mam\u0002baQuant [39], Quamba [4], and Mamba-PTQ [30] all examine the\nsensitivity of the linear projections and the SSM layer of the Mamba\narchitecture through the lens of the prevalence of outlier channels in\neach component of the model. While Mamba-PTQ and MambaQuant\nmotivate the challenge of investigating quantization dynamics for state\nspace models with the observation of activation outliers, Quamba goes\neven further in demonstrating that SSMs show distinct outlier pat\u0002terns compared to those observed in the self-attention mechanism.\nThese early investigations into quantization dynamics for Mamba\ndemonstrate that model architecture, namely the transition from\ntransformers to state space models, figures significantly in the char\u0002acterization of the quantization design space and the recipe that\neffectively mitigates accuracy loss at inference time.\n4.3 data dependence of the quantization design space\nWhile the contrast between dynamics found in transformers and those\nfound for state space models demonstrates the architecture-dependent\nnature of the challenges posed by quantization, a key gap in our un\u000233\nderstanding of these interactions left unanswered by any existing work\nconcerns the data-dependent nature of the challenges of quantization.\nThe four works mentioned in this chapter use standard language mod\u0002eling datasets such as the Pile [12], WikiText2 [16], or Common Crawl\n[31] and language & reasoning benchmarks such as LAMBADA [27],\nPIQA [3], ARC [5], and HellaSwag [40] to evaluate model degradation\ndue to quantization at inference time, the prevalence of degradation\u0002causing activation outliers, or, when relevant, the calibration dataset\nused to tune quantization hyperparameters. MambaQuant also uses\nthe image classification dataset ImageNet, given that it also evaluates\nits method for Mamba-based vision models. While these benchmarks\nand datasets may be standard practice for evaluating the performance\nof models in their most common use cases, the lack of any charac\u0002terization of quantization dynamics beyond these common datasets\nand applications severely limits our understanding of how quanti\u0002zation techniques respond to the diversity of datasets that matter in\nreal-world applications of AI models.\nThe evaluation and characterization of quantization in a data-aware\nmanner is critical to our understanding of this technique because quan\u0002tization is fundamentally a data-dependent operation. If the success of\nquantization techniques were entirely independent of data distribution\nand different computational operations’ sensitivity to the shapes of\nthese distributions, then existing techniques such as pre-quantization\ncalibration or quantization-aware training would not be necessary\nto mitigate accuracy loss of quantized models. Although the current\nbody of literature on quantization heavily focuses on model archi\u0002tecture as a source of variance in the ability of tailored quantization\nsolutions for preserving accuracy at quantized inference time, the\nquestion of whether properties of the training dataset impact the loss\nconvergence and downstream accuracy degradation of quantized\nmodels remains unexplored.\n34\n4.4 new challenges posed by fp8 training\nThe increasing adoption of the FP8 quantization paradigm, which\nenables both training and inference in 8-bit floating point formats,\nopens up new degrees of freedom in the quantization design space\nwhich integer quantization and post-training quantization do not\npose. Quantized training involves performing model optimization and\ngradient descent with the weights in 8-bit floating point. Inputs to\nquantized matrix multiplications are also downcasted before being\npassed to FP8 matrix multiplication kernels, which are performed\non FP8 tensor cores for computational speedup. Quantized model\noptimization further deepens the interactions between data, model\narchitecture, and quantization. When optimizing with 8-bit and higher\nprecision floating point parameters, updates to both the unquantized\nand quantized layers of the model can be affected by quantization\nerror propagated through the model from layer to layer but also from\nbatch to batch. In other words, there are two dimensions along which\noptimization can cause quantization error to travel:\n1. in the layer dimension, where updates to later layers are impacted\nby quantization error accrued in earlier layers, and\n2. in the temporal dimension, where quantization error accrued in\nearlier phases of training affect the optimization trajectory in\nlater phases of training.\nThe layer dimension of quantization error propagation is observable\nin transformers, but we hypothesize that this effect can be further\nexacerbated by model architectures and computational models where\nquantization error may be accumulated multiplicatively rather than\nadditively in the sequence length dimension. In transformers, aggre\u0002gations across tokens are performed additively, implying that any\nquantization error accrued from output token to output token would\naccumulate additively as well. However, the Mamba and Mamba-2\nalgorithms involve a continuous self-multiplication of the parameter\n35\nmatrix, suggesting that quantization error might accumulate much\nmore rapidly. This observation is, in fact, already made by Xu et al.\n[39], and informs their analysis of post-training quantization error for\nMamba.\nThe temporal aspect of quantization error propagation gives rise\nto a new degree of freedom for investigation: at what point during\nquantized training should FP8 precision be activated? In other words,\nat what point during the optimization process should we start FP8\ntraining such that the quantization error accrued during quantized\noptimization does not exceed our tolerance for accuracy degradation?\nPrevious work in precision scheduling for quantization and in\nFP8 training supports the intuition that the phase at which quan\u0002tized optimization begins during the training process has implica\u0002tions for the numerical stability of training. DeepSpeed’s Mixture-of\u0002Quantization method [22], which schedules the reduction of precision\nin quantization-aware training for integer formats, reduces the pre\u0002cision of layers with larger second-order gradient eigenvalues more\ngradually than less sensitive layers with smaller Hessian spectra. Even\nthough MoQ is applied in quantization-aware training, the use of\nthe gradient eigenvalues as a proxy for sensitivity of neural network\nlayers to sensitivity during the optimization process signals that opti\u0002mizing jointly with noise from quantization error requires temporal\nvariations to mitigate accuracy loss. Furthermore, Fishman et al. [11]\nfound that, while scaling FP8 training to two-trillion tokens, numeri\u0002cal instabilities and loss divergence while training Llama2 7B in FP8\ncompared to BF16 learning curves occured after processing 220 billion\ntokens due to spikes in outliers in the SwiGLU activations of the\nmodel. The fact that these observations were made after scaling up the\nscale and duration of training suggests that quantization error, and\nthus numerical instability caused by this noise, is propagated not just\nthrough the model but also through time as the weights are optimized\njointly with quantization error. Thus, the empirical observations of\nFP8 training instabilities in these models and early investigations of\n36\nprecision scheduling motivates further examination of the temporal\ndegree of freedom of quantization.\n4.5 structuring the problem space of quantization\nOur analysis of how quantization outcomes depend on model architec\u0002ture, data, hardware-enabled FP8 training, and quantized optimization\nenables us to devise a structured navigation of the quantization design\nspace. This formalization gives rise to a set of experiments which,\ngiven a model architecture and a set of relevant datasets, can be used\nto create a context-aware characterization of quantization across these\ndata regimes. These insights can be used to create observations of\nquantization success and failure modes for a given context, which in\nturn inform model, data, hardware, and phase-aware quantization\nrecipe design.\nThere are two design principles that an acceptable quantization\nrecipe for a given context must meet:\n1. Maximizing compute utilization: quantization must provide\nsufficient computational throughout gains and memory savings\nat training and inference time that justify implementation and\nalgorithmic complexity\n2. Training stability: quantized training must not exhibit loss di\u0002vergences or instabilities that render the model too sensitive to\nhyperparameter or data perturbations to be applicable\n3. Downstream accuracy: quantized models must not exhibit ac\u0002curacy degradation on downstream tasks beyond the user’s\ntolerance for such error\nThe principle of compute efficiency requires that engineering tech\u0002niques such as operator fusion, hardware sharing, and kernel launch\noverhead reduction to ensure that the latency introduced by scaling\nand casting operations does not outpace the computational gains\n37\nprovided by tensor core-based matrix multiplications. We provide\nan overview of benchmarking of FP8 training and inference, as well\nas techniques for reducing additional overhead and realizing FP8\nacceleration in practice, in this chapter.\nThe principles of training stability and downstream accuracy mo\u0002tivate a framework for the systematic characterization of the quanti\u0002zation landscape based on the quantization context–model architec\u0002ture, dataset, quantization technique, downstream tasks, and accuracy\ndegradation tolerance–in order to better understand how to devise\nviable quantization recipes. We establish a collection of research ques\u0002tions and their corresponding experiments to answer these questions\nfor developing such a characterization.\n4.6 conclusion\nIn this chapter, we analyze the contextual dimensions of the quantiza\u0002tion design space to motivate a systematic and structured navigating\nthe complex landscape of quantization in a architecture-, dataset-, and\ntask-aware manner. Architectural advancements to push the scalabil\u0002ity frontier of large and capable AI models has created the need for\nunderstanding numerical systems techniques like quantization across\na more diverse range of computational models. In addition, we argue\nthat the inherent data-dependent nature of quantization and the chal\u0002lenges posed by emergent outliers implies that the limited range of\ndatasets used for evaluating quantization techniques is insufficient for\na comprehensive understanding of their dynamics. With the increas\u0002ing adoption of FP8 training, we introduce the temporal dimension\nof quantized optimization, where the phase at which quantization is\napplied affects the loss convergence and downstream properties of the\nmodel.\nWith these insights in mind, we formulate a three-part structure to\nthe problem space of quantization: maximizing compute utilization,\n38\ntraining stability, and downstream accuracy. Maximizing through\u0002put and hardware efficiency is critical in rendering any quantization\ntechnique effective for providing computational speedups, allowing\nquantization to be deployed and become useful for a given model ar\u0002chitecture. Numerical stability and downstream performance require a\nquantitative analysis of the effect of quantization on loss convergence\nand accuracy at inference time, with careful experiments and ablations\nto isolate the effects of lower precision on optimization stability and\nmodel performance. We delve into the specific considerations for each\nof these design principles in the next chapter.\n39\n\n5\nT O WA R D S A U N I F I E D S T R AT E G Y F O R\nC O N T E X T- AWA R E Q U A N T I Z AT I O N\nBuilding upon the structure we endowed the problem space of quanti\u0002zation with a dimensional analysis of model architecture, data depen\u0002dence, and the advent of FP8 training, this chapter presents a unified\nframework of optimization and evaluation strategies for the metrics\nwe defined in Chapter 4: maximizing compute utilization, training sta\u0002bility, and downstream accuracy. We first demonstrate the challenges\nposed by realizing the computational gains of quantization specific to\nFP8 and propose optimization techniques such as operator fusion and\nkernel launch overhead reduction to mitigate these overheads. We then\nestablish the key research questions critical to building a context-aware\ncharacterization of model performance across the quantization design\nspace. These questions go hand in hand with our evaluation metrics\nfor quantized model performance, which are carefully constructed\nto provide useful ablations of quantized optimization and training,\nand a gradient-based diagnostic metric for explaining instabilities or\naccuracy degradations.\n5.1 performance engineering for fp8 training & infer\u0002ence\nWe provide an overview of float8 training and inference in PyTorch,\nimplemented in the torchao library, and illustrates the systems en\u0002gineering challenges presented by the additional computational and\nmemory overhead of FP8 techniques for realizing performance gains.\n41\n5.1.1 Float8 Training and Inference in PyTorch\nIn order to preserve accuracy when downcasting to lower precision,\nFP8 quantization is typically applied on a per-tensor basis, meaning\nthat a scaling factor is maintained for each quantized tensor and may\nbe updated independently from those of other tensors. Given the\ngranularity of this technique and that each tensor must be scaled\nindividually before matrix multiplications are applied, PyTorch im\u0002plements FP8 training with a model-rewrite-based approach. Linear\nlayers, which perform matrix multiplications between two inputs, are\nswapped out for Float8Linear modules by passing the model through\nthe function convert_to_float8_training. The Float8Linear mod\u0002ules downcast input parameters to FP8 using their per-tensor scaling\nfactors before the forward pass, which calls the cuBLAS GEMM kernel\nfor FP8 operations on H100 tensor cores.\nPyTorch provides three modes for per-tensor FP8 scaling: dynamic,\ndelayed, and static. Dynamic scaling scales each tensor based on the\nabsolute maximum of the tensor at runtime:\n1 amax = abs(max(x))\nscale = amax_to_scale(amax, torch.float8_e4m3fn)\nx_fp8 = (x * scale).to(torch.float8_e4m3fn)\nListing 5.1: Dynamic scaling for FP8 scales tensors at runtime by the absolute\u0002maximum of the tensor.\nThe scaling operation is performed before each FP8 matrix mul\u0002tiplication, incurring additional computational overhead. However,\ndynamic scaling is the most accurate scaling mode, since the scale is\nalways determined by the current runtime value of each tensor.\nDelayed scaling uses a running history of absolute maximum values\nof the same tensor to compute an approximate scale for downcasting.\nThis approach incurs additional implementation complexity as well\nas memory overhead, as a buffer of previous absolute maximum\n42\nvalues must be maintained for every tensor. However, this approach\nlends itself better than dynamic scaling to fusing the calculation of\nthe current runtime absolute maximum to be stored in the history\nwith the downcasting operation itself, yielding potential performance\nbenefits.\nFinally, static scaling requires that a scale be precomputed and\nkept constant throughout training. Thus, while this approach has\nno additional overhead other than casting itself, it is most suitable\nfor inference and, even so, reduces accuracy since the scales are not\ndetermined based on runtime values. A possible compromise between\nthese tradeoffs is to use static scales for weights but dynamic scales\nfor activations.\nEach mode of FP8 training has its own tradeoffs with respect to\nper-tensor quantization accuracy, performance, and opportunities for\nkernel fusion optimization. In this study, we opt for dynamic scal\u0002ing due to its accuracy compared to delayed and static scaling. We\nprovide benchmarking results for FP8 training with dynamic scaling\nand inference, profiling to demonstrate opportunities for optimiza\u0002tion, and different approaches to reducing overhead and realizing\ncomputational gains from deploying models in 8-bit floating point.\n5.1.2 Benchmarking FP8 Training and Inference\nBenchmarking FP8 training and inference on a single NVIDIA H100\nGPU when naïvely converting the linear projections of the Mamba2\narchitecture at three model sizes–780M, 1.3B, and 2.7B parameters–\nto Float8Linear modules can cause slowdowns of up to 40% for\ninference and up to 45% for training. Benchmarking was performed\nfor batch sizes ranging from 8 to 256 examples for inference and 8 to\n32 examples for training, subject to GPU memory constraints for each\nmodel size, for a sequence length of 1024.\n43\nFigure 3: FP8 quantization without systems optimization of additional\ncasting overhead causes significant slowdowns at inference time.\nSlowdown is expressed for each model size and batch size as a\nproportion of per-iteration inference latency for the BF16 model.\nFigures 3 and 4 illustrate that naïvely applying FP8 quantization\nleads to scaling and casting overheads that significantly outpace the\ncomputational gains created by performing matrix multiplications\non tensor cores. The slowdown becomes less severe for larger model\nsizes and larger batch sizes, especially at inference time, as the size of\nthe matrix multiplications performed on the model parameters hides\nmore of the additional casting overhead.\nFigure 4: FP8 training exhibits similar overhead issues that hinder the\nrealization of computational gains from FP8 matrix multiplications.\nThese benchmarks demonstrate the need to more carefully examine\nthe sources of overhead introduced by FP8 linear projections with\nprofiling, which can help to determine performance bottlenecks and\ntarget areas for optimization.\n44\n5.1.3 Profiling FP8 Inference\nIn order to better understand where the significant additional over\u0002head introduced by FP8 linear layers and the optimizations required\nto realize their computational gains, we profile FP8 inference for all\nthree model sizes. Profiling FP8 inference reveals that the operations\nfor computing scaling factors for each tensor before FP8 matrix multi\u0002plications and casting tensors to 8-bit formats causes gains from the\nvastly faster FP8 matrix multiplication cuBLAS kernels to be hidden,\nleading to an overall slowdown in these linear projections.\nFigure 5: A profile of BF16 inference on the Mamba2-780M model for a batch\nsize of 32 demonstrates that, though the BF16 GEMM kernel has\nsome kernel launch overhead, there are no intermediate kernels\ncalled between the preceding LayerNorm kernel and the GEMM\nkernel for the linear layer. The time from the completion of the\nLayerNorm kernel to the completion of the GEMM kernel is 1.539\nms.\nThe BF16 inference profile shown in Figure 5 and FP8 profile in\nFigure 6 show the kernels called for the same projection module of\na Mamba2-780M model processing a batch size of 32. We choose a\nsmaller model size to profile to show a more extreme case of the\nslowdowns observed in benchmarking.\nThe empty spaces in each timeline show GPU idle time, where the\nGPU is not performing any computation. These idle times occur due\nto kernel launch overhead, which can become dominant in overall\nexecution time when many smaller kernels are launched in succession.\nAlthough the BF16 GEMM kernel shown in figure 5 has its own kernel\n45\nFigure 6: In contrast to figure 5, this profile of FP8 inference on the same\n780M parameter model with batch size of 32 shows that many\nkernels, performing scaling and casting operations on the input\ntensors before the matrix multiplication, are launched before the\nFP8 GEMM kernel is called. Therefore, although the FP8 GEMM\nkernel itself is 1.88× faster than the BF16 GEMM kernel, the entire\nprojection operation from the completing of the LayerNorm to the\ncompletion of the projection takes 2.884 ms, nearly twice as long.\nlaunch time, no other kernels are launched before it, since the output\nof the LayerNorm kernel is already in BF16. However, since tensors\nare dynamically scaled and cast to FP8 during FP8 inference, many\nsmaller kernels that compute the absolute maximum of the kernel,\nclamping tensor values, and casting the kernel to the E4M3 floating\npoint format must be executed, each with its own overhead. Therefore,\neven though the FP8 GEMM kernel takes only 462.7 µs, nearly twice\nas fast compared to the BF16 GEMM kernel’s 866.6 µs, the overall\nlatency of the projection computation increases by 80%.\nThese observations lead to a set of systems performance optimiza\u0002tions that are needed to mitigate these overheads and realize the\ncomputational gains promised by FP8.\n5.1.4 Optimizations for Efficient FP8 Computation\nWe identify two key performance optimizations for mitigating slow\u0002downs from the computational overhead of downcasting tensors to\nFP8 and kernel launch overhead from these operations. Careful appli\u0002cation of these techniques will allow the multiplicative gains of FP8\n46\nFigure 7: Using the TorchInductor compiler fuses the 15-20 individual cast\u0002ing and scaling kernels preceding FP8 GEMM kernels into four\noperations with custom Triton kernels. The use of fused kernels\nlimits the end-to-end time of the projection operation to 1.464 ms.\nGEMM kernels to be realized in end-to-end training and inference\ntimes.\noperation fusion & hardware sharing Profiling FP8 in\u0002ference reveals that a single FP8 GEMM kernel is preceded by 15-20\nindividual kernels for computing the absolute maximum of the tensor\nfor the scaling factor, performing scaling and casting, and memory\nreads & writes, before the efficient GEMM can even be executed. In\nscenarios where many small operations must be performed on a sin\u0002gle tensor, operator fusion can be used to combine multiple CUDA\nkernels into a single kernel. Each individual kernel requires loading\ntensors from memory, executing the operation, and writing them back\nto memory. The execution of the operation is often not even the bottle\u0002neck of the operation, as these pointwise operations can be become\nmemory-bound. The problem is further exacerbated by the fact that\nmany of these kernels need to be launched and executed to complete\nsuch custom operators like scaling and casting. Therefore, fusing these\noperations into a smaller number of kernels can be beneficial for\ncoalescing memory reads and writes [29].\nThe TorchInductor compiler in PyTorch automatically fuses kernels\ninto custom Triton code. To compile the FP8 linear projections of the\nmodel, torch.compile is called on each input and output projection,\nsuch that a fused kernel is generated for the scaling and casting\n47\noperations. Figure 7 shows a profile of inference with fused kernels,\ndemonstrating how the individual downcasting kernels are fused into\na smaller set of Triton kernels, reducing the overhead of the linear\nprojection operation to be on par with BF16 inference.\nAlthough more custom fusion is possible by writing a custom FP8\nlinear kernel which combines scaling, casting, and the matrix multi\u0002plication itself into a single kernel, this example demonstrates how\noperation fusion can be used to coalesce data movement and reduce\ncomputational overhead of tensor operations.\nFigure 8: Benchmarking data for Mamba2 inference with FP8 and operation\nfusion with torch.compile with max-autotune mode. Fusing scal\u0002ing operations allows FP8 inference to be competitive with BF16\ninference, but still only provides up to 5% of speedup over higher\nprecision.\nkernel launch overhead reduction Operator fusion brings\nthe latency of the FP8 linear projection to be similar to the BF16 infer\u0002ence time. However, this is still far from the multiplicative speedups\npromised by FP8. Furthermore, it is still not enough to ensure that\nthe nearly 2× acceleration of the FP8 GEMM is seen in the end-to-end\ninference latency, as shown in Figure 8.\nThe idle time shown by the empty spaces in the timeline in Figure 7\ndemonstrate that kernel launch overhead is still a dominating factor in\nthe end-to-end latency of FP8 inference. Even though fusing scaling op\u0002erators mitigated some of the overhead introduced by downcasting, it\nalso lead to the proportion of GPU idle time and kernel execution time\nto degrade. Therefore, operator fusion and reducing kernel launch\n48\noverhead need to be combined in order to allow the accelerated FP8\nGEMM time to dominate the linear projection operation.\nCUDA Graphs [13] enable sequences of GPU operations to be de\u0002fined as graphs, allowing a series of kernels to be launched through\na single CPU operation, thus reducing kernel launch overhead from\nindividual kernels. In PyTorch, a CUDA graph can be constructed\nby capturing operations executed in a CUDA stream, recording the\nwork performed in that stream as a graph of operations. The work\ncaptured in the graph can then be replayed, launching the entire se\u0002quence operations at once. Since the same memory addresses are used,\nby replacing the data in those memory addresses, different batches\nof data can be processes in the same way that inference and training\nwould normally be performed.\nGiven that compiled FP8 inference introduces significant kernel\nlaunch overhead from multiple small kernels being launched preced\u0002ing the GEMM operation, a CUDA graph can be used to capture the\nscaling, casting, and matrix multiplication operations into a single\ngraph. Then, the graph can be replayed in lieu of the separate kernels\nbeing launched.\n5.2 characterizing model performance for fp8 training\n& inference\nWhile existing work has introduced the concept and implementations\nof FP8 training and demonstrated its success in training transformer\u0002based models such as GPT and Llama models, how FP8 affects both\nloss convergence with higher-precision training and accuracy on down\u0002stream tasks on a wider variety of model architectures and datasets is\nnot well characterized or understood. To this end, we propose three\nresearch questions which help us to build a better understanding of\nwhen applying FP8 training is actually beneficial in terms of both\nperformance and accuracy:\n49\n1. Does FP8 training always achieve loss parity, or approximate\nloss convergence at almost every step of training when keeping\ndatasets and initialization equal, with BF16 training?\n2. Does FP8 training improve or degrade quantized performance\non downstream tasks?\n3. How does switching to FP8 training at different temporal phases\nof fine-tuning impact training stability?\nThe first question regarding loss parity is the preliminary metric\nof whether or not the quantization error introduced by FP8 training\ncauses divergences in loss trajectories. Since the loss curve during\ntraining is the most direct view of the optimization trajectory of the\nmodel, it is the first stage of our investigation into instabilities or\ndegradation which FP8 training may cause.\nWhile loss parity may be achieved, an equal loss may not always\ncorrespond to equal performance on relevant benchmarks. Thus, it\nis critical not just to examine whether equal loss is achieved and if\nloss is numerically stable, but whether or not quantization impacts\nperformance on downstream tasks, particularly those for which it is\nfine-tuned.\nFinally, inspired by early work in precision scheduling, we investi\u0002gate how applying FP8 at various points of fine-tuning impacts its loss\nstability. This investigation opens up a frontier of training time, given\nthat FP8 provides computational savings, and accuracy loss, for which\nwe can use intermediate metrics and signals to predict and navigate.\n5.2.1 Metrics for Model Performance in Quantized Finetuning\nWe identify three metrics for evaluating models finetuned with FP8\nquantized training, each of which signal different information about\nthe effectiveness of fine-tuning and the impact of quantization on the\nmodel architecture based on the training dataset. Evaluating models\n50\non all three metrics is key to building a comprehensive, context-aware\nunderstanding of how quantization impacts downstream accuracy as\nwell as the optimization dynamics of quantized training.\n1. Improvement in downstream task accuracy between baseline\npretrained model and finetuned model for BF16 training and FP8\ntraining\n2. Full-precision-to-quantized inference accuracy divergence when\nusing BF16 and FP8 training\n3. Quantized inference accuracy divergence between BF16 and FP8\ntraining\nThe first metric provides a holistic view of how responsive the\nmodel is to the finetuning dataset–in other words, how difficult the\ntask is relative to the pretraining dataset and the model architecture,\nand how quantization impacts the model’s ability to learn the task.\nThe second and third metrics provide two subtly-different abla\u0002tions to carefully study the quantization dynamics of the model. Full\u0002precision-to-quantized inference accuracy divergence when using BF16\nand FP8 training signifies the effectiveness of quantized training at\ninjecting quantization error into the optimization objective during\ntraining, making the finetuned model more robust to quantization\nerror at inference time. To measure this metric for a finetuned model,\nthe training regime (model, dataset, use of quantization) is kept con\u0002stant, while the precision of the weights at inference time are varied\nfrom high precision to FP8. Quantized inference accuracy divergence\nbetween BF16 and BF16, on the other hand, requires that the inference\nprecision is kept constant at 8-bit floating point, whereas the training\nregime is varied from quantized training to BF16 training. This met\u0002ric measures whether quantized optimization, rather than inference\nquantization, corrupts the model’s downstream performance.\n51\n5.3 experimental methodology\nIn order to answer each of the proposed research questions and collect\nobservations with respect to the metrics of model performance, we\ndevise a set of experiments for building a structured characterization\nof the quantization landscape.\nFor each model and dataset pair, a series of quantized and high\u0002precision training runs is performed to answer each of the research\nquestions and ablate the use of FP8 training and examine its impact\non each metric of interest. For every set of hyperparameters that\nmay be used in training or finetuning, a pair of training runs must\nbe conducted–one with FP8 training, and one with high-precision\ntraining.\nAcross each training run in BF16, 10 model checkpoints per epoch\nare also taken and saved to disk. Each of these checkpoints is then\nused as a starting point for FP8 training, forming the experimental\nstudy of the temporal degree of freedom of quantized optimization.\nThus, across 2 epochs, we will have 20 joint BF16/FP8 loss trajectories\nwith different starting points across the optimization process. The\ngranularity of starting points can be adjusted based on the sensitivity\nof the model to scheduled quantization.\nEach of the training runs yields a final checkpoint, which can be\nused to evaluate the downstream accuracy of the model with respect\nto the relevant task or application. Inference can be performed with\nvarying inference quantization configurations. For example, for FP8\ninference, we can choose to quantize only the weights or dynamically\nquantize activations as well.\n5.3.1 Diagnosing Quantization Instability with Gradient Norms\nFor all of these investigations, it is important not only to empirically\nobserve these trends, but also to understand how the underlying me\u000252\nchanics of the model architecture and of training may be predictive of\nor even explain these observations. In this study, we use the Frobenius\nnorms of the gradients of each layer of the model at various points of\ntraining as a proxy for the smoothness and numerical stability of optimiza\u0002tion. By studying how applying quantization perturbs trends in the\ngradient norm, we form the basis for how we might be able to guide\na user towards the optimal quantization strategy using a early diag\u0002nostic metric. Devising such a metric is particularly important given\nthat existing literature and intuition does not provide a systematic\nand quantitative manner for studying quantized optimization. Our\nhypothesis is that gradient norms can provide a diagnostic signal for\nthe sensitivity of the model and individual layers of the architecture of\nquantization during optimization, rather than a more static measure\nbased on the properties of the layer outside of its training context.\n5.4 conclusion\nIn this chapter, we present a unified framework for developing a\ncontext-aware and exhaustive characterization of the quantization\nlandscape with respect to the metrics of hardware utilization, training\nstability, and downstream accuracy.\nWe demonstrate with benchmarking data and CUDA kernel profil\u0002ing that realizing the computational gains promised by quantized train\u0002ing and inference requires significant systems performance engineer\u0002ing. We propose techniques for fusing casting and scaling operations\nwith FP8 matrix multiplications and for reducing kernel launch over\u0002head to bring quantized training closer to the throughput speedups\npromised by tensor core hardware.\nWe then establish the key research questions that must be answered\nabout each quantization configuration in the design space to character\u0002ize the impact of quantized training on its convergence properties and\ndownstream accuracy. In order to investigate each of these questions,\n53\nwe propose a series of experiments and metrics for evaluating the via\u0002bility of a quantization recipe while isolating the effect of quantization\non downstream accuracy and training stability.\nTogether, the systems engineering considerations we propose as\nwell as the experimental methodology for characterizing the impact of\nquantized training on numerical stability and accuracy form a frame\u0002work for comprehensively understanding how a model architecture,\ndataset, and downstream task respond to various choices in the design\nspace of quantized optimization and inference.\n54\n6\nC A S E S T U D Y O F F P8 Q U A N T I Z E D T R A I N I N G F O R\nM A M B A2\nThis chapter presents an empirical case study of FP8 quantized train\u0002ing for Mamba2, based on the experimental framework for structured\nunderstanding of quantization dynamics proposed in the previous\nchapter. This study serves as a proof-of-concept of our strategy for\nbuilding a structured characterization of quantization dynamics for a\ngiven context, including a model architecture, dataset(s), and down\u0002stream tasks.\n6.1 data regimes\nThe three research questions posed in the previous chapter for struc\u0002turing our interrogation of the quantization landscape provide the\nintuition and motivation for a set of experiments for devising a context\u0002aware case study of quantization. We conduct an empirical study of\nFP8 training dynamics for Mamba2 across model sizes in a data-aware\nmanner on three finetuning datasets and tasks:\n1. Orca-Math-dataset [21], a synthetic dataset of 200K math prob\u0002lems paired with solutions from GPT-4-Turbo, evaluated against\nthe GSM8K benchmark of 8.5K grade school math problems,\n2. The Chinese language split of xP3 (Crosslingual Public Pool\nof Prompts) [23], a multilingual instruction-following dataset,\nevaluated on CMMLU, a multitask language understanding\ndataset in Chinese, and\n3. Vezora’s CodeTester dataset, a Python dataset with over 143K\nexamples of working Python code in a question-and-answer\n55\nformat, evaluated on HumanEval, a code generation benchmark\nfor LLMs\nAll datasets were sourced from HuggingFace, and evaluations were\nperformed using LM-EVAL [2]. For xP3, given the large size of the\ndataset, a subsample of 200K examples were used as the training split.\nThese three datasets were chosen to test how Mamba2 would re\u0002spond to finetuning on tasks that are intuitively different from those\nincluded in its pretraining dataset, the Pile [12], and the benchmarks\non which it is commonly evaluated. Using a more diverse collection of\ndatasets allows us to draw insights on a wider range of contexts, based\non the difficult of each task for the model, the information content of\neach dataset compared to the pretraining dataset, and the difficult of\nthe downstream task conditioned on learning from the dataset.\n6.2 experimental setup\nWe fine-tune Mamba2 at three different sizes–780M, 1.3B, and 2.7B–\nfrom the models available on HuggingFace. These models were pre\u0002trained on the Pile, a common language modeling dataset, and have\ndemonstrated competitive performance with state-of-the-art trans\u0002former models on standard language modeling and reasoning bench\u0002marks. We use the float8 API from PyTorch to perform model\nrewrites and invoke FP8 training and inference. Training is performed\non 4 NVIDIA H100 GPUs on the Harvard FASRC research cluster,\nusing Fully-Sharded Data Parallelism (FSDP) with float8 all-gather\nsynchronization to shard the model across GPUs. Following from the\npre-training recipe given for Mamba2, we fine-tune the model with\nthe following hyperparameters:\n• Cosine annealing learning rate decay with linear warmup from\n1/10th of peak learning rate\n• Batch size of 0.5 million tokens (sequence length of 1024, batch\nsize of 16 examples, gradient accumulation of 32 steps)\n56\nWe sweep learning rates α ∈ {10−3\n, 5 × 10−3, 10−4, 5 × 10−4}. Mod\u0002els were fine-tuned on 2 epochs of the training split of each dataset.\nAs a baseline, we use PyTorch AMP’s mixed-precision training with\nBF16. All training and evaluation is conducted with the same random\nseed to ensure that the same batches of tokens are processed by all\nruns. For each of these experiments, gradient norms are recorded 100\ntimes per epoch of training (or 200 times total, given that models were\nfinetuned for two epochs).\nWe also take model checkpoints throughout BF16 training for con\u0002verting to FP8 training at various phases of fine-tuning. Since the\nmodel is sharded across GPUs, we used the Distributed Checkpointing\n(DCP) feature of PyTorch, such that each GPU saves its own sharded\nweights of the model in separate disk files, for seamless resumption of\ndistributed training.\n6.3 fp8 loss parity\nOur study of BF16/FP8 loss parity across the three datasets reveals\ndata-dependent instabilities and loss divergences that arise during\nfinetuning, with loss spikes and instabilities being especially prevalent\nin higher learning rate regimes. Full learning curves for all three model\nsizes and the three datasets are shown in Appendix A.1.\nAlthough we demonstrate loss parity between quantized and un\u0002quantized finetuning on the xP3 Chinese reasoning dataset (Figure 9)\nacross learning rates, instabilities begin to occur for the Orca-Math\nand CodeTester datasets for the highest learning rate in the sweep. In\nthe grade school math dataset (Figure 10), a loss spike not observed\nin BF16 finetuning occurs after around 5 billion tokens are processed,\ncausing an unrecoverable loss divergence in this training run. In the\nCodeTester Python dataset (Figure 11), while loss instability is ob\u0002served for both BF16 and FP8 training, they occur differently for the\ntwo regimes. In both BF16 and FP8 training, loss spikes begin after\n57\nFigure 9: Mamba2 1.3B FP8 and BF16 learning curves across learning rates\nfor the xP3 Chinese reasoning dataset demonstrate that loss parity\ncan be achieved for FP8 finetuning.\naround 0.5B tokens are processed. However, stability is recovered in\nBF16 after around 100 million additional tokens are processed, whereas\nFP8 training takes almost twice as long to recover.\nThe variance in the effects of FP8 quantized training from dataset\nto dataset confirms the data-dependent nature of quantized opti\u0002mization dynamics. While some datasets, like the xP3 Chinese dataset,\nmay not require variation across layers or phases in quantization preci\u0002sion or technique to achieve loss parity with higher-precision training,\nothers like Orca-Math datasets and CodeTester datasets trigger more\nsensitivies to quantization error during optimization.\n6.3.1 Layerwise Clustering in Gradient Norm Behavior\nClustering between the gradient norms of early and late Mamba2\nblocks in FP8 training suggests that quantized optimization pro\u0002motes gradient propagation across the model architecture. For the\ngradient norms collected at periodic steps for each weight during\ntraining, norms were grouped based on which quartile of layers they\n58\nFigure 10: Mamba2 1.3B FP8 and BF16 learning curves across learning rates\nfor the Orca-Math grade school math dataset reveal loss spikes that\nmay be caused by quantization error in FP8 quantized training, as\nthey do not manifest in identical BF16 runs.\nbelonged to (e.g. the first quarter of blocks, the second quarter of\nblocks, etc.). Full gradient norm and loss trajectories can be found in\nAppendix A.2.\nWe then compare gradient norm trajectories, keeping dataset and\nlearning rate constant, across training precisions. Figures 12, 13, and\n14 show the loss and gradient norm trajectories across the four layer\ngroups of Mamba2 1.3B for the CodeTester, Orca-Math, and xP3 (Chi\u0002nese reasoning) datasets, respectively.\nBy grouping the gradient norms into earlier and later blocks of the\nmodel, we reveal that gradient norms exhibit a stratified pattern with\nrespect to layer group, with later layers (darker gradient norm lines)\nhaving lower gradient norm magnitudes than earlier layers (lighter\ngradient norm lines) in BF16 training. On the other hand, training\nwith quantized optimization in FP8 leads to the magnitude lag of late\ngradient norms to become less apparent, with these gradients tending\nto rise to be closer to the center of the gradient norm distribution.\nFigure 15 shows the distribution of absolute differences between\nthe log median of group 4 gradient norms and the mean log gradient\n59\nFigure 11: Mamba2 1.3B FP8 and BF16 learning curves across learning rates\nfor the CodeTester dataset suggest that loss instabilities occur\ndifferently with respect to tokens processed when instabilities\noccur based on training precision.\nnorm, isolated by training precision. The histograms clearly show a\ndistributional shift in gradient norm variance between BF16 and\nFP8| training, indicating that FP8 training has noteable implications\non the optimization trajectory of the model and layerwise dynamics\nduring training.\nThese observations confirm that gradient norms can be an effective\ndiagnostic tool for the stability of optimization and the sensitivity\nof the model and dataset to propagation of gradients and errors\nacross the model architecture. They also show that there is a strong\narchitectural component to the effect that FP8 quantized training has\non the optimization dynamics of the model, which may correlate\nwith Mamba2’s computational model. More comparison with other\ntraining dynamics of other models would reveal further similarities\nand differences between the effect of quantized training on the shape\nof the gradient norm distribution across layers.\nFurthermore, the differences between how quantized training\nimpacts the gradient norm relationships across layers from dataset\nto dataset further supports the data-dependent nature of quantized\n60\nFigure 12: Loss and gradient norm trajectories for BF16 and FP8 training\nof Mamba2 1.3B on the CodeTester dataset. Although the BF16\ntraining run shows greater fluctuations in gradient norms across\nlayers (around the 11Kth step), comparing the two layer group\u0002wise patterns demonstrates that the magnitudes of late layers’\ngradients are amplified by FP8 training.\nFigure 13: Loss and gradient norm trajectories for BF16 and FP8 training of\nMamba2 1.3B on the Orca-Math dataset visually illustrates the\nphenomenon of clustering of later layer gradient norms towards\nthe center of the norm distribution.\ntraining dynamics. As we can see from the differences in gradient\nnorm patterns between datasets (e.g. between figures 12, 13, and 14),\nnot only do the gradient norms associated with training on each\ndataset have vastly different oscillatory behavior, but they also exhibit\ndifferent spreads between layers and gradient norms and sensitivity\nto gradient norm spikes.\n61\nFigure 14: Loss and gradient norm trajectories for BF16 and FP8 training of\nMamba2 1.3B on the xP3 Chinese reasoning dataset, like Figure\n13, illustrates the clustering effect of late gradient norms.\nFigure 15: FP8 training promotes gradient norm clustering of the final layer\ngroup towards the gradient norm mean when considering norms\nstepwise.\n6.4 effect of fp8 quantization on downstream task per\u0002formance\nBased on each of the three evaluative metrics established in Chapter 5\nfor understanding the effect of FP8 quantization on model performance\nand, specifically, downstream task accuracy, we present findings that\ndemonstrate the FP8 does not consistently lead to improved or more\nrobust quantized accuracy on downstream tasks, and can in fact lead\nto catastrophic accuracy degradation where the quantized, finetuned\nmodel fails to perform on a single downstream task instance. Full\ndownstream accuracy results across model sizes, finetuning datasets,\nand hyperparameters can be found in Appendix A.3.\n62\nFigure 16: Improvement after BF16 and FP8 finetuning compared to baseline\npretrained model performance on relevant downstream tasks for\nOrca-Math, xP3 (Chinese reasoning), and CodeTester datasets\nillustrates how the three datasets represent three different data\nregimes based on task difficulty and dataset learnability.\nFigure 16 validates that the three datasets used for these experiments\ndo indeed represent three different data regimes, based on the diffi\u0002culty of learning the downstream task by finetuning from the training\ndataset. The Orca-Math dataset is remarkable effective at improving\nMamba2 performance on the GSM8K grade school math benchmark.\nOn the other hand, Chinese reasoning ability as measured by the\nCMMLU benchmark is more difficult for an English-trained model to\nacquire, with finetuning only demonstrating modest sub-1% improve\u0002ments to accuracy. Surpringly, finetuning on the Alpaca-instruction\u0002following Python dataset CodeTester degrades model accuracy at\ncode generation as measured by HumanEval by single digit percent\u0002age points. In all three of these tasks, FP8 finetuning limits the\ncapabilities of the model, either degrading the model accuracy more\nthan BF16 training as is the case with code generation or curbing\nperformance improvement as in the grade school math dataset.\nInference divergence on each downstream task based on whether the\nmodel was quantized at inference time grouped by training precision\nis shown in figure 17. In all but one setting, in the case of the Chinese\nreasoning dataset and task, quantized training does not improve upon\nmodel accuracy at inference time when the model is quantized, even\nthough the model weights are produced by a quantized optimization\n63\nFigure 17: Model accuracy divergence on downstream tasks between full\u0002precision and 8-bit floating point quantized inference, grouped by\ntraining precision (BF16 or FP8), demonstrates that FP8 training\ndoes not show a clear benefit for the robustness of models to\nquantization error. Positive bar values indicate that quantized\ninference performed better than unquantized inference. Bars are\ngrouped by dataset and are in order of increasing learning rate.\nprocess. In most cases, we see an up to 3.7% accuracy degradation due\nto quantization at inference time. While overall this is a relatively slight\ndrop in accuracy, the sensitivity of the natural language output quality\nto performance degradations can vary from task to task. Therefore,\nbuilding a characterization such as this of the effects of inference\nquantization can be helpful for a user to assess different strategies\nbased on the task’s error tolerance.\nFigure 18: Model accuracy divergence on downstream tasks for 8-bit floating\npoint quantized models based on training precision. FP8 training\ntends to cause degradations in best quantized model accuracy,\nwith the degree of accuracy degradation being positive correlated\nwith model size.\n64\nIsolating the quantized model accuracy and analyzing divergence\nfrom the perspective of training precision, our final evaluative metric,\ntells a more dire story for FP8 quantization. Figure 18 features exam\u0002ples of severe degradation of a model due to quantized training, where\nusing FP8 training sets back the quantized model accuracy by over\n15%. In fact, multiple training runs exhibit catastrophic degradation,\nwhere the model accuracy is at a constant 0% after FP8 finetuning.\nThe demonstration of such instability is a warning sign for the use\nof FP8 training and is critical to identify for architecture-data con\u0002texts, suggesting that the optimization path has the ability to destroy\nthe model’s ability to perform the downstream task on any problem\ninstance.\nFigure 19: Catastrophic degradation events are more common in optimiza\u0002tion pathways with FP8 training compared to BF16.\nEnumerating cases of catastrophic degradation, cases where the\nmodel accuracy is driven to exactly 0 due to finetuning, demonstrates\nthat this phenomenon is more common when FP8 training is in\u0002volved in the finetuning process than with BF16 training (Figure\n19). While this finding is not necessarily surprising given the intuition\nthat FP8 optimization removes critical information from the process of\nlearning from the finetuning dataset such that the model’s weights are\n“driven off course,\" the prevalence of this event across quantization\nrecipes for a single context is alarming and noteable.\n65\n6.5 scheduling fp8 training\nScheduling FP8 training–beginning finetuning in BF16 before convert\u0002ing linear layers to FP8 for the remainder of finetuning steps–reveals\nnovel insights into how the loss convergence, gradient norm, and\ndownstream accuracy divergence properties of a model archiecture\nand dataset context respond to a temporal quantization scheduling\ntechnique that varies the precision of optimization across time. The\nfull set of scheduled FP8 training experiments, with their learning\ncurve and gradient norm trajectories, can be found in Appendix A.4.\nFigure 20: Scheduled quantization for BF16 → FP8 finetuning of Mamba2\n1.3B on the xP3 dataset (learning rate 1 × 10−4) demonstrates that\nthe change in gradient norm behavior can be attributable to the\nconversion to 8-bit floating point precision.\nFigure 20 shows two instances of scheduled FP8 finetuning, where\nthe conversion to 8-bit floating point occurs at an earlier and later stage\nof finetuning on the xP3 Chinese reasoning dataset. Applying this\nscheduled technique for this model-dataset context does not introduce\nclear loss divergence or discontinuities at the BF16/FP8 boundary,\nindicated by the red vertical lines. However, visualizing the gradient\nnorms further reinforces our finding that the change in gradient norm\nbehavior–the clustering of gradient norms of later layers together with\nthose of the rest of the model–can be attributable to the 8-bit quantized\noptimization, even when applied in the middle of finetuning.\nThe same scheduled quantization strategy applied on the Orca-Math\ngrade school math dataset reveals not only that conversion to FP8\n66\nFigure 21: Scheduled quantization for BF16 → FP8 finetuning of Mamba2\n1.3B on the Orca-Math dataset (learning rate 5 × 10−3) reveals that\nstarting 8-bit floating point optimization can introduced numerical\ninstabilities that manifest loss spikes and divergences coupled with\ngradient norm explosions both at the conversion boundary and\nlater on in training.\nFigure 22: Scheduled quantization for BF16 → FP8 finetuning of Mamba2\n1.3B on the CodeTester Python dataset (learning rate 1 × 10−4)\ndemonstrates that gradient norm discontinuities can occur even\nin the absence of loss spikes or divergences.\ntraining can introduce loss divergence and gradient norm explosions,\nbut also that they can occur both at the conversion boundary (the left\nplot in figure 21) or later on during quantized training (the right plot\nof figure 21), even when the loss curve and gradient norm trajectory\nis continuous at the BF16/FP8 boundary. These observations are a\ncritical finding for our characterization of how quantization error and\nnumerical instabilities from 8-bit floating point optimization can\npropagate both in the layer dimension, leading to gradient norm\nirregularities, and the temporal dimension, causing these instabilities\nto manifest billions of tokens later in the finetuning process.\n67\nFinally, applying scheduled quantization while finetuning on the\nCodeTester Python-generation dataset reveals that gradient norm dis\u0002continuities can occur without observing loss spikes or learning curve\ndivergences. The gradient norms to the left of the FP8 start boundary\non the right-side graph of Figure 22 shows the magnitudes of gradi\u0002ents in BF16. Comparing the magnitude of gradients at step 1887 in\nthe BF16 training run compared to the gradient norm magnitude spike\nshown on the left-side graph reveals that the FP8 training conversion\ncauses gradient norms to be magnified. However, these training runs\npreserve loss convergence with their corresponding BF16 learning\ncurve, demonstrating that gradient norm irregularities and loss spikes\nare not perfectly coupled. The independence of these observations\nmotivates the study of gradient norms in addition to only loss curves\nas diagnostic of potential instabilities introduced by quantization,\nas these could be correlated to accuracy degradation at inference time.\n6.6 conclusion\nThe empirical case study of FP8 quantized training for Mamba2 re\u0002veals novel contributions to our understanding of quantization dynam\u0002ics during optimization and inference, providing a robust proof-of\u0002concept of the structured framework introduced in Chapter 5 for build\u0002ing insightful and comprehensive characterizations of the quantization\nlandscape in a context-aware manner. Our exhaustive evaluation and\nnavigation of this design space reveals the following insights about\nquantization for Mamba2:\n(a) We observe data-dependent loss instabilities and divergences\nduring quantized fine-tuning of Mamba2, confirming our hy\u0002pothesis that numerical instabilities introduced by quantization\nmust be characterized in a dataset-aware manner (Section 6.3)\n(b) With gradient norms as a proxy for the smoothness or instability\nof optimization, we discover that FP8 quantized training pro\u000268\nmotes clustering of the gradient norms in later layers towards\nthe mean gradient norm. The distributional shift of gradient\nnorms across layer groups indicates that quantized optimization\nhas implications for how quantization error is propagated across\nlayers, and that this shift depends on the dataset. (Section 6.3.1)\n(c) Using the evaluative metrics for downstream performance of\nquantized models and ablative studies of quantization at infer\u0002ence and training time, we reveal that FP8 does not always lead\nto superior or more robust quantized model performance in\nterms of accuracy. In fact, we observe that quantized training in\nFP8 increases the likelihood that finetuning leads to catastrophic\ndegradation events, where the model fails to correctly answer a\nsingle problem instance of the downstream task (Section 6.4)\n(d) We explore the implications of scheduled quantized training,\nwhere FP8 quantization is introduced a various phases of fine\u0002tuning, on gradient norm and loss convergence properties of the\nmodel-data context. (Section 6.5)\nOur findings confirm that:\na) Gradient norm clustering is attributable to conversion to 8-\nbit floating point optimization, even with FP8 is introduced\nin the middle of BF16 finetuning\nb) Quantization error and loss instabilities can propagate in\nboth the layer dimension and the temporal dimension, lead\u0002ing divergences to occur both at or billions of tokens after\nthe BF16/FP8 conversion boundary\nc) Gradient norm instabilities observed in the absence of loss\ndivergences or discontinuities motivates the study of gradi\u0002ent norms as a diagnostic metric, even when learning curve\nparity is present\nThe rich characterization of FP8 training and inference dynamics for\nMamba2, with a host of novel contributions to our understanding of\n69\nquantized state space models, demonstrates the power of our unified\nframework for strategic quantization for building a systematic under\u0002standing of this complex design space to form the basis for guiding a\nuser towards viable quantization recipes.\n70\n7\nC O N C L U S I O N\nThis thesis presents a novel, unified framework for building exhaustive\nand comprehensive characterizations of the quantization design space\nfor the specific context–model architecture, dataset, and downstream\ntask–to which quantization is applied. We propose a structuring of\nthe problem of quantization, establishing that architectural develop\u0002ments, dataset & task diversity, and new advances in GPU tensor core\nhardware to support FP8 quantized optimization, define the dimen\u0002sionality of the challenge of determining the optimal quantization\nstrategy for a given application. The combinatorial complexity of this\ndesign space motivates a unified framework for understanding and\nnavigating choices within that space that goes beyond existing, ad-hoc\nsolutions that are specific to particular model architectures (especially\ntransformers) and a limited range of datasets and downstream tasks.\nThis unified framework is founded upon two design principles for\nquantization–systems performance and model performance–for which\nwe propose evaluation metrics and critical research question to under\u0002stand their dynamics for a quantization context. With these questions\nin mind, we design a collection of diagnostic and ablative experiments\nfor deeply and systematically characterizing the quantization land\u0002scape and gleaning meaningful insights that can lead us towards a\nnavigation strategy for this design space.\nAs a proof-of-concept of our structured framework, we present\na case study of FP8 quantized training and inference dynamics for\nMamba2, motivated by its promise as an alternative architecture to\nthe attention mechanism and its distinct computational model. With\nthe experiments we propose, we reveal several novel contributions\nto our understanding of quantization dynamics for state space mod\u000271\nels, such as distinct patterns of gradient norm clustering due to the\npropagation of numerical instabilities and quantization error in both\nthe layer and temporal dimensions of quantized training, evidence\nof catastrophic accuracy degradation due to FP8 quantization, and\nthe data-dependence of loss divergence and gradient norm explosion\nbehaviors.\nfuture work\nOur contributions open up a new paradigm for the study of the quan\u0002tization design space in terms of quantization context. Our proposed\nunified framework for characterizing the quantization landscape leads\nto several avenues of potential future research:\n(a) Additional degrees of freedom: incorporating quantization gran\u0002ularity (tile-wise, axis-wise, group-wise, channel-wise) into our\nstructured characterization framework\n(b) Strategic navigation of the quantization design space: with an\nexhaustive characterization of the quantization design space,\nwe can automatically or semi-automatically guide a user to\u0002wards the optimal quantization framework, either by developing\nmathematical formulations or empirical heuristics for ruling\nout catastrophic quantization recipes or promoting promising\nconfigurations\n(c) Activation outliers: building upon existing work and studies of\nthe impact of activation outliers on quantization error, we can\nadd activation outliers as an additional diagnostic metric for\nnumerical instability in tandem with gradient norms\n(d) More models and datasets: additional case studies with our\nframework of other model architectures and modalities (e.g.\nimage, audio, video) to better understand the challenges of the\nquantization design space\n72\nA P P E N D I X\n73\n\nA\nE X P E R I M E N TA L R E S U LT S F O R F P8 Q U A N T I Z E D\nF I N E T U N I N G O F M A M B A2\na.1 loss parity experiments\nFigures 23, 24, and 25 show full learning curve results for FP8/BF16\nfine-tuning of three sizes of Mamba2 models across the three datasets\nfrom Chapter 6.\nFigure 23: Learning curves for Mamba2 models on the Orca-Math grade\nschool math dataset\n75\nFigure 24: Learning curves for Mamba2 models on the CodeTester Python\ndataset\nFigure 25: Learning curves for Mamba2 models on the xP3 Chinese reasoning\ndataset\n76\na.2 gradient norms\nThese figures show layer-wise gradient norm and loss curves for\nfinetuning across Mamba2 model sizes and datasets. The gradient\nnorms are grouped in terms of layer group, where there are four\ngroups across each model architecture.\nFigure 26: Layer-wise gradient norms and loss curves for xP3 fine-tuning of\nMamba2 780M.\n77\nFigure 27: Layer-wise gradient norms and loss curves for xP3 fine-tuning of\nMamba2 1.3B.\n78\nFigure 28: Layer-wise gradient norms and loss curves for xP3 fine-tuning of\nMamba2 2.7B.\n79\nFigure 29: Layer-wise gradient norms and loss curves for Orca-Math fine\u0002tuning of Mamba2 780M.\n80\nFigure 30: Layer-wise gradient norms and loss curves for Orca-Math fine\u0002tuning of Mamba2 1.3B.\n81\nFigure 31: Layer-wise gradient norms and loss curves for Orca-Math fine\u0002tuning of Mamba2 2.7B.\n82\nFigure 32: Layer-wise gradient norms and loss curves for CodeTester fine\u0002tuning of Mamba2 780M.\n83\nFigure 33: Layer-wise gradient norms and loss curves for CodeTester fine\u0002tuning of Mamba2 1.3B.\n84\nFigure 34: Layer-wise gradient norms and loss curves for CodeTester fine\u0002tuning of Mamba2 2.7B.\n85\na.3 downstream task performance\nThis section shows full downstream task performance for each model,\ndataset, learning rate, training precision, and inference precision con\u0002figuration. In the tables below, W8A8 indicates 8-bit weight quantiza\u0002tion and 8-bit dynamic activation quantization, whereas W8 indicates\n8-bit weight-only quantization. The dataset refers to the finetuning\ndataset.\nTable 2: Downstream task accuracy for Mamba2 780M\nDataset Learning\nRate\nTraining\nPrecision\nInference\nPrecision\nCMMLU\n(Acc.)\nCMMLU\n(Std.\nErr.)\nGSM8K\n(Acc.)\nGSM8K\n(Std.\nErr.)\nHumanEval\n(Acc.)\nHumanEval\n(Std.\nErr.)\nCodeTester 1E-04 BF16 BF16 0.2519 0.0040 0.0197 0.0038 0.0854 0.0219\nCodeTester 1E-04 BF16 W8 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nCodeTester 1E-04 BF16 W8A8 0.2518 0.0040 0.0144 0.0033 0.0305 0.0135\nCodeTester 1E-04 FP8 BF16 0.2467 0.0040 0.0159 0.0034 0.0366 0.0147\nCodeTester 1E-04 FP8 W8 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nCodeTester 1E-04 FP8 W8A8 0.2481 0.0040 0.0212 0.0040 0.0488 0.0169\nCodeTester 5E-04 BF16 BF16 0.2555 0.0041 0.0205 0.0039 0.0000 0.0000\nCodeTester 5E-04 BF16 W8 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nCodeTester 5E-04 BF16 W8A8 0.2513 0.0040 0.0129 0.0031 0.0061 0.0061\nCodeTester 5E-04 FP8 BF16 0.2489 0.0040 0.0190 0.0038 0.0000 0.0000\nCodeTester 5E-04 FP8 W8 0.2483 0.0040 0.0167 0.0035 0.0000 0.0000\nCodeTester 5E-04 FP8 W8A8 0.2555 0.0041 0.0136 0.0032 0.0000 0.0000\nCodeTester 1E-03 BF16 BF16 0.2532 0.0040 0.0212 0.0040 0.0000 0.0000\nCodeTester 1E-03 BF16 W8 0.2526 0.0040 0.0212 0.0040 0.0000 0.0000\nCodeTester 1E-03 BF16 W8A8 0.2510 0.0040 0.0144 0.0033 0.0000 0.0000\nCodeTester 1E-03 FP8 BF16 0.2488 0.0040 0.0182 0.0037 0.0000 0.0000\nCodeTester 1E-03 FP8 W8 0.2427 0.0040 0.0076 0.0024 0.0000 0.0000\nCodeTester 1E-03 FP8 W8A8 0.2502 0.0040 0.0091 0.0026 0.0000 0.0000\nCodeTester 5E-03 BF16 BF16 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nCodeTester 5E-03 BF16 W8 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nCodeTester 5E-03 BF16 W8A8 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nCodeTester 5E-03 FP8 BF16 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nCodeTester 5E-03 FP8 W8 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nCodeTester 5E-03 FP8 W8A8 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nOrca\u0002Math1E-04 BF16 BF16 0.2513 0.0040 0.0553 0.0063 0.0427 0.0158\nOrca\u0002Math1E-04 BF16 W8 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nOrca\u0002Math1E-04 BF16 W8A8 0.2496 0.0040 0.0675 0.0069 0.0305 0.0135\nContinued on next page\n86\nTable 2: Downstream task accuracy for Mamba2 780M (Continued)\nOrca\u0002Math1E-04 FP8 BF16 0.2523 0.0040 0.0455 0.0057 0.0061 0.0061\nOrca\u0002Math1E-04 FP8 W8 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nOrca\u0002Math\n1E-04 FP8 W8A8 0.2526 0.0040 0.0425 0.0056 0.0183 0.0105\nOrca\u0002Math5E-04 BF16 BF16 0.2526 0.0040 0.1463 0.0097 0.0183 0.0105\nOrca\u0002Math5E-04 BF16 W8 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nOrca\u0002Math5E-04 BF16 W8A8 0.2495 0.0040 0.1509 0.0099 0.0000 0.0000\nOrca\u0002Math5E-04 FP8 BF16 0.2481 0.0040 0.1114 0.0087 0.0000 0.0000\nOrca\u0002Math5E-04 FP8 W8 0.2491 0.0040 0.1001 0.0083 0.0000 0.0000\nOrca\u0002Math5E-04 FP8 W8A8 0.2526 0.0040 0.1183 0.0089 0.0000 0.0000\nOrca\u0002Math\n1E-03 BF16 BF16 0.2526 0.0040 0.2138 0.0113 0.0000 0.0000\nOrca\u0002Math1E-03 BF16 W8 0.2522 0.0040 0.1903 0.0108 0.0000 0.0000\nOrca\u0002Math1E-03 BF16 W8A8 0.2526 0.0040 0.1895 0.0108 0.0061 0.0061\nOrca\u0002Math1E-03 FP8 BF16 0.2511 0.0040 0.1615 0.0101 0.0000 0.0000\nOrca\u0002Math1E-03 FP8 W8 0.2519 0.0040 0.1524 0.0099 0.0000 0.0000\nOrca\u0002Math1E-03 FP8 W8A8 0.2520 0.0040 0.1539 0.0099 0.0000 0.0000\nOrca\u0002Math\n5E-03 BF16 BF16 0.2526 0.0040 0.2851 0.0124 0.0000 0.0000\nOrca\u0002Math5E-03 BF16 W8 0.2521 0.0040 0.2843 0.0124 0.0000 0.0000\nOrca\u0002Math5E-03 BF16 W8A8 0.2516 0.0040 0.2889 0.0125 0.0000 0.0000\nOrca\u0002Math\n5E-03 FP8 BF16 0.2525 0.0040 0.2684 0.0122 0.0000 0.0000\nOrca\u0002Math5E-03 FP8 W8 0.2524 0.0040 0.2229 0.0115 0.0000 0.0000\nOrca\u0002Math5E-03 FP8 W8A8 0.2525 0.0040 0.2646 0.0122 0.0000 0.0000\nxP3 1E-04 BF16 BF16 0.2495 0.0040 0.0265 0.0044 0.0366 0.0147\nxP3 1E-04 BF16 W8 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nxP3 1E-04 BF16 W8A8 0.2501 0.0040 0.0250 0.0043 0.0183 0.0105\nxP3 1E-04 FP8 BF16 0.2512 0.0040 0.0281 0.0045 0.0000 0.0000\nxP3 1E-04 FP8 W8 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nxP3 1E-04 FP8 W8A8 0.2484 0.0040 0.0212 0.0040 0.0122 0.0086\nxP3 5E-04 BF16 BF16 0.2571 0.0041 0.0243 0.0042 0.0122 0.0086\nxP3 5E-04 BF16 W8 0.2495 0.0040 0.0235 0.0042 0.0000 0.0000\nContinued on next page\n87\nTable 2: Downstream task accuracy for Mamba2 780M (Continued)\nxP3 5E-04 BF16 W8A8 0.2544 0.0040 0.0220 0.0040 0.0000 0.0000\nxP3 5E-04 FP8 BF16 0.2506 0.0040 0.0281 0.0045 0.0000 0.0000\nxP3 5E-04 FP8 W8 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nxP3 5E-04 FP8 W8A8 0.2522 0.0040 0.0243 0.0042 0.0000 0.0000\nxP3 1E-03 BF16 BF16 0.2525 0.0040 0.0212 0.0040 0.0000 0.0000\nxP3 1E-03 BF16 W8 0.2565 0.0041 0.0235 0.0042 0.0000 0.0000\nxP3 1E-03 BF16 W8A8 0.2563 0.0041 0.0220 0.0040 0.0000 0.0000\nxP3 1E-03 FP8 BF16 0.2535 0.0040 0.0273 0.0045 0.0000 0.0000\nxP3 1E-03 FP8 W8 0.2489 0.0040 0.0205 0.0039 0.0000 0.0000\nxP3 1E-03 FP8 W8A8 0.2529 0.0040 0.0296 0.0047 0.0000 0.0000\nxP3 5E-03 BF16 BF16 0.2551 0.0041 0.0159 0.0034 0.0000 0.0000\nxP3 5E-03 BF16 W8 0.2540 0.0040 0.0167 0.0035 0.0000 0.0000\nxP3 5E-03 BF16 W8A8 0.2492 0.0040 0.0190 0.0038 0.0000 0.0000\nxP3 5E-03 FP8 BF16 0.2526 0.0040 0.0091 0.0026 0.0000 0.0000\nxP3 5E-03 FP8 W8 0.2527 0.0040 0.0114 0.0029 0.0000 0.0000\nxP3 5E-03 FP8 W8A8 0.2479 0.0040 0.0152 0.0034 0.0000 0.0000\nTable 3: Downstream task accuracy for finetuned Mamba2 1.3B\nDataset Learning\nRate\nTraining\nPrecision\nInference\nPrecision\nCMMLU\n(Acc.)\nCMMLU\n(Std.\nErr.)\nGSM8K\n(Acc.)\nGSM8K\n(Std.\nErr.)\nHumanEval\n(Acc.)\nHumanEval\n(Std.\nErr.)\nCodeTester 1E-04 BF16 BF16 0.2495 0.0040 0.0182 0.0037 0.0976 0.0232\nCodeTester 1E-04 BF16 BF16 0.2493 0.0040 0.0159 0.0034 0.0976 0.0232\nCodeTester 1E-04 BF16 W8 0.2527 0.0040 0.0205 0.0039 0.0915 0.0226\nCodeTester 1E-04 BF16 W8 0.2524 0.0040 0.0182 0.0037 0.0915 0.0226\nCodeTester 1E-04 BF16 W8A8 0.2538 0.0040 0.0182 0.0037 0.0610 0.0187\nCodeTester 1E-04 BF16 W8A8 0.2528 0.0040 0.0227 0.0041 0.0610 0.0187\nCodeTester 1E-04 FP8 BF16 0.2490 0.0040 0.0205 0.0039 0.0854 0.0219\nCodeTester 1E-04 FP8 W8 0.2496 0.0040 0.0212 0.0040 0.0732 0.0204\nCodeTester 1E-04 FP8 W8A8 0.2535 0.0040 0.0190 0.0038 0.0488 0.0169\nCodeTester 5E-04 BF16 BF16 0.2532 0.0040 0.0243 0.0042 0.0061 0.0061\nCodeTester 5E-04 BF16 BF16 0.2513 0.0040 0.0258 0.0044 0.0366 0.0147\nCodeTester 5E-04 BF16 W8 0.2498 0.0040 0.0205 0.0039 0.0061 0.0061\nCodeTester 5E-04 BF16 W8 0.2507 0.0040 0.0197 0.0038 0.0366 0.0147\nCodeTester 5E-04 BF16 W8A8 0.2478 0.0040 0.0235 0.0042 0.0061 0.0061\nCodeTester 5E-04 BF16 W8A8 0.2500 0.0040 0.0227 0.0041 0.0427 0.0158\nCodeTester 5E-04 FP8 BF16 0.2490 0.0040 0.0144 0.0033 0.0000 0.0000\nCodeTester 5E-04 FP8 W8 0.2525 0.0040 0.0174 0.0036 0.0000 0.0000\nCodeTester 5E-04 FP8 W8A8 0.2516 0.0040 0.0129 0.0031 0.0000 0.0000\nCodeTester 1E-03 BF16 BF16 0.2464 0.0040 0.0167 0.0035 0.0000 0.0000\nCodeTester 1E-03 BF16 BF16 0.2491 0.0040 0.0182 0.0037 0.0366 0.0147\nCodeTester 1E-03 BF16 W8 0.2463 0.0040 0.0174 0.0036 0.0000 0.0000\nCodeTester 1E-03 BF16 W8 0.2452 0.0040 0.0174 0.0036 0.0610 0.0187\nContinued on next page\n88\nTable 3: Downstream task accuracy for finetuned Mamba2 1.3B (Continued)\nCodeTester 1E-03 BF16 W8A8 0.2456 0.0040 0.0106 0.0028 0.0000 0.0000\nCodeTester 1E-03 BF16 W8A8 0.2501 0.0040 0.0227 0.0041 0.0488 0.0169\nCodeTester 1E-03 FP8 BF16 0.2459 0.0040 0.0129 0.0031 0.0000 0.0000\nCodeTester 1E-03 FP8 BF16 0.2481 0.0040 0.0243 0.0042 0.0305 0.0135\nCodeTester 1E-03 FP8 W8 0.2499 0.0040 0.0159 0.0034 0.0000 0.0000\nCodeTester 1E-03 FP8 W8 0.2422 0.0040 0.0129 0.0031 0.0305 0.0135\nCodeTester 1E-03 FP8 W8A8 0.2486 0.0040 0.0144 0.0033 0.0000 0.0000\nCodeTester 1E-03 FP8 W8A8 0.2412 0.0040 0.0212 0.0040 0.0244 0.0121\nCodeTester 5E-03 BF16 BF16 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nCodeTester 5E-03 BF16 BF16 0.2504 0.0040 0.0023 0.0013 0.0000 0.0000\nCodeTester 5E-03 BF16 W8 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nCodeTester 5E-03 BF16 W8 0.2506 0.0040 0.0023 0.0013 0.0000 0.0000\nCodeTester 5E-03 BF16 W8A8 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nCodeTester 5E-03 FP8 BF16 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nCodeTester 5E-03 FP8 W8 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nCodeTester 5E-03 FP8 W8A8 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nOrca\u0002Math1E-04 BF16 BF16 0.2511 0.0040 0.0917 0.0080 0.0427 0.0158\nOrca\u0002Math1E-04 BF16 W8 0.2483 0.0040 0.1077 0.0085 0.0366 0.0147\nOrca\u0002Math1E-04 BF16 W8A8 0.2533 0.0041 0.0970 0.0082 0.0610 0.0187\nOrca\u0002Math1E-04 FP8 BF16 0.2521 0.0040 0.1039 0.0084 0.0183 0.0105\nOrca\u0002Math1E-04 FP8 W8 0.2509 0.0040 0.1061 0.0085 0.0244 0.0121\nOrca\u0002Math1E-04 FP8 W8A8 0.2517 0.0040 0.0978 0.0082 0.0244 0.0121\nOrca\u0002Math5E-04 BF16 BF16 0.2532 0.0040 0.2335 0.0117 0.0366 0.0147\nOrca\u0002Math5E-04 BF16 W8 0.2521 0.0040 0.2214 0.0114 0.0366 0.0147\nOrca\u0002Math5E-04 BF16 W8A8 0.2573 0.0041 0.2146 0.0113 0.0244 0.0121\nOrca\u0002Math\n5E-04 FP8 BF16 0.2522 0.0040 0.2191 0.0114 0.0000 0.0000\nOrca\u0002Math5E-04 FP8 W8 0.2508 0.0040 0.2077 0.0112 0.0000 0.0000\nOrca\u0002Math5E-04 FP8 W8A8 0.2517 0.0040 0.2115 0.0112 0.0000 0.0000\nOrca\u0002Math1E-03 BF16 BF16 0.2542 0.0041 0.2699 0.0122 0.0000 0.0000\nOrca\u0002Math1E-03 BF16 W8 0.2545 0.0041 0.2714 0.0122 0.0000 0.0000\nOrca\u0002Math\n1E-03 BF16 W8A8 0.2570 0.0041 0.2699 0.0122 0.0000 0.0000\nOrca\u0002Math5E-03 BF16 BF16 0.2526 0.0040 0.3404 0.0131 0.0000 0.0000\nContinued on next page\n89\nTable 3: Downstream task accuracy for finetuned Mamba2 1.3B (Continued)\nOrca\u0002Math5E-03 BF16 W8 0.2522 0.0040 0.3389 0.0130 0.0000 0.0000\nOrca\u0002Math5E-03 BF16 W8A8 0.2523 0.0040 0.3434 0.0131 0.0000 0.0000\nOrca\u0002Math\n5E-03 FP8 BF16 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nOrca\u0002Math5E-03 FP8 W8 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nOrca\u0002Math5E-03 FP8 W8A8 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nxP3 1E-04 BF16 BF16 0.2543 0.0041 0.0174 0.0036 0.0671 0.0196\nxP3 1E-04 BF16 W8 0.2539 0.0040 0.0174 0.0036 0.0549 0.0178\nxP3 1E-04 BF16 W8A8 0.2538 0.0040 0.0152 0.0034 0.0488 0.0169\nxP3 1E-04 FP8 BF16 0.2555 0.0041 0.0243 0.0042 0.0549 0.0178\nxP3 1E-04 FP8 W8 0.2576 0.0041 0.0227 0.0041 0.0366 0.0147\nxP3 1E-04 FP8 W8A8 0.2532 0.0040 0.0190 0.0038 0.0549 0.0178\nxP3 5E-04 BF16 BF16 0.2485 0.0040 0.0205 0.0039 0.0244 0.0121\nxP3 5E-04 BF16 W8 0.2481 0.0040 0.0197 0.0038 0.0305 0.0135\nxP3 5E-04 BF16 W8A8 0.2505 0.0040 0.0174 0.0036 0.0244 0.0121\nxP3 5E-04 FP8 BF16 0.2482 0.0040 0.0190 0.0038 0.0061 0.0061\nxP3 5E-04 FP8 W8 0.2463 0.0040 0.0205 0.0039 0.0061 0.0061\nxP3 5E-04 FP8 W8A8 0.2467 0.0040 0.0265 0.0044 0.0000 0.0000\nxP3 1E-03 BF16 BF16 0.2469 0.0040 0.0235 0.0042 0.0122 0.0086\nxP3 1E-03 BF16 W8 0.2473 0.0040 0.0212 0.0040 0.0122 0.0086\nxP3 1E-03 BF16 W8A8 0.2472 0.0040 0.0220 0.0040 0.0183 0.0105\nxP3 1E-03 FP8 BF16 0.2466 0.0040 0.0220 0.0040 0.0000 0.0000\nxP3 1E-03 FP8 W8 0.2459 0.0040 0.0182 0.0037 0.0000 0.0000\nxP3 1E-03 FP8 W8A8 0.2491 0.0040 0.0190 0.0038 0.0000 0.0000\nxP3 5E-03 BF16 BF16 0.2509 0.0040 0.0220 0.0040 0.0000 0.0000\nxP3 5E-03 BF16 W8 0.2487 0.0040 0.0220 0.0040 0.0000 0.0000\nxP3 5E-03 BF16 W8A8 0.2491 0.0040 0.0235 0.0042 0.0000 0.0000\nxP3 5E-03 FP8 BF16 0.2523 0.0040 0.0159 0.0034 0.0000 0.0000\nxP3 5E-03 FP8 W8 0.2516 0.0040 0.0152 0.0034 0.0000 0.0000\nxP3 5E-03 FP8 W8A8 0.2474 0.0040 0.0144 0.0033 0.0000 0.0000\nTable 4: Downstream task accuracy for Mamba2 2.7B\nDataset Learning\nRate\nTraining\nPrecision\nInference\nPrecision\nCMMLU\n(Acc.)\nCMMLU\n(Std.\nErr.)\nGSM8K\n(Acc.)\nGSM8K\n(Std.\nErr.)\nHumanEval\n(Acc.)\nHumanEval\n(Std.\nErr.)\nCodeTester 1E-04 BF16 BF16 0.2495 0.0040 0.0182 0.0037 0.0976 0.0232\nCodeTester 1E-04 BF16 BF16 0.2493 0.0040 0.0159 0.0034 0.0976 0.0232\nCodeTester 1E-04 BF16 W8 0.2527 0.0040 0.0205 0.0039 0.0915 0.0226\nCodeTester 1E-04 BF16 W8 0.2524 0.0040 0.0182 0.0037 0.0915 0.0226\nCodeTester 1E-04 BF16 W8A8 0.2538 0.0040 0.0182 0.0037 0.0610 0.0187\nContinued on next page\n90\nTable 4: Downstream task accuracy for Mamba2 2.7B (Continued)\nCodeTester 1E-04 BF16 W8A8 0.2528 0.0040 0.0227 0.0041 0.0610 0.0187\nCodeTester 1E-04 FP8 BF16 0.2490 0.0040 0.0205 0.0039 0.0854 0.0219\nCodeTester 1E-04 FP8 W8 0.2496 0.0040 0.0212 0.0040 0.0732 0.0204\nCodeTester 1E-04 FP8 W8A8 0.2535 0.0040 0.0190 0.0038 0.0488 0.0169\nCodeTester 5E-04 BF16 BF16 0.2532 0.0040 0.0243 0.0042 0.0061 0.0061\nCodeTester 5E-04 BF16 BF16 0.2513 0.0040 0.0258 0.0044 0.0366 0.0147\nCodeTester 5E-04 BF16 W8 0.2498 0.0040 0.0205 0.0039 0.0061 0.0061\nCodeTester 5E-04 BF16 W8 0.2507 0.0040 0.0197 0.0038 0.0366 0.0147\nCodeTester 5E-04 BF16 W8A8 0.2478 0.0040 0.0235 0.0042 0.0061 0.0061\nCodeTester 5E-04 BF16 W8A8 0.2500 0.0040 0.0227 0.0041 0.0427 0.0158\nCodeTester 5E-04 FP8 BF16 0.2490 0.0040 0.0144 0.0033 0.0000 0.0000\nCodeTester 5E-04 FP8 W8 0.2525 0.0040 0.0174 0.0036 0.0000 0.0000\nCodeTester 5E-04 FP8 W8A8 0.2516 0.0040 0.0129 0.0031 0.0000 0.0000\nCodeTester 1E-03 BF16 BF16 0.2464 0.0040 0.0167 0.0035 0.0000 0.0000\nCodeTester 1E-03 BF16 BF16 0.2491 0.0040 0.0182 0.0037 0.0366 0.0147\nCodeTester 1E-03 BF16 W8 0.2463 0.0040 0.0174 0.0036 0.0000 0.0000\nCodeTester 1E-03 BF16 W8 0.2452 0.0040 0.0174 0.0036 0.0610 0.0187\nCodeTester 1E-03 BF16 W8A8 0.2456 0.0040 0.0106 0.0028 0.0000 0.0000\nCodeTester 1E-03 BF16 W8A8 0.2501 0.0040 0.0227 0.0041 0.0488 0.0169\nCodeTester 1E-03 FP8 BF16 0.2459 0.0040 0.0129 0.0031 0.0000 0.0000\nCodeTester 1E-03 FP8 BF16 0.2481 0.0040 0.0243 0.0042 0.0305 0.0135\nCodeTester 1E-03 FP8 W8 0.2499 0.0040 0.0159 0.0034 0.0000 0.0000\nCodeTester 1E-03 FP8 W8 0.2422 0.0040 0.0129 0.0031 0.0305 0.0135\nCodeTester 1E-03 FP8 W8A8 0.2486 0.0040 0.0144 0.0033 0.0000 0.0000\nCodeTester 1E-03 FP8 W8A8 0.2412 0.0040 0.0212 0.0040 0.0244 0.0121\nCodeTester 5E-03 BF16 BF16 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nCodeTester 5E-03 BF16 BF16 0.2504 0.0040 0.0023 0.0013 0.0000 0.0000\nCodeTester 5E-03 BF16 W8 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nCodeTester 5E-03 BF16 W8 0.2506 0.0040 0.0023 0.0013 0.0000 0.0000\nCodeTester 5E-03 BF16 W8A8 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nCodeTester 5E-03 FP8 BF16 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nCodeTester 5E-03 FP8 W8 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nCodeTester 5E-03 FP8 W8A8 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nOrca\u0002Math1E-04 BF16 BF16 0.2511 0.0040 0.0917 0.0080 0.0427 0.0158\nOrca\u0002Math1E-04 BF16 W8 0.2483 0.0040 0.1077 0.0085 0.0366 0.0147\nOrca\u0002Math1E-04 BF16 W8A8 0.2533 0.0041 0.0970 0.0082 0.0610 0.0187\nOrca\u0002Math\n1E-04 FP8 BF16 0.2521 0.0040 0.1039 0.0084 0.0183 0.0105\nOrca\u0002Math\n1E-04 FP8 W8 0.2509 0.0040 0.1061 0.0085 0.0244 0.0121\nOrca\u0002Math1E-04 FP8 W8A8 0.2517 0.0040 0.0978 0.0082 0.0244 0.0121\nContinued on next page\n91\nTable 4: Downstream task accuracy for Mamba2 2.7B (Continued)\nOrca\u0002Math5E-04 BF16 BF16 0.2532 0.0040 0.2335 0.0117 0.0366 0.0147\nOrca\u0002Math5E-04 BF16 W8 0.2521 0.0040 0.2214 0.0114 0.0366 0.0147\nOrca\u0002Math\n5E-04 BF16 W8A8 0.2573 0.0041 0.2146 0.0113 0.0244 0.0121\nOrca\u0002Math5E-04 FP8 BF16 0.2522 0.0040 0.2191 0.0114 0.0000 0.0000\nOrca\u0002Math5E-04 FP8 W8 0.2508 0.0040 0.2077 0.0112 0.0000 0.0000\nOrca\u0002Math5E-04 FP8 W8A8 0.2517 0.0040 0.2115 0.0112 0.0000 0.0000\nOrca\u0002Math1E-03 BF16 BF16 0.2542 0.0041 0.2699 0.0122 0.0000 0.0000\nOrca\u0002Math1E-03 BF16 W8 0.2545 0.0041 0.2714 0.0122 0.0000 0.0000\nOrca\u0002Math1E-03 BF16 W8A8 0.2570 0.0041 0.2699 0.0122 0.0000 0.0000\nOrca\u0002Math\n5E-03 BF16 BF16 0.2526 0.0040 0.3404 0.0131 0.0000 0.0000\nOrca\u0002Math5E-03 BF16 W8 0.2522 0.0040 0.3389 0.0130 0.0000 0.0000\nOrca\u0002Math5E-03 BF16 W8A8 0.2523 0.0040 0.3434 0.0131 0.0000 0.0000\nOrca\u0002Math5E-03 FP8 BF16 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nOrca\u0002Math5E-03 FP8 W8 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nOrca\u0002Math5E-03 FP8 W8A8 0.2526 0.0040 0.0000 0.0000 0.0000 0.0000\nxP3 1E-04 BF16 BF16 0.2543 0.0041 0.0174 0.0036 0.0671 0.0196\nxP3 1E-04 BF16 W8 0.2539 0.0040 0.0174 0.0036 0.0549 0.0178\nxP3 1E-04 BF16 W8A8 0.2538 0.0040 0.0152 0.0034 0.0488 0.0169\nxP3 1E-04 FP8 BF16 0.2555 0.0041 0.0243 0.0042 0.0549 0.0178\nxP3 1E-04 FP8 W8 0.2576 0.0041 0.0227 0.0041 0.0366 0.0147\nxP3 1E-04 FP8 W8A8 0.2532 0.0040 0.0190 0.0038 0.0549 0.0178\nxP3 5E-04 BF16 BF16 0.2485 0.0040 0.0205 0.0039 0.0244 0.0121\nxP3 5E-04 BF16 W8 0.2481 0.0040 0.0197 0.0038 0.0305 0.0135\nxP3 5E-04 BF16 W8A8 0.2505 0.0040 0.0174 0.0036 0.0244 0.0121\nxP3 5E-04 FP8 BF16 0.2482 0.0040 0.0190 0.0038 0.0061 0.0061\nxP3 5E-04 FP8 W8 0.2463 0.0040 0.0205 0.0039 0.0061 0.0061\nxP3 5E-04 FP8 W8A8 0.2467 0.0040 0.0265 0.0044 0.0000 0.0000\nxP3 1E-03 BF16 BF16 0.2469 0.0040 0.0235 0.0042 0.0122 0.0086\nxP3 1E-03 BF16 W8 0.2473 0.0040 0.0212 0.0040 0.0122 0.0086\nxP3 1E-03 BF16 W8A8 0.2472 0.0040 0.0220 0.0040 0.0183 0.0105\nxP3 1E-03 FP8 BF16 0.2466 0.0040 0.0220 0.0040 0.0000 0.0000\nxP3 1E-03 FP8 W8 0.2459 0.0040 0.0182 0.0037 0.0000 0.0000\nxP3 1E-03 FP8 W8A8 0.2491 0.0040 0.0190 0.0038 0.0000 0.0000\nContinued on next page\n92\nTable 4: Downstream task accuracy for Mamba2 2.7B (Continued)\nxP3 5E-03 BF16 BF16 0.2509 0.0040 0.0220 0.0040 0.0000 0.0000\nxP3 5E-03 BF16 W8 0.2487 0.0040 0.0220 0.0040 0.0000 0.0000\nxP3 5E-03 BF16 W8A8 0.2491 0.0040 0.0235 0.0042 0.0000 0.0000\nxP3 5E-03 FP8 BF16 0.2523 0.0040 0.0159 0.0034 0.0000 0.0000\nxP3 5E-03 FP8 W8 0.2516 0.0040 0.0152 0.0034 0.0000 0.0000\nxP3 5E-03 FP8 W8A8 0.2474 0.0040 0.0144 0.0033 0.0000 0.0000\na.4 scheduled fp8 training experiments\nThis section shows all training runs for scheduled FP8 quantization,\nwhere the model was converted to have 8-bit floating point Linear\nlayers at some non-zero step of fine-tuning.\nFigure 35: Scheduled FP8 training of Mamba2 1.3B on xP3 dataset\n93\nFigure 36: Scheduled FP8 training of Mamba2 1.3B on Orca-Math dataset\n94\nFigure 37: Scheduled FP8 training of Mamba2 1.3B on Orca-Math dataset\n95\n\nB I B L I O G R A P H Y\n[1] Automatic Mixed Precision package - torch.amp 2014; PyTorch 2.6 doc\u0002umentation — pytorch.org. https://pytorch.org/docs/stable/\namp.html. [Accessed 18-02-2025].\n[2] Stella Biderman et al. “Lessons from the trenches on repro\u0002ducible evaluation of language models.” In: (2024). eprint: 2405.\n14782 (cs.CL).\n[3] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and\nYejin Choi. “PIQA: Reasoning about physical commonsense in\nnatural language.” In: (2019). eprint: 1911.11641 (cs.CL).\n[4] Hung-Yueh Chiang, Chi-Chih Chang, Natalia Frumkin, Kai\u0002Chiang Wu, and Diana Marculescu. “Quamba: A post-training\nquantization recipe for selective state Space Models.” In: (2024).\neprint: 2410.13229 (cs.LG).\n[5] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish\nSabharwal, Carissa Schoenick, and Oyvind Tafjord. “Think you\nhave Solved Question Answering? Try ARC, the AI2 Reasoning\nChallenge.” In: (2018). eprint: 1803.05457 (cs.AI).\n[6] Tri Dao and Albert Gu. “Transformers are SSMs: Generalized\nmodels and efficient algorithms through structured state space\nduality.” In: (2024). eprint: 2405.21060 (cs.LG).\n[7] DeepSeek-AI et al. “DeepSeek-V3 Technical Report.” In: (Dec.\n2024). arXiv: 2412.19437 [cs.CL].\n[8] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettle\u0002moyer. “LLM.int8(): 8-bit Matrix Multiplication for Transformers\nat Scale.” In: (2022). eprint: 2208.07339 (cs.LG).\n97\n[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n“BERT: Pre-training of deep bidirectional Transformers for lan\u0002guage understanding.” In: (2018). eprint: 1810.04805 (cs.CL).\n[10] Alexey Dosovitskiy et al. “An image is worth 16x16 words:\nTransformers for image recognition at scale.” In: (2020). eprint:\n2010.11929 (cs.CV).\n[11] Maxim Fishman, Brian Chmiel, Ron Banner, and Daniel Soudry.\n“Scaling FP8 training to trillion-token LLMs.” In: (2024). eprint:\n2409.12517 (cs.LG).\n[12] Leo Gao et al. “The Pile: An 800GB dataset of diverse text for\nlanguage modeling.” In: (2021). eprint: 2101.00027 (cs.CL).\n[13] Getting Started with CUDA Graphs | NVIDIA Technical Blog —\ndeveloper.nvidia.com. https : / / developer . nvidia . com / blog /\ncuda-graphs/. [Accessed 22-03-2025].\n[14] Albert Gu and Tri Dao. “Mamba: Linear-time sequence model\u0002ing with selective state spaces.” In: (2023). eprint: 2312.00752\n(cs.LG).\n[15] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao,\nAtri Rudra, and Christopher Ré. “Combining recurrent, convo\u0002lutional, and continuous-time models with Linear State-space\nlayers.” In: (2021). eprint: 2110.13985 (cs.LG).\n[16] David Herel and Tomas Mikolov. “Advancing state of the art in\nlanguage modeling.” In: (2023). eprint: 2312.03735 (cs.CL).\n[17] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and\nFrançois Fleuret. “Transformers are RNNs: Fast autoregressive\ntransformers with linear attention.” In: (2020). eprint: 2006 .\n16236 (cs.LG).\n[18] Sehoon Kim et al. “Full stack optimization of Transformer infer\u0002ence: A survey.” In: arXiv [cs.CL] (2023).\n98\n[19] Tanishq Kumar, Zachary Ankner, Benjamin F Spector, Blake\nBordelon, Niklas Muennighoff, Mansheej Paul, Cengiz Pehle\u0002van, Christopher Ré, and Aditi Raghunathan. “Scaling laws for\nprecision.” In: (2024). eprint: 2411.04330 (cs.LG).\n[20] Paulius Micikevicius et al. “FP8 Formats for Deep Learning.” In:\n(2022). eprint: 2209.05433 (cs.LG).\n[21] Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed\nAwadallah. “Orca-Math: Unlocking the potential of SLMs in\nGrade School Math.” In: (2024). eprint: 2402.14830 (cs.CL).\n[22] Mixture-of-Quantization: A novel quantization approach for reducing\nmodel size with minimal accuracy impact — deepspeed.ai. https :\n//www.deepspeed.ai/2021/05/04/MoQ.html. [Accessed 21-03-\n2025].\n[23] Niklas Muennighoff et al. “Crosslingual Generalization through\nMultitask Finetuning.” In: (2022). eprint: 2211.01786 (cs.CL).\n[24] NVIDIA Blackwell Architecture — nvidia.com. https://www.nvidia.\ncom/en-us/data-center/technologies/blackwell-architecture/.\n[Accessed 04-03-2025].\n[25] NVIDIA Hopper Architecture In-Depth | NVIDIA Technical Blog\n— developer.nvidia.com. https://developer.nvidia.com/blog/\nnvidia - hopper - architecture - in - depth/. [Accessed 18-02-\n2025].\n[26] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei\nBondarenko, Mart van Baalen, and Tijmen Blankevoort. “A white\npaper on neural network quantization.” In: (2021). eprint: 2106.\n08295 (cs.LG).\n[27] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan\nNgoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Ba\u0002roni, Gemma Boleda, and Raquel Fernández. “The LAMBADA\ndataset: Word prediction requiring a broad discourse context.”\nIn: (2016). eprint: 1606.06031 (cs.CL).\n99\n[28] Houwen Peng et al. “FP8-LM: Training FP8 Large Language\nModels.” In: (2023). eprint: 2310.18313 (cs.LG).\n[29] Performance Tuning Guide 2014; PyTorch Tutorials 2.6.0+cu124\ndocumentation — pytorch.org. https://pytorch.org/tutorials/\nrecipes/recipes/tuning_guide.html. [Accessed 22-03-2025].\n[30] Alessandro Pierro and Steven Abreu. “Mamba-PTQ: Outlier\nchannels in recurrent large language models.” In: (2024). eprint:\n2407.12397 (cs.LG).\n[31] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sha\u0002ran Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J\nLiu. “Exploring the limits of transfer learning with a unified\ntext-to-text transformer.” In: (2019). eprint: 1910.10683 (cs.LG).\n[32] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir\nGholami, Michael W Mahoney, and Kurt Keutzer. “Q-BERT:\nHessian based ultra low precision quantization of BERT.” In:\narXiv [cs.CL] (2019).\n[33] Jimmy T H Smith, Andrew Warrington, and Scott W Linderman.\n“Simplified state space layers for sequence modeling.” In: (2022).\neprint: 2208.04933 (cs.LG).\n[34] Matthias Sperber, Jan Niehues, Graham Neubig, Sebastian Stüker,\nand Alex Waibel. “Self-Attentional Acoustic Models.” In: (2018).\neprint: 1803.09519 (cs.CL).\n[35] Using FP8 with Transformer Engine 2014; Transformer Engine 2.1.0\ndocumentation — docs.nvidia.com. https://docs.nvidia.com/\ndeeplearning / transformer - engine / user - guide / examples /\nfp8_primer.html. [Accessed 28-03-2025].\n[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\nLlion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polo\u0002sukhin. “Attention is all you need.” In: (2017). eprint: 1706 .\n03762 (cs.CL).\n100\n[37] Lilian Weng. Large Transformer Model Inference Optimization —\nlilianweng.github.io. https : / / lilianweng . github . io / posts /\n2023-01-10-inference-optimization/. [Accessed 28-03-2025].\n[38] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien De\u0002mouth, and Song Han. “SmoothQuant: Accurate and efficient\npost-training quantization for large language models.” In: (2022).\neprint: 2211.10438 (cs.CL).\n[39] Zukang Xu, Yuxuan Yue, Xing Hu, Zhihang Yuan, Zixu Jiang,\nZhixuan Chen, Jiangyong Yu, Chen Xu, Sifan Zhou, and Dawei\nYang. “MambaQuant: Quantizing the Mamba family with vari\u0002ance aligned rotation methods.” In: (2025). eprint: 2501.13484\n(cs.LG).\n[40] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and\nYejin Choi. “HellaSwag: Can a machine really finish your sen\u0002tence?” In: (2019). eprint: 1905.07830 (cs.CL).\n[41] Susan Zhang et al. “OPT: Open Pre-trained transformer lan\u0002guage models.” In: (2022). eprint: 2205.01068 (cs.CL).\n101",
    "length": 156919,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Todd Essig: Will Love for Learning Matter Anymore? | Department of the History of Science",
    "url": "https://histsci.fas.harvard.edu/event/todd-essig-faculty-seminar-ai-and-university",
    "text": "Todd Essig: Will Love for Learning Matter Anymore? | Department of the History of Science[\nSkip to main contentarrow\\_circle\\_down\n] \n[![Harvard University]] \n[\nDepartment of the History of Science\nFaculty of Arts and Sciences (FAS)\n] \nmenucloseMenu\nSearch\nSearchsearch\n[\nDepartment of the History of Science\nFaculty of Arts and Sciences (FAS)\n] \n# Todd Essig: Will Love for Learning Matter Anymore?\n![Event Poster for Todd Essig&#039;s AI Seminar Talk] \n#### calendar\\_todayDate and Time\n**November 4, 2025**\n12:00PM - 01:30PM EST\n#### pin\\_dropLocation\n**\nScience Center 469\n**\nVegetarian Lunch Provided. Registration Required.\n[Register on Harvard SharePoint (Request Access with HarvardKey)arrow\\_circle\\_right] \nTodd Essig begins discussion of this month's faculty seminar on*Knowledge Production and the University in the Age of AI*. His talk is titled \"Will love for learning matter anymore? Understanding the complex psychology of AI relationality.\" Faculty and researchers from all Harvard schools welcome. Registration required.\nThis seminar is sponsored by the Department of the History of Science and by the Harvard Data Science Initiative.\n## Will Love for Learning Matter Anymore? Understanding the Complex Psychology of AI Relationality\nUniversities are confronting a future in which emerging, alien intelligences promise to augment research and teaching while also threatening a world of professor-bots that can reasonably replicate, even extend, the work of expert educators and scholars. In such AI universities, what will remain for humans to do? The rise of AI companions, lovers, and “therapy-bots” shows the urgency of this question. Narcissistic confidence that “no bot could ever do what I do” is a dead end. Instead, understanding the complex psychology of AI relationality may reveal what is uniquely human and worth protecting. For this purpose, I have been developing the concept of techno-subjunctivity, a psychoanalytic framework naming the new psychic space in which empathy is simulated, asymmetry feels like mutuality, and yet emotional significance remains. It clarifies that higher education’s task in the AI age includes teaching students to value human processes over mere instrumental utility by recognizing that behavior born of a love of learning is meaningfully different from behavior emerging from stochastic computation. Human experience matters.\n### Todd Essig\n![professional headshot of Todd Essig in a navy blazer with arms folded] \nTodd Essig, Ph.D., is Faculty and Training &amp;&amp; Supervising Psychoanalyst at the William Alanson White Institute, Adjunct Clinical Professor at the NYU Postdoctoral Program in Psychoanalysis, and Psychotherapy Action Network (PsiAN) Advisory Board member. Widely known as a pioneer in the innovative uses of mental health technologies, he publishes and lectures widely at the intersections of psychoanalysis and artificial intelligence, including screen relations based psychoanalytic care. He founded and currently chairs the American Psychoanalytic Association President’s Commission on Artificial Intelligence (CAI) and the soon to launch International Psychoanalytic Association Committee on Artificial Intelligence. Previously, he co-authored the IPA Task Force Report on the use of telesessions in psychoanalytic education and chaired APsA’s Covid-19 Advisory Team. For 10 years, until the pandemic hit, he wrote \"Managing Mental Wealth\" for*Forbes*where he explored the intersections of technology, psychology, and culture. In his clinical practice he treats individuals and couples.\n### Seminar Readings\nPlease come to seminar having read the following article.\n[&quot;What Will Remain for People to Do?&quot; by Daniel Susskindchevron\\_right] \n![stack of books with top book open] \n### Knowledge Production and the University in the Age of AI\nFaculty Seminar\nAt this time of seismic change for the university, and the research systems of which we are a central part, it is hard to find a moment to pause and ask:*how should we make knowledge in the future?*\n[Learn More About the Seminarchevron\\_right] \n![18th century watercolor view of harvard] \n## Seminar Sponsors\nThis seminar is co-sponsored by the Department of the History of Science and the Harvard Data Science Initiative.\n![History of Science Logo] \n[### Department of the History of Science\n] \n![Harvard Date Science Initiative Shield Logo] \n[### Harvard Data Science Initiative\n] \nAttachments\n* [picture\\_as\\_pdfAI Seminar Essig-1-Main Poster.pdf] \nShare on:\n* [Facebook] \n* [Twitter] \n* [Linkedin] \nSave:[Add to calendarcalendar\\_today] Copy linklink",
    "length": 4608,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "SPH Generative AI Sandbox Now Available | Harvard T.H. Chan School of Public Health",
    "url": "https://hsph.harvard.edu/information-technology/news/sph-generative-ai-sandbox-now-available/",
    "text": "SPH Generative AI Sandbox Now Available | Harvard T.H. Chan School of Public Health[Skip to main content] \nSupport the School\n[Your gift] powers excellence in research and education to advance public health.\nClose alert\nSearch Harvard Chan:SearchSearchClose search bar\n[Giving] \n[Harvard T.H. Chan School of Public Health] \nOpen Search Bar\n[Home] /[News] /[Department of Information Technology] /SPH Generative AI Sandbox Now Available\n# SPH Generative AI Sandbox Now Available\nBy\nStaff Writer\nNovember 30, 2023\nJump to Section\nShare Post\n* [LinkedIn] \n* [Facebook] \n* [WhatsApp] \n* [Email] \n* Copy link\nDear SPH Staff,\nWith interest in Generative AI very high, we are pleased to announce the launch of a SPH Generative AI sandbox for secure experimentation. This sandbox may be used to explore large language models (LLMs), search for content, create/edit/summarize content, assist with code writing/debugging, analyze information and much more.\n## SPH AI Sandbox\n[SPH’s sandbox may be found here], and is part of Harvard University’s larger AI sandbox. Additional information on getting started with the Sandbox may be found in the email below from Harvard University Information Technology.\n## SPH Survey\nTo assist SPH and the University in planning for future Generative AI needs, we ask that you complete a brief survey,[Staff GAI survey], by December 15.\n## Workshops\nKeep an eye out for SPH workshops and informational sessions on experimenting with generative AI in teaching and learning, administration and operations, and research.We anticipate that the sandbox will be available through the end of the academic year.\nBest,\nKate Calvin, Executive Dean for Administration\nErin Driver-Linn, Dean for Education\nDeane Eastwood, Chief Information Officer\nDear SPH Staff,\nWe’re pleased to announce that the pilot version of the Generative AI Sandbox is now ready for you to use.\nAs a reminder, the[AI Sandbox] has been developed by HUIT in collaboration with VPAL, the FAS Division of Science, and colleagues across the University, to enable Harvard community members to securely access Large Language Models (LLM). The AI Sandbox offers a single interface that enables access to four different Large Language Models (LLM): Azure OpenAI GPT-3.5 and GPT-4, Anthropic Claude 2, and Google PaLM 2 Bison. It provides a “walled-off,” secure environment in which to experiment with generative AI, mitigating many security and privacy risks and ensuring the data entered will not be used to train any public AI tools.\nDuring this first pilot period, the AI Sandbox:\n* Is approved for use with[Medium Risk Confidential data (L3)] and below.\n* Is free to use.\n* Features a simple log in process using your existing HarvardKey credentials.\n* Cannot be custom trained on a corpus of materials.\n* Will experience occasional errors and interruptions in service, which may reset your session.\nCurrently, API access is not available through the AI Sandbox. Please visit[HUIT&#8217;s website on exploring Generative AI tools for more information about OpenAI API usage].\n## About Generative AI\nHarvard has launched an informational page about Generative AI at[https://huit.harvard.edu/ai/about].\n## Before you use the AI Sandbox:\nPlease review “[Getting started with the AI Sandbox],” which includes guidelines, terms of use, tool instructions, and ideas for prompts.\n## To access the AI Sandbox:\n* Go to[https://hsph.sandbox.ai.huit.harvard.edu/] \n* When prompted, log in with your HarvardKey username and password.## Feedback:\nYour feedback will help us develop the AI Sandbox further and inform future University investments in AI technology.[Please use this survey to submit your comments].\n## Accessibility:\nThe pilot version of the AI Sandbox contains elements that are not fully accessible. If you or any participants need accessibility assistance, the following resources are available:\nStudent Local Disability Coordinators (LDCs):[https://accessibility.harvard.edu/student-coordinators].\nIf you are unable to determine who the appropriate contact may be, University Disability Resources can be contacted via[accessibility@harvard.edu] or Digital Accessibility Services via[digitalaccessibility@harvard.edu].\nThank you,\nAcademic Technology\nHarvard University Information Technology\n## Last Updated\nNovember 22, 2024\n## Related News\n* ![] \n### [Affirming care] \nJanuary 13, 2026\n* ![Lyndsey Garrett] \n### [Protected: Reproductive rights advocate Lyndsey Garrett, MPH ’27, pursues Harvard Chan training to expand fight for bodily autonomy] \nJanuary 13, 2026\n* In the Media![Smokestack pipes emitting co2 from coal thermal power plant into atmosphere] \n### [The EPA is changing how it considers the costs and benefits of air pollution rules] \nJanuary 13, 2026\n## Get the latest public health news\nStay connected with Harvard Chan School\n[Subscribe to our newsletters] \n## Unleash your potential at Harvard Chan School.\nIn addition to our degree programs, we offer highly targeted executive and continuing education, directed and taught by Harvard faculty.\n[Degree Programs] \n[How to Apply] \n[Executive and Continuing Education]",
    "length": 5121,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Using Permissions in Dataverse | Dataverse Support",
    "url": "https://support.dataverse.harvard.edu/event/using-permissions-dataverse",
    "text": "Using Permissions in Dataverse | Dataverse Support[\nSkip to main contentarrow\\_circle\\_down\n] \n[![Harvard University]] \n[\nHarvard\nDataverse Support\n] \nmenucloseMenu\nSearch\nSearchsearch\n[\nHarvard\nDataverse Support\n] \n# Using Permissions in Dataverse\n#### calendar\\_todayDate and Time\n**February 26, 2025**\n09:30AM - 10:30AM EST\n[Registration Requiredarrow\\_circle\\_right] \nLearn how to manage permissions in a Dataverse repository to manage deposit workflows and access to your data. This webinar will provide an overview of the permissions framework, helping you understand roles, access levels, and how to customize permission at the collection, dataset, and file levels.\nKey topics include:\n* Overview of roles: Administrators, contributors, curators, data users\n* Configuring dataset-level and collection-level permissions\n* Managing guest access and link sharing\n* Best practices for balancing openness and data protection\nThis session is designed for researchers, data managers, and repository administrators who want to maximize collaboration and manage data access. Whether you're new to Dataverse or looking to refine your repository practices, this webinar will equip you with the tools to navigate permissions effectively.\nPresenters:\nSonia Barbosa*(The Dataverse Project, IQSS)*\nGustavo Durand*(The Dataverse Project, IQSS)*\nShare on:\n* [Facebook] \n* [Twitter] \n* [Linkedin] \nSave:[Add to calendarcalendar\\_today] Copy linklink\n## Dataverse Community Calls\nEvery month, the Dataverse Project hosts a Zoom call to discuss upcoming Dataverse releases, community contributions, and more. Visit[https://dataverse.org/community-calls] to learn more and join the calls.",
    "length": 1674,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "AI's Impact on Science, Law, and Society | Berkman Klein Center",
    "url": "https://cyber.harvard.edu/events/ais-impact-science-law-and-society",
    "text": "AI&#039;s Impact on Science, Law, and Society | Berkman Klein Center\n[Skip to the main content] \nMenu[Harvard Berkman Klein Center Logo] \n[Harvard Berkman Klein Center Logo] \nSearcho-icon\\_\\_search\n![AI&#039;s Impact on Science, Law, and Society] \n# AI&#039;s Impact on Science, Law, and Society\nSpring Speaker Series\n[Ethics and Governance of AI] \n![Sayash Kapoor] \n[\nSayash Kapoor\n] \nShare To[icon--facebook] [icon--twitter] \n[![Embedded YouTube video]] \n#### **BKC**[**Spring Speaker Series**] **Event**\nThe promise of AI agents has led to claims of imminent and rapid adoption across fields. Companies have even promised to build AI agents that can automate all legal and scientific tasks. At the same time, there are concerns about their misuse leading to catastrophic risks such as bio and cybersecurity risks.\nIn this talk, Sayash Kapoor goes over three case studies to foreground the importance of evidence-based AI analysis. First, while AI has been claimed to automate all of science, existing adoption has been plagued by severe reproducibility failures that lead to overoptimistic results across dozens of fields. Recent empirical work shows that current models fall well short of accomplishing far simpler tasks, such as reproducing a paper’s results even when the code and data are provided. Second, for legal applications, tasks that would lead to the most significant changes to the legal profession are also the ones most prone to overoptimism about AI capabilities, as they are harder to evaluate. Third, for analyzing safety risks, it is important to analyze the marginal risk of AI over and above existing technology to evaluate the effectiveness of policy interventions. Kapoor concludes with a discussion of how to effectively conduct empirical analysis of AI.\n##### Speaker\nSayash Kapoor is a computer science Ph.D. candidate at Princeton University's Center for Information Technology Policy and a co-author of AI Snake Oil. His research focuses on the societal impact of AI. He is a recipient of a best paper award at ACM FAccT, an impact recognition award at ACM CSCW, and was included in TIME's inaugural list of the 100 most influential people in AI.\n**Past Event**Wednesday, April 2, 2025\n**Time**\n12:30 PM - 1:30 PM ET\n**Location**\n1557 Massachusetts Ave.\nMultipurpose Room, 5th Floor\nCambridge,MA02138US\n*Last updatedMay 16, 2025*\n### You might also like\n* community\n[Inside the Black Box] \n* community\n[Fact checking Moravec&#039;s paradox] \n* community\n[How AI deepfakes have skirted revenge porn laws] \n## Events03\n[] \nEvent\nMar 12, 2025 @ 12:30 PM\n### [Open Source Lawfare: AI Regulation After DeepSeek] \nBKC Spring Speaker Series EventBKC Fellow Ben Brooks dives beyond the splashy AI headlines to the important policy forces shaping our regulatory landscape now and in the future…\n[] \nEvent\nApr 30, 2025 @ 12:30 PM\n### [Radical Optionality: A Governance Strategy for Managing Uncertainty] \nSpring Speaker Series\nIn this talk, Mackenzie Arnold will outline a third option for how to govern AI systems: “radical optionality.”\n[] \nEvent\nApr 16, 2025 @ 12:30 PM\n### [New Legal Directions for a Global AI Commons] \nSpring Speaker Series\nIn the debate about the future of intellectual property in an AI world, there exists an opportunity to build new legal and technical infrastructure that preserves the ethos of the…",
    "length": 3349,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Getting Started with HUIT-Supported AI Tools",
    "url": "https://bokcenter.harvard.edu/getting-started-huit-supported-ai-tools",
    "text": "Getting Started with HUIT-Supported AI Tools | The Derek Bok Center for Teaching and Learning[\nSkip to main contentarrow\\_circle\\_down\n] \nannouncement\n**The Bok Center is currently developing new programming and new resources for the FAS community. Thank you for your patience as we revise our website to reflect these changes.**\nclose\n[![Harvard University]] \n[\n![The Derek Bok Center for Teaching and Learning] \n![The Derek Bok Center for Teaching and Learning] \n] \nmenucloseMenu\nSearch\nSearchsearch\n[\n![The Derek Bok Center for Teaching and Learning] \n![The Derek Bok Center for Teaching and Learning] \n] \n# Getting Started with HUIT-Supported AI Tools\nHarvard provides access to secure and robust AI platforms to safeguard privacy.\n### **HUIT AI Sandbox**\n* Grants access to popular large language models (LLMs), including OpenAI’s GPT-4o, Anthropic’s Claude, Google’s Gemini, and Meta’s Llama.\n* Ensures privacy and security by preventing third-party access to chats.\n* Can be used for image generation, data visualization, and the ability to upload multiple files.\n* Does not support creating custom GPTs.\n* Faculty, instructional staff, and administrative staff[can log in to the HUIT AI Sandbox here].\nHUIT has provided[initial guidelines for the use of generative AI tools at Harvard].\n### **Google Gemini**\n* As part of Harvard's Google Workspace, Gemini is integrated directly into Harvard Google accounts (Gmail, Docs, Drive, etc.).\n* Provides all FAS students, faculty, and staff with access to Gemini as part of the standard Harvard Google Workspace tools. Instructors should note that**Gemini will be the primary AI service available to all FAS students**in the coming academic year, making it a good option if they wish to incorporate AI into coursework.\n* Includes access to NotebookLM, Google’s AI-powered research and note-taking tool.\n* For assistance with Gemini access or account setup, contact HUIT (see the[Harvard Google Workspace] page for details).\n### **OpenAI ChatGPT Edu**\n* Provides access to advanced LLMs like GPT-4o and DALL-E.\n* Enables the creation of custom GPTs, allowing faculty to design tailored chatbots with user-defined personas and knowledge bases for specific instructional needs (e.g., \"tutorbots\").\n* [Members of the FAS can find information about accessing their accounts here].\n[![Embedded YouTube video]] \n[![Embedded YouTube video]] \n## **The Bok Center is currently developing new programming and new resources for the FAS community. Thank you for your patience as we revise our website to reflect these changes.**",
    "length": 2567,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "What is ‘original scholarship’ in the age of AI?",
    "url": "https://news.harvard.edu/gazette/story/2024/05/how-is-generative-ai-changing-education-artificial-intelligence/",
    "text": "While moderating a talk on artificial intelligence last week, [Latanya Sweeney] posed a thought experiment. Picture three to five years from now. AI companies are continuing to scrape the internet for data to feed their large language models. But unlike today’s internet, which is largely human-generated content, most of that future internet’s content has been generated by … large language models.\n\nThe scenario is not farfetched considering the explosive growth of generative AI in the last two years, suggested the Faculty of Arts and Sciences and Harvard Kennedy School professor.\n\nSweeney’s panel was part of a daylong [symposium] on AI hosted by the FAS last week that considered questions such as: How are generative AI technologies such as ChatGPT disrupting what it means to own one’s work? How can AI be leveraged thoughtfully while maintaining academic and research integrity? Just how good are these large language model-based programs going to get? (Very, very good.)\n\n“Here at the FAS, we’re in a unique position to explore questions and challenges that come from this new technology,” said [Hopi Hoekstra], Edgerley Family Dean of the Faculty of Arts and Sciences, during her opening remarks. “Our community is full of brilliant thinkers, curious researchers, and knowledgeable scholars, all able to lend their variety of expertise to tackling the big questions in AI, from ethics to societal implications.”\n\nIn an all-student panel, philosophy and math concentrator Chinmay Deshpande ’24 compared the present moment to the advent of the internet, and how that revolutionary technology forced academic institutions to rethink how to test knowledge. “Regardless of what we think AI will look like down the line, I think it’s clear it’s starting to have an impact that’s qualitatively similar to the impact of the internet,” Deshpande said. “And thinking about pedagogy, we should think about AI along somewhat similar lines.”\n\n![Students Naomi Bashkansky, Fred Heiding, and Chloe Loughridge discuss generative AI at the symposium.] Students Naomi Bashkansky (from left), Kevin Wei, and Chloe Loughridge discuss their experiences with AI.\n\nComputer science concentrator and master’s degree student Naomi Bashkansky ’25, who is exploring [AI safety issues] with fellow students, urged Harvard to provide thought leadership on the implications of an AI-saturated world, in part by offering courses that integrate the basics of large language models into subjects like biology or writing.\n\nHarvard Law School student Kevin Wei agreed.\n\n“We’re not grappling sufficiently with the way the world will change, and especially the way the economy and labor market will change, with the rise of generative AI systems,” Wei said. “Anything Harvard can do to take a leading role in doing that … in discussions with government, academia, and civil society … I would like to see a much larger role for the University.”\n\nThe day opened with a panel on original scholarship, co-sponsored by the [Mahindra Humanities Center] and the [Edmond & Lily Safra Center for Ethics]. Panelists explored ethics of authorship in the age of instant access to information and blurred lines of citation and copyright, and how those considerations vary between disciplines.\n\n[David Joselit], the Arthur Kingsley Professor of Art, Film, and Visual Studies, said challenges wrought by AI have precedent in the history of art; the idea of “authorship” has been undermined in the modern era because artists have often focused on the idea as what counts as the artwork, rather than its physical execution. “It seems to me that AI is a mechanization of that kind of distribution of authorship,” Joselit said. He posed the idea that AI should be understood “as its own genre, not exclusively as a tool.”\n\nAnother symposium topic included a review of Harvard Library’s law, information policy, and AI survey research revealing how students are using AI for academic work. Administrators from across the FAS also shared examples of how they are experimenting with AI tools to enhance their productivity. Panelists from the Bok Center shared how AI has been used in teaching this year, and Harvard University Information Technology gave insight into tools it is building to support instructors.\n\nThroughout the ground floor of the Northwest Building, where the symposium took place, was a poster fair keying off final projects from Sweeney’s course “Tech Science to Save the World,” in which students explored how scientific experimentation and technology can be used to solve real-world problems. Among the posters: “Viral or Volatile? TikTok and Democracy,” and “Campaign Ads in the Age of AI: Can Voters Tell the Difference?”\n\nStudents from the inaugural General Education class “ [Rise of the Machines?] ” capped the day, sharing final projects illustrating current and future aspects of generative AI.\n\n#### Share this article\n\n- ![] [Share on Facebook] \n- ![] [Share on LinkedIn] \n- ![] [Email article] \n- ![] [Print/PDF] \n\n## You might like\n\n- ![Bonobo vocalizing.] \n[Science & Tech] \n\n### [Turns out, bonobos ‘talk’ a lot like humans] \n\n\n\nResearchers compile dictionary of vocalizations suggesting the animals use equivalent of word compounds, phrasings to communicate complex social situations\n\n[![Findings series]] \n\n4 min read\n\n- ![Don Ingber in his lab.] \n[Science & Tech] \n\n### [He got the stop-work order. Then the scrambling began.] \n\n\n\nWyss’ Don Ingber details rush to hold onto consequential projects, talented researchers — and system that has driven American innovation\n\n\n\n5 min read\n\n- ![Gary Ruvkun, circa 2000, next to a computer screen showing the roundworm C. elegans.] \n[Science & Tech] \n\n### [Long trail from 1992 discovery to 2024 Nobel] \n\n\n\nGary Ruvkun recounts years of research, which gradually drew interest, mostly fueled by NIH grants\n\n\n\n4 min read\n\n\n## Trending\n\n1. [Science & Tech] \n\n### [Long trail from 1992 discovery to 2024 Nobel] \n\n\n\nGary Ruvkun recounts years of research, which gradually drew interest, mostly fueled by NIH grants\n\n\n\n\n\n4 min read\n\n2. [Science & Tech] \n\n### [He got the stop-work order. Then the scrambling began.] \n\n\n\nWyss’ Don Ingber details rush to hold onto consequential projects, talented researchers — and system that has driven American innovation\n\n\n\n\n\n5 min read\n\n3. [Campus & Community] \n\n### [Harvard files lawsuit against Trump administration] \n\n\n\nFiling argues freeze of research funding violates First Amendment, laws, procedures\n\n\n\n\n\n5 min read",
    "length": 6488,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "AI Lab",
    "url": "https://communicate.gse.harvard.edu/our-mission-2",
    "text": "AI Lab | HGSE Peer-to-Peer Services[\nSkip to main contentarrow\\_circle\\_down\n] \n[![Harvard University]] \n[\nHGSE Peer-to-Peer Services\nYour home for writing, verbal communication, AI , and data support\n] \nmenucloseMenu\nSearch\nSearchsearch\n[\nHGSE Peer-to-Peer Services\nYour home for writing, verbal communication, AI , and data support\n] \n# AI Lab\n![robot and human shaking hands] \n**The HGSE AI Lab**is dedicated to empowering our community to navigate and harness the transformative potential of Generative AI technologies in education. Recognizing the profound impact these advancements have on learning paradigms, our mission is to facilitate a smooth integration of AI tools within our academic ecosystem, addressing the diverse levels of familiarity and acceptance among students and faculty. We are committed to bridging knowledge and communication gaps through personalized consultations and interactive workshops, aiming to create a cohesive environment where all members feel supported and informed.\nFurthermore, the AI Lab aspires to be a nexus of insight and innovation, fostering a deep understanding of GenAI's role in shaping the academic and professional futures of our students. By acting as a trusted mediator, we aim to amplify the voices of both faculty and students, facilitating a rich exchange of perspectives and accelerating the adoption of GenAI in a manner that is ethical, equitable, and reflective of our diverse community. Encouraging the exploration of novel applications and pedagogical approaches, we envision the lab as a crucible for ideation and experimentation, where novel uses of AI are welcomed and best practices are shared widely. Our goal is not only to adapt to the evolving educational landscape but to actively shape it, promoting a forward-thinking culture that anticipates the future of learning and teaching.\n## Services\nThe HGSE AI Lab supports our community in the dynamic field of Generative AI (GenAI) technologies in education. With services tailored to both students and faculty, we offer personalized 1-on-1 consultations and interactive workshops aimed at enhancing GenAI literacy, fostering innovation, and facilitating effective integration into academic practices.\nUnderstanding the rapid evolution of AI, our team acknowledges that we may not have all the answers to the specific challenges and opportunities GenAI presents. However, we are committed to acting as a trusted advisor and partner, helping you navigate these complexities. We encourage feedback and insights from our community to refine our services and better meet your needs.\nco\\_present\n[### 1-on-1 Consultations\n] \nWe provide 30-minute Zoom consultations for both students and faculty on a range of topics related to GenAI, from practical tool use and policy understanding for students to teaching integration and policy development for faculty. These sessions are tailored to address individual questions and promote a deeper understanding of GenAI at HGSE.\ngroups\n[### Roundtables\n] \nOur flagship HGSE AI &amp;&amp; Education Roundtable brings together leading voices in AI and education to explore a wide range of topics—from academic integrity and education ventures to the disruption of higher education and beyond.\nrocket\\_launch\n[### AI Initiative Fund\n] \nDo you have ideas to promote collective learning in AI and education at HGSE?\nWe have a microgrant to support you. Apply now!",
    "length": 3414,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "Institutional Voice",
    "url": "https://www.harvard.edu/president/news/2024/institutional-voice/",
    "text": "Institutional Voice - Harvard University President\n[Skip to main content] \n[![HARVARD UNIVERSITY]] [Visit Harvard.edu] \n[![Harvard Office of the President logo]] \nExplore More**\n* [News] \n# Institutional Voice\nDear Members of the Harvard Community,\nIn April, the[Institutional Voice Working Group] was established to consider whether and when our institution should issue official statements on publicly salient issues. We write today to share the Working Group’s report and recommendations.\nThe Working Group was a faculty committee drawn from across the University, reflecting an array of backgrounds and expertise. They conducted extensive outreach to the Harvard community, gathering input from every school and more than 1,000 faculty, students, staff, and alumni through 31 focus groups, an online poll, and a dedicated email address. The Working Group also researched existing University and School-based practices relating to Harvard’s institutional voice and considered approaches at other universities.\nToday, we are delighted to share with the Harvard community[the Working Group’s report], containing a set of principles and recommendations that ground the use of institutional voice in the University’s mission of “seeking truth through open inquiry, debate, and weighing . . . evidence.” In particular, the report concludes that “[t]he university and its leaders should not . . .  issue official statements about public matters that do not directly affect the university’s core function” as an academic institution. It reasons that when the University “speaks officially on matters outside its institutional area of expertise,” such statements risk compromising the “integrity and credibility” of our academic mission and may undermine open inquiry and academic freedom by making “it more difficult for some members of the community to express their views when they differ from the university’s official position.”\nWe have accepted the faculty Working Group’s report and recommendations, which also have been endorsed by the Harvard Corporation. The process of translating these principles into concrete practice will, of course, require time and experience, and we look forward to the work ahead.\nWe encourage you to read the report and a related[Gazette Q&amp;A] ****with the Working Group’s co-chairs, Professors Noah Feldman and Alison Simmons. We are grateful to them, the members of the Working Group, and all who contributed ideas and insights that informed these thoughtful recommendations.\nSincerely,\nAlan M. Garber\nInterim President\nJohn F. Manning\nInterim Provost\nMeredith Weenick\nExecutive Vice President\nAndrea Baccarelli\nDean, Harvard T.H. Chan School of Public Health\nTomiko Brown-Nagin\nDean, Harvard Radcliffe Institute\nNancy Coleman\nDean, Division of Continuing Education and University Extension\nGeorge Q. Daley\nDean, Harvard Medical School\nSrikant Datar\nDean, Harvard Business School\nEmma Dench\nDean, Harvard Kenneth C. Griffin Graduate School of Arts and Sciences\nDouglas W. Elmendorf\nDean, Harvard Kennedy School of Government\nMarla Frederick\nDean, Harvard Divinity School\nWilliam V. Giannobile\nDean, Harvard School of Dental Medicine\nJohn C.P. Goldberg\nInterim Dean, Harvard Law School\nHopi E. Hoekstra\nEdgerley Family Dean, Faculty of Arts and Sciences\nRakesh Khurana\nDean, Harvard College\nBridget Terry Long\nDean, Harvard Graduate School of Education\nDavid C. Parkes\nDean, Harvard John A. Paulson School of Engineering and Applied Sciences\nSarah M. Whiting\nDean, Harvard Graduate School of Design",
    "length": 3534,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "AI Tools Available to the FAS",
    "url": "https://atg.fas.harvard.edu/ai-at-fas",
    "text": "AI Tools Available to the FAS | Academic Technology[\nSkip to main contentarrow\\_circle\\_down\n] \n[![Harvard University]] \n[\nAcademic Technology\nfor the Faculty of Arts and Sciences\n] \nmenucloseMenu\nSearch\nSearchsearch\n[\nAcademic Technology\nfor the Faculty of Arts and Sciences\n] \n# AI Tools Available to the FAS\nSort\n[**Harvard AI Sandbox**] \n|\nExperiment with multiple LLMs in secure environment.\n[Click here to see our short video on the AI sandbox] \n**Features:**Code generation, Creative writing, Data analysis, Summarizing, Text generation and editing, Image generation, Translation\n|\nManaged by HUIT. Available to FAS faculty, staff, and students.\n|\n[**Google Gemini**] \n|\nVersatile chatbot able to generate text, code, and more.\n[Click here to see our short video on Gemini] \n**Features:**Code generation, Creative writing, Data analysis, Image and storybook generation, Summarizing, Text generation and editing, Translation\n|\nAvailable to FAS faculty, staff, students, and researchers as part of Harvard's[Google Workspace] license.\n|\n[PingPong] \n|\nA platform for using large language models for teaching and learning.\n**Features:**Create and share custom bots for specific tasks, like serving as a virtual teaching assistant with access to course documents.\n|\nAvailable to faculty and instructors for use in FAS courses.\n[Pilot info and quick start guide here] \n|\n[**OpenAI ChatGPT Edu**] \n|\nVersatile chatbot able to generate text, code, images, and more.\n**Features:**Chatbot customization, Code generation, Creative writing, Data analysis, Image generation, Summarizing, Text generation and editing, Translation\n|\nAvailable to FAS faculty, staff, students for academic year 2025-26 as part of the FAS's[ChatGPT Edu] license. Use of OpenAI enterprise accounts after June 2026 will require administrative and budgetary approval.\n|\n[**Adobe Firefly**] \n|\nGenerate images and text effects by simply typing key words or a description. Trained on stock images, openly licensed and public domain content. Also integrated into Adobe apps.\n**Features:**Image generation, Image editing\n|\nAvailable to FAS faculty, staff, students, and researchers as part of Harvard's[Adobe Creative Cloud] license.\n|\n**API Access**\n|\nUse code to access OpenAI models for AI functionality in scripts, tools, and other programming projects.\n**Features:**Text generation, Summarizing, Code generation, Translation, Data analysis\n|\nAvailable to FAS faculty for teaching and learning use. Please contact[atg@fas.harvard.edu] for details. OpenAI for Community Developers API access is available to FAS constituents for teaching, learning, and experimentation. Each individual who registers for access and claims an API key will have access to $10/month in OpenAI API credits.\n|\n## Where can I get help?\n* If you need help with**account creation or access**, please[contact the HUIT Service Desk].## More resources:\n* [University guidelines on the use of generative AI] \n* [Bok Center AI resources] \n* [AI @ the FAS] \n* [HUIT Generative AI homepage] \n* [University Generative AI homepage] \nSee also:\n* [Technology]",
    "length": 3092,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "",
    "url": "https://academicunionization.harvard.edu/sites/g/files/omnuum3256/files/2025-09/Memo%20of%20Understanding%20on%20Artificial%20Intelligence%208-4-25.pdf",
    "text": "\n \n \n \n \n \n \n \n \nUniversity \n8-4-25 \nMemo of Understanding on Artificial Intelligence \nThe Union has proposed inclusion of an article on Artificial Intelligence (“AI”) in the \ncollective bargaining agreement. The University has rejected this proposal. Nevertheless, \nthe parties recognize that AI is an emerging technology that may impact working \nconditions and therefore, recognize the benefit for designated discussion forums between \nthe parties. \nConsistent with its management rights, the University is entitled to implement and utilize AI \nin its sole discretion. If concerns arise regarding the use of AI, the Union may raise those \nconcerns at the Labor Management Committee for discussion. \nWithout restricting the University’s use of AI in any way, the parties agree that should the \nUniversity’s implementation of AI directly impact the working conditions of Employees, \nthe parties will discuss such use and, to the extent required by the National Labor \nRelations Act, will engage in impact bargaining over the effects of such implementation. \nThis MOU will only remain in effect for the life of this collective bargaining agreement. \nThe Union will withdraw its proposals on Artificial Intelligence.",
    "length": 1214,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "AI Literacy & Ethics",
    "url": "https://bokcenter.harvard.edu/ai-literacy-and-ethics",
    "text": "AI Literacy &amp; Ethics | The Derek Bok Center for Teaching and Learning[\nSkip to main contentarrow\\_circle\\_down\n] \nannouncement\n**The Bok Center is currently developing new programming and new resources for the FAS community. Thank you for your patience as we revise our website to reflect these changes.**\nclose\n[![Harvard University]] \n[\n![The Derek Bok Center for Teaching and Learning] \n![The Derek Bok Center for Teaching and Learning] \n] \nmenucloseMenu\nSearch\nSearchsearch\n[\n![The Derek Bok Center for Teaching and Learning] \n![The Derek Bok Center for Teaching and Learning] \n] \n# AI Literacy &amp; Ethics\nRecent studies of AI use in higher education (e.g.[Lund et al., 2025];[Yusuf et al., 2024]) report that students are increasingly interested in learning how to use generative AI tools responsibly and ethically, but often feel uncertain about appropriate practices. A 2024 Harvard undergraduate survey similarly highlights that students want explicit, consistent rules about AI use in their courses. Educators can help students navigate generative AI with confidence and integrity by providing clear guidelines and open communication.\n### **AI Literacy For Students**\nA crucial aspect of communicating with students about AI is supporting their AI literacy. This means helping students understand what generative AI tools are, how they work, and where their strengths and limitations lie. Discussing how AI generates content, why it can sometimes produce errors or “hallucinations,” and how to responsibly use and cite these tools equips students to make informed decisions in their academic work. Some resources for this include:\n* [The AI Pedagogy Project] by metaLab at Harvard\n* The[Harvard Libraries Artificial Intelligence for Research and Scholarship Guide], which includes information about citing AI.\nBuilding students’ AI literacy can start with a few key steps you take in your own course policies and classroom conversations. Here are some recommendations for how to communicate about AI use with your students:\n* Include an AI policy on your syllabus. Be transparent with students about why you are asking them to complete a particular assignment and explain how using or not using AI tools will affect those goals. Syllabus statement advice is available through:\n* [The Office of Undergraduate Education website] \n* [The Bok Center’s Illustrated Rubric for Syllabus Statements on Generative AI] \n* Having a clear AI policy on your syllabus is a good start, but you might also want to check in regularly with your students by posting the policy on your Canvas site, mentioning it in assignment prompts, and discussing it during class meetings and office hours. Regular reminders can help ensure that students understand and remember your approach to AI use.\n* Require students to disclose their use of AI, whether for[brainstorming, drafting, or other purposes.] \n* If students are not permitted to use AI, design assignments that minimize AI’s utility, such as personalized reflections, oral presentations, or in-class tasks, ensuring that students engage deeply with the material and demonstrate their own understanding.\n* Model responsible AI use for your students by being transparent about how you incorporate AI in tasks such as preparing materials, generating content, or providing feedback. Demonstrating thoughtful use of AI can help students understand its potential as a tool for learning while reinforcing ethical and effective practices.\nYou can also encourage students to explore further resources for developing AI literacy, such as attending workshops, experimenting with new tools, or discussing their experiences with peers. Actively supporting students in building AI literacy can help them use these tools more thoughtfully and effectively. For example, students can use AI as a tool for deeper learning by generating their own study questions, analyzing content from multiple perspectives, or engaging in reflective critique of AI-generated outputs. Framing AI as both a partner in and an object of learning can help students build critical thinking skills and engage more meaningfully with course material.\n### **Key Debates And Ethical Questions**\nAs AI becomes more integrated into academic life, it raises important questions around academic honesty, fairness, and responsible technology use. Addressing these topics with students is essential to help them understand the potential impacts of AI on their learning and future careers.\n#### **Bias and Fairness in AI Systems**\nAI systems are trained on[vast datasets] – predominantly collected from the internet– and therefore[incorporate the biases and stereotypes] embedded in those datasets. AI-generated content reflects dominant cultural norms and can pose the risk of marginalizing or misrepresenting, reinforcing harmful stereotypes and perpetuating inequality.\nFor the same reason, the use of AI might also undercut the learning goals of intellectual vitality.[Intellectual vitality] encourages students to question assumptions and resist arriving at premature conclusions— two areas where AI-generated output often falls short.\nAnother consideration is how certain AI tools can disadvantage certain student groups. For instance, AI systems that process language may struggle with non-standard dialects or multilingual speakers, leading to inaccuracies or misunderstandings.[Vigilance in recognizing and addressing AI’s limitations] is essential in diverse classroom settings.\n#### **Privacy and Data Security**\nUsing AI in educational settings[raises concerns about privacy and data security]. Many AI tools require users to input personal information or academic work into platforms that collect and store data. In some cases, this data may be used for purposes beyond the immediate educational context, such as marketing or[further training of AI models], often without the user's explicit consent. This raises ethical questions regarding the ownership and control over one's intellectual property and private information.\nTo combat this, FAS members are encouraged to use Harvard-approved tools (Harvard’s[AI Sandbox] and[ChatGPT Edu] Workspace). These options allow for the upload of confidential materials (specifically, those materials[Level Three] and below).\n#### **Environmental Impact**\nTraining and running large AI models require[substantial computational power,] which in turn consumes significant energy. This energy consumption is not unique to AI; many digital processes, such as video streaming and cloud storage, also demand significant resources.[But as AI use expands] in education,[its contribution to carbon emissions and other environmental harms likewise grows].\n#### **Copyright and Intellectual Property**\nGenerative AI tools rely on massive datasets that include[copyrighted material.] This can raise ethical and legal concerns around[ownership, use, and attribution of content produced with AI.] \n**The Bok Center is currently developing new programming and new resources for the FAS community. Thank you for your patience as we revise our website to reflect these changes.**",
    "length": 7113,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  },
  {
    "query": "site:harvard.edu Harvard University generative AI policy",
    "title": "AI APIs and developer tools",
    "url": "https://www.huit.harvard.edu/ai-developer-tools",
    "text": "[Skip to main contentarrow\\_circle\\_down] \n\n# AI APIs and developer tools\n\n# AI APIs and developer tools\n\nTools & services\n\n[Request a consultationarrow\\_circle\\_right] [Sign into the API Catalogarrow\\_circle\\_right] \n\n## Overview\n\nHUIT offers developers guidance on and access to generative AI APIs and frameworks, enabling the integration of Large Language Models (LLMs) into their applications or services. This includes chatbot creation and customization, building and testing applications, access to model training and deployment, coding, predictive analytics, and more. Code and low/no-code offerings are available. These tools are subject to change based on availability. Self-service AI API access for eligible users is available through [Harvard’s API Platform]. These include pay-as-you-go or limited-access, credit-redemption options, including the [FAS OpenAI “Community Developer” API] (HarvardKey-protected). For additional guidance on these and other AI offerings, [request a consultation].\n\nCompliance with [University guidelines for use of generative AI] required for use.\n\n### Eligibility\n\n**Faculty, staff, researchers,** and **students** in Central Administration, FAS, College, DCE, GSAS, SEAS, GSD, GSE, HBS, HKS, HDS, HLS, HMS (Quad), HSDM (Quad), Radcliffe, SPH\n\n_Some self-service AI APIs in the portal have limited availability. Where this is the case, it's noted in the documentation for the API_\n\n### Data classification level\n\nApproved for up to [Level 3 confidential data] \n\n### Quick links\n\n- [Get access to AI APIs through the Harvard API Portal] \n- [Generative AI at Harvard]",
    "length": 1608,
    "domain_restricted": true,
    "university": "Harvard University",
    "domain": "harvard.edu"
  }
]