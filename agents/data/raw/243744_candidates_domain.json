[
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Course Policies on Generative AI Use - Teaching and Learning Hub",
    "url": "https://tlhub.stanford.edu/docs/course-policies-on-generative-ai-use/",
    "text": "Course Policies on Generative AI Use - Teaching and Learning Hub\n[Skip to content] \nStanford University\n**\nFor immediate technical assistance during class time, call the GSB classroom support hotline: 1(650)736-3342\nOr submit a[support ticket] to Stanford Service-Now\n[![Teaching and Learning Hub, Stanford GSB]] \n[**MenuClose] \n# Course Policies on Generative AI Use\n![] \n# Engaging Students\n9\n* **[Pre-recording Videos to Share with Your Students] \n* **[Handout: Zoom Instructions for Remote Guest Speakers] \n* **[Faculty Tips: Teaching with Presence and Connection] \n* **[Tools and Technology] \n* **[Asynchronous Course Discussions] \n* **[Conducting Role-Plays] \n* **[Using Polls in the GSB Classroom] \n* **[Building Classroom Community] \n* **[Leading Effective Discussions] \n![] \n# Feedback\n1\n* **[Collecting Mid-quarter Student Feedback] \n![] \n# Resources for Remote Teaching\n1\n* **[Teaching in the Virtual Classroom] \n![] \n# Open Discourse\n6\n* **[Teaching During an Election Season] \n* **[Handling Sensitive Topics] \n* **[Writing Content Notices for Sensitive Content] \n* **[GSB Students on Open Discourse: May 24, 2023 Panel Highlights] \n* **[Engaging with Difference in Course Content] \n* **[GSB Faculty on Open Discourse and Participation: April 13, 2022 Panel Highlights] \n![betterdocs-category-icon] \n# Teaching and AI\n6\n* **[GSB Student Voices Panel on Exploring AI: Event Highlights] \n* **[Faculty Showcase on AI and Teaching] \n* **[Starting Small with AI in the Classroom] \n* **[Teaching in the AI Era] \n* **[Course Policies on Generative AI Use] \n* **[Quick Start Guide to AI and Teaching] \n[] \n* [Teaching and AI] \n# Course Policies on Generative AI Use\n* updated onDecember 11, 2025\nTable of contents\n* [GSB Guidelines on Classroom AI Use] \n* [GSB Policy] \n* [Tips for Setting Your Approach to AI Use] \n* [Citing Generative AI Use] \n* [Template Syllabus Statements for Generative AI Use] \n* [Additional Resources] \n## GSB Guidelines on Classroom AI Use\n#### GSB Policy\n**MBA/MSx courses: Instructors may not ban student use of AI tools for take-home coursework**, including assignments and exams.**Instructors may choose whether to allow student use of AI tools for in-class work**, including exams. Except for in-class exams, students should be allowed to use AI tools in cases where they are able to use the internet and electronic devices. In-class assignments for which electronic devices are prohibited inherently also prohibit use of the internet, AI tools, tablets, laptops, calculators, etc.\n**PhD/Undergraduate courses:**Follow the[Generative AI Policy Guidance] from Stanford&#8217;s Office of Community Standards.\n#### Tips for Setting Your Approach to AI Use\n* **Consider your course goals**to inform how you guide students on using AI in your course.\n* **Explain your choices about AI use to students.**Connect your guidance to the learning goals of the course, industry standards (e.g., publication standards), or common industry cases (e.g., needing to give thoughtful/persuasive/clear input on the spot during board meetings).\n* **State your policy in your course syllabus and the[Course Policies and Norms form] **, and then follow up in class with your students.\n## Citing Generative AI Use\nGSB students are expected to**cite all resources, including generative AI tools**, used to prepare their academic work.See the MBA and MSx[assignment preparation policy].\nNevertheless, many students are hesitant to report AI use. It can help to ask all students to report*how*they used AI (instead of*whether*they used AI). Asking students to regularly report their AI use can help normalize transparent AI use.\nHere are some ways you can**ask students to acknowledge AI use**:\n* Submit chat logs associated with major assignments or exams\n* Disclose AI use according to your industry&#8217;s emerging standards, if available\n* Include statements with coursework to disclose how AI was used\n> Example statement: *> Gemini was used to gather sources for this project and create an initial written draft based on the author’s outline. Substantial revisions were made by the author, with additional fact-checking with ChatGPT. Claude was used to generate data visualizations. The ideas discussed in this report are those of the author alone.\n*\n* Reflect on how they used AI for the assignment\n> Sample prompt: *> How did you use AI in this assignment? What was or wasn’t effective? What will you do differently next time?\n*\n* Follow Stanford Library’s[guide to citing AI] in academic research, including[guidelines] and[examples] \n* Follow examples from Stanford UIT on[citing AI-generated content] (see the FAQ “Can you provide some examples of how to cite when content is generated by AI?”).\n## Template Syllabus Statements for Generative AI Use\nAdapt the template syllabus statements below to fit your course.\n**\n### AI Tools Permitted for All Coursework\nIn this course, you may use generative AI tools for all coursework, including in-class work, according to the following guidelines for use.\nGuidelines for use:\n* The following**Stanford-approved generative AI tools**should be used whenever possible when using AI tools for your coursework. These secure platforms may be used for most prompts, materials, and data (except[high risk data]).\n* [Stanford AI Playground] \n* [Google Gemini] and[NotebookLM] (only when authenticated using your SUNet ID)\n* Nectir AI (accessed through Canvas)[*Include if you have enabled this tool for your course.*]\n* [*Optional: Share links to additional tools that are recommended for your course.*]\n* Review and**follow the guidelines provided in Stanford IT’s resource on[Responsible AI at Stanford] **.\n* **When using a third-party, non-Stanford-approved AI tool**such as a personal ChatGPT account, make sure to check the fine print terms before signing up.**Avoid inputting information that should not be made public**. This includes personal or confidential information of your own or that others share with you, as well as proprietary or copyrighted materials ([*include relevant material types for your course:*e.g., case studies, data sets, assignment prompts]) that may be included in your coursework.\n[*Optional: Select one or more additional guidelines for use from the list below, according to what applies for your course. Highlighted text should be edited to fit your course policies.*]\n* In this course,**you will work with[sensitive / copyrighted / personal]data**, including[*examples from your course: e.g.,*data sets, slide decks, case studies, interview transcripts]. When working with this data, you**may only use the Stanford-approved tools**[listed above]****. You may not input this data into third-party AI tools, according to the guidelines provided in[Responsible AI at Stanford]. Note that[high risk data] is never appropriate for use with any AI tool.\n* **Cite all AI-generated material and/or explain how you have drawn on AI-generated material in your work.**Please cite generative AI use that contributes to your coursework in the following way:[*describe your preferred format (see examples above)*].\n* **Be prepared to fact-check and critically evaluate all AI-generated information.**Generative AI tools can provide false information (called ‘hallucinations’), perpetuate biases and/or stereotypes, or draw on copyrighted information without proper attribution, and such problematic information is often presented very convincingly. The materials these tools generate does not necessarily meet the standards of this course.\n**\n### AI Tools Permitted for Some In-class Work\nIn this course, you may use generative AI tools for select in-class work and take-home coursework, if you choose, according to the following guidelines.\n[*3 options for explaining for which in-class work AI tools will be allowed:*]\n[*Option 1*]**Refer to the Course Policies and Norms form for the assignment types on which AI tools are allowed.**\n[*Option 2*]**You may use AI tools during class when electronic devices are allowed.**\n[*Option 3*]**You may use generative AI tools for**[*specify allowable assignment types, e.g.,*homework assignments, problem sets, projects/papers, and exam prep]**, but not on**[*specify not allowable assignment types, e.g.,*for in-class exercises, on exams or quizzes].[*May parallel the categories given in and/or refer to the[Course Policies and Norms form].*]\n[*May follow with rationale of why AI use is restricted for certain assignments or exams.*]\nGuidelines for use:\n* The following**Stanford-approved generative AI tools**should be used whenever possible when using AI tools for your coursework. These secure platforms may be used for most prompts, materials, and data (except[high risk data]).\n* [Stanford AI Playground] \n* [Google Gemini] and[NotebookLM] (only when authenticated using your SUNet ID)\n* Nectir AI (accessed through Canvas)[*Include if you have enabled this tool for your course.*]\n* [*Optional: Share links to additional tools that are recommended for your course.*]\n* Review and**follow the guidelines provided in Stanford IT’s resource on[Responsible AI at Stanford] **.\n* **When using a third-party, non-Stanford-approved AI tool**such as a personal ChatGPT account, make sure to check the fine print terms before signing up.**Avoid inputting information that should not be made public**. This includes personal or confidential information of your own or that others share with you, as well as proprietary or copyrighted materials ([*include relevant material types for your course:*e.g., case studies, data sets, assignment prompts]) that may be included in your coursework.\n[*Optional: Select one or more additional guidelines for use from the list below, according to what applies for your course. Highlighted text should be edited to fit your course policies.*]\n* In this course,**you will work with[sensitive / copyrighted / personal]data**, including[*examples from your course: e.g.,*data sets, slide decks, case studies, interview transcripts]. When working with this data, you**may only use the Stanford-approved tools**[listed above]****. You may not input this data into third-party AI tools, according to the guidelines provided in[Responsible AI at Stanford]. Note that[high risk data] is never appropriate for use with any AI tool.\n* **Cite all AI-generated material and/or explain how you have drawn on AI-generated material in your work.**Please cite generative AI use that contributes to your coursework in the following way:[*describe your preferred format (see examples above)*].\n* **Be prepared to fact-check and critically evaluate all AI-generated information.**Generative AI tools can provide false information (called ‘hallucinations’), perpetuate biases and/or stereotypes, or draw on copyrighted information without proper attribution, and such problematic information is often presented very convincingly. The materials these tools generate does not necessarily meet the standards of this course.\n**\n### AI Tools Permitted for Take-home Work Only\nIn this course, you may not use generative AI tools for in-class work.[*May include rationale, e.g.,*So that we can continue to get the most out of our in-person interactions. / This course is designed to challenge your creative, analytic, and critical thinking skills, and using generative AI tools may not always support your learning of these skills. This is true even in cases where generative AI tools may be used to replicate portions of work we are asking you to complete.]\nYou may use AI tools for coursework completed outside of class, if you choose, according to the following guidelines.\nGuidelines for use:\n* The following**Stanford-approved generative AI tools**should be used whenever possible when using AI tools for your coursework. These secure platforms may be used for most prompts, materials, and data (except[high risk data]).\n* [Stanford AI Playground] \n* [Google Gemini] and[NotebookLM] (only when authenticated using your SUNet ID)\n* Nectir AI (accessed through Canvas)[*Include if you have enabled this tool for your course.*]\n* [*Optional: Share links to additional tools that are recommended for your course.*]\n* Review and**follow the guidelines provided in Stanford IT’s resource on[Responsible AI at Stanford] **.\n* **When using a third-party, non-Stanford-approved AI tool**such as a personal ChatGPT account, make sure to check the fine print terms before signing up.**Avoid inputting information that should not be made public**. This includes personal or confidential information of your own or that others share with you, as well as proprietary or copyrighted materials ([*include relevant material types for your course:*e.g., case studies, data sets, assignment prompts]) that may be included in your coursework.\n[*Optional: Select one or more additional guidelines for use from the list below, according to what applies for your course. Highlighted text should be edited to fit your course policies.*]\n* In this course,**you will work with[sensitive / copyrighted / personal]data**, including[*examples from your course: e.g.,*data sets, slide decks, case studies, interview transcripts]. When working with this data, you**may only use the Stanford-approved tools**[listed above]****. You may not input this data into third-party AI tools, according to the guidelines provided in[Responsible AI at Stanford]. Note that[high risk data] is never appropriate for use with any AI tool.\n* **Cite all AI-generated material and/or explain how you have drawn on AI-generated material in your work.**Please cite generative AI use that contributes to your coursework in the following way:[*describe your preferred format (see examples above)*].\n* **Be prepared to fact-check and critically evaluate all AI-generated information.**Generative AI tools can provide false information (called ‘hallucinations’), perpetuate biases and/or stereotypes, or draw on copyrighted information without proper attribution, and such problematic information is often presented very convincingly. The materials these tools generate does not necessarily meet the standards of this course.\n## Additional Resources\n* [Quick Start Guide to AI and Teaching], GSB Teaching and Learning Hub\n* [Teaching in the AI Era], GSB Teaching and Learning Hub\n* [AI Playground], Stanford UIT\n* [Responsible AI at Stanford], Stanford UIT\n**\n### Important Note:\nTechnology is changing at a rapid pace. While we make every attempt to ensure our content is updated to reflect changes to the interface and functionality, we can only guarantee the accuracy of the content on this resource page when it was written or recorded. Please be sure to check the software developer's website for the latest updates and release notes for the most up to date information. If you have questions or concerns, or need additional support, please contact us.\n[![Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License logo]] \n[Unless otherwise noted, you may copy and redistribute this resource in its unmodified form under the terms of the Creative Commons BY-NC-ND 4.0 International license (attribution, non-commercial, no-derivatives).] \n[**]",
    "length": 15154,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Report of the AI at Stanford Advisory Committee",
    "url": "https://provost.stanford.edu/2025/01/09/report-of-the-ai-at-stanford-advisory-committee/",
    "text": "Report of the AI at Stanford Advisory Committee | Office of the Provost\n[Skip to main content] [Skip to secondary navigation] \n[Stanford University(link is external)] \n[\nStanford\nOffice of the Provost\n] \nSearch this siteSubmit Search\nMain content start\n[From the Provost] \n# Report of the AI at Stanford Advisory Committee\nOn March 18, 2024, the Provost charged the AI at Stanford Advisory Committee to assess the role of AI at Stanford.\nJanuary 9, 2025\nOn March 18, 2024, the Provost charged the AI at Stanford Advisory Committee to assess the role of AI at Stanford in administration, education, and research to identify potential policy gaps and other needs to advance the responsible use of AI at Stanford. The committee met seven times between March 18 and June 19 to assess potential policy gaps in the areas of administration, education, and research at Stanford. They were informed by reports, policies, and resources from peer institutions that have assessed similar questions over the last year (e.g.[Cornell],[Michigan],[Harvard],[Yale], and[Princeton]).\n## Stanford and the Rapidly Changing Landscape of AI Use\nThere are great opportunities and great challenges associated with the newest generation of AI technologies, particularly large language models (LLMs). They represent powerful transformative tools for productivity because of their ability to generate/draft, summarize, and analyze text, images and other media. At the same time, they are new and have not systematically been assessed (for example, with respect to biases within the data used to train them, and their performance on critical tasks that are fact-based). Their attractive output may lull users into a misplaced sense of quality, precision, and accuracy. Most importantly, these technologies are rapidly evolving in both their capabilities and the degree to which they can be inspected, tested, verified and validated. And, while such models continue to improve, their already-known shortcomings require that care be taken when using these systems in many contexts.\nStanford is a leader in the development and application of AI. The university community includes experts in the creation, validation and extension of the core capabilities of AI. Across all schools and units, many colleagues are already using AI to advance teaching and research in their disciplines. Therefore, it seems prudent to articulate a core set of policies, standards, and best practices for the use of AI in the administrative, educational and research functions of the university. A fine balance is required. The committee wishes to encourage experimentation with the technology as it evolves, to maximally benefit the university, the research enterprise, and teaching and learning, while ensuring that key principles of the university are honored. Thus, our group approached its charge through the lens of providing guardrails to support productive uses of AI and not to stifle innovation, creativity or exploration.\nPer its charge, the committee sought to identify potential policy gaps. We list these, along with other considerations and recommendations for university activities in administration, education and research. Given the pace of change in the AI field, the committee cautions against creating fixed, rigid policies (except where required by law or other compelling considerations). Rather, we recommend the adoption of approaches that are flexible and do not unduly limit the creative use of AI to support the university’s mission. In some cases, policies are not required—guidelines and best practices will suffice and provide more flexibility. At a minimum, any best practices, standards or policies promulgated by the university should be evaluated and updated regularly based on their relevance and effectiveness.\nThe committee also arrived at a set of general principles to help guide the approach to AI, since many situations and challenges may not be anticipated in advance. These principles should help guide the*ad hoc*management of new issues as they arise. Our group learned that there are hundreds of uses of AI each day conducted by faculty, staff, students/trainees. General principles to help guide these “experiments” may be more useful to individual users and practitioners than any policy to cover the specific situations and use cases that we were able to anticipate and describe below. Of course, consultation with the Office of the General Counsel (OGC) is important when potential legal issues arise.\n## Guiding Principles for AI at Stanford\n[[1]] \nWe encourage the university to view goals in this space in line with the philosophy of Stanford’s Institute for Human-Centered AI (HAI) to make sure that the use of AI is centered on improving the human condition. Thus, we prefer to focus on “augmenting human capabilities” versus “replacing humans.” Many of us at Stanford aspire towards a goal where AI positively impacts our community. We must take into consideration the fair distribution of benefits that come from AI. We seek to remove or repair AI systems that tend to harm individuals or groups disproportionately. For critical systems affecting human health and welfare, we aspire for a high bar for validation and verification of AI accuracy, applicability, fairness, and equity before deployment. Moreover, we should resist the tendency to assume that existing laws, regulations, and university policies are not applicable in the context of AI use, a tendency we call “AI exceptionalism.” In short, we believe that the use of AI at Stanford should be human-centered.\nWe modify from a[Cornell report] a review of the elements of human-centeredness to formulate the following guiding principles that we can strive towards in our deployment of AI at Stanford:\nHuman Oversight. Humans should take responsibility for the AI systems used or created at Stanford. Each system should aim to have a clear line of authority and responsibility for system procurement/creation, maintenance, monitoring and sunsetting. There should be plans for re-engaging human control of systems when they do not work, and there should be contingency plans for management of high-risk AI systems.\nHuman Alignment. AI systems should be built/procured to support Stanford’s mission and core values. When appropriate, systems should be adopted and deployed in consultation with diverse stakeholders to identify risks and mitigation strategies–particularly for disparate negative impacts. The scope of use, system capabilities, and limitations of systems should be documented in an ongoing process and evaluated regularly for consistency with community standards.\nHuman Professionalism. All members of the Stanford community should adhere to standards and aim for high levels of rigor and quality in their work. We expect that they will exercise their best judgment and critical thinking in the use of AI tools. All community members are responsible for the content of our work and must be willing to take responsibility for that work, regardless of whether it was assisted by AI. They are also responsible for the quality of reasoning and veracity of assumptions that underlie the work. Community members are also responsible for abiding by applicable laws, university policies, and must consider the expectations and norms within the multiple professional communities to which they belong.[[2]] \nEthical and Safe Use. AI should be used to improve university functions. Decision makers should aim to understand the full implication of using AI systems in university functions and should embark upon evaluations when these implications are not fully understood. All community members should aspire to ensure that AI services promote justice and individual autonomy and are free from bias and unlawful discrimination.[[3]] The community should have processes for identifying and decommissioning previously deployed AI systems that are not safe and/or ethical.\nPrivacy, Security, and Confidentiality. When AI systems use personal data (especially sensitive data, as defined by law or common practice), the legality and impact should be appropriately assessed. If personal data is used for decision-making by AI systems, Stanford should reasonably aim to be transparent about how the decisions are made, whether they are inspectable and the degree to which they have been validated and verified independently. Some uses of data or application of AI systems may require consent, such as attorney-client privileged information, medical records and psychotherapist-patient information. Generative AI tools (such as Open AI and Chat GPT) save, reuse, and share the information entered with their affiliates including confidential or sensitive information. Any confidential or legally privileged information of Stanford or a third-party may not be provided to generative AI tools.\nData Quality and Control. All data used to create new AI systems with university resources should be collected in legal and ethical ways, and data provenance should be explicitly documented and managed as part of the system design. The university should consider building and maintaining a world class data-centric infrastructure that prospectively plans for data use in AI across the research lifecycle. AI systems should be tested and their risks documented. Technical and organizational controls should mitigate the risk of interference or exploitation by bad actors.\nAn AI Golden Rule. During a time of great change in the capabilities and uses of AI, it may be useful to “use or share AI outputs as you would have others use or share AI output with you.” One test would be for an individual to consider if they would be comfortable if the roles of AI user and recipient were reversed. Another test might be for an AI user to consider whether they would be willing to transparently disclose the details of their use with those affected by the AI output. These assessments would likely change over time, and would be based on individual judgements as well as evolving community norms for use of AI. They would be combined with the other principles to inform decision making about the use of AI.\n## Policy Areas\nWhile surveying the current uses and activities related to AI at Stanford, the committee found some critical areas that may require further guidance and support from the university. Appropriate groups (indicated where possible) may wish to evaluate the need for, and content of, additional policies related to the use of AI in conducting Stanford activities.\n### AI in university administration processes\n* **Hiring.**There are concerns of bias and risk of litigation around the use of AI to review, screen or filter applicants for jobs or roles at the university. There is also concern about bias in the use of AI to prepare job descriptions and job ads.*(University Human Resources.)*\n* **Performance Reviews.**The use of AI in the generation of performance review materials may raise issues of trust and staff morale, and should be managed very carefully. In addition, there may be legal implications to AI-generated materials in performance reviews.*(University Human Resources.)*\n* **Admissions.**Like hiring, the use and influence of AI in admissions decisions and application review at the undergraduate and graduate level raise concerns around bias, risk of litigation, and reputational harm. The committee recommends not using generative AI in the admissions process without careful and documented assessment of the performance of these systems. We recognize that AI may be helpful in creating communication materials for recruitment, but it is important to use it carefully in ways consistent with the guiding principles articulated above.*(Faculty Senate Committees on Undergraduate Admissions and Financial Aid and Graduate Studies.)*\n* **Communications.**Staff in communications roles across the university may need guidance in the appropriate use of AI for generating university content.*(University Communications.)*\n* **Surveillance.**Any data collection about the activities of members of the Stanford community is a potential threat to privacy, confidentiality, personal autonomy and the trust between the university and the members. At the same time, some of these technologies may be useful for tracking student classroom attendance. Therefore, there should be guidance on the procedure for deciding on surveillance, the expectations for transparency (including potentially consent), the expectations for data management, maintenance and deletion.*(University Privacy Office and the Faculty Senate Committee on Academic Computing and Information Systems.)*\n*Other recommendations and considerations:*\n* **Education and training on sensitive data.**Existing data security and privacy policies for computing resources cover many of the uses of AI services. However, there may be a sense that AI tools are exceptional or are not covered by these rules. For example, it is easy to paste text into ChatGPT and then forget that it might be inappropriate to provide sensitive university data to an external entity (like OpenAI) except under clearly prescribed conditions. The university should consider methods to inform university community members with access to these data of these risks.\n* **University-provided access to LLMs**. There are equity issues in access to LLMs, because they can be expensive and thus access across faculty, staff, students/trainees may be uneven. At the same time, most of the major vendors of LLMs are not transparent about the data used to train their models, and the models may not have been evaluated systematically for bias, fairness, precision or accuracy. The committee supports investigation into ways to make access to these tools more balanced, while offering choice and asking vendors to help characterize their products beyond “see what you think.” If LLMs are provided by the university, the committee sees an opportunity for education about how to use them and how to be wary of their outputs.\n* **Vision for AI and staffing.**It is our view that the best use cases of AI are where they help and enable staff in their work. The university should consider describing its vision for AI systems to augment and not replace staff work, and to emphasize ethical issues and ways that these should be mitigated. Staff may be especially concerned about job loss in the context of AI, and so the university leadership may want to consider how a vision of “augment do not replace” might be adopted. Such a policy should be complemented by readily available education in how staff (and others) can use AI to augment their work.*(University Human Resources.)*\n* **Streamlined procurement process.**It is clear that Stanford community members are using Stanford resources to purchase AI systems for a variety of purposes. This is consistent with the vision of experimentation and testing of AI. However, it also may create risks for the university if the procured software is not consistent with the guiding principles we outlined above (e.g. professional activities of staff are captured and saved, student information is captured, surveillance data is used for non-Stanford purposes). The committee recommends that high risk software be identified and evaluated, so that the community can learn about AI without subjecting community members to risks that violate our guiding principles.\n* **A website to communicate guidelines and resources to the community.**The university should continue to expand web resources that provide the Stanford community with easy access to information about AI resources, policies, and other supporting information. UIT’s[GenAI] website currently provides a central site with easy navigation to the[HAI] and[Responsible AI] websites while presenting an overview of gen AI to the broader Stanford audience. The site’s[GenAI Evaluation Matrix] helps Stanford community members understand the risk classifications for many commonly used GenAI tools. Collectively, pages like these may provide resources to improve AI literacy and present approaches for engaging with gen AI platforms while raising awareness about security and privacy concerns in order to use AI most productively.\n* **Letters of recommendation.**Letters of evaluation and recommendation are key to academic advancement, hiring and promotions. These letters ideally contain honest and original descriptions of candidate activities, and so the use of AI to generate text as a substantial component of letters of recommendation could have unintended impacts—such as recipients recognizing that the letter was produced with AI and thereby diminishing the value and weight of the letter. At the same time, an AI-generated letter as a first draft followed by substantial editing and preservation of original descriptions and detailed (often coded) evaluation language could lead to the more rapid creation of high-quality, accurate, personalized and useful letters. The committee recognizes that faculty have a large number of letters to write annually for a variety of purposes, and so generative AI may have some utility, if used appropriately. The AI Golden rule may be a useful principle in these activities.## Applications of AI to support education\n* **Assessment and grading**. The committee sees several potential risks in use of AI software to grade, assess, detect plagiarism, or provide feedback on student work, including (but not limited to) quality, accuracy, fairness, and potential bias. In addition, there are potential privacy concerns when using third-party tools and any FERPA implications. This is another rapidly evolving area, and it seems prudent to recommend or require instructor disclosures and justifications when AI is used for student assessment or student feedback.*(Faculty Senate Committees on Undergraduate Standards and Policy and Graduate Studies.)*\n* **Student useof AI.**Students have generally been early adopters, particularly of generative AI for writing–as well as other uses. They are preparing for careers where AI will likely be ubiquitous and increasingly powerful. The committee recognizes that there is an existing Honor Code policy about the use of AI. This should be examined for revision or updating; some faculty have found the[guidance] unclear, while students and others have also voiced that the policy is unenforceable and therefore not effective or useful. AI should be considered in the context of evolving changes to the Stanford Honor Code. Given predominant concerns about cheating, it may also be valuable to provide finer distinctions in the ways students (and faculty) may use AI to support their work, for example as a source of ideas or feedback versus a wholescale use of AI to take or grade exams or homework. Policies may be quite different for different uses of AI for learning.*(Board on Conduct Affairs.)*\n*Other recommendations and considerations:*\n* **Accommodations.**The committee recognizes that AI tools have the potential to be tools to address issues of accessible education. This is a complex topic that goes to the heart of how accessible education should be approached and requires careful deliberation on ensuring optimal educational and learning goals, while considering equity and applicable laws.\n* **Sandbox forusers.**The committee noted multiple concerns about the lack of university-sanctioned AI services, guided training programs, and best practices may have deterred Stanford workers from experimenting with AI for their job (administration, education, research). This is particularly acute in administration where efficiencies may be found, and education where instructors and students are interested in learning how to best use the tools. The university should consider offering a makerspace, sandbox, or other setting where community members can experiment with AI tools. For example, the Graduate School of Education has created an “AI Tinkery” (a digital maker space), where educators can explore different AI tools with the help of dedicated staff, who also provide regular workshops.*(Center for Teaching and Learning and Learning and Technology Services could providea makerspace open to all instructors, utilizing Stanford’s*[*AI Playground*] *.)*\n* **Access to some LLM tool for all students/campus.**The issue of universal access to one or more LLM tools for student equity came up repeatedly in our discussions and is related to the Sandbox consideration above. Student access to AI and LLMs is a fundamental but difficult issue. At a minimum, access to LLMs may be considered as an educational cost (like textbooks) in financial aid and “cost of college” calculations. There should be a process for determining the process for evaluating, procuring and distributing site licenses for current and future LLMs. Issues of evaluation–accuracy, precision, validation, verification will likely become increasingly prominent in such decisions.*(University IT.)*\n* **Resources for teaching with AI.**There is considerable variability across fields and between individual faculty in how best to approach teaching with AI. The university should consider providing frameworks and worked-out examples to help instructors think through all aspects of pedagogy impacted by AI (assessment, 1:1 tutoring, the definition of cheating and its detection). For example, a selection of sample course policies in addition to the existing training resources and workshops on offer through CTL and the Stanford Accelerator for Learning in the GSE is sponsoring seed grants and community events on the development of new AI-infused pedagogies.*(Leverage expertise in the Graduate School of Education together with the Center for Teaching and Learning.)*\n* **License for plagiarism checkers.**Access to plagiarism checking software for researchers is increasingly required for author self-checking against inadvertent and unintentional plagiarism.*(Stanford Libraries.)*## Uses of AI in research\n* **Authorship.**The university’s current[policy on authorship], written at a time where multi-disciplinary research was growing, may be due for an update to address potential issues around attribution between individual authors and AI. Although journals, publishers, and the government will no doubt create policies around this, Stanford should aim to have basic statements of authorship consistent with academic values, processes, and assessment of merit. In particular, the risk of quoting text beyond currently acceptable academic standards is present in some of the current LLMs and requires author professionalism.*(The Vice Provost and Dean of Research and the Faculty Senate Committee on Research.)*\n* **Misconduct.**AI tools, in particular AI plagiarism detectors, are already leading to a higher volume of allegations (including spurious allegations), and this creates a huge burden on university resources in adjudicating these allegations. The university’s current misconduct policy and rules for investigating individual allegations will need to be re-evaluated and updated in accordance with the new federal policy.[[4]] *(The Vice Provost and Dean of Research.)*\n* **Review and writing of proposals.**There is a growing prevalence of AI content in research proposals and reviews of research proposals. Grant agencies are developing rules around such uses of AI; the university may similarly consider its own policy or supplement[existing guidance]. Whereas the use of AI in brainstorming, copy-editing or otherwise refining grant proposals may be very useful, the use of AI as a proxy reviewer may violate fundamental academic standards.\n* **Training AI on student work.**There are active and exciting efforts to train AI systems to provide tutoring and evaluation of student work. These systems often require large amounts of student work for training, and the use of student work-product without permission and appropriate approval processes may be inappropriate or even illegal in some cases.*(SDOC, the Student Data Oversight Committee, should provide guidelines on appropriate use of student data for AI research.)*\n* **Oversight on using data for AI research.**AI research often involves the use of large amounts of training data which may have legal and/or ethical concerns. Given that much of health AI research is exempt from IRB review under federal regulations, the university should clarify the process for reviewing and providing support for responsible and compliant use of data.*(The Vice Provost and Dean of Research.)*\n*Other recommendations and considerations:*\n* **Legal issues.**As mentioned above, there may be a tendency to think of AI as an exception to existing laws, rules, and university policies; in the research sphere. Examples of risk areas include copyright and trademark infringement disputes (e.g. when curating and distributing datasets for AI that contains copyrighted work), breach of contractual agreements (e.g. violating our agreement with data owners), potential tort and defamation liabilities (e.g. if the AI tools produce fake, or harmful information or advice), and violation of privacy rights of others and related regulations (e.g. using or sharing PHI or other personal data in violation of existing policies and protocols, or failing to obtain adequate consents to use personal data). Litigation risk will continue to shift to the users of AI (as opposed to the initial focus on developers) as the use of this technology becomes more commonplace. Researchers should be reminded or made aware of risks and obligations in contexts where AI is used for research and updated as the legal risk profile changes.\n**Computing support for campus AI.**The university should consider ways to expand computing resources to enable AI-powered research and experimentation to ensure that Stanford administrators, educators and researchers remain leaders in the productive and human-centered use of these technologies.\n[Report of the AI at Stanford Advisory Committee] \n**AI at Stanford Advisory Committee members:**\n**Russ Altman**, Kenneth Fong Professor and Professor of Bioengineering, of Genetics, of Medicine, of Biomedical Data Science, and Senior Fellow at the Stanford Institute for HAI*(Committee Chair)*\n**Yi-An Chen**, Senior University Counsel\n**Michele Elam**, William Robertson Coe Professor of Humanities and Senior Fellow at the Stanford Institute for HAI\n**Zephyr Frank**, Gildred Professor of Latin American Studies and Professor at the Stanford Doerr School of Sustainability\n**Steve Gallagher**, Chief Information Officer, University IT\n**Stephanie Kalfayan**, Vice Provost for Academic Affairs\n**Serena Rao**, Senior Associate Dean for Finance and Administration, VPDOR\n**Mehran Sahami**, Tencent Chair of the of the Computer Science Department and James and Ellenor Chesebrough Professor\n**Dan Schwartz**, Dean of the Graduate School of Education\n**Zack Al-Witri**, Assistant Vice Provost for Academic Affairs*(Staff to the Committee)*\n[1]Examples of other related guiding principles and resources:\n* [Stanford Research Policy Handbook research principles 1.1] \n* [Whitehouse Executive Order on Safe, Secure and Trustworthy AI] \n* [NIST Risk Management Framework for AI 1.0] \n* [National Academies of Science, Engineering and Medicine publications on AI] \n[2]These policies and expectations, as we recommend below, ought to be clearly communicated to the Stanford community and accessible through an online portal.\n[3]HHS recently issued rules for healthcare providers to follow to make sure decision-making systems (such as AI tools) are non-discriminatory.[eCFR :: 45 CFR 92.210 —Nondiscrimination in the use of patient care decision support tool]. While the rules are intended to apply to patient care settings, it may provide a helpful framework to consider for evaluation of responsible use of AI in decision making generally.\n[4]The Office of Research Integrity under the US Department of Health and Human Services has recently published the[rule on research misconduct].\n## More News Topics\n[From the Provost] \n## More News\n* [\n### Welcome to a fall quarter\n] \nSeptember 22, 2025\nThe president and provost shared their priorities at the start of a new academic year.\n* [From the Provost] \n* [\n### Letter to the Class of 2029\n] \nMay 9, 2025\nIn a letter welcoming incoming students, the president and provost reflected on Stanford&#039;s values and culture of curiosity and respect.\n* [From the Provost] \n* [\n### Welcome to spring quarter\n] \nMarch 31, 2025\nThe president and provost welcome the community to spring quarter.\n* [From the Provost] \nBack to Top",
    "length": 28610,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Generative AI Policy Guidance | Office of Community Standards",
    "url": "https://communitystandards.stanford.edu/generative-ai-policy-guidance",
    "text": "Generative AI Policy Guidance | Office of Community Standards\n[Skip to main content] [Skip to secondary navigation] \n[Stanford University(link is external)] \n[\nStanford\nOffice of Community Standards\n] \nSearch this siteSubmit Search\n![Chatbot Chat with AI. Credit: serba011d64_201 / iStock] \n# Generative AI Policy Guidance\nMain content start\nGuidance adopted on February 16, 2023\n![Decorative Cardinal Red accent line.] \n## Honor Code Implications of Generative AI Tools\nThe Board on Conduct Affairs (BCA) has been asked to address the Honor Code implications of generative AI tools such as ChatGPT, Bard, DALL-E, and Stable Diffusion. These are novel tools, and both students and instructors have been experimenting with their use in academic settings.\nWhile these tools have applications that foster student learning and understanding, these tools can also be used in ways that bypass key learning objectives.\nTo give sufficient space for instructors to explore uses of generative AI tools in their courses, and to set clear guidelines to students about what uses are and are not consistent with the Stanford Honor Code, the BCA has set forth the following policy guidance regarding generative AI in the context of coursework:\n*Absent a clear statement from a course instructor, use of or consultation with generative AI shall be treated analogously to assistance from another person. In particular, using generative AI tools to substantially complete an assignment or exam (e.g. by entering exam or assignment questions) is not permitted. Students should acknowledge the use of generative AI (other than incidental use) and default to disclosing such assistance when in doubt.*\n*Individual course instructors are free to set their own policies regulating the use of generative AI tools in their courses, including allowing or disallowing some or all uses of such tools. Course instructors should set such policies in their course syllabi and clearly communicate such policies to students. Students who are unsure of policies regarding generative AI tools are encouraged to ask their instructors for clarification.*\nThe BCA will continue to monitor developments in these tools and their use in academic settings and may update this guidance. Members of the community are encouraged to contact the BCA to provide input, suggestions, and comments on this policy.\nNOTE: As part of the BCA’s guidance on clear communication of a course’s generative AI policy, OCS recommends course instructors provide clear advance notice that they may use detection software to review work submitted for use of generative AI.Other helpful information for faculty and course assistants can be found[HERE].\n![Decorative Cardinal Red accent line.] \nIf you are in doubt about whether a generative AI source (or any source) is permitted aid in the context of a particular assignment, talk with the instructor.\n![] \nBack to Top",
    "length": 2904,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "PWR Policy on the Use of GenAI and LLMs",
    "url": "https://pwr.stanford.edu/about-pwr/pwr-policies/pwr-policy-use-genai-and-llms",
    "text": "PWR Policy on the Use of GenAI and LLMs | Program in Writing and Rhetoric\n[Skip to main content] [Skip to secondary navigation] \n[\n![Stanford VPUE Logo] \nStanford\nUndergrad\nProgram inWriting and Rhetoric\n] \nSearch this siteSubmit Search\n# PWR Policy on the Use of GenAI and LLMs\n## Main navigation\n[Skip Secondary Navigation] \nMain content start\nThe Program in Writing and Rhetoric’s policy about GenAI use in PWR courses is grounded in our focus on the learning objectives that constitute the foundation of the Writing and Rhetoric requirement, including -\n1. Students in PWR courses learn invention strategies to help them develop drafts in response to essay assignments, with the aim of exploring their ideas and making connections between ideas. They also learn revision strategies focused on further developing and refining their ideas and making decisions about organization and style to engage and guide the reader.\n2. Students in PWR courses complete research assignments in several stages, from crafting and refining initial research questions to engaging with a range of sources to developing an argument to add to the conversation.\nWe developed these guidelines for the use of LLMs in PWR classrooms to create and foster a learning environment focused on the growth of all students as writers, researchers, speakers, and thinkers.\nIn addition, the Program recognizes[the impact of AI use on the environment] (through water and electricity usage) and is committed to supporting a sustainable approach to writing and research practices.\n## PWR Policy on GenAI Use in PWR classes\nStudents may not use generative AI or LLMs to compose the drafts or revisions for any of the major assignments in their PWR classes. This includes using generative AI to compose or revise portions of their essay or scripts (from individual phrases or sentences to longer passages) or including in their essays/scripts paraphrases of LLM-generated writing or paraphrases of source material generated by LLMs. In addition, students may not rely on generative AI summaries of sources for their research and instead must actively read and engage themselves with the sources they draw on for their research projects and that they incorporate into their writing.\nBeyond this program policy, individual lecturers may craft their own supplemental policies specifically addressing the use of genAI for initial scaffolding activities focused on brainstorming and exploration as well as for classroom work and other writing and research activities. Instructors will clearly explain these policies on the syllabus as well as in the assignment sheets; students should ask their instructors if they have questions about any aspect of this program policy or the policy in the instructors’ syllabus.\nViolation of PWR’s AI policy is considered an Honor Code violation and will result in the involvement of Stanford’s Office of Community Standards (OCS).\nIf you have questions or concerns about our policy, please contact PWR Associate Director, Christine Alfano ([alfano@stanford.edu]).\n## GenAI Policy FAQs\nRead our FAQ section for answers to questions about specific types of GenAI usage, such as using Grammarly, using GenAI for research, etc.\n[Read our FAQs] \nBack to Top",
    "length": 3246,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Creating your course policy on AI - Stanford Teaching Commons",
    "url": "https://teachingcommons.stanford.edu/teaching-guides/artificial-intelligence-teaching-guide/creating-your-course-policy-ai",
    "text": "Creating your course policy on AI | Teaching Commons\n[Skip to main content] [Skip to secondary navigation] \n[Stanford University(link is external)] \n[\nStanford\nTeaching Commons\n] \nSearch this siteSubmit Search\n# Creating your course policy on AI\n## Main navigation\n[Skip Secondary Navigation] \nMain content start\nHere we offer example syllabus statements, suggestions for what to include, and sample sentences that you might use as you think through your own course policy on AI and begin writing a statement to put in your syllabus.\n## Analyzing the implications of AI chatbots\nKey points from the previous module\n* Academic integrity, student success, and workload balance represent three important areas of focus regarding the impact of AI chatbots.\n* The Office of Community Standards has stated that absent a clear statement from the instructors, the use of generative AI will be treated the same as assistance from another person.\n* Evaluate your own courses carefully when thinking about if and how you would use AI chatbots and how you would direct students to use them.[Go to the previous module] \n## Outcomes for this module\nHere we aim to guide you through developing a draft course policy on AI use that you can include in your syllabi.\nAfter completing this module, you should be able to:\n* Reflect on the purpose(s) of an AI policy in your course syllabi.\n* Describe some options forAI policy syllabus statements.\n* Create an AI policy for your course that takes into consideration academic integrity, student success, and workload balance.## Thinking about your syllabus\nA syllabus serves many purposes and can have multiple audiences depending on how and when it is used. Let's focus on the syllabus as a way to communicate to students the most important aspects and learning goals of your course. Before you begin to write your course policy on AI, consider the tone and voice of your syllabus, course policies already in place, and so on.\nWe offer the prompt \"What is one complexity about your course that could be clarified or improved for your students?\" that you can respond to in the poll below.\n## Explore example syllabi statements\nConsider these example syllabi statements on AI use:\n* [Classroom Policies for AI Generative Tools], created by Lance Eaton\n* [Sample text for syllabi], from the Sentient Syllabus Project\n* Sample language from the[AI Guidance &amp; FAQs] web page from the Harvard Office of Undergraduate Education (at the end of the page)\n## Reflection prompt\nWhat important conversations do you want to have with your students about AI and learning in your course?\n## What to include in your AI course policy statement\nAn effective syllabus is designed to motivate learning, define learning goals, explain the structure of the course, offer support, and so on—which also applies to any statements about AI. Many of the strategies for writing effective syllabi in general, such as using student-centered language, aiming for transparency and precision, providing examples, and using the syllabus as a starting point for conversations with students, would apply here as well (Gannon, 2023). Remember that some students may not be aware of campus policies or have varying degrees of preparation for navigating such issues, so they would likely benefit from a clear and comprehensive statement about AI use in your course.\nConsider the following when developing your AI course policy:\n* What is the policy and what tools does it apply to specifically?\n* When does it apply? What conditions or contexts allow or preclude the use of AI?\n* What processes and consequences result from non-compliance?\n* What rationale and reasoning guide this policy?\n* How do you provide support to students in relation to this policy?\n* How does the policy show support for student well-being?## Ideas to get started\nBelow we provide options for ideas and wordings that you might use to begin crafting your own course policy. The following worksheet also contains more sample language and a chatbot prompt to help you refine your policy statement.\n[Worksheet for creating your AI course policy] (129.75 KB)\n### Type of policy\n* Yes, always allowed\n* Yes, but...\n* No, but...\n* No, never allowed### Specific tools it applies to\n* AI chatbots (such as ChatGPT, Google Bard, Claude, Bing Chat)\n* AI image generators (such as DALL-E, Midjourney, Stable Diffusion, Adobe Firefly)\n* AI code generators (such as CoPilot, Tabnine, Cody)\n* AI audio or music generators (such as Amper, AIVA, Soundful)### Conditions and contexts\n* Only for specific assignments\n* Only specified AI tools\n* Only when students openly explain how and why they used AI tools\n* Only when students and the teaching team consent to having their data entered into AI tools\n* Only when used for purposes that are not private, sensitive, or high-risk\n* Only with supervision during class, section, or office hours\n* Only after students have gained skills for using chatbots appropriately\n* Only by request and in consultation with the instructor or teaching team\n* Only with your own data. Do not enter private, sensitive, or copyrighted data from others into AI tools without their consent.\n* Only prohibited for graded assignments; for non-graded assignments, students may use chatbots\n* Must cite the AI tools and prompts they use### Rationales\n* Yes, always because...\n* \"The students in this course have strong learning skills and have shown themselves to be responsible, effective, self-directed learners.\"\n* \"I’ve designed robust assessments and learning activities in this course that have value regardless of the use of chatbots.\"\n* \"The use of chatbots aligns with the goals of the course in a way that enhances learning.\"\n* \"I consider learning to use chatbots an important skill in the discipline.\"\n* \"Students are informed about AI, its risks and benefits, and can decide for themselves if and how they would use AI tools.\"\n* Yes, but... because...\n* \"Students need to first develop their skills to demonstrate they can use chatbots effectively and responsibly.\"\n* \"Chatbots would enhance learning in certain specific situations but could be a detriment to learning outside of those situations.\"\n* \"The teaching team only has enough resources to support students working with chatbots in limited ways.\"\n* \"Students first need to understand issues around privacy and data security and consent to using a chatbot.\"\n* No, but... because...\n* \"It could enhance learning for certain students in unique circumstances but otherwise would likely inhibit learning.\"\n* \"It depends on the individual student's goals, situation, skills, and needs that need to be evaluated on a case-by-case basis.\"\n* \"The teaching team only has enough resources to support a limited number of students with the greatest need to use chatbots for learning.\"\n* \"As chatbots rapidly evolve, the teaching team needs more time to adapt the course to properly support students.\"\n* \"Content in this course is private, sensitive, or copyrighted, and should only be entered into a chatbot under certain circumstances.\"\n* No, never because...\n* \"The teaching team considers chatbots incompatible with the course goals and likely a detriment to student learning.\"\n* \"Content in this course is private, sensitive, or copyrighted and should never be entered into a chatbot.\"### Process\n* \"All students should contact the teaching team if they have questions about anything in this policy.\"\n* \"All students must attend at least one office hour session to discuss using AI tools.\"\n* \"Students must consent to and follow class community agreements about responsible use of AI.\"\n* \"Student work that does not include proper citation of chatbot use...\"### Consequences\n* \"If a student is suspected of or reportedfor not following this policy, the teaching team will consult with the Office of Community Standards in accordance with the[student accountability process].\"### Support resources\n* \"My course policy aligns with[guidance from the Office of Community Standards]. Please familiarize yourself with their policy, as it applies to all courses, not just this one.\"\n* \"If you have any questions, please talk to me or someone from the teaching team. The best way to contact us is...\"\n* \"All students and teaching team members should follow the recommendations on privacy, data security, and responsible use of AI tools described on University IT's[Responsible AI at Stanford] webpage.\"\n* \"For the specified assignments where AI tools are allowed, we will go over in class how to access anduse them.\"\n* \"AI use is not required and is entirely optional. Equivalent alternatives are provided for all students whether they choose to use AI tools or not.\"### Inclusive language\n* \"If you as a studentare struggling and feeling too much pressure in this course, please don't resort to chatbots as a shortcut to completing assignments. Many Stanford students feel stressed and pressured. It is completely natural, as this is a challenging course and the university can be a high-pressure environment. Butthere are a lot of support resources available to you, and I believe that you can succeed here.Please contact me anytime and let's talk about it. I am open to extending due dates or adjusting the assignments to fit your situation. I will work with you to support your success in this course!\"\n* \"We recognize that you may have concerns around privacy and security, or have ethical or other reasons why you do not want to use AI tools in this class. This is completely understandable and we respect your choices. I and the teaching team are here to help you succeed in this course. Please email, visit office hours, or speak to me at any time so we can help you. I can accommodate or adapt course assignments for most students' situations. In the instances where I cannot, I can connect you to other campus resources that can help you.\"## Assess and reinforce your learning\nWe offer this activity for you to self-assess and reflect on what you learned in this module.\n**Stanford affiliates**\n* Go to the[Stanford-only version of this activity] \n* Use your Stanford-provided Google account to respond.\n* You have the option of receiving an email summary of your responses\n* After submitting your responses, you will have the option to view the anonymized responses of other Stanford community members by clicking**Show previous responses**.\n**Non-Stanford users**\n* Complete the activity embedded below.\n* You have the option of receiving an email summary of your responses.\n* Your responses will only be seen by the creators of these modules.\nLoading…\n## Learn more\n* [How to Cite ChatGPT], APA Style\n* [CTL Syllabus Template], Stanford Center for Teaching and Learning\n* [Should You Add an AI Policy to Your Syllabus?], Chronicle of Higher Education\n* [The Sentient Syllabus Project], Sentient Syllabus Project## Works cited\n*AI Guidance &amp; FAQs*. (n.d.). Harvard Office of Undergraduate Education. Retrieved July 26, 2023, from[https://oue.fas.harvard.edu/ai-guidance] \n*CTL Syllabus Template | Center for Teaching and Learning*. (n.d.). Retrieved July 28, 2023, from[https://ctl.stanford.edu/syllabus-template] \nEaton, L. (n.d.).*Classroom Policies for AI Generative Tools—Google Docs*. Retrieved July 28, 2023, from[https://docs.google.com/document/d/1RMVwzjc1o0Mi8Blw\\_-JUTcXv02b2WRH86vw7mi16W3U/edit] \nGannon, K. (2023, July 31).*Advice | Should You Add an AI Policy to Your Syllabus?*The Chronicle of Higher Education.[https://www.chronicle.com/article/should-you-add-an-ai-policy-to-your-syllabus] \nMcAdoo, T. (2023, April 7).*How to cite ChatGPT*. Https://Apastyle.Apa.Org.[https://apastyle.apa.org/blog/how-to-cite-chatgpt] \n*Syllabus Resources*. (n.d.). Google Docs. Retrieved July 31, 2023, from[https://docs.google.com/document/d/1O1\\_uUvF8OYbleru5QyjjuNP\\_On7h5vaVQC2GaSQ315U/edit?usp=sharing] \n## Integrating AI chatbots into assignments\nPreview of the next module\nStrategies and perspectives on integrating AI tools into assignments and activities used to assess student learning.\n[Go to the next module] \n![] \n**Learning together with others can deepen the learning experience.**We encourage you to organize your colleagues to complete these modules together or facilitate a workshop using our[Do-it-yourself Workshop Kits] on AI in education.\nThe Center for Teaching and Learning also coordinates the[AI Meets Education at Stanford (AIMES)] initiative. AIMES resources and programs include the[AIMES Library of Examples,] containing exemplary learning objects from Stanford instructors, and the[Critical AI Literacy for Instructors] self-paced Canvas course.\nIf you have any questions, contact us at[TeachingCommons@stanford.edu]. This guideis licensed under[Creative Commons BY-NC-SA 4.0] (attribution, non-commercial, share-alike) and should be attributed to Stanford Teaching Commons.\nBack to Top",
    "length": 12867,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Research Repository | SCALE Initiative",
    "url": "https://scale.stanford.edu/genai/research-repository",
    "text": "Research Repository | SCALE Initiative\n[Skip to main content] \n[![Stanford University]] \n# Research Repository\nA collection of all generative AI research on education (with a focus on K12) including descriptive research (product development and implementation and use), quasi-experimental, randomized controlled trials, and systematic reviews.\nDisplaying 1 - 31 of 31\n* ##### [Tutor CoPilot: A Human-AI Approach for Scaling Real-Time Expertise] \nRose E. Wang, Ana T. Ribeiro, Carly D. Robinson, Susanna Loeb, Dorottya Demszky. (2025).*National Student Support Accelerator*.[https://studentsupportaccelerator.org/studies/tutor-copilot-human-ai-approach-s…] \n* ##### [Automated Feedback Improves Teachers’ Questioning Quality in Brick-and-Mortar Classrooms: Opportunities for Further Enhancement] \nDorottya Demszky, Jing Liu, Heather C. Hill, Shyamoli Sanghi, Ariel Chun. (2024).*EdWorkingPapers.com*.[https://doi.org/10.26300/8pnw-5q67] \n* ##### [Backwards Planning with Generative AI: Case Study Evidence from US K12 Teachers] \nSamantha Keppler, Wichinpong Park Sinchaisri, Clare Snyder. (2024).*SSRN*.[https://dx.doi.org/10.2139/ssrn.4924786] \n* ##### [The Feedback Prize: A Case Study in Assisted Writing Feedback Tools] \nPerpetual Baffour, Scott Crossley, Yu Tian, Alex Franklin, Natalie Rambis, Meg Benner, Ulrich Boser. (2024).*The Learning Agency Lab*.[https://the-learning-agency-lab.com/] \n* ##### [Can Large Language Models Make the Grade? An Empirical Study Evaluating LLMs Ability To Mark Short Answer Questions in K-12 Education] \nOwen Henkel, Libby Hills, Adam Boxer, Bill Roberts, Zach Levonian. (2024).*L@S &#039;24: Proceedings of the Eleventh ACM Conference on Learning @ Scale*.\n* ##### [How do students use ChatGPT as a writing support?] \nSarah W. Beck, Chris Mah, Lena Phalen, Jaylen Pittman. (2024).*International Literacy Association*.[https://doi.org/10.1002/jaal.1373] \n* ##### [Generative AI Can Harm Learning] \nHamsa Bastani, Osbert Bastani, Alp Sungu, Haosen Ge, Özge Kabakcı, Rei Mariman. (2024).*SSRN*.[https://dx.doi.org/10.2139/ssrn.4895486] \n* ##### [Harvard Undergraduate Survey on Generative AI] \nShikoh Hirabayashi, Rishab Jain, Nikola Jurković, Gabriel Wu. (2024).*arXiv*.[https://arxiv.org/pdf/2406.00833] \n* ##### [What Conversational AI Can (and Can’t) Tell Us About Peer Interaction in Higher Education] \nPeters Vanessa, Emily Pressler, Julie Neisler, Barbara Means. (2024).*ISLS Repository*.[https://repository.isls.org/handle/1/11077] \n* ##### [Grade Like a Human: Rethinking Automated Assessment with Large Language Models] \nWenjing Xie, Juxin Niu, Chun Jason Xue, Nan Guan. (2024).*arXiv*.[https://doi.org/10.48550/arXiv.2405.19694] \n* ##### [Improving Teaching at Scale: Can AI Be Incorporated Into Professional Development to Create Interactive, Personalized Learning for Teachers?] \nYasemin Copur-Gencturk, Jingxian Li, Sebnem Atabas. (2024).*American Educational Research Journal*.[https://doi.org/10.3102/00028312241248514] \n* ##### [AI Tutoring Outperforms Active Learning] \nGregory Kestin, Kelly Miller, Anna Klales, Timothy Milbourne, Gregorio Ponti. (2024).*Research Square*.[https://doi.org/10.21203/rs.3.rs-4243877/v1] \n* ##### [Artificial Intelligence and Educational Measurement: Opportunities and Threats] \nAndrew D. Ho. (2024).*DASH Harvard*.[https://nrs.harvard.edu/URN-3:HUL.INSTREPOS:37379195] \n* ##### [ChatGPT-generated help produces learning gains equivalent to human tutor-authored help on mathematics skills] \nZachary A. Pardos, Shreya Bhandari. (2024).*PLOS ONE*.[https://doi.org/10.1371/journal.pone.0304013] \n* ##### [Effective and Scalable Math Support: Experimental Evidence on the Impact of an AI- Math Tutor in Ghana] \nOwen Henkel, Hannah Horne-Robinson, Nessie Kozhakhmetova, Amanda Lee. (2024).*arXiv*.[https://arxiv.org/pdf/2402.09809] \n* ##### [The GPT Surprise: Offering Large Language Model Chat in a Massive Coding Class Reduced Engagement but Increased Adopters Exam Performances] \nAllen Nie, Yash Chandak, Miroslav Suzara, Ali Malik, Juliette Woodrow, Matt Peng, Mehran Sahami, Emma Brunskill, Chris Piech. (2024).*MyIDEAS*.[https://ideas.repec.org/p/osf/osfxxx/qy8zd.html] \n* ##### [Improving Student Learning with Hybrid Human-AI Tutoring: A Three-Study Quasi-Experimental Investigation] \nDanielle R. Thomas, Jionghao Lin, Erin Gatz, Ashish Gurung, Shivang Gupta, Kole Norberg, Stephen E. Fancsali, Vincent Aleven, Lee Branstetter, Emma Brunskill, Kenneth R. Koedinger. (2024).*Association for Computing Machinery*.[https://doi.org/10.1145/3636555.3636896] \n* ##### [Generative artificial intelligence enhances creativity but reduces the diversity of novel content] \nAnil R. Doshi, Oliver P. Hauser. (2024).*arXiv*.[https://arxiv.org/abs/2312.00506] \n* ##### [Improving Assessment of Tutoring Practices using Retrieval-Augmented Generation] \nZifei FeiFei Han, Jionghao Lin, Ashish Gurung, Danielle R. Thomas, Eason Chen, Conrad Borchers, Shivang Gupta, Kenneth R. Koedinger. (2024).*arXiv*.[https://arxiv.org/abs/2402.14594] \n* ##### [Supporting Teachers’ Professional Development With Generative AI: The Effects on Higher Order Thinking and Self-Efficacy] \nJijian Lu, Ruxin Zheng, Zikun Gong, Huifen Xu. (2024).*IEEE Xplore*.[https://ieeexplore.ieee.org/abstract/document/10444988] \n* ##### [Human-AI Collaborative Essay Scoring: A Dual-Process Framework with LLMs] \nChangrong Xiao, Wenxing Ma, Qingping Song, Sean Xin Xu, Kunpeng Zhang, Yufang Wang, Qi Fu. (2024).*arXiv*.[https://doi.org/10.48550/arXiv.2401.06431] \n* ##### [Deciphering Stereotypes in Pre-Trained Language Models] \nWeicheng Ma, Henry Scheible, Brian Wang, Goutham Veeramachaneni, Pratim Chowdhary, Alan Sun, Andrew Koulogeorge, Lili Wang, Diyi Yang, Soroush Vosoughi. (2023).*ACL Anthology*.[https://doi.org/10.18653/v1/2023.emnlp-main.697] \n* ##### [The Impact of Student-AI Collaborative Feedback Generation on Learning Outcomes] \nAnjali Singh, Christopher Brooks, Xu Wang. (2023).*OpenReview*.[https://openreview.net/forum?id=IQZ2dcsVq0] \n* ##### [Let’s Chat: Leveraging Chatbot Outreach for Improved Course Performance] \nKatharine Meyer, Lindsay C. Page, Catherine Mata, Eric Smith, B. Tyler Walsh, C. Lindsey Fifield, Amy Eremionkhale, Michael Evans, Shelby Frost. (2023).*EdWorkingPapers.com*.[https://doi.org/10.26300/es6b-sm82] \n* ##### [Experimental evidence on the productivity effects of generative artificial intelligence] \nShakked Noy, Whitney Zhang. (2023).*Science*.[https://www.science.org/doi/10.1126/science.adh2586] \n* ##### [GPT detectors are biased against non-native English writers] \nWeixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, James Zou. (2023).*ScienceDirect*.[https://doi.org/10.1016/j.patter.2023.100779] \n* ##### [The Impact of an Artificial Intelligence (AI) Project-Based Learning (PBL) Course on Middle-School Students&#039; Interest, Knowledge, and Career Aspiration in the AI Field] \nRoozbeh Aliabadi. (2023).*Robert Morris University*.[https://eric.ed.gov/ED633474] \n* ##### [M-Powering Teachers: Natural Language Processing Powered Feedback Improves 1:1 Instruction and Student Outcomes] \nDorottya Demszky, Jing Liu. (2023).*EdWorkingPapers.com*.[https://edworkingpapers.com/ai23-759] \n* ##### [The effect of generative artificial intelligence (AI)-based tool use on students&#039; computational thinking skills, programming self-efficacy and motivation] \nRamazan Yilmaz, Fatma Gizem, Karaoglan Yilmaz. (2023).*Computers and Education: Artificial Intelligence*.[https://doi.org/10.1016/j.caeai.2023.100147] \n* ##### [Logic Against Bias: Textual Entailment Mitigates Stereotypical Sentence Reasoning] \nHongyin Luo, James Glass. (2023).*arXiv*.[https://arxiv.org/abs/2303.05670] \n* ##### [Proactive student support using artificially intelligent conversational chatbots: The importance of targeting the technology] \nAizat Nurshatayeva, Lindsay C. Page, Carol C. White, Hunter Gehlbach. (2020).*EdWorkingPapers.com*.[https://doi.org/10.26300/mp4q-4x12] \n[![Stanford University]] \n* [Stanford Home] \n* [Maps &amp; Directions] \n* [Search Stanford] \n* [Emergency Info] \n* [Privacy] \n* [Copyright] \n* [Trademarks] \n* [Non-Discrimination] \n* [Accessibility] \n©Stanford University. Stanford, California 94305.",
    "length": 8199,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Responsible AI at Stanford - University IT",
    "url": "https://uit.stanford.edu/security/responsibleai",
    "text": "Responsible AI at Stanford | University IT[Skip to main content] \n[![Stanford] University IT] Main Menu\nTopic Menu\n# Responsible AI at Stanford\nEnabling innovation through AI best practices\n* [Overview] \n* [Safety measures] \n* [Risk factors] \n* [Guidance] \n* [Tools] \n* [Resources and help] \n* [More on GenAI] \nGenerative artificial intelligence (AI) is built using algorithms that can generate text, images, videos, audio, and 3D models in response to prompts. Popular examples of generative AI include ChatGPT and Google Gemini.\nThis type of AI can introduce both positive and negative impacts.\n​With this guide, learn how to more confidently use AI tools and models while keeping Stanford's data safe.\n![] \n## ![] \nSafety measures\nLet's take a look at several key ways we can increase privacy and security when using third-partyAI platforms and tools.\n### Be aware\nA critical step to using AI is to build your awareness of how the platform you are using works.\nImportantly, any data put into third-party AI systems is transmitted and stored on external third-party servers over which Stanford has no direct control. This can introduce risks of compromising these data, or even data loss.\nYou can build awareness of[how large language models work] and[more] from theNew York Times(free for Stanford students, faculty, and staff through[Stanford Libraries]).\n### Be careful\nIn general, err on the side of caution when it comes to what you input with a generative AI platform.\nConsider disabling options related to saved history to prevent the information from being logged or tracked for training model purposes.\nAvoid inputting any sensitive data, such as[Moderate or High Risk Data], whether using a personal or Stanford accountwith a third-party AI platform or tool that is not covered by a Stanford Business Associates Agreement. Review Stanford[approved services] by data risk classification.\nHave more questions?[Start a discussion with ISO].\n### Be transparent\nIf your final product is significantly influenced by an AI platform, consider informing people how you used AI and[cite] appropriately.\nTransparency will help your audiences understand more about the role of AI in your work and build trust in your integrity.\n## ![] \nRisk factors\nLet's now look at the main risk factors when working with generative AI platforms.\n### Compromising sensitive data\nTypically you needto provide some information, even if just a prompt, to bring outa result from generative AI.\nYet these inputs often train the model and might be used to form future responses for others.\nWhen creating prompts: Avoid inputting any sensitive data, such as[Moderate Risk or High Risk Data].\\*\n*\\*This includes: Home addresses, passport numbers, personal health information, passwords, financial data, or intellectual property. This also includes: Controlled Unclassified Information (CUI), International Traffic in Arms Regulations (ITAR) data, as well as proprietary source code, and any information that would be protected under non-disclosure agreements (NDA).*\n### Inaccurate results\nInformation provided by AI can be incorrect or can create “hallucinations.”\nA[hallucination] by AI is when false or incorrect information is provided by AI, but in a very convincing manner. AI platforms typically will not introduce any skepticism about the false information, nor will theyfact-check or provide citations.\nAdditionally, if prompted forcitations and references, AI platforms have been known to generate inaccurate(but convincing) source information.\nIn short: Wecan't trust results from generative AI completely.\n*This can also apply to using AI to generate code, whichcould be badly constructed, insecure, create backdoors, and even risk intellectual property infringement.*\n## ![] \nExplore best practices\nDiscover security and privacy considerations for working with generative AI today.\nThe information provided for these context areas is not considered complete or exhaustive.\n|Area|Best Practices|\nData Privacy &amp; Usage|Avoid inputting data into generative AI about others that you wouldn’t want them to input about you.|\nData Privacy &amp; Usage|Avoid inputting any sensitive data, such as[Moderate or High Risk Data], whether using a personal or Stanford account with a third-party AI platform or tool that is not covered by a Stanford Business Associates Agreement. Review Stanford[approved services] by data risk classification.|\nData Privacy &amp; Usage|If inputting[Low Risk Data], think about whether you want it to be public.|\nData Privacy &amp; Usage|It's recommended to opt out of sharing data for AI iterative learning wherever possible.|\nData Privacy &amp; Usage|If generative AI is to be used to interact with users, obtain their informed consent. Users must be informed about how their data is being used and have the option to opt-out or delete their data.|\nEmerging Technology|To keep meetings secure and private,[a] [void potentially risky third-party bots] and integrations. (Third-party tools may have the ability to scrape your calendar for information, unknowingly transcribe or record meetings, save meetings in unknown places, and join meetings even when you’re not present.)|\nRecommended Best Practices|For content creation: If use of generative AI is permitted at all, one should always transparently cite its use.|\nRecommended Best Practices|Always refer to the specific policies and statements of discipline-relevant journals, publishers, and professional groups.|\nPromoting Discourse|Discuss opportunities for AI to contribute positively to your goals.|\nPromoting Discourse|Have conversations around ethical issues and limitations related to AI use and development.|\n## ![] \nTools being explored\nView a list of generative AI tools being evaluated by University IT (UIT) for potential implementation in various contexts, according to the needs of the Stanford community.\n[GenAI Tool Evaluation Matrix] \n## ![] \nResources and help\nWhen reviewing the resources and policies shown on this page, keep in mind that the legality and ethics of how AI is developed and used is still evolving.\nFor example, the growth of the AI industry has sparked an increase in \"data scraping\" (or copying and using) huge amounts of information on the internet to train new AI models. While this practice is common, its legality and potential outcomes are not clearly decided at this time.\nOther concerns related to generative AI are similarly under debate and scrutiny.\n### **\nMore resources and communities at Stanford\nStanford University community members are part of the ground-breaking research and work around generative AI.\nYou can explore more perspectives and explorative efforts with these sites:\n* [Artificial Intelligence Teaching Guide | Stanford Teaching Commons] \n* [Generative AI: Perspectives from Stanford HAI] \n* [The Stanford Artificial Intelligence Laboratory (SAIL)] \n* [Stanford Trustworthy AI] \nAnd you can build knowledge about AI with this useful newsletter series from the New York Times(available free for Stanford students, staff, and faculty through[Stanford Libraries]):\n* [On Tech: A.I. Newsletter Series (New York Times)] \n### **\nHelp\nFor privacy questions:\n* [​Submit a help request for the University Privacy Office(UPO)].\nFor IT security questions:\n* [Submit a help request for the Information Security Office(ISO)].\nConsidering a generative AIthird-party tool?\n* [Start a discussion with ISO].\nTo report an incident (such as a data breach or system compromise):\n[Report an incident »] \n### **\nRelevant policies at Stanford\nRemember to review and consider how Stanford's security and privacy policiesapply to usingAI:\n* [Generative AI Policy Guidance at Stanford] \n* [Minimum Security Standards at Stanford] \n* [Minimum Privacy Standards at Stanford] \n* [Health Insurance Portability and Accountability Act (HIPAA)] \n* [European Union’s General Data Protection Regulation (GDPR)] \n* [Federal Family Educational Rights and Privacy Act (FERPA)] \n* [Research Policy Handbook and HRPP] \nConsider other policies that might apply to your work and scholarship, as well.\n### **\nEducation and awareness\nGrow your awareness,build an education plan, and keep track of AI-related developments worldwidewith these resources:\n* [UIT Tech Training] \n* [AI Programs Comparison Chart by Stanford Online] \n* [LinkedIn Learning: Artificial Intelligence] \n* Coursera ([log in with your Stanford ID to audit])\n* [AI for Everyone] \n* [Prompt Engineering for ChatGPT] \n* [AI Watch: Global Regulatory Tracker] \nDo you have feedback about this page?\n[Share Feedback] \n![] \n[![Stanford University]] \n* [Stanford Home] \n* [Maps & Directions] \n* [Search Stanford] \n* [Emergency Info] \n* [Terms of Use] \n* [Privacy] \n* [Copyright] \n* [Trademarks] \n* [Non-Discrimination] \n* [Accessibility] \n©CopyrightStanford University.Stanford,California94305.",
    "length": 8847,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Academic Integrity Working Group addresses generative AI and exam policies",
    "url": "https://studentaffairs.stanford.edu/news/academic-integrity-working-group-addresses-generative-ai-and-exam-policies",
    "text": "Academic Integrity Working Group addresses generative AI and exam policies | Student Affairs\n[Skip to main content] [Skip to secondary navigation] \n[Stanford University(link is external)] \n[\nStanford\nStudent Affairs\n] \nSearch this siteSubmit Search\nMain content start\n[Academic Integrity Working Group] \n# Academic Integrity Working Group addresses generative AI and exam policies\nAs it enters its third year of a proctoring pilot, the university’s Academic Integrity Working Group offers guidance and resources for faculty.\nDecember 11, 2025\n|\nDeanna Grasser\n![The landmark 1941 Hoover Tower dominates the campus skyline of Stanford University.] \nThe campus skyline of Stanford University.\n**As it enters its third year of a proctoring pilot, the university’s Academic Integrity Working Group offers guidance and resources**for faculty.\n![Decorative Cardinal Red accent line. ] \nAs the university’s[Academic Integrity Working Group] proctoring pilot enters its third year, the group has partnered with faculty and departments across campus to update guidance for instructors on generative AI use and test administration.\n“Our work with campus partners is helping instructors preserve the importance of independent student work while also making sure our students are prepared for a future in which AI tools will be integral to many professions,” said AIWG Faculty Co-Chair Jennifer Schwartz Poehlmann.\nThe Academic Integrity Working Group (AIWG) was[formed in winter 2024] to study Stanford’s academic landscape and identify the scope of academic dishonesty, including its root causes and its relationship to teaching practices. A multi-year pilot study of equitable proctoring practices launched in spring 2024 with seven courses. More than 50 courses across multiple schools are participating in the pilot this quarter.\n“The AIWG’s work reinforces the university’s commitment to advancing academic integrity initiatives and leverages many campus partners,” Provost Jenny Martinez said. “As strategic campus partners in academic integrity, departments can lead these conversations, developing approaches that reflect the unique contexts of their disciplines in a changing educational landscape.”\nAs professors across the university prepare for midterms and exams this fall, the AIWG, with support from the provost’s office, has several recommendations to share with faculty. The group is also calling for departments to consider their role in initiating critical dialogue and discipline-specific policy around generative AI.\n## Proctoring and in-person assessments\nFor courses not participating in the proctoring pilot –which is currently the only place proctoring is permitted –the[AIWG website] outlines[best practices] for faculty conducting their own in-person exams, ranging from seating strategies to recommendations around cell phone use.\n“The proctoring pilot remains an important tool for creating consistent and equitable testing conditions,” Poehlmann said. “However, we want instructors to also feel empowered to create their own strategies for managing in-person exams and know that there are tools available to help.”\nFor instructors who wish to limit students’ use of AI in specific circumstances, including for high-stakes assessments, the group recommends[in-person formats] such as oral exams and in-class writing assignments. In their study of generative AI in higher education, the group identified several issues with tools designed to detect AI use in student work, including bias and occurrences of false positives and negatives. They also found that these tools often fail to reliably assess the extent of AI involvement in texts that mix AI and human writing, making them unsuitable for high-stakes situations, especially as evidence in academic misconduct cases.\n## **Departmental policies and preparing students for an AI-enabled future**\nThe AIWG emphasizes that the most effective approaches to student AI use go beyond individual course and exam management.\n“Departments and multi-section courses are encouraged to establish consistent and enforceable AI policies that can be applied fairly across sections. These policies should be reasonable, communicated clearly, and discussed with students regularly throughout the academic year,” Poehlmann said.\nThe[AIMES website], launched by the[Center for Teaching and Learning (CTL)] and VPUE, provides guidance and examples of how AI can be integrated into coursework to support student learning and career preparation. In addition, the CTL, a key partner in AIWG’s work and a resource for all faculty, encourages departments to[reach out] for individual consultations and department-level workshops.\nBringing students into the conversation about academic integrity and why policies matter can help strengthen understanding and buy-in. The[AIMES website] also includes a guide specifically for students,[AI and Your Learning: A Guide for Students], with practical advice on using AI responsibly.\n“We recognize that restrictions on AI use are essential to achieving certain learning goals, and at the same time, faculty are exploring constructive ways of weaving AI into their teaching and student learning,” said Cassandra Volpe Horri, associate vice provost for education and director of the Center for Teaching and Learning.\n## For more information\nThe AIWG continues to collect[faculty interest for the continuing proctoring pilot] in the 2025-2026 academic year. Submissions will be reviewed on a rolling basis before each quarter.\nPer the Stanford Honor Code, proctoring is currently not required and is only permitted in courses participating in the[AIWG proctoring pilot].\n[Access the original article at the Stanford Report] \n![Cover slide of the AIWG - Fall Recommendations PowerPoint presentation. ] \n## AIWG - Fall Recommendations, 10.20.25\nHere, you can review the recent Chairs &amp;&amp; Deans presentation associated with this article.\n[Access presentation] \n## Quick links\n* [Academic Integrity Working Group] \n* [AY 25-26 Stanford Proctoring Pilot: Faculty Interest Form] \n* [AIWG Best Practices for In-Person Exams] \n* [AIMES: AI Meets Education at Stanford] \n* [August 2025 policy brief on AI in Education] \n## More News Topics\n[Academic Integrity Working Group] \n## More News\n* [\n![] \n### The Flourish | Making Yourself the Priority\n] \nJanuary 28, 2026\nAs winter deepens, it’s the perfect time to prioritize your well-being and combat those winter blues.\n* [What&#039;s Flourishing] \n* [\n![] \n### The Flourish, February 2026\n] \nJanuary 28, 2026\nThe Flourish is here to help you enjoy a winter of wellness, allowing you to prioritize your well-being even when the world and campus get stressful.\n* [The Flourish] \n* [\n![] \n### The Flourish | Supporting You Through Every Season\n] \nJanuary 14, 2026\nAs we transition into a new year, it’s the perfect time to reflect and reset.\n* [What&#039;s Flourishing] \nBack to Top",
    "length": 6912,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Academic Integrity Working Group addresses generative AI and ...",
    "url": "https://news.stanford.edu/stories/2025/10/academic-integrity-working-group-generative-ai-exam-policies",
    "text": "[Skip to content] \n\n## Stanford Report cookie usage information\n\nWe want to provide stories, announcements, events, leadership messages and resources that are relevant to you. Your selection is stored in a browser cookie which you can remove at any time by visiting the \"Show me...\" menu at the top right of the page. For more, read our [cookie policy].\n\nAcceptDecline\n\nOctober 31st, 2025\\| 1 min read [Institutional News] \n\n# Academic Integrity Working Group addresses generative AI and exam policies\n\nAs it enters its third year of a proctoring pilot, the university’s Academic Integrity Working Group offers guidance and resources for faculty.\n\nAs the university’s [Academic Integrity Working Group] proctoring pilot enters its third year, the group has partnered with faculty and departments across campus to update guidance for instructors on generative AI use and test administration.\n\n“Our work with campus partners is helping instructors preserve the importance of independent student work while also making sure our students are prepared for a future in which AI tools will be integral to many professions,” said AIWG Faculty Co-Chair Jennifer Schwartz Poehlmann.\n\n### Quick links\n\n- [Academic Integrity Working Group] \n- [AY 25-26 Stanford Proctoring Pilot: Faculty Interest Form] \n- [AIWG Best Practices for In-Person Exams] \n- [AIMES: AI Meets Education at Stanford] \n- [August 2025 policy brief on AI in Education] \n\nThe Academic Integrity Working Group (AIWG) was [formed in winter 2024] to study Stanford’s academic landscape and identify the scope of academic dishonesty, including its root causes and its relationship to teaching practices. A multi-year pilot study of equitable proctoring practices launched in spring 2024 with seven courses. More than 50 courses across multiple schools are participating in the pilot this quarter.\n\n“The AIWG’s work reinforces the university’s commitment to advancing academic integrity initiatives and leverages many campus partners,” Provost Jenny Martinez said. “As strategic campus partners in academic integrity, departments can lead these conversations, developing approaches that reflect the unique contexts of their disciplines in a changing educational landscape.”\n\nAs professors across the university prepare for midterms and exams this fall, the AIWG, with support from the provost’s office, has several recommendations to share with faculty. The group is also calling for departments to consider their role in initiating critical dialogue and discipline-specific policy around generative AI.\n\n## Proctoring and in-person assessments\n\nFor courses not participating in the proctoring pilot – which is currently the only place proctoring is permitted – the [AIWG website] outlines [best practices] for faculty conducting their own in-person exams, ranging from seating strategies to recommendations around cell phone use.\n\n“The proctoring pilot remains an important tool for creating consistent and equitable testing conditions,” Poehlmann said. “However, we want instructors to also feel empowered to create their own strategies for managing in-person exams and know that there are tools available to help.”\n\nFor instructors who wish to limit students’ use of AI in specific circumstances, including for high-stakes assessments, the group recommends [in-person formats] such as oral exams and in-class writing assignments. In their study of generative AI in higher education, the group identified several issues with tools designed to detect AI use in student work, including bias and occurrences of false positives and negatives. They also found that these tools often fail to reliably assess the extent of AI involvement in texts that mix AI and human writing, making them unsuitable for high-stakes situations, especially as evidence in academic misconduct cases.\n\n## **Departmental policies and preparing students for an AI-enabled future**\n\nThe AIWG emphasizes that the most effective approaches to student AI use go beyond individual course and exam management.\n\n“Departments and multi-section courses are encouraged to establish consistent and enforceable AI policies that can be applied fairly across sections. These policies should be reasonable, communicated clearly, and discussed with students regularly throughout the academic year,” Poehlmann said.\n\nThe [AIMES website], launched by the [Center for Teaching and Learning (CTL)] and VPUE, provides guidance and examples of how AI can be integrated into coursework to support student learning and career preparation. In addition, the CTL, a key partner in AIWG’s work and a resource for all faculty, encourages departments to [reach out] for individual consultations and department-level workshops.\n\nBringing students into the conversation about academic integrity and why policies matter can help strengthen understanding and buy-in. The [AIMES website] also includes a guide specifically for students, [AI and Your Learning: A Guide for Students], with practical advice on using AI responsibly.\n\n“We recognize that restrictions on AI use are essential to achieving certain learning goals, and at the same time, faculty are exploring constructive ways of weaving AI into their teaching and student learning,” said Cassandra Volpe Horri, associate vice provost for education and director of the Center for Teaching and Learning.\n\n## For more information\n\nThe AIWG continues to collect [faculty interest for the continuing proctoring pilot] in the 2025-2026 academic year. Submissions will be reviewed on a rolling basis before each quarter.\n\nPer the Stanford Honor Code, proctoring is currently not required and is only permitted in courses participating in the [AIWG proctoring pilot].\n\n### Writer\n\nDeanna Graesser\n\n### Related topics\n\n[Institutional News] \n\n[Academics] \n\n[Artificial Intelligence] \n\n### Share this story\n\nCopy link\n\n## Read next\n\n[View allRead next] \n\n## [Stanford University reports return on investment portfolio, value of endowment] \n\n[Institutional News] \n\nNews\n\n## [Stanford Institute for Theoretical Physics named for Leinweber Foundation gift] \n\n[Institutional News] \n\nNews\n\n## [Lessons learned by Financial Management Services] \n\n[Institutional News] \n\nNews\n\n## [How Stanford is simplifying decisions and processes] \n\n[Institutional News] \n\nNews\n\n## [Stanford streamlines the process of faculty appointments and promotions] \n\n[Institutional News] \n\nNews\n\n## [Three simplification projects target research support] \n\n[Institutional News] \n\nNews\n\nPreviousNext",
    "length": 6509,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "PWR GenAI Policy FAQs",
    "url": "https://pwr.stanford.edu/about-pwr/pwr-policies/pwr-policy-use-genai-and-llms/pwr-genai-policy-faqs",
    "text": "# PWR GenAI Policy FAQs\n\nMain content start\n\n## Does using platforms like Grammarly violate the PWR AI policy?\n\nUse of any platform that suggests substitutions or changes specifically related to tone, structure, content, concision, or addressing a specific audience is prohibited by the PWR policy.  Specifically in relation to Grammarly, while students can safely use some of its basic autocorrect features (see FAQ #2), using the enhanced features of Grammarly Pro (which the website advertises as an “AI partner”) or features that suggest substitutions related to word choice or organization violates policy in that the software program actively rewrites/revises student prose, changing the language in relation to both style and substance.\n\n## Does autocorrect on Google Docs or Microsoft Word count as AI?\n\nWhile Google Docs, Word, and other writing platforms are beginning to incorporate AI, at this point simple autocorrect (when the platform suggests a correct spelling or punctuation as you write – not when you input text for “correction” and revision) is acceptable. Grammarly’s basic (free) autocorrect functions, through which it highlights/identifies typos, punctuation errors, and grammar issues, are acceptable to use for the same reason.  Again, consider the difference between a program that corrects misspellings and relatively minor surface matters and programs that revise the writing in relation to content, context, organization, and style.\n\n## Can students use AI to help them understand an assignment in PWR?\n\nPWR assignment sheets are the intellectual property of the lecturer and Stanford University, and it is against PWR policy to upload PWR assignment sheets or course materials (including sample essays)  unless the PWR lecturer has given explicit approval to do so. That said, you can ask AI questions about guidelines for writing a particular type of assignment, as long as you do all the writing yourself in completing the assignment.\n\n## Can students use AI to help them with the peer review or peer feedback process?\n\nStudents should not upload other students’ work (considered their intellectual property) – whether that be their classmates’ work contributed for peer feedback sessions or sample student essays – into an LLM or genAI to prompt the tool to generate suggestions for revision.  This includes any examples of student writing from previous students provided by the instructor. Using AI to craft, expand, or polish peer review end comments based on the peer reviewers’ notes or input is not explicitly against policy, although individual instructors may decide to prohibit this use case in their class. Check with your instructor about what is permissible for your PWR section.\n\n## Can students use AI to find sources for their PWR projects?\n\nAccording to PWR policies, AI can be used to find sources as part of conducting research, as long as students read the sources themselves and do not rely on AI summary or analysis when engaging with the text to incorporate into their research-based writing. It is strongly recommended that, even if the PWR instructor allows AI use for researching, students also incorporate searches using Stanford’s Searchworks and Articles+ resources as well as Google Scholar. This policy may be modified by individual PWR instructors to be more restrictive, depending on their course design and individual course policies.\n\n## Can students use AI to help them format citations in a particular academic style?\n\nIt does not violate general PWR policy for students to ask genAI to help them format citations in a particular style, such as MLA, APA, etc. This functionality operates in a similar way to citation management platforms like RefWorks and Zotero, tools specifically designed to produce correct citations, the use of which is in fact encouraged by many PWR instructors. Note that AI tools cannot be counted on to generate correct citation formats (in part due to universal updates to citation style), so students always should proofread AI-generated citations and are ultimately responsible for their accuracy.  Also note that some lecturers may impose more stringent AI guidelines that would prohibit using AI in this way; when in doubt, students should double-check with their instructor.\n\n## How should students navigate built-in AI aids they encounter while researching, such as JSTOR summaries of articles or AI overviews on Google?\n\nAI-generated summaries and overviews that increasingly appear automatically in search engines as embedded features can provide helpful summaries while you’re trying to quickly assess whether an article is appropriate for your research project. However, these summaries are not public domain (and not always reliable) and should not be inserted into your writing and should not substitute for engagement with the sources you plan to use in your research-based writing. In addition, students should not prompt genAI to provide summaries for readings; rather they should read the complete article/text themselves. Again, although the summaries generated by AI might serve as a starting point, they should lead you to engage with the sources and write about them yourself as part of the research process. Note that this policy may be modified by individual PWR instructors to be more restrictive, depending on their course design and individual course policies.\n\n## Can students use AI to produce annotations for an annotated bibliography?\n\nPWR policy prohibits students from incorporating writing produced by genAI for their assignments, which includes annotations and summaries for an annotated bibliography. Even if a research platform, such as JSTOR, has provided an AI summary of an article or other text, students are expected to write the annotations for the sources in their bibliography themselves, in their own words, informed by their engagement with the text in question.\n\n## Can students use AI to create slides or slide decks for presentations?\n\nIn terms of the general PWR policy, students may not use AI or advanced AI-enhanced templates to create slides based on prompts that supply the AI with raw material to turn into slides or ask AI to generate material for the slide. Using non-AI generated design templates (where the student manually inserts their own content and writing) from sites like Canva and Google Slides is discouraged but not prohibited by PWR’s genAI policy. Note that this policy related to slides may be modified by individual PWR instructors to be more restrictive, depending on their course design and individual course policies.\n\n## Can students use AI to create infographics or imagery for use in presentations or essays?\n\nStudents may use AI to generate infographics and other imagery for presentations as long as they (students) provide the data and use AI simply to design the visual element. Data contained in the infographic must be appropriately cited in the presentation or essay and AI usage in this regard (since the AI is the “author” of the infographic) should be cited as well. An exception would be a class where the infographic or image itself constitutes one of the major assignments for the course; in this case, AI cannot be used to generate the infographic or imagery. As stated in reference to the above questions, PWR instructors may implement more restrictive policies related to genAI use for infographics and visual texts in alignment with their individual class policies.\n\n## Does the PWR AI policy apply to advanced PWR courses or just PWR 1 and PWR 2?\n\nThe PWR AI policy applies to all courses taught through the Program in Writing and Rhetoric, including advanced PWR courses.\n\nBack to Top",
    "length": 7679,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "3.32: Generative Artificial Intelligence (AI) Policy | MD Program",
    "url": "https://med.stanford.edu/md/mdhandbook/section-3-md-requirements-procedures/3-32--generative-artificial-intelligence--ai--policy.html",
    "text": "3.32: Generative Artificial Intelligence (AI) Policy | MD Program | Stanford Medicine\n[Skip to Content] \n[Stanford Medicine] [MD Program] Site Nav\nMenu\nMD Program\nStudent Handbook\nClose menu\n## Generative Artificial Intelligence (AI) Policy\nArtificial Intelligence (AI) refers to a broad range of technologies that enable computers and machines to simulate human learning through advanced techniques and processes that perform complex tasks, such as large language models (LLMs), machine learning (ML), and natural language processing (NLP). AI encompasses a range of technologies that enable machines to interpret data, recognize patterns, make decisions, and solve problems. In the context of medicine, this may include a range of technologies and applications that enhance clinical decision-making, improve patient care, and streamline healthcare operations.\nThis policy aims to create a framework for the responsible integration of generative AI in the MD and MSPA Programs, ensuring that students develop the necessary skills and knowledge to utilize these tools effectively while maintaining the highest standards of academic integrity and patient care.\nThe MD and MSPA Programs emphasize the importance of developing clinical reasoning and skills. While artificial intelligence (AI) can serve as a valuable resource for learning and exploration, it is crucial that students engage actively with the material, applying their knowledge and critical thinking to clinical scenarios. AI can be a collaborative tool that enhances understanding and skills, but not a substitute for the rigorous training and cognitive skills necessary for effective medical practice.\n**The intent is to support the judicious use of AI as educational tools while safeguarding academic integrity, patient confidentiality, and the development of critical reasoning and clinical skills.****AI should enhance, not replace, the foundational knowledge and clinical competencies essential for medical practice.**\nThe guidance and policy will be reviewed and revised as necessary to reflect changes in technology, educational practices, and institutional standards.\nPermitted Uses and Privacy\n* Learning and Clarification: Students may utilize AI to enhance their understanding of medical concepts, definitions, and for grammar/style editing, provided it does not conflict with specific assignment instructions or be inconsistent with faculty-authorized tasks as detailed below.\n* Faculty-Authorized Tasks: Any use of AI must align with explicit faculty guidance as outlined in course syllabi. Faculty may designate appropriate contexts for AI use in assignments and projects.\n* Platforms: Stanford has several tools that have been specifically designed for use within the Stanford Medicine community that are compliant with institutional policies and suitable for handling sensitive information (e.g., patient health information). When permitted for use, please use these tools to maintain confidentiality and compliance with Stanford Medicine policies.\n* Stanford Healthcare Secure GPT:[https://securegpt.stanfordhealthcare.org/] \n* Stanford AI Playground:[https://] [aiplayground] [.stanford.edu] \n* Responsible AI at Stanford:[https://uit.stanford.edu/security/responsibleai] \n* Stanford Security Risk Classifications:[https://uit.stanford.edu/guide/riskclassifications] \n* To ensure that patient confidentiality is maintained at all times, no confidential research, patient data, or other protected health information (PHI) is to be entered into public AI platforms (e.g., ChatGPT, Gemini, OpenEvidence, Doximity, etc.).\n* Note that anonymized, general inputs are still appropriate (e.g., “Middle-aged woman with asthma exacerbation and aspirin allergy complaining of worsening symptoms…”) but directly copying in a patient progress note into any such public system would be a clear privacy violation.\nResponsible Use of AI Tools\nWhile we recognize that restricting the use of AI tools cannot be strictly enforced, we expect students to optimize their learning progress, which is likely best achieved through a balanced approach that includes both AI use and traditional methods, and students will be held accountable for demonstrating competency in clinical skills and reasoning without the aid of AI tools.\n* Assessments: The use of AI tools is discouraged for any activities in which students are evaluated on their own knowledge or skills, unless explicitly granted permission by the faculty. Unless explicitly advised by individual instructors, the use of AI tools is prohibited for any “closed book” exams or assignments where internet use is restricted.\n* Clinical Documentation: Use of AI for clinical documentation must adhere to the policies and guidelines for the clinical site and comply with HIPAA regulations. Unless supported by the Electronic Health Record, students should refrain from using AI for clinical documentation, including writing the History and Physical (H&amp;Ps) unless permitted by the clerkship.\n* Patient Confidential Data: The use of patient-identifying information or protected health information (PHI) in public AI tools is strictly forbidden.\n* Note that individual policies around AI use may vary by course and clerkships and this information will be outlined in the course syllabus. Students are responsible for verifying with faculty whether the use of generative AI tools is permissible for specific assignments or tasks.\nAcknowledgement and Citation\n* Any substantial contributions from AI tools on assignments, presentations, and scholarly abstracts or proposals must be disclosed and properly cited. This includes the tool name, model/version, date, query, and output excerpt. For example: ChatGPT (GPT-4; June 10, 2025), prompt: “Explain ARDS pathophysiology,” output used in Section 2.\n* AI tools may not be listed as authors on scholarly works.\nAccuracy &amp; Accountability\n* Students are responsible for any AI-generated content they submit, even if flawed or biased. Verification of information and judgment of quality is expected.\nEthics &amp; Education\n* Students should develop AI literacy, understanding the limitations, potential for inaccuracies (hallucinations), biases, and privacy risks associated with AI.\n* AI education will be integrated into the curriculum to provide students with additional guidance and training on the proper and effective use of AI.\nAll MD and MSPA students are expected to comply with this policy. Violations may result in disciplinary action in accordance with the School of Medicine’s policies on academic integrity and professional conduct.\nFor more information refer to Stanford University's[Generative AI Policy Guidance].\n*updated August 2025*\n[**]",
    "length": 6719,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "AI guidelines for marketing and communications",
    "url": "https://ucomm.stanford.edu/policies-and-guidance/ai-guidelines-marketing-and-communications",
    "text": "AI guidelines for marketing and communications | University Communications\n[Skip to main content] [Skip to secondary navigation] \n[Stanford University(link is external)] \n[\nStanford\nUniversity Communications\n] \nSearch this siteSubmit Search\n# AI guidelines for marketing and communications\n## Main navigation\n[Skip Secondary Navigation] \nMain content start\n## Introduction, Scope of Guidelines, and Relevant Definitions\nThese guidelines recognize the transformative potential of artificial intelligence (“AI”) technologies to enhance operational efficiency, creativity, innovation, and decision-making in the marketing and communications functions at Stanford. At the same time, these tools necessitate careful usage to ensure responsible, legal, and ethical implementation. These guidelines outline the acceptable uses and expectations for the use of AI, supporting fully the use of these technologies, while ensuring such activities are done in the best traditions of the university.\nAs such, these guidelines build on Stanford’s philosophy that the use of AI is centered on improving the human condition and that AI should be used to augment, not replace, human work. In this context, the university expects that content generation using AI will be done ethically and with human oversight.\nThese guidelines apply to all regular staff, interns, casual employees, and consultants. Within the context of this document, “AI” and “generative AI” mean software and/or machines that can create new content, such as text, images, video, and other rich media. (Examples include, but are not limited to, ChatGPT, DALL-E, Gemini, and Claude.) Additionally, any software and/or machines that leverage existing data and use that knowledge to generate new, original content should also be considered within this scope.\nSpecific guidance for a select number of common use cases also is outlined below. The absence of a use case should not, however, be read as prohibition against that case. The bias of these guidelines is toward thoughtful experimentation and continuous growth.\nProviding guidance on how to use AI tools is not within the scope of these guidelines. Employees interested in a primer in this area should consult the[Stanford AI Playground].\nFinally, these guidelines are intended to complement existing university policy. If there is a conflict between guidance in this document and a policy in the Administrative Guide, then the Administrative Guide controls.\n## Responsible and Ethical Usage\nConsistent with the[Report of the AI at Stanford Advisory Committee] (which is incorporated herein by reference and paraphrased below for ease of reference), there are seven key guiding principles that employees in the marketing and communications functions should strive for in their use of AI tools:\n* **Human oversight:**Humans must take responsibility for the AI tools and systems they use at Stanford. You are responsible for oversight of any content you produce using AI to ensure that content is accurate, in alignment with institutional values, and in compliance with the policies set forth in this document.\n* You are personally responsible for conforming to this requirement and this responsibility may not be delegated to another employee.\n* **Alignment with university values:**AI systems should be built and/or procured to support Stanford’s mission and values.\n* **Human professionalism:**All employees in communications and marketing should adhere to professional standards and high levels of quality in their work. Moreover, they should exercise their best judgment and critical thinking in the use of these tools.\n* **Ethical and safe deployment:**These tools should improve university functions. Employees should understand the full implications of an AI system prior to deploying it in their functional area. If the system is not fully understood, an evaluation of the system should be undertaken prior to its implementation.\n* **Privacy, security, and confidentiality:**When using AI tools that involve personal data, the legality and impact of the application should be carefully considered prior to implementation. Some uses of data —such as those involving medical records, privileged information, or student or employee records —require express consent.\n* Please review the[university risk classifications] for assistance in determining what kind of data you may be working with.\n* As of this writing, the[Stanford AI Playground] is not approved for high-risk data.\n* You may not provide any confidential or legally privileged information of Stanford or a third party to generative AI tools.\n* **Data quality and control:**All data used to create new AI applications with university resources should be collected in legal and ethical ways and you should document the provenance of any data you use with these tools.\n* **An AI “golden rule”:**As stated in the committee report, you should “use or share AI outputs as you would have others use or share output with you.”\nReaders of these guidelines are strongly encouraged to familiarize themselves with the full content of the committee report.\n## Compliance and Intellectual Property Considerations\nThe following guidelines apply to all uses of AI technology without limitation:\n* Irrespective of the application, you must adhere to the[University Code of Conduct],[Information Security], and[Privacy Policies] when using AI technologies.\n* As in other contexts, you may not use AI tools to promote for-profit organizations, engage in commercial activities, or provide explicit or implicit commercial endorsements.\n* Please carefully review the terms of use (“TOU”) for any model you use. Many common TOUs include language that grants the provider the right to use your name and Stanford’s name in their marketing activities, which is not permitted under Administrative Guide 1.5.4.\n* Similarly, you may not use an AI to advocate on behalf of Stanford for any political position or political party, consistent with[Administrative Guide 1.5.1].\n* Do not use[high-risk data] in your prompts or include such data as attachments to your prompts.\n* Examples of such information include, but are not limited to, protected health information (PHI), student records, donor information, employee information, home addresses, and social security numbers. You should familiarize yourself with these risk classifications before providing data to an AI tool.\n* Additionally, exercise good judgment and extreme caution when providing[even moderate-risk] or[low-risk data] to a model, whether as prompt, attachment, through an API, or otherwise.\n* Ensure that your usage of AI complies with intellectual property rights and laws such as copyright, trademark, and trade secrecy protections. Use of third-party copyrighted or trademarked material or use of a person’s likeness without permission in interacting with an AI model may be illegal and could expose Stanford to significant financial liability.\n* Similarly, you are responsible for ensuring that any generative model you use has obtained, and provides to the university, a license for any outputs from that model.## Continuous Evaluation\nGiven the rapidly changing nature of this field, you are encouraged to regularly review and assess the models you are using to ensure they are meeting expectations and these guidelines.\nAll employees communicating on behalf of Stanford are encouraged to engage in a regular discussion of their AI practices with colleagues and, where appropriate, their supervisors. In the initial phase of any trials, experimentation, or evaluations, you should consider discussing with your supervisor(s) how you are using AI, how you plan to validate its output, and any additional AI-related activities you may plan to undertake in your work. This allows both parties to be aware of the usage, to discuss it in an open fashion, and ensure alignment on the application.\n## Recommended Platform: Stanford AI Playground\nThe Stanford AI Playground offers an excellent opportunity to trial a variety of models in a safe environment. We strongly recommend exploring your use cases in this environment, to the greatest extent possible, as you begin to leverage these technologies. The AI Playground includes various large language models (“LLMs”), a form of generative AI specializing in text-based content. Users can also access additional LLM plugins for image generation, web scraping, and AI-assisted Google services.\nMoreover, Stanford’s AI Playground supports data privacy by taking advantage of Stanford’s infrastructure and vendor partnerships. Files uploaded to the Playground are not shared externally or used to train models. More information is available in[a January 2025 Stanford Report article] and in the[AI Playground Quick Start Guide].\n## Specific Use Cases in the Communications and Marketing Functions\nFollowing is guidance for a number of common use cases in the marketing and communications functions. (As noted above, the absence of a use case should not be construed as a prohibition on that activity.) All applications, however, should conform to the principles, as well as the legal and policy considerations, outlined above.\n* **Writing and/or research for news articles (including Stanford Report), press releases, institutional statements, and similar material:**When using AI tools for drafting written content, you are responsible for: (i) carefully reviewing and fact-checking the output; (ii) ensuring the finished product complies with these guidelines and other university policy; (iii) ensuring the relevant tool does not borrow text verbatim from other sources; and (iv) disclosing the presence of an AI in the drafting and/or research process to the relevant editor.\n* **Drafting statements:**When using AI tools for brainstorming or outlining statements, you are responsible for: (i) carefully reviewing and fact-checking AI output; (ii) ensuring the finished product complies with these guidelines and other university policy; (iii) disclosing the presence of an AI in the drafting and/or research process to the relevant editor; and (iv) disclosing the presence of an AI in that statement to the Vice President for University Communications.\n* **Image generation and enhancement:**\n* **Enhancement:**Many image and video editing tools now employ AI models to assist with tasks such as color grading and/or noise reduction. These are generally acceptable applications of an AI tool, so long as the usage conforms to the guidelines above and does not substantially alter the authenticity of an image. If scientific data is being presented, any enhancement(s) should be reviewed with the scientists who generated the data to ensure it is acceptable.\n* **Generation:**In the case of image generation, any such activities should be consistent with the intellectual property guidelines above and you should credit/indicate the model that created the image in any photo credit line. As detailed above, if scientific data is being used to generate an image, the output should be carefully reviewed with the scientists who generated the data to be sure that it is acceptable.\n* **Social media:**Some general considerations in using AI tools for the following activities:\n* **Monitoring:**When using AI models, consistent with these guidelines, to monitor and summarize social media sentiment, you are encouraged to validate the output against a random sample of the underlying data.\n* **Writing:**When using an AI as an authoring partner, you are responsible for confirming accuracy of its output and its appropriateness for your audience.\n* **Marketing:**Again, some general considerations:\n* **Segment and personalize:**When using an AI to develop audience insights, segment audiences, and deliver personalized content to those audiences, you must ensure that all applications remain in compliance with these guidelines.\n* **Chatbots and other search agents:**Chatbots that utilize AI models to retrieve and present content to audiences (for example, as in a retrieval-augmented generative approach) may be deployed only with approval from your unit head of marketing and/or communications.\n* **Other activities:**\n* **Presentation preparation:**Using AI tools (such as those built into common presentation tools) to enhance the efficiency and effectiveness of presentations should also remain consistent with these guidelines.\n* **Event transcription:**When using transcription tools for live, public events, or livestreamed events, you are responsible for reviewing the output for accuracy. If you use an AI tool to transcribe a meeting, you are responsible for ensuring: (i) its use conforms fully to these guidelines; (ii) the transcripts are not used to train non-Stanford models; and (iii) the data is retained consistent with university policy.## Specific Guidelines for Stanford Report Content\nYou may use AI tools to help produce content bound for publication in Stanford Report. The following are permitted uses:\n* Post-production, enhancing images, optimizing image quality, and retouching minor sections of an image. Generated images will be accepted at the discretion of the Stanford Report editorial team.\n* Translation and captioning of accompanying video and/or motion assets. All captions should be reviewed by a human for accuracy.\n* As a partner in drafting portions of written editorial content, including:\n* Brainstorming and suggesting questions for an interview.\n* Proposing an outline for an article.\n* Suggesting refinements to a narrative for clarity or consistency.\n* Researching the subject of the content.\n* Capturing and transcribing interview notes.\n* Preliminary editing to reduce grammatical, spelling, or other typographical errors.\nIn these cases, a resulting content product may be submitted to the editors of Stanford Report without prior consultation, but the author must disclose the use of an AI to the receiving editors in University Communications. Similarly, editors involved in the review of material prior to submission to University Communications must adhere to the same standards. The author(s), and not the AI, are responsible for reviewing and ensuring the accuracy of any images, content, or research in which an AI was utilized.\nWe recognize a wider range of applications and use cases may be applied to producing content for Stanford Report than what is indicated in the above. In those cases, the author(s) of the content should consult with the editors of Stanford Report prior to submitting the content for evaluation.\nPlease note that compliance with these guidelines does not guarantee placement in Stanford Report. All other[editorial standards and content considerations] are still effective. University Communications exercises discretion in determining which content appears in Stanford Report. This process involves rigorous editorial review during which a variety of factors, including university policies, are carefully considered. As a result, not all content suggested for publication, whether developed using an AI or not, is accepted.\n## Concluding Remarks\nThis is a rapidly developing area where all employees will benefit from continuous learning and experimentation. We expect that the scope of application will soon exceed what is enumerated in these guidelines. While we will make every effort to keep it current, as you encounter novel use cases, please share those with your colleagues for their own learning and development.\nThese guidelines were drafted in collaboration and consultation with colleagues from University IT, the Graduate School of Education, the School of Engineering, the Office of Development, the Graduate School of Business, the Stanford Institute for Human-Centered Artificial Intelligence, and University Communications. The designated point of contact is John Stafford, Assistant Vice President for Marketing &amp; Digital Strategy. Please[contact John] with any questions, concerns, or suggestions you may have.\n*Consistent with these guidelines, gpt-4o-mini and the Google plug-in on*[*Stanford’s AI Playground*] *were used to conduct research related to the drafting of these guidelines.*\nBack to Top",
    "length": 16172,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Adapting University Policies for Generative AI: Opportunities, Challenges, and Policy Solutions in Higher Education",
    "url": "https://scale.stanford.edu/ai/repository/adapting-university-policies-generative-ai-opportunities-challenges-and-policy",
    "text": "Adapting University Policies for Generative AI: Opportunities, Challenges, and Policy Solutions in Higher Education | SCALE Initiative\n[Skip to main content] \n[![Stanford University]] \n## Search and Filter\nWhat is the application?\nTeaching –Instructional Materials[**] \nTeaching –Assessment and Feedback[**] \nTeaching –Professional Learning[**] \nLearning –Student Support[**] \nCommunicating / Social Tools[**] \nOrganizing[**] \nAnalyzing[**] \nWho is the user?\nStudent[**] \nParent/Caregiver[**] \nEducator[**] \nSchool Leader[**] \nOthers[**] \nWhich age?\n0-3 years\nElementary (PK5)\nMiddle School (6-8)\nHigh School (9-12)\nPost-Secondary\nAdult\nWhy use AI?\nEfficiency[**] \nOutcomes –Literacy[**] \nOutcomes –Numeracy[**] \nOutcomes –Other Academic[**] \nOutcomes –Differentiation[**] \nOutcomes –Social Emotional[**] \nOutcomes –Durable Skills[**] \nReimagined Schooling[**] \nOther[**] \nStudy design\nDescriptive –Implementation and Use[**] \nDescriptive –Product Development[**] \nImpact –Randomized Controlled Trial[**] \nImpact –Quasi-experimental[**] \nSystematic Review[**] \nTechnical –Computational\nQuantitative –Others\n## Submit a research study\nContribute to the repository:\n[Add a paper] \n# Adapting University Policies for Generative AI: Opportunities, Challenges, and Policy Solutions in Higher Education\nAuthors\nRussell Beale\nDate\n06/2025\nPublisher\narXiv\nLink\n[http://arxiv.org/pdf/2506.22231v1] \nThe rapid proliferation of generative artificial intelligence (AI) tools -\nespecially large language models (LLMs) such as ChatGPT - has ushered in a\ntransformative era in higher education. Universities in developed regions are\nincreasingly integrating these technologies into research, teaching, and\nassessment. On one hand, LLMs can enhance productivity by streamlining\nliterature reviews, facilitating idea generation, assisting with coding and\ndata analysis, and even supporting grant proposal drafting. On the other hand,\ntheir use raises significant concerns regarding academic integrity, ethical\nboundaries, and equitable access. Recent empirical studies indicate that nearly\n47% of students use LLMs in their coursework - with 39% using them for exam\nquestions and 7% for entire assignments - while detection tools currently\nachieve around 88% accuracy, leaving a 12% error margin. This article\ncritically examines the opportunities offered by generative AI, explores the\nmultifaceted challenges it poses, and outlines robust policy solutions.\nEmphasis is placed on redesigning assessments to be AI-resilient, enhancing\nstaff and student training, implementing multi-layered enforcement mechanisms,\nand defining acceptable use. By synthesizing data from recent research and case\nstudies, the article argues that proactive policy adaptation is imperative to\nharness AI's potential while safeguarding the core values of academic integrity\nand equity.\nWhat is the application?\n[Teaching –Instructional Materials],\n[Teaching –Assessment and Feedback],\n[Teaching –Professional Learning],\n[Learning –Student Support],\n[Analyzing] \nWho is the user?\n[Student],\n[Educator],\n[Others] \nWho age?\n[Post-Secondary] \nWhy use AI?\n[Efficiency],\n[Outcomes –Literacy],\n[Outcomes –Other Academic],\n[Outcomes –Differentiation],\n[Outcomes –Durable Skills] \nStudy design\n[Systematic Review] \n[![Stanford University]] \n* [Stanford Home] \n* [Maps &amp; Directions] \n* [Search Stanford] \n* [Emergency Info] \n* [Privacy] \n* [Copyright] \n* [Trademarks] \n* [Non-Discrimination] \n* [Accessibility] \n©Stanford University. Stanford, California 94305.",
    "length": 3516,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "How Stanford educators are bringing AI into the classroom",
    "url": "https://news.stanford.edu/stories/2025/10/stanford-educators-ai-coursework-classroom-ideas",
    "text": "[Skip to content] \n\n## Stanford Report cookie usage information\n\nWe want to provide stories, announcements, events, leadership messages and resources that are relevant to you. Your selection is stored in a browser cookie which you can remove at any time by visiting the \"Show me...\" menu at the top right of the page. For more, read our [cookie policy].\n\nAcceptDecline\n\nOctober 22nd, 2025\\| 1 min read [Academics] \n\n# How Stanford educators are bringing AI into the classroom\n\nA new initiative highlights how instructors are leveraging generative AI while imposing constraints on student use to foster critical thinking.\n\nAn instructor and student work on a syllabus review activity at the Center for Teaching and Learning Course Design Institute. \\| Center for Teaching and Learning\n\nRecent advances in easy-to-access artificial intelligence tools have brought with them a quandary about AI’s place in education. Discussions about if and how generative AI should be used in classrooms are going strong. In the meantime, the tools are widely available to students and teachers alike.\n\nTo keep pace with change – and to help chart productive paths forward for AI use in the classroom – educators at Stanford are testing different ways to apply generative AI in coursework and sharing their ideas with others.\n\nThe [Office of the Vice Provost for Undergraduate Education] (VPUE) and the [Center for Teaching and Learning] (CTL) have launched a new effort called [AI Meets Education at Stanford] (AIMES), which provides a window into how courses across disciplines are leveraging generative AI tools for learning, as well as how they are placing important constraints on students’ uses of AI to foster deep learning and critical thinking.\n\n“As AI evolves, faculty are changing class policies and assignments,” said [James T. Hamilton], the Freeman-Thornton Vice Provost for Undergraduate Education. “AIMES makes it easier for faculty to share ideas and approaches to encouraging or restricting AI use in their courses.”\n\nVPUE also [recently welcomed Michele Elam] as one of two new senior associate vice provosts for undergraduate education. Elam, the William Robertson Coe Professor in the School of Humanities and Sciences (H&S) and a senior fellow at the [Stanford Institute for Human-Centered AI], will co-lead the AIMES initiative along with CTL’s associate vice provost for education and director, Cassandra Volpe Horii.\n\n“As our teaching and learning community considers whether, how, and when to use AI in courses, it is essential to do so within the broader context of Stanford’s enduring mission prioritizing open inquiry and ethical citizenship,” said Elam. “We also recognize that AIMES’s work occurs within the context of often wildly conflicting information about the possibilities, limitations, and harms of artificial intelligence. Therefore, one of our new initiatives includes helping the campus community navigate and evaluate all this incoming by becoming a resource for the latest, most thoroughly vetted research and scholarship on AI and education – leveraging especially the expertise on campus – to help educators and students make the most critically informed, evidence-driven decisions and choices about AI.”\n\n> As our teaching and learning community considers whether, how, and when to use AI in courses, it is essential to do so within the broader context of Stanford’s enduring mission prioritizing open inquiry and ethical citizenship.\n>\n> Michele ElamAIMES Co-Leader\n\nBelow are four examples from AIMES that illustrate how instructors are bringing AI into the classroom. A [library of these examples] is available on the CTL website, along with synthesized insights and professional development resources on university teaching and AI.\n\n1. **Program in Writing and Rhetoric (PWR) courses** provide some of the earliest practice and feedback on thinking-through-writing that students encounter at Stanford. The PWR lecturers are taking a variety of approaches to AI.For example, [Shay Brawn], an advanced lecturer, teaches the PWR 1 course _Rhetoric of Robots and AI_, which grapples with AI topics. Brawn provides clear guidance about allowed and disallowed uses of AI. While she acknowledges that generative AI might have valid uses in other contexts, her policy is that students may not use any writing generated by an AI for class assignments and may not use AI summaries as a replacement for engaging directly with actual sources. Students may, however, use AI to locate sources, identify key concepts to aid in research, and correct grammar, but Brawn warns about AI hallucinations. She is also clear that AI use should be transparently cited. Brawn separates AI use that is not permitted in her class into uses that run counter to the learning goals of the course and uses that are unethical, giving students more to consider about these categories. Brawn credits the work of Lisa Swan and Valerie Kinsey, who also teach in PWR, for earlier course policies that she adapted.\n2. **Art practice** instructor [Morehshin Allahyari], assistant professor of art and art history in H&S, allows the use of AI as a tool for generating ideas or visual sketches for projects but not for replacing the critical thinking, communication, and research skills needed to complete assignments. She also co-creates an agreement on the use of AI tools with her students, within boundaries. Allahyari emphasizes students’ effort and learning throughout their process, rather than just the final product of their work.\n3. **Philosophy 20N**, _Philosophy of Artificial Intelligence_, is a seminar that grapples with questions at the core of the field of AI through the perspectives of philosophy of mind, epistemology (the philosophy of knowledge), and ethics. In this course, [John Etchemendy], professor of philosophy in H&S, has students write regular journal entries about this topic and a longer final paper. In an intro note to students, Etchemendy urges students not to use LLMs to help write their work for the course: “By the end of the class, the three of us \\[faculty and teaching assistants\\] will come to know each of you well enough to recognize whether your paper reflects the way you think and express yourself. Believe me, ChatGPT isn’t going to capture your distinctive voice and way of thinking.”\n4. In the **Computer Science** course CS 121, _Equity and Governance for Artificial Intelligence_, senior lecturer [Cynthia Bailey] has students take on authentic policy-making tasks, such as making legislative recommendations about AI from the perspective of a staffer advising a U.S. Congress member or state legislator. The assignment aims to “evoke the reality of being a congressional staffer, to foster intrinsic motivation to do the assignment well,” explained Bailey. Congressional staffers must digest, understand, and recall large volumes of information at a blistering pace. To do that work, they use web searches, Wikipedia, and, in some cases, generative AI. Likewise, Bailey’s students are allowed limited use of generative AI for this project, such as to help them analyze detailed legislative and technical material in a short amount of time. In the course of this work, students are held to exceptionally high standards of integrity, excellence, and discretion – as though they were real-world congressional staff – and must disclose their use of AI.\n\n## For more information\n\nElam is also a professor of English in the [School of Humanities and Sciences] (H&S), a race and technology affiliate at the [Center for Comparative Studies in Race and Ethnicity], and an affiliate of the [Clayman Institute for Gender Research] and the [Wu Tsai Neurosciences Institute]. Etchemendy is also the Patrick Suppes Family Professor in H&S, the Denning Co-Director of [Stanford HAI], and provost emeritus. Hamilton is also the Hearst Professor and professor of communication in H&S, and a senior fellow at the [Stanford Institute for Economic Policy Research (SIEPR)].\n\n### Writer\n\nTaylor Kubota\n\n### Campus units\n\n[Center for Teaching & Learning] \n\n[Office of the Vice Provost for Undergraduate Education] \n\n### Related topics\n\n[Academics] \n\n[Artificial Intelligence] \n\n### Share this story\n\nCopy link\n\n## Read next\n\n[View allRead next] \n\n## [Fellowship positions students to lead in tech ethics and policy] \n\n[Academics] \n\nNews\n\n## [Academic Integrity Working Group addresses generative AI and exam policies] \n\n[Institutional News] \n\nNews\n\n## [‘Science thrives when everyone feels like they belong’] \n\n[Awards, Honors & Appointments] \n\nNews\n\n## [New fellowship connects students to the richness of rural life] \n\n[Academics] \n\nNews\n\n## [New programs bridge data science with humanities and the arts] \n\n[Academics] \n\nNews\n\n## [Stanford’s farm celebrates 10 years of sustainable education] \n\n[Academics] \n\nNews\n\nPreviousNext",
    "length": 8855,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "AI Guidelines for Marketing and Communications | University Communications",
    "url": "https://ucomm.stanford.edu/policies/ai-guidelines-for-marketing-and-communications/",
    "text": "AI guidelines for marketing and communications | University Communications\n[Skip to main content] [Skip to secondary navigation] \n[Stanford University(link is external)] \n[\nStanford\nUniversity Communications\n] \nSearch this siteSubmit Search\n# AI guidelines for marketing and communications\n## Main navigation\n[Skip Secondary Navigation] \nMain content start\n## Introduction, Scope of Guidelines, and Relevant Definitions\nThese guidelines recognize the transformative potential of artificial intelligence (“AI”) technologies to enhance operational efficiency, creativity, innovation, and decision-making in the marketing and communications functions at Stanford. At the same time, these tools necessitate careful usage to ensure responsible, legal, and ethical implementation. These guidelines outline the acceptable uses and expectations for the use of AI, supporting fully the use of these technologies, while ensuring such activities are done in the best traditions of the university.\nAs such, these guidelines build on Stanford’s philosophy that the use of AI is centered on improving the human condition and that AI should be used to augment, not replace, human work. In this context, the university expects that content generation using AI will be done ethically and with human oversight.\nThese guidelines apply to all regular staff, interns, casual employees, and consultants. Within the context of this document, “AI” and “generative AI” mean software and/or machines that can create new content, such as text, images, video, and other rich media. (Examples include, but are not limited to, ChatGPT, DALL-E, Gemini, and Claude.) Additionally, any software and/or machines that leverage existing data and use that knowledge to generate new, original content should also be considered within this scope.\nSpecific guidance for a select number of common use cases also is outlined below. The absence of a use case should not, however, be read as prohibition against that case. The bias of these guidelines is toward thoughtful experimentation and continuous growth.\nProviding guidance on how to use AI tools is not within the scope of these guidelines. Employees interested in a primer in this area should consult the[Stanford AI Playground].\nFinally, these guidelines are intended to complement existing university policy. If there is a conflict between guidance in this document and a policy in the Administrative Guide, then the Administrative Guide controls.\n## Responsible and Ethical Usage\nConsistent with the[Report of the AI at Stanford Advisory Committee] (which is incorporated herein by reference and paraphrased below for ease of reference), there are seven key guiding principles that employees in the marketing and communications functions should strive for in their use of AI tools:\n* **Human oversight:**Humans must take responsibility for the AI tools and systems they use at Stanford. You are responsible for oversight of any content you produce using AI to ensure that content is accurate, in alignment with institutional values, and in compliance with the policies set forth in this document.\n* You are personally responsible for conforming to this requirement and this responsibility may not be delegated to another employee.\n* **Alignment with university values:**AI systems should be built and/or procured to support Stanford’s mission and values.\n* **Human professionalism:**All employees in communications and marketing should adhere to professional standards and high levels of quality in their work. Moreover, they should exercise their best judgment and critical thinking in the use of these tools.\n* **Ethical and safe deployment:**These tools should improve university functions. Employees should understand the full implications of an AI system prior to deploying it in their functional area. If the system is not fully understood, an evaluation of the system should be undertaken prior to its implementation.\n* **Privacy, security, and confidentiality:**When using AI tools that involve personal data, the legality and impact of the application should be carefully considered prior to implementation. Some uses of data —such as those involving medical records, privileged information, or student or employee records —require express consent.\n* Please review the[university risk classifications] for assistance in determining what kind of data you may be working with.\n* As of this writing, the[Stanford AI Playground] is not approved for high-risk data.\n* You may not provide any confidential or legally privileged information of Stanford or a third party to generative AI tools.\n* **Data quality and control:**All data used to create new AI applications with university resources should be collected in legal and ethical ways and you should document the provenance of any data you use with these tools.\n* **An AI “golden rule”:**As stated in the committee report, you should “use or share AI outputs as you would have others use or share output with you.”\nReaders of these guidelines are strongly encouraged to familiarize themselves with the full content of the committee report.\n## Compliance and Intellectual Property Considerations\nThe following guidelines apply to all uses of AI technology without limitation:\n* Irrespective of the application, you must adhere to the[University Code of Conduct],[Information Security], and[Privacy Policies] when using AI technologies.\n* As in other contexts, you may not use AI tools to promote for-profit organizations, engage in commercial activities, or provide explicit or implicit commercial endorsements.\n* Please carefully review the terms of use (“TOU”) for any model you use. Many common TOUs include language that grants the provider the right to use your name and Stanford’s name in their marketing activities, which is not permitted under Administrative Guide 1.5.4.\n* Similarly, you may not use an AI to advocate on behalf of Stanford for any political position or political party, consistent with[Administrative Guide 1.5.1].\n* Do not use[high-risk data] in your prompts or include such data as attachments to your prompts.\n* Examples of such information include, but are not limited to, protected health information (PHI), student records, donor information, employee information, home addresses, and social security numbers. You should familiarize yourself with these risk classifications before providing data to an AI tool.\n* Additionally, exercise good judgment and extreme caution when providing[even moderate-risk] or[low-risk data] to a model, whether as prompt, attachment, through an API, or otherwise.\n* Ensure that your usage of AI complies with intellectual property rights and laws such as copyright, trademark, and trade secrecy protections. Use of third-party copyrighted or trademarked material or use of a person’s likeness without permission in interacting with an AI model may be illegal and could expose Stanford to significant financial liability.\n* Similarly, you are responsible for ensuring that any generative model you use has obtained, and provides to the university, a license for any outputs from that model.## Continuous Evaluation\nGiven the rapidly changing nature of this field, you are encouraged to regularly review and assess the models you are using to ensure they are meeting expectations and these guidelines.\nAll employees communicating on behalf of Stanford are encouraged to engage in a regular discussion of their AI practices with colleagues and, where appropriate, their supervisors. In the initial phase of any trials, experimentation, or evaluations, you should consider discussing with your supervisor(s) how you are using AI, how you plan to validate its output, and any additional AI-related activities you may plan to undertake in your work. This allows both parties to be aware of the usage, to discuss it in an open fashion, and ensure alignment on the application.\n## Recommended Platform: Stanford AI Playground\nThe Stanford AI Playground offers an excellent opportunity to trial a variety of models in a safe environment. We strongly recommend exploring your use cases in this environment, to the greatest extent possible, as you begin to leverage these technologies. The AI Playground includes various large language models (“LLMs”), a form of generative AI specializing in text-based content. Users can also access additional LLM plugins for image generation, web scraping, and AI-assisted Google services.\nMoreover, Stanford’s AI Playground supports data privacy by taking advantage of Stanford’s infrastructure and vendor partnerships. Files uploaded to the Playground are not shared externally or used to train models. More information is available in[a January 2025 Stanford Report article] and in the[AI Playground Quick Start Guide].\n## Specific Use Cases in the Communications and Marketing Functions\nFollowing is guidance for a number of common use cases in the marketing and communications functions. (As noted above, the absence of a use case should not be construed as a prohibition on that activity.) All applications, however, should conform to the principles, as well as the legal and policy considerations, outlined above.\n* **Writing and/or research for news articles (including Stanford Report), press releases, institutional statements, and similar material:**When using AI tools for drafting written content, you are responsible for: (i) carefully reviewing and fact-checking the output; (ii) ensuring the finished product complies with these guidelines and other university policy; (iii) ensuring the relevant tool does not borrow text verbatim from other sources; and (iv) disclosing the presence of an AI in the drafting and/or research process to the relevant editor.\n* **Drafting statements:**When using AI tools for brainstorming or outlining statements, you are responsible for: (i) carefully reviewing and fact-checking AI output; (ii) ensuring the finished product complies with these guidelines and other university policy; (iii) disclosing the presence of an AI in the drafting and/or research process to the relevant editor; and (iv) disclosing the presence of an AI in that statement to the Vice President for University Communications.\n* **Image generation and enhancement:**\n* **Enhancement:**Many image and video editing tools now employ AI models to assist with tasks such as color grading and/or noise reduction. These are generally acceptable applications of an AI tool, so long as the usage conforms to the guidelines above and does not substantially alter the authenticity of an image. If scientific data is being presented, any enhancement(s) should be reviewed with the scientists who generated the data to ensure it is acceptable.\n* **Generation:**In the case of image generation, any such activities should be consistent with the intellectual property guidelines above and you should credit/indicate the model that created the image in any photo credit line. As detailed above, if scientific data is being used to generate an image, the output should be carefully reviewed with the scientists who generated the data to be sure that it is acceptable.\n* **Social media:**Some general considerations in using AI tools for the following activities:\n* **Monitoring:**When using AI models, consistent with these guidelines, to monitor and summarize social media sentiment, you are encouraged to validate the output against a random sample of the underlying data.\n* **Writing:**When using an AI as an authoring partner, you are responsible for confirming accuracy of its output and its appropriateness for your audience.\n* **Marketing:**Again, some general considerations:\n* **Segment and personalize:**When using an AI to develop audience insights, segment audiences, and deliver personalized content to those audiences, you must ensure that all applications remain in compliance with these guidelines.\n* **Chatbots and other search agents:**Chatbots that utilize AI models to retrieve and present content to audiences (for example, as in a retrieval-augmented generative approach) may be deployed only with approval from your unit head of marketing and/or communications.\n* **Other activities:**\n* **Presentation preparation:**Using AI tools (such as those built into common presentation tools) to enhance the efficiency and effectiveness of presentations should also remain consistent with these guidelines.\n* **Event transcription:**When using transcription tools for live, public events, or livestreamed events, you are responsible for reviewing the output for accuracy. If you use an AI tool to transcribe a meeting, you are responsible for ensuring: (i) its use conforms fully to these guidelines; (ii) the transcripts are not used to train non-Stanford models; and (iii) the data is retained consistent with university policy.## Specific Guidelines for Stanford Report Content\nYou may use AI tools to help produce content bound for publication in Stanford Report. The following are permitted uses:\n* Post-production, enhancing images, optimizing image quality, and retouching minor sections of an image. Generated images will be accepted at the discretion of the Stanford Report editorial team.\n* Translation and captioning of accompanying video and/or motion assets. All captions should be reviewed by a human for accuracy.\n* As a partner in drafting portions of written editorial content, including:\n* Brainstorming and suggesting questions for an interview.\n* Proposing an outline for an article.\n* Suggesting refinements to a narrative for clarity or consistency.\n* Researching the subject of the content.\n* Capturing and transcribing interview notes.\n* Preliminary editing to reduce grammatical, spelling, or other typographical errors.\nIn these cases, a resulting content product may be submitted to the editors of Stanford Report without prior consultation, but the author must disclose the use of an AI to the receiving editors in University Communications. Similarly, editors involved in the review of material prior to submission to University Communications must adhere to the same standards. The author(s), and not the AI, are responsible for reviewing and ensuring the accuracy of any images, content, or research in which an AI was utilized.\nWe recognize a wider range of applications and use cases may be applied to producing content for Stanford Report than what is indicated in the above. In those cases, the author(s) of the content should consult with the editors of Stanford Report prior to submitting the content for evaluation.\nPlease note that compliance with these guidelines does not guarantee placement in Stanford Report. All other[editorial standards and content considerations] are still effective. University Communications exercises discretion in determining which content appears in Stanford Report. This process involves rigorous editorial review during which a variety of factors, including university policies, are carefully considered. As a result, not all content suggested for publication, whether developed using an AI or not, is accepted.\n## Concluding Remarks\nThis is a rapidly developing area where all employees will benefit from continuous learning and experimentation. We expect that the scope of application will soon exceed what is enumerated in these guidelines. While we will make every effort to keep it current, as you encounter novel use cases, please share those with your colleagues for their own learning and development.\nThese guidelines were drafted in collaboration and consultation with colleagues from University IT, the Graduate School of Education, the School of Engineering, the Office of Development, the Graduate School of Business, the Stanford Institute for Human-Centered Artificial Intelligence, and University Communications. The designated point of contact is John Stafford, Assistant Vice President for Marketing &amp; Digital Strategy. Please[contact John] with any questions, concerns, or suggestions you may have.\n*Consistent with these guidelines, gpt-4o-mini and the Google plug-in on*[*Stanford’s AI Playground*] *were used to conduct research related to the drafting of these guidelines.*\nBack to Top",
    "length": 16172,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Use of Generative AI Technology - Student Affairs",
    "url": "https://law.stanford.edu/office-of-student-affairs/use-of-generative-ai-technology/",
    "text": "Use of Generative AI Technology - Student Affairs - Stanford Law School\nlogo-footerlogo-fulllogo-stanford-universitylogomenu-closemenuClose IconIcon with an X to denote closing.Play IconPlay icon in a circular border.\n[Skip to main content] \n![Sls logo] \n# Use of Generative AI Technology\nGenerative AI tools (e.g., ChatGPT) are increasingly used in legal practice and may have some beneficial uses to support learning. However, these tools come with risk: they may produce incorrect responses and otherwise inhibit learning, and some uses would constitute professional malpractice in the provision of legal services. Law school instructors should therefore exercise discretion to choose the appropriate AI policy for their learning goals.\nIndividual course instructors are free to set their own policies regulating the use of generative AI tools in their courses, including allowing or disallowing some or all uses of such tools, provided that the permitted uses of AI do not authorize students to contravene standard academic norms concerning plagiarism and accuracy.[Plagiarism] includes using an idea obtained from AI without attribution or submitting AI-generated text verbatim without quotation marks; accuracy norms require verifying assertions in submitted work. In coursework that involves attorney client representation, providing legal advice or services, or that would otherwise constitute the practice of law, AI use must meet all applicable rules of professional responsibility. Course instructors must state these policies in their course syllabi and clearly communicate these policies to students. Students who are unsure of policies regarding generative AI tools are encouraged to ask their instructors for clarification. This policy applies to all Stanford Law School courses, including clinics.\nIn the absence of a course-specific AI policy set by the instructor, students may use generative AI tools to support their learning and to aid in the development or refinement of their own ideas, provided they do not use such tools to generate content that is then presented as their own work. The use of generative AI while taking an exam or to draft or revise any portion of submitted work (e.g., drafting or revising the text of a paper or clinic work product; generating citations; fixing citation format) is not permitted unless (1) fully disclosed in advance of its use by the student to the instructor, and (2) explicitly authorized by the instructor in writing prior to the student’s use of the tool. In all cases, the student’s use of AI cannot contravene standard academic norms concerning plagiarism and accuracy, and in coursework involving the practice of law, AI must meet all applicable rules of professional responsibility. Students who are unsure whether a particular AI use is permitted should default to asking the instructor and disclosing the use.\nCitations to sources that do not exist will raise a presumption of generative AI use. Unauthorized use of AI tools may result in a lower grade, including a grade of F, and/or a referral to Stanford’s Office of Community Standards.\nThe following examples illustrate the application of this policy in the absence of a course-specific Generative AI policy:\n* A student uses an AI tool to organize her study outline in preparation for an exam in a doctrinal course. This use would not be prohibited.\n* A student uses AI to fix the Bluebooking of a paper and discloses in advance. This involves the use of AI to revise submitted work, so it must be authorized by the instructor in writing in advance of the student’s use of AI.\n* A student uses AI to prepare for being on panel in class. This use is not prohibited.\n* A student uses AI to write a paper or to produce clinic work product and discloses in advance prior to using such tools. This involves the use of AI to draft or revise submitted work, so it must be authorized by the instructor in writing in advance and cannot contravene standard academic norms concerning plagiarism and accuracy, or relevant professional standards for legal practice.\n* A student uses AI to write a paper or to produce clinic work product and does not disclose in advance. This is a violation of the AI policy and could result in an F and/or a referral to Stanford’s Office of Community Standards.\n* A student uses AI during an exam and does not disclose. This is a violation of the AI policy and could result in an F and/or a referral to Stanford’s Office of Community Standards.\n**Back to the Top",
    "length": 4512,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Guidance on technology tools for academic integrity",
    "url": "https://teachingcommons.stanford.edu/news/guidance-technology-tools-academic-integrity",
    "text": "Guidance on technology tools for academic integrity | Teaching Commons\n[Skip to main content] [Skip to secondary navigation] \n[Stanford University(link is external)] \n[\nStanford\nTeaching Commons\n] \nSearch this siteSubmit Search\nMain content start\n[Policy],[Technology-enhanced],[Artificial Intelligence (AI)] \n# Guidance on technology tools for academic integrity\nDiscussion about technology tools for plagiarism detection and online proctoring.\nApril 26, 2023\n|\nTeaching Commons\nAcademic integrity is an important concern among instructors, particularly in light of new developments in generative AI technology. Some instructors may look to technological tools to address these concerns. This page discusses the many factors and issues related to academic integrity and technology tools at Stanford.\n## Current policy guidance related to technology\nIntegrity, honesty, and ethical behavior are fundamental to campus life at Stanford. With regard to teaching and learning, the[Honor Code],[Student Judicial Charter], and the guidance provided by the Board on Judicial Affairs available on the[Office of Community Standards (OCS)] website provides guidance and policy on matters involving cheating,[what constitutes plagiarism], unpermitted aid, and so on.\n## Generative AI policy guidance\nThe Board of Judicial Affairs (BJA) has stated that individual instructors are free to set their own course policies regarding the use of generative AI tools.\nOCS has stated: \"Absent a clear statement from a course instructor, use of or consultation with generative AI shall be treated analogously to assistance from another person.\"\nTherefore, instructors should decide their own course policies regarding the use of generative AI, state those policies in syllabi, and clearly communicate them to students.\nOCS provides more details on its[Generative AI Policy Guidance webpage].\n## Technology tools to enforce academic integrity\nWhile there are many types of educational technology tools that might relate to academic integrity, most instructor inquiries are about plagiarism detection and proctoring.\n### Plagiarism detection tools\nTools, such as Turnitin, Unicheck, and Grammarly typically cross-reference uploaded student work to a database of other works to determine the originality and sources of the student work. Some plagiarism detection tools also leverage AI technology and may detect AI-generated text to varying degrees of accuracy.\n#### Current policy guidance on plagiarism detection tools\nRefer to OCS’s[Tips for Faculty &amp; Teaching Assistants] web page for policy guidance on the use of plagiarism detection software.\n#### How plagiarism detection software works\nStudents typically upload their work to a plagiarism detection platform in their campus learning management system. The platform checks the submitted work against a database of copyrighted or other external works and creates a report for each uploaded work that highlights any sections of writing or code that are similar to existing content. The report may also provide a link to the existing original content. This report is visible to the instructor and may be made visible to students. Uploaded works that are highly similar to other works are flagged for review.\n#### How to respond to suspected plagiarism\nIf there is a concern that academic dishonesty may have occurred, for example when plagiarism detection tools flag a student’s work for potential plagiarism or improper citations, it is the instructor’s responsibility under the Honor Code to[contact the Office of Community Standards]. OCS staff will consult with you to determine the best way forward.\nThe instructor should not discuss the matter with the student ahead of consulting with OCS. Instructors may not penalize grades (reducing the student’s grade for engaging in suspected academic dishonesty before the student has been found responsible). Consulting with OCS staff does not obligate the person to file a concern; investigations do not begin unless and until an official written concern is filed.\nThis guidance applies to any assignment or activity that is used as the basis of grading. Suspected plagiarism on an ungraded assignment, such as a draft of an essay, may be resolved with the student directly and OCS does not need to be consulted.\nIn the case of suspected unpermitted aid from generative AI tools, verification through plagiarism detection tools is not required before submitting a concern to OCS.\nAfter reporting a concern, the instructor will still have the opportunity to provide input on how the case is resolved. For example, the instructor could ask for a non-disciplinary resolution if the student has taken responsibility and the instructor feels the violation is minor.\n#### Issues with plagiarism detection tools\nSome plagiarism detection tools store uploaded student work which allows future submissions to be checked against the database. While some tools allow users to opt out or to keep separate institutional databases of submitted work, questions about maintaining the privacy and intellectual property rights of students should be addressed when adopting such tools.\nFor some students, submitting their work to plagiarism detection tools may erode feelings of trust and belonging. Some students may have had experiences in previous educational settings where such tools were associated with punitive measures, may have misconceptions about what plagiarism is, or not understand how plagiarism detection tools work. If unaddressed, this can contribute to distrust and disengagement in the classroom.\nPlagiarism detection platforms have varying levels of efficacy in detecting text generated by AI tools. Relying solely on plagiarism detection tools to identify if students have not complied with an AI-related course policy is not advised.\nLicensing costs for plagiarism detection tools can vary. Stanford currently does not support any campus-wide plagiarism detection tools. Individual schools, departments, or units within the university would cover any costs and provide instructor support if they choose to adopt such tools.\n#### Benefits of plagiarism detection tools\nThese tools can be helpful when used thoughtfully to teach what plagiarism and academic integrity are and to practice writing skills such as researching, note-taking, paraphrasing, citing sources, and so on. Students might submit ungraded drafts of their writing to a plagiarism detection platform to identify areas to improve.\nThese tools can also be useful in non-instructional contexts, such as submitting works to a journal for publication.\nPlagiarism detection tools are often bundled with other useful grading and writing tools, such as dynamic rubrics and grading annotation.\n### Remote proctoring tools\nOnline proctoring tools, such as Proctorio or ProctorU, typically use software installed on the students' computers to verify students’ identity, restrict certain apps, and activate the students’ web cameras to record or observe them while taking an exam. There are generally two types of online proctoring tools: ones that use automated software and ones that use live human proctors. Both will then report suspicious behavior to the instructor for follow-up.\n#### Current policy guidance on proctoring tools\nPolicy around proctoring is evolving. The Office of Community Standards provides up-to-date information in the[Honor Code section of its website].\n## Consider adopting these tools carefully\nStanford currently does not support any campus-wide plagiarism detection or proctoring tools for general use for many reasons.\nAdoption of any academic technology tool at the campus-wide level, particularly tools that intersect with established policy, requires a careful and comprehensive process of evaluating the tool for security, privacy, accessibility, cost, technical support services, policy considerations, and so forth. Campus leaders are aware of these tools and the interest from some instructors in adopting them, and are continuously evaluating their feasibility. However, currently, such tools have not been adopted for general use.\nTherefore, if you are in a specific situation with a compelling reason to use such tools, first contact the[Office of Community Standards] for the most up-to-date policy guidance. Then contact your department leaders to explore the adoption of these tools for specific use within your course, department, or program. Some departments have already adopted tools, notably the Computer Science department, for checking the originality of student work in programming languages.\n## Strategies to promote academic honesty\nRegardless of the technology tools, it is important to remember that plagiarism detection and proctoring are based on a model of enforcement and punitive consequences that only address certain factors related to academic integrity.\nA more inclusive and comprehensive approach might address factors that motivate ethical or unethical behavior, the deeper goals of education, and the human needs of instructors and students. For example, you might encourage students to consider the writing process as essential to their learning and work with them to foster the habits of academic integrity, such as accurately noting and acknowledging research sources.\nRather than only asking “How can we stop students from cheating?”, we might also ask, “How can we encourage students to behave honorably and ethically?”\nFor further discussion on this topic see[Teaching Strategies to Support the Honor Code and Student Learning].\n## More News Topics\n[Policy] [Technology-enhanced] [Artificial Intelligence (AI)] [Assessment] \n## More News\n* [\n### How Stanford educators are bringing AI into the classroom\n] \nOctober 23, 2025\nAI Meets Education at Stanford (AIMES), with examples from writing, art, philosophy, and computer science\n* [Artificial Intelligence (AI)] \n* [Instructor Exemplars] \n* [\n### Canvas Video Tutorials\n] \nOctober 20, 2025\nExplore Canvas tutorials that review various tools and features. These short, informative videos are designed to help you navigate and utilize Canvas effectively.\n* [Technology-enhanced] \n* [\n### AI Meets Education At Stanford (AIMES) Library of Examples\n] \nOctober 7, 2025\nFind examples of generative AI use, policies, assignments, and more from Stanford courses to spark ideas and showcase a variety of approaches to AI.\n* [Artificial Intelligence (AI)] \n* [Instructor Exemplars] \nBack to Top",
    "length": 10453,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Report outlines Stanford principles for use of AI",
    "url": "https://news.stanford.edu/stories/2025/01/report-outlines-stanford-principles-for-use-of-ai",
    "text": "[Skip to content] \n\n## Stanford Report cookie usage information\n\nWe want to provide stories, announcements, events, leadership messages and resources that are relevant to you. Your selection is stored in a browser cookie which you can remove at any time by visiting the \"Show me...\" menu at the top right of the page. For more, read our [cookie policy].\n\nAcceptDecline\n\nJanuary 9th, 2025\\| 1 min read [Leadership & Governance] \n\n# Report outlines Stanford principles for use of AI\n\nThe AI at Stanford Advisory Committee’s report includes recommendations for AI use in the university’s administration, education, and research.\n\nAs artificial intelligence (AI) continues to reshape education, research, and administration at Stanford, a [new report] from the university’s AI at Stanford Advisory Committee calls for balancing innovation with responsibility and alignment with the university’s key values.\n\nUnderscoring both the opportunities and risks of AI, the “AI at Stanford” report sets forth guiding principles to encourage experimentation and creativity while addressing challenges such as plagiarism, authorship, and ethical use.\n\n“The growth of AI technologies has huge implications for higher education, from the classroom to the research lab,” said Provost Jenny Martinez. “The possibilities are incredibly exciting, and I’m confident Stanford will continue to be a leader in this area. As we support advances in this technology, it’s crucial to assess how AI is being used at Stanford today, consider how it may be used in the future, and identify any policy gaps that may exist. I’m grateful for the advisory committee’s thoughtful and thorough work, and the guidance it has provided in advancing the responsible use of AI at Stanford.”\n\nIn March, the provost charged the AI at Stanford Advisory Committee to evaluate the role of AI in administration, education, and research, and to identify what’s needed to use AI responsibly at the university.\n\nVarious university offices and committees will consider and take on recommendations from the report, and the committee chair will provide a presentation to the Faculty Senate in the winter quarter. The committee, made up of 10 faculty and staff members from various campus units, will continue to meet to address future issues related to AI use at Stanford.\n\n## Balancing experimentation with principles\n\nWhile the committee acknowledged legitimate concerns about AI, it sought to avoid rigid policies that might deter the technology’s potential benefits. “We wanted to first encourage experimentation in safe spaces to learn what it can do and how it might help us pursue our mission,” said Committee Chair Russ Altman. “But then we also wanted to establish clear ‘hot button’ areas where people should proceed with an abundance of caution.”\n\nThe report highlights potential policy gaps for the university, noting that many situations and challenges can’t be anticipated, and provides general principles to guide AI usage at the university, such as requiring human oversight and ethical considerations in all AI usage.\n\n“There should always be professionalism and personal responsibility. Whenever somebody uses AI, even if the AI does some work, they need to take responsibility for the output, and if there’s mistakes, it’s on them,” said Altman, the Kenneth Fong Professor and professor of bioengineering, of genetics, of medicine, of biomedical data science, and senior fellow at the Stanford Institute for Human-Centered Artificial Intelligence.\n\nPeople should resist lapsing into AI exceptionalism– assuming that existing laws, regulations, and university policies aren’t applicable to AI, according to the report. “It seems seductively capable,” Altman said, “and so people let their guard down and use it in ways that suggest they are forgetting what it is and how it works.”\n\nAs a guiding principle, the report recommends an “AI golden rule”: use AI with others as you would want them to use AI with you. For example, would you want AI to be used to review your proposal? This assessment would be based on individual judgments, evolving community norms, and combined with other principles to inform AI use.\n\n## Education\n\nOne primary area the committee examined is how AI affects education. Students have already adopted AI technologies such as ChatGPT, meaning the Honor Code and individual classroom policies may need to be revisited, the committee noted.\n\nAt the same time, many faculty aren’t experienced with AI and are unaware of how they can use it, said Dan Schwartz, committee member and dean of the Graduate School of Education (GSE).\n\n“Students have been exploring it much more than the faculty, and that’s why it’s important to find ways to educate the faculty, which I think is also true on the research side,” Schwartz said. “The question is slowly changing from what are our students going to cheat with, to a more sophisticated question of what counts as cheating, how they can use AI in their work, and how to make it part of the assignment.”\n\nTo help faculty navigate these questions, the committee recommends frameworks that can be tailored to different classroom needs. “It was something that students as much as faculty wanted so they can understand what’s permissible and productive,” Schwartz said.\n\nThe GSE has already created the [AI Tinkery], part of the [Stanford Accelerator for Learning], to provide a collaborative space for educators to explore the possibilities of AI.\n\n## Research\n\nIn research, AI raises complex challenges, including the question of whether AI can or should be credited as an author on publications. Other concerns include the use of AI in reviewing and writing proposals, training AI on student work, using data for AI research, addressing potential copyright violations in LLM outputs, and detecting fraudulent behavior.\n\nFor example, the use of AI detectors is already leading to a higher volume of plagiarism allegations, including spurious ones, which creates huge burdens on the offices that handle research misconduct. The university’s misconduct policy and rules for investigating allegations may also need to be updated in accordance with new federal policy.\n\nMany of these issues emerged from conversations with those who work in research administration, Altman said.\n\nResearchers should also be reminded or made aware of potential risks in AI use and updated as legal risk profile changes, the report says. The committee also encourages the university to consider ways to expand computing resources to ensure Stanford remains a leader in the productive use of these technologies.\n\n## Administration\n\nRegarding AI use in university administration processes, the committee found several areas that may require more guidance and additional policies, such as hiring, performance reviews, admissions, communications, and surveillance. The report also included recommendations on education and training for use of sensitive data, a streamlined procurement process for AI systems, and letters of recommendation.\n\nThe committee acknowledges that not all AI uses have been fully identified. “We were very aware that we couldn’t uncover all uses of AI on campus, and this is what led us to articulate the guiding principles that we hope are useful for folks evaluating new AI opportunities to see if there are any red flags,” Altman said. “In the end, these principles are probably more useful, and general purpose, than the specific issues we surfaced, which may or may not remain issues over the next few years.”\n\nThe committee is just one facet of the university’s ongoing incorporation of AI into its everyday work, which also includes University IT [resources] like the Stanford AI Playground where staff, students, and faculty can access a range of AI tools. At the same time, on the research side, faculty across the university are pushing the frontiers of knowledge in the development and use of AI across a wide variety of disciplines, supported by programs like the [Stanford Institute for Human-Centered Artificial Intelligence] and [Stanford Data Science] as well as the new [Marlowe], Stanford’s newest high-performance computing cluster.\n\n### Related story\n\n[**AI Playground offers a safe place to explore and experiment** \\\n\\\nUniversity IT began piloting the Stanford AI Playground earlier this year, providing the Stanford community a secure platform to explore various AI tools.] \n\n### Writer\n\nChelcey Adami\n\n### Campus unit\n\n[Office of the Provost] \n\n### Related topics\n\n[Leadership & Governance] \n\n### Share this story\n\nCopy link\n\n## Read next\n\n[View allRead next] \n\n## [Board of Trustees holds final meeting of 2024-25 academic year] \n\n[Leadership & Governance] \n\nNews\n\n## [Provost presents preliminary 2025-26 budget plan, says cuts are likely] \n\n[Leadership & Governance] \n\nNews\n\n## [Craig Carnaroli named Stanford’s senior vice president for finance and administration] \n\n[Leadership & Governance] \n\nNews\n\n## [Faculty Senate hears updates on the undergraduate experience, emeriti] \n\n[Leadership & Governance] \n\nNews\n\n## [Faculty Senate hears report on proctoring pilot, academic integrity issues] \n\n[Leadership & Governance] \n\nNews\n\n## [President Levin expands on university’s guiding principles] \n\n[Leadership & Governance] \n\nNews\n\nPreviousNext",
    "length": 9290,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Clinic, Law School, and University AI Policies and Syllabus Language | Stanford Law School",
    "url": "https://law.stanford.edu/juelsgaard-intellectual-property-and-innovation-clinic/clinic-and-law-school-policies-and-syllabus-language/",
    "text": "Clinic, Law School, and University AI Policies and Syllabus Language - Juelsgaard Intellectual Property and Innovation Clinic - Stanford Law School\nlogo-footerlogo-fulllogo-stanford-universitylogomenu-closemenuClose IconIcon with an X to denote closing.Play IconPlay icon in a circular border.\n[Skip to main content] \n![Sls logo] \n# Clinic, Law School, and University AI Policies and Syllabus Language\n#### Universities, law schools, and clinics have rushed to adopt policies about student use of AI tools in a variety of settings. Some are flat (and fairly mindless) prohibitions. Others embrace the pedagogical possibilities of student AI use and attempt to guide it while preserving important underlying learning and ethical principles. Most are somewhere in between.\n#### Mills Legal Clinic Policies and Syllabus Language\n* **SLS Juelsgaard Clinic, Syllabus Language (as of Spring 2024): Use of generative AI tools such as ChatGPT:**\n* **&#8220;*As a default, you should not use ChatGPT or any other generative AI tools in completing any of your clinic assignments. As you’re no doubt aware, several recent cases have involved attorneys using generative AI to prepare briefs or other legal materials, with fairly disastrous results. Various state bars and courts have now instituted rules about the use of generative AI. In JIPIC, our focus on technology and innovation means that we have a strong interest in studying and experimenting with the role that generative AI may play in legal practice. But we also must be careful and ensure that we comply with all quality requirements and ethical responsibilities. So anytime you think that the engaging such technology in a clinic case or project would be interesting, instructive, or productive, talk to your instructors and evaluate what might be appropriate in that case. But you must always receive express approval before actually using any AI tools in our practice.****&#8220;*\n* **SLS Juelsgaard Clinic Syllabus &#8212; Spring 2024[JIPIC AI Module] on Use of AI in Legal Practice**#### SLS and Stanford Policies and Syllabus Language\n* **[Syllabus language] from SLS Prof. Jan Martinez&#8217;s 2023 (Re)Designing Dispute Systems course (LAW 7806):**\n* **&#8220;*Generative AI (Artificial Intelligence that can produce contents) is now widely available to produce text, images, and other media. I encourage the use of such AI resources to inform yourself about the field, to understand the contributions that AI can make, and to help your learning. However, keep the following three***\n***principles in mind: (1) AI contributions must be attributed and true so ideas and facts must be referenced and validated; (2) The use of AI resources must be open and documented; and (3) An AI generated submission on its own cannot******achieve a passing grade. This is necessary to ensure you are competent to surpass generative AI in the future –whether in academia, research, the workplace, or other domains of society. Your task will be to surpass them, so whatever you submit will be compared with solely AI-generated solutions. For your weekly reflections, please use your own ideas to critique the assigned readings or speaker writings. If you choose to supplement with AI, provide transparent citations, according to the Honor Code. We will discuss the use of AI in our first class as a preliminary ground rule.&#8221;***\n* **SLS Crown Library EdTech Hub: &#8220;[Syllabus Statements for Generative AI Usage] &#8220;**\n* **Stanford Office of Community Standards: &#8220;[Policy Guidance: Honor Code Implications of Generative AI Tools],&#8221; February 16, 2023**\n* **Stanford Center for Teaching and Learning, Learning Commons: &#8220;[Creating Your Course Policy on AI] &#8220;**#### Other Institutions&#8217; Policies and Syllabus Language\n* **Berkeley Law &#8220;[AI Rule on Exams and Assessments] &#8221; April 2023**\n* **AALS: &#8220;[Faculty Perspectives: Does Your Law School Need a Policy on Generative Artificial Intelligence?] &#8221; (discussing the creation of Berkeley Law&#8217;s policy)**\n* **Duke Learning Innovation and Lifetime Education: &#8220;[Artificial Intelligence Policies: Guidelines and Considerations] &#8221; (updated January 24, 2024) (suggestions for teachers developing their own AI use policies)**\n* **Harvard College Office of Undergraduate Education:[AI Guidance and FAQs &#8212; Policies for the Use of AI in Courses] (providing &#8220;maximally restrictive,&#8221; &#8220;fully encouraging,&#8221; and &#8220;mixed&#8221; exemplar policy language)**\n* **MetaLAB (at) Harvard[AI Pedagogy Project]: &#8220;[Proposed Harvard AI Code of Conduct] &#8221;**\n* **[Compilation of US University Policies on Use of Generative AI] (Compiled by Scribbr, updated regularly)**\n* **&#8220;[Syllabi Policies for AI Generative Tools] &#8221; (collection of over 130 AI policies by different types of educational institutions, though only a handful of law schools. Created by educational consultant Lance Eaton**#### General Resources on AI Policies\n* **Educause Review &#8212; Esther Brandon, Lance Eaton, Dana Gavin and Allison Papini: &#8220;[Cross-Campus Approaches to Building a Generative AI Policy] &#8221; December 12, 2023**\n**Back to the Top",
    "length": 5226,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Generative AI in Higher Education: Seeing ChatGPT Through Universities' Policies, Resources, and Guidelines",
    "url": "https://scale.stanford.edu/genai/repository/generative-ai-higher-education-seeing-chatgpt-through-universities-policies",
    "text": "Generative AI in Higher Education: Seeing ChatGPT Through Universities&#039; Policies, Resources, and Guidelines | SCALE Initiative\n[Skip to main content] \n[![Stanford University]] \n## Search and Filter\nWhat is the application?\nTeaching –Instructional Materials[**] \nTeaching –Assessment and Feedback[**] \nTeaching –Professional Learning[**] \nLearning –Student Support[**] \nCommunicating / Social Tools[**] \nOrganizing[**] \nAnalyzing[**] \nWho is the user?\nStudent[**] \nParent/Caregiver[**] \nEducator[**] \nSchool Leader[**] \nOthers[**] \nWhich age?\n0-3 years\nElementary (PK5)\nMiddle School (6-8)\nHigh School (9-12)\nPost-Secondary\nAdult\nWhy use AI?\nEfficiency[**] \nOutcomes –Literacy[**] \nOutcomes –Numeracy[**] \nOutcomes –Other Academic[**] \nOutcomes –Differentiation[**] \nOutcomes –Social Emotional[**] \nOutcomes –Durable Skills[**] \nReimagined Schooling[**] \nOther[**] \nStudy design\nDescriptive –Implementation and Use[**] \nDescriptive –Product Development[**] \nImpact –Randomized Controlled Trial[**] \nImpact –Quasi-experimental[**] \nSystematic Review[**] \n## Submit a research study\nContribute to the repository:\n[Add a paper] \n# Generative AI in Higher Education: Seeing ChatGPT Through Universities&#039; Policies, Resources, and Guidelines\nAuthors\nHui Wang, Anh Dang, Zihao Wu, Son Mac\nDate\n07/2024\nPublisher\narXiv\nLink\n[https://arxiv.org/abs/2312.05235] \nThe advancements in Generative Artificial Intelligence (GenAI) provide opportunities to enrich educational experiences, but also raise concerns about academic integrity. Many educators have expressed anxiety and hesitation in integrating GenAI in their teaching practices, and are in needs of recommendations and guidance from their institutions that can support them to incorporate GenAI in their classrooms effectively. In order to respond to higher educators' needs, this study aims to explore how universities and educators respond and adapt to the development of GenAI in their academic contexts by analyzing academic policies and guidelines established by top-ranked U.S. universities regarding the use of GenAI, especially ChatGPT. Data sources include academic policies, statements, guidelines, and relevant resources provided by the top 100 universities in the U.S. Results show that the majority of these universities adopt an open but cautious approach towards GenAI. Primary concerns lie in ethical usage, accuracy, and data privacy. Most universities actively respond and provide diverse types of resources, such as syllabus templates, workshops, shared articles, and one-on-one consultations focusing on a range of topics: general technical introduction, ethical concerns, pedagogical applications, preventive strategies, data privacy, limitations, and detective tools. The findings provide four practical pedagogical implications for educators in teaching practices: accept its presence, align its use with learning objectives, evolve curriculum to prevent misuse, and adopt multifaceted evaluation strategies rather than relying on AI detectors. Two recommendations are suggested for educators in policy making: establish discipline-specific policies and guidelines, and manage sensitive information carefully.\nWhat is the application?\n[Teaching –Instructional Materials],\n[Teaching –Assessment and Feedback],\n[Teaching –Professional Learning] \nWho is the user?\n[Educator],\n[School Leader] \nWho age?\n[Post-Secondary] \nWhy use AI?\n[Efficiency],\n[Outcomes –Other Academic] \nStudy design\n[Descriptive –Implementation and Use],\n[Systematic Review] \n[![Stanford University]] \n* [Stanford Home] \n* [Maps &amp; Directions] \n* [Search Stanford] \n* [Emergency Info] \n* [Privacy] \n* [Copyright] \n* [Trademarks] \n* [Non-Discrimination] \n* [Accessibility] \n©Stanford University. Stanford, California 94305.",
    "length": 3771,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "AI Governance in Higher Education: Case Studies of Guidance at Big Ten Universities",
    "url": "https://scale.stanford.edu/genai/repository/ai-governance-higher-education-case-studies-guidance-big-ten-universities",
    "text": "AI Governance in Higher Education: Case Studies of Guidance at Big Ten Universities | SCALE Initiative\n[Skip to main content] \n[![Stanford University]] \n## Search and Filter\nWhat is the application?\nTeaching –Instructional Materials[**] \nTeaching –Assessment and Feedback[**] \nTeaching –Professional Learning[**] \nLearning –Student Support[**] \nCommunicating / Social Tools[**] \nOrganizing[**] \nAnalyzing[**] \nWho is the user?\nStudent[**] \nParent/Caregiver[**] \nEducator[**] \nSchool Leader[**] \nOthers[**] \nWhich age?\n0-3 years\nElementary (PK5)\nMiddle School (6-8)\nHigh School (9-12)\nPost-Secondary\nAdult\nWhy use AI?\nEfficiency[**] \nOutcomes –Literacy[**] \nOutcomes –Numeracy[**] \nOutcomes –Other Academic[**] \nOutcomes –Differentiation[**] \nOutcomes –Social Emotional[**] \nOutcomes –Durable Skills[**] \nReimagined Schooling[**] \nOther[**] \nStudy design\nDescriptive –Implementation and Use[**] \nDescriptive –Product Development[**] \nImpact –Randomized Controlled Trial[**] \nImpact –Quasi-experimental[**] \nSystematic Review[**] \n## Submit a research study\nContribute to the repository:\n[Add a paper] \n# AI Governance in Higher Education: Case Studies of Guidance at Big Ten Universities\nAuthors\nChuhao Wu, He Zhang, John M. Carroll\nDate\n09/2024\nPublisher\narXiv\nLink\n[https://arxiv.org/abs/2409.02017] \nGenerative AI has drawn significant attention from stakeholders in higher education. As it introduces new opportunities for personalized learning and tutoring support, it simultaneously poses challenges to academic integrity and leads to ethical issues. Consequently, governing responsible AI usage within higher education institutions (HEIs) becomes increasingly important. Leading universities have already published guidelines on Generative AI, with most attempting to embrace this technology responsibly. This study provides a new perspective by focusing on strategies for responsible AI governance as demonstrated in these guidelines. Through a case study of 14 prestigious universities in the United States, we identified the multi-unit governance of AI, the role-specific governance of AI, and the academic characteristics of AI governance from their AI guidelines. The strengths and potential limitations of these strategies and characteristics are discussed. The findings offer practical implications for guiding responsible AI usage in HEIs and beyond.\nWhat is the application?\n[Teaching –Instructional Materials],\n[Teaching –Assessment and Feedback],\n[Teaching –Professional Learning],\n[Learning –Student Support],\n[Analyzing] \nWho is the user?\n[Student],\n[Educator],\n[School Leader],\n[Others] \nWho age?\n[Post-Secondary] \nWhy use AI?\n[Efficiency],\n[Outcomes –Other Academic],\n[Outcomes –Differentiation],\n[Other] \nStudy design\n[Descriptive –Implementation and Use] \n[![Stanford University]] \n* [Stanford Home] \n* [Maps &amp; Directions] \n* [Search Stanford] \n* [Emergency Info] \n* [Privacy] \n* [Copyright] \n* [Trademarks] \n* [Non-Discrimination] \n* [Accessibility] \n©Stanford University. Stanford, California 94305.",
    "length": 3028,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "On the Societal Impact of Open Foundation Models",
    "url": "https://crfm.stanford.edu/open-fms",
    "text": "On the Societal Impact of Open Foundation Models\n# **On the Societal Impact of\nOpen Foundation Models**\n**Analyzing the benefits and risks of foundation models with widely available weights**\n[Paper (ICML 2024 Oral)] [Blog] [Policy brief] [Authors] \n**Context.**One of the biggest tech policy debates today is about the future of AI,\nespecially foundation models and generative AI. Should open AI models be restricted? This\nquestion is central to\nseveral policy efforts like the EU AI Act and the U.S. Executive Order on Safe, Secure, and\nTrustworthy AI.\n**Status quo.**Open foundation models, defined here as models with widely available weights,\nenable greater customization and deeper inspection.\nHowever, their downstream use cannot be monitored\nor moderated. As a result, risks relating to biosecurity, cybersecurity, disinformation, and\nnon-consensual\ndeepfakes have prompted pushback.\n**Contributions.**We analyze the benefits and risks of open foundation models. In\nparticular, we present a framework to assess their*marginal*risk compared to closed\nmodels or\nexisting technology. The framework helps explain why the marginal risk is low in some cases,\nclarifies disagreements in past studies by revealing the different assumptions about risk, and\ncan help foster more\nconstructive debate going forward.\n#### Key contributions\n* **Identifying distinctive properties.**Foundation models released with widely available\nweights have\ndistinctive properties that lead to both their benefits and risks. We outline five\nproperties\nthat inform our analysis of their societal impact: broader access, greater customizability, the\nability for local inference and adaptability, an inability to rescind model weights once\nreleased, and an inability to monitor or moderate usage.\n* **Connecting properties to benefits.**Open foundation models can distribute decision-making\npower, reduce market\nconcentration,\nincrease innovation, accelerate science, and enable transparency.\nWe highlight considerations that may temper these benefits in practice (for example, model\nweights\nare\nsufficient for some forms of science, but access to training data is necessary for others and is\nnot\nguaranteed by release of weights).\n* **Developing a risk assessment framework.**We present a framework for conceptualizing the*marginal\nrisk*of open foundation models:\nthe extent to which these models increase societal risk by intentional misuse beyond\nclosed foundation models or pre-existing technologies (such as web search on the internet).\n* **Re-assessing past studies.**Surveying seven common misuse vectors described for open\nfoundation\nmodels (such as disinformation,\nbiosecurity, cybersecurity, non-consensual intimate imagery, scams), we find that past studies\ndo\nnot\nclearly assess the marginal risk in most cases.\nIn particular, we encourage more grounded research on characterizing the marginal risk,\nespecially as both model capabilities and societal defenses evolve.\n#### Benefits of Open Foundation Models\nThe distinctive properties of open foundation models allow us to critically analyze key benefits for\nopen\nfoundation models that emerge from these properties.\n##### Distributing who defines acceptable model behavior**\n*Broader access and\ngreater customizability expand who is able to decide\nacceptable model behavior.*\nDevelopers of closed foundation models exercise unilateral control in determining\nwhat is and is not acceptable model behavior.\nGiven that foundation models increasingly intermediate critical societal processes,\nmuch as social media platforms do\ntoday, the definition of what is acceptable model behavior is a consequential\ndecision that should take into account the views of stakeholders and the context\nwhere the model is applied.\nIn contrast, while developers may initially specify and control how the model\nresponds to user queries, downstream developers who use open foundation models can\nmodify them to specify alternative behavior.\nOpen foundation models allow for greater diversity in defining what model behavior\nis acceptable, whereas closed foundation models implicitly impose a monolithic view\nthat is determined unilaterally by the foundation model developer.\n##### Increasing innovation**\n*Broader access, greater customizability, and local inference\nexpand how foundation models are used to develop applications.*\nSince open foundation models can be more aggressively customized, they better support\ninnovation across a range of applications.\nIn particular, since adaptation and inference can be performed locally, application\ndevelopers can more easily adapt or fine-tune models on large proprietary datasets\nwithout data protection and privacy concerns. Similarly, the customizability of open\nmodels allows improvements such as furthering the state-of-the-art across different[languages].\nWhile some developers of closed foundation models provide mechanisms for users to opt\nout of data collection, the data storage, sharing, and usage practices of foundation\nmodel developers are not always transparent.\nHowever, the benefits of open foundation models for innovation may have limits due to\npotential comparative disadvantages in improving open foundation models over time. For\nexample, open foundation model developers generally do not have access to user feedback\nand interaction logs that closed model developers do for improving models over time.\nFurther, because open foundation models are generally more heavily customized, model\nusage becomes more fragmented and lessens the potential for strong economies of scale.\nHowever, new research directions such as[merging\nmodels] might allow open foundation\nmodel developers to reap some of these benefits (akin to open source\nsoftware).\nMore generally, the usability of foundation models strongly[influences\ninnovation]:\nfactors beyond whether a model is released openly such\nas the capabilities of the model and the quality of potential inference APIs shape\nusability.\n##### Accelerating science**\n*Broader access and greater customizability facilitate scientific\nresearch. The availability of other key assets (such as training data) would further\naccelerate scientific research.*\nFoundation models are critical to modern scientific research, within and beyond the\nfield of artificial intelligence.\nBroader access to foundation models enables greater inclusion in scientific research,\nand model weights are essential for several forms of research across AI\ninterpretability, security, and safety.\nEnsuring ongoing access to specific models is essential for the scientific\nreproducibility of research, something that has been undermined to date by the business\npractice of closed model developers to retire models\nregularly.\nAnd since closed foundation models are often instrumented by safety measures by\ndevelopers, these measures can complicate or render some research impossible.\nHowever, model weights alone are insufficient for several forms of scientific research.\nOther assets, especially the data used to build the model, are necessary.\nFor example, to understand how biases propagate, and are potentially amplified, requires\ncomparisons of data biases to model biases, which in turn requires access to the\ntraining data.\nAccess to data and other assets, such as model checkpoints, has already enabled\nwide-ranging downstream\nresearch.\nWhile some projects such as BLOOM and Pythia prioritize accessibility to such assets\nwith the stated goal of\nadvancing scientific research on foundation\nmodels, it is not common for open models in\ngeneral.\nIn fact, while evaluations have received widespread attention for their potential to\nclarify the capabilities and risks of foundation models, correctly interpreting the\nresults of evaluations requires understanding the relationship between the evaluation\ndata and a model's training data.\n##### Enabling transparency**\n*Broad access to weights enables some forms of transparency. The\navailability of other key assets (such as documentation and training data) would further\nimprove transparency.*\nTransparency is a vital precondition for responsible innovation and public\naccountability.\nYet digital technologies are plagued by problematic opacity. Widely available model\nweights enable external researchers, auditors, and journalists to investigate and\nscrutinize foundation models more deeply. In particular, such inclusion is especially\nvaluable given that the foundation model developers often underrepresent marginalized\ncommunities that are likely to be subject to the harms of foundation models.\nThe history of digital technology demonstrates that broader scrutiny, including by those\nbelonging to marginalized groups that experience harm most acutely, reveals concerns\nmissed by developers. The 2023 Foundation Model Transparency Index indicates that\ndevelopers of major open foundation models tend to be more transparent than their closed\ncounterparts.\nStill, model weights only make some types of transparency (such as\nevaluations\nof risk) possible, but they do not guarantee such transparency will manifest.\nMore generally, model weights do not guarantee transparency on the upstream resources\nused to build the foundation model (e.g., data sources, labor practices, energy\nexpenditure) nor transparency on the downstream impact of the foundation model (e.g.,\naffected markets, adverse events, usage policy enforcement).\nSuch transparency can help address prominent societal concerns surrounding bias,\nprivacy, copyright, labor, usage practices, and demonstrated harms.\n##### Mitigating monoculture and market concentration**\n*Greater customizability mitigates the harms of monoculture and\nbroader access reduces market concentration.*\nFoundation models function as infrastructure for building downstream applications,\nspanning market sectors.\nBy design, they contribute to the rise of algorithmic monoculture: many downstream\napplications depend on the same foundation model.\nMonocultures often yield poor societal resilience and are susceptible to widespread\nsystemic risk: consider the Meltdown and Spectre attacks, which led to massive security\nrisks because of the widespread dependence on Intel and ARM-based microprocessors.\nFurther, foundation model monocultures have been conjectured to lead to correlated\nfailures\nand cultural homogenization. Since open foundation models are more easily and deeply\ncustomized,\nthey may yield more diverse downstream model behavior, thereby reducing the severity of\nhomogeneous outcomes.\nBroad access to model weights and greater customizability further enable greater\ncompetition in downstream markets, helping to reduce market concentration at the\nfoundation model level from vertical cascading.\nIn the foundation model market, there are barriers to entry for low-resource actors in\ndeveloping foundation models given their significant capital costs.\nFurther, while open foundation models may increase competition in some regions of the AI\nsupply chain, they are unlikely to reduce market concentration in the highly\nconcentrated upstream markets of computing and specialized hardware providers.\n#### A Framework for Analyzing the Marginal Risk of Open Foundation Models\nTechnologists and policymakers have worried that open foundation models present risks. To better\nunderstand the nature and\nseverity of these risks, we present a framework that centers the*marginal*risk: what additional risk is society subject to because of open foundation models\nrelative to pre-existing technologies or other relevant reference points? The framework consists of\nsix parts:\n##### Threat identification**\nAll misuse analyses should systematically identify and characterize the potential\nthreats being analyzed.\nIn the context of open foundation models, this would involve naming the misuse\nvector, such as spear-phishing scams or influence operations, as well as detailing\nthe manner in which the misuse would be executed.\nTo present clear assumptions, this step should clarify the potential malicious\nactors and their resources: individual hackers are likely to employ different\nmethods and wield different resources relative to state-sponsored entities.\n##### Existing risk (absent open foundation models)**\nGiven a threat, misuse analyses should clarify the existing misuse risk in society.\nFor example, Seger et al. (2023) outline the misuse potential for open foundation\nmodels via disinformation on social media, spear-phishing scams over email, and\ncyberattacks on critical infrastructure.\nEach of these misuse vectors already are subject to risk*absent*open foundation\nmodels. So understanding the pre-existing level of risk contextualizes and baselines any\nnew risk introduced by open foundation models.\n##### Existing defenses (absent open foundation models)**\nAssuming that risks exist for the misuse vector in question, misuse analyses should\nclarify how society (or specific entities or jurisdictions) defends against these risks.\nDefenses can include technical interventions (e.g., spam filters to detect and remove\nspear-phishing emails) and regulatory interventions (e.g., laws punishing the\ndistribution\nof child sexual abuse material).\nUnderstanding the current defensive landscape informs the efficacy, and sufficiency,\nwith which new risks introduced by open foundation models will be addressed.\n##### Evidence of marginal risk of open FMs**\nThe threat identification, paired with an analysis of existing risks and defenses,\nprovides the conceptual foundation for reasoning about the risks of open foundation\nmodels.\nNamely, subject to the status quo, we can evaluate the marginal risk of open\nfoundation models.\nBeing aware of existing risk clarifies instances where open foundation models simply\nduplicate existing risk (e.g., an open language model providing biological information\navailable via Wikipedia).\nSimilarly, being aware of existing defenses clarifies instances where open foundation\nmodels introduce concerns that are well-addressed by existing measures.\nConversely, we can identify critical instances where new risks are introduced (e.g.,\nfine tuning models to create non-consensual intimate imagery of specific people) or\nwhere existing defenses\nwill be inadequate (e.g., AI-generated child sexual abuse material may overwhelm\nexisting law enforcement resources).\nFurther, the marginal risk analysis need not only be conducted relative to the status\nquo, but potentially relative to other (possibly hypothetical) baselines.\nFor example, understanding the marginal risk of open release relative to a more\nrestricted release (e.g., API release of a closed foundation model) requires reasoning\nabout the relevant existing defenses for said restricted release.\nThis perspective ensures greater care is taken to not assume that closed releases are\nintrinsically more safe and, instead, to interrogate the quality of existing defenses.\n##### Ease of defending against new risks**\nWhile existing defenses provide a baseline for addressing new risks introduced by open\nfoundation models, they do not fully clarify the marginal risk.\nIn particular, new defenses can be implemented or existing defenses can be modified to\naddress the increase in overall risk.\nTherefore, characterizations of the marginal risk should anticipate how defenses will\nevolve in reaction to risk: for example, (open) foundation models may also contribute to\nsuch defenses (e.g., the creation of better disinformation detectors or code fuzzers).\n##### Uncertainty and assumptions**\nFinally, it is imperative to articulate the uncertainties and assumptions that underpin\nthe risk assessment framework for any given misuse risk.\nThis may encompass assumptions related to the trajectory of technological development,\nthe agility of threat actors in adapting to new technologies, and the potential\neffectiveness of novel defense strategies.\nFor example, forecasts of how model capabilities will improve or how the costs of model\ninference will decrease would influence assessments of misuse efficacy and scalability.\nThe risk framework enables precision in discussing the misuse risk of open foundation models and is\nbased on the threat modeling framework in computer security. For example, without clearly\narticulating the marginal risk of biosecurity concerns stemming from the use of open language\nmodels, researchers might come to completely different conclusions about whether they pose risks:\nopen language models can generate accurate information about pandemic-causing pathogens, yet such\ninformation is publicly available on the Internet, even without\nthe use of open language models.\nUsing this framework, we assess prior studies that span different risk vectors (biosecurity risk,\ncybersecurity risk, disinformation, non-consensual intimate imagery, child sexual abuse materials,\nspear-phishing scams, and voice-cloning scams) in our\npaper.\nWe find that the risk analysis is incomplete for six of the seven studies we analyze.\nTo be clear, incomplete assessments do not necessarily indicate that the analysis in prior studies is\nflawed, only\nthat these studies, on their own, are insufficient evidence to demonstrate increased marginal societal\nrisk from\nopen foundation models.\nScoring studies that analyze the risk from open foundation models using our framework.\n⬤indicates the step of our framework is clearly addressed; ◑indicates partial completion;\n◯indicates the step is absent in the misuse analysis.|Misuse risk|Paper|Threat identification|Existing risk|Existing defenses|Marginal risk evidence|Ease of defense|Uncertainty/ assumptions|\nSpear-phishing scams|[Hazell (2023)] |![full circle] |![full circle] |![full circle] |![full circle] |![full circle] |![full circle] |\nCybersecurity risk|[Seger\net al. (2023)] |![half circle] |![empty circle] |![half circle] |![empty circle] |![half circle] |![empty circle] |\nDisinformation|[Musser (2023)] |![full circle] |![half circle] |![empty circle] |![empty circle] |![half circle] |![full circle] |\nBiosecurity risk|[Gopal et al. (2023)] |![full circle] |![empty circle] |![half circle] |![empty circle] |![empty circle] |![empty circle] |\nVoice-cloning|[Ovadya et al. (2019)] |![full circle] |![half circle] |![half circle] |![half circle] |![half circle] |![full circle] |\nNon-consensual intimate imagery|[Lakatos\n(2023)] |![full circle] |![half circle] |![empty circle] |![half circle] |![half circle] |![empty circle] |\nChild sexual abuse material|[Thiel\net al. (2023)] |![full circle] |![full circle] |![full circle] |![full circle] |![full circle] |![full circle] |\n#### Authors\nThe 25 authors span 16 organizations across academia, industry, and civil society.\n*\\* denotes equal contribution. Contact: sayashk@princeton.edu, nlprishi@stanford.edu*\n|Name|Affiliation|\nSayash Kapoor \\*|Princeton University|\nRishi Bommasani \\*|Stanford University|\nKevin Klyman|Stanford University|\nShayne Longpre|Massachusetts Institute of Technology|\nAshwin Ramaswami|Georgetown University|\nPeter Cihon|GitHub|\nAspen Hopkins|Massachusetts Institute of Technology|\nKevin Bankston|Center for Democracy and Technology, Georgetown University|\nStella Biderman|Eleuther AI|\nMiranda Bogen|Center for Democracy and Technology, Princeton University|\nRumman Chowdhury|Humane Intelligence|\nAlex Engler|Work done while at Brookings Institution|\nPeter Henderson|Princeton University|\nYacine Jernite|Hugging Face|\nSeth Lazar|Australian National University|\nStefano Maffulli|Open Source Initiative|\nAlondra Nelson|Institute for Advanced Study|\nJoelle Pineau|Meta|\nAviya Skowron|Eleuther AI|\nDawn Song|University of California, Berkeley|\nVictor Storchan|Mozilla AI|\nDaniel Zhang|Stanford University|\nDaniel E. Ho|Stanford University|\nPercy Liang|Stanford University|\nArvind Narayanan|Princeton University|\n*Note: The views and\nopinions expressed in this paper are those of the authors and do not necessarily reflect the\nofficial\npolicy or position of their employers.*",
    "length": 19884,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Policy | Stanford HAI",
    "url": "https://hai.stanford.edu/policy",
    "text": "[Skip to content] \n[] \n[] \n* [] \n* [] \n* [] \n* [] \n* [] \n* [] \n[] \nPolicy | Stanford HAI\n# Policy\nHAI produces critical scholarship on AI governance and convenes national and global AI discussions. We engage with leaders and policymakers worldwide to ensure AI is designed to augment human capabilities and benefit society.\n## Recently Published in Policy Publications\n[See all Publications] \n##### [Response to OSTP&#x27;s Request for Information on Accelerating the American Scientific Enterprise] \n[Rishi Bommasani,] [John Etchemendy,] [Surya Ganguli,] [Daniel E. Ho,] [Guido Imbens,] [James Landay,] [Fei-Fei Li,] [Russell Wald] \nQuick ReadDec 26, 2025\nResponse to Request\n![] \nStanford scholars respond to a federal RFI on scientific discovery, calling for the government to support a new “team science” academic research model for AI-enabled discovery.\n#### [Response to OSTP&#x27;s Request for Information on Accelerating the American Scientific Enterprise] \n[Rishi Bommasani,] [John Etchemendy,] [Surya Ganguli,] [Daniel E. Ho,] [Guido Imbens,] [James Landay,] [Fei-Fei Li,] [Russell Wald] \nQuick ReadDec 26, 2025\nStanford scholars respond to a federal RFI on scientific discovery, calling for the government to support a new “team science” academic research model for AI-enabled discovery.\n[\nSciences (Social, Health, Biological, Physical)\n] [\nRegulation, Policy, Governance\n] \n![] \nResponse to Request\n##### [Beyond DeepSeek: China&#x27;s Diverse Open-Weight AI Ecosystem and Its Policy Implications] \n[Caroline Meinhardt,] [Sabina Nong,] [Graham Webster,] [Tatsunori Hashimoto,] [Christopher Manning] \nDeep DiveDec 16, 2025\nIssue Brief\n![] \nAlmost one year after the “DeepSeek moment,” this brief analyzes China’s diverse open-model ecosystem and examines the policy implications of their widespread global diffusion.\n#### [Beyond DeepSeek: China&#x27;s Diverse Open-Weight AI Ecosystem and Its Policy Implications] \n[Caroline Meinhardt,] [Sabina Nong,] [Graham Webster,] [Tatsunori Hashimoto,] [Christopher Manning] \nDeep DiveDec 16, 2025\nAlmost one year after the “DeepSeek moment,” this brief analyzes China’s diverse open-model ecosystem and examines the policy implications of their widespread global diffusion.\n[\nFoundation Models\n] [\nInternational Affairs, International Security, International Development\n] \n![] \nIssue Brief\n##### [Response to FDA&#x27;s Request for Comment on AI-Enabled Medical Devices] \n[Desmond C. Ong,] [Jared Moore,] [Nicole Martinez-Martin,] [Caroline Meinhardt,] [Eric Lin,] [William Agnew] \nQuick ReadDec 02, 2025\nResponse to Request\n![] \nStanford scholars respond to a federal RFC on evaluating AI-enabled medical devices, recommending policy interventions to help mitigate the harms of AI-powered chatbots used as therapists.\n#### [Response to FDA&#x27;s Request for Comment on AI-Enabled Medical Devices] \n[Desmond C. Ong,] [Jared Moore,] [Nicole Martinez-Martin,] [Caroline Meinhardt,] [Eric Lin,] [William Agnew] \nQuick ReadDec 02, 2025\nStanford scholars respond to a federal RFC on evaluating AI-enabled medical devices, recommending policy interventions to help mitigate the harms of AI-powered chatbots used as therapists.\n[\nHealthcare\n] [\nRegulation, Policy, Governance\n] \n![] \nResponse to Request\n##### [Moving Beyond the Term &quot;Global South&quot; in AI Ethics and Policy] \n[Evani Radiya-Dixit,] [Angèle Christin] \nQuick ReadNov 19, 2025\nIssue Brief\n![] \nThis brief examines the limitations of the term &quot;Global South&quot; in AI ethics and policy, and highlights the importance of grounding such work in specific regions and power structures.\n#### [Moving Beyond the Term &quot;Global South&quot; in AI Ethics and Policy] \n[Evani Radiya-Dixit,] [Angèle Christin] \nQuick ReadNov 19, 2025\nThis brief examines the limitations of the term &quot;Global South&quot; in AI ethics and policy, and highlights the importance of grounding such work in specific regions and power structures.\n[\nEthics, Equity, Inclusion\n] [\nInternational Affairs, International Security, International Development\n] \n![] \nIssue Brief\nSkip Carousel Content\n![Fei-Fei Li and Condoleezza Rice] \n[\nLearn about policymaker education\n] \nHAI Co-Director Fei-Fei Li and Advisory Council Member Condoleezza Rice speaking at the Congressional Boot Camp on AI.\nChristine Baker\n[\nLearn about policymaker education\n] \n![] \nHAI Senior Fellow Daniel E. Ho testified before the Senate Committee on Homeland Security and Governmental Affairs in May 2023 on AI in government.\n![] \n[\nRead Article\n] \nHAI Faculty Affiliate Jeff Hancock and Sanmi Koyejo discussed AI&#x27;s potential and misuse during a workshop with members of the Association of Southeast Asian Nations community.\n[\nRead Article\n] \n![] \n[\n**Read**the article\n] \nThe[**Stanford Robotics Center**] and HAI are partnering to define the responsible use of AI in the robotics field and produce original policy insights to help govern AI-enabled robotics.\n[\n**Read**the article\n] \n## Policy Publications\nStanford HAI leverages the university’s strength across disciplines—including computer science, business, economics, education, law, literature, medicine, neuroscience, philosophy, sustainability, and more—to create innovative research on AI.\nWe translate cutting-edge, multidisciplinary AI research for the policy audience and produce original AI-related policy research to equip policymakers with tools to understand and govern the technology.\n[\nView all**Policy Publications**\n] [\nSign up for the**Policy Newsletter**\n] \n## Training for Policymakers\nPolicymakers and civil servants are at the front lines of decision-making on emerging technologies such as AI. Recognizing the valuable role they play in the AI governance ecosystem, Stanford HAI has developed specialized training programs to meet their needs.\n![Amy Zegart presenting at Congressional Boot Camp] \n### [Congressional Boot Camp on AI] \nHAI hosts a 3-day boot camp at Stanford for senior congressional staffers every August. This bipartisan, bicameral program breaks down complex AI concepts and unpacks the impact of AI on healthcare, education, climate, and democracy featuring scholars, Silicon Valley leaders, and civil society pioneers.\n### [AI Training Series for U.S. Government Employees] \nHAI tailored this program in collaboration with the General Services Administration and the White House Office of Management and Budget to inform and educate U.S. government employees at all levels.\n### [AI Fundamentals for Public Servants] \nHAI launched an online course in partnership with the learning platform Apolitical and Stanford Online, featuring instruction from multiple HAI experts. The course is provided free-of-charge and can be accessed on-demand for a broad audience of global policymakers.\n## Policy Initiatives &amp; Research\nAt HAI, we believe AI should be inspired by human intelligence, developed to augment human capabilities, and designed and applied with consideration for its impact on people and society.\n![Patrick Suppes Family Professor of Humanities and Sciences; Denning Co-Director of Stanford HAI, and provost emeritus John Etchemendy introduces U.S. Representative Anna Eshoo, co-chair of the Congressional AI Caucus and co-sponsor for the CREATE AI Act, before her keynote address at the Unlocking Public Sector AI Innovation Seminar at Gates Computer Science Building on Tuesday, October 31, 2023 in Stanford, California. Panelists Jennifer King, Fei-Fei Li, and Russ B. Altman addressed the packed house on developing advancements and innovations in the field of AI, significant in part by the previous days passing of an Executive Order by American President Joe Biden. (] \n### [National AI Research Resource] \nHAI led efforts with top universities and lawmakers to create a National AI Research Resource that would provide researchers with compute power and government datasets needed for education and research, and democratize AI research, education, and innovation.\n### [Healthcare AI Policy] \nHAI&#x27;s multidisciplinary committee of Stanford faculty and scholars conducts interdisciplinary research, convenes multi-stakeholder discussions, and develops tangible recommendations for policymakers that help ensure healthcare AI can benefit patients, doctors, and developers alike.\n### [Tracking U.S. Executive Action on AI] \nHAI scholars have analyzed the state of implementation of a variety of AI-related executive actions released since 2019 across administrations, providing insights into federal efforts to lead in and govern AI.\n### [Governance of Foundation Models] \nIn collaboration with the Stanford Center for Research on Foundation Models (CRFM) and the Regulation, Evaluation, and Governance (RegLab), HAI blends knowledge with comprehensive legal and policy insights through high-impact research to inform U.S. and global AI governance discussions and legislative proposals.\n## Get Involved\n### [Tech Ethics &amp; Policy Summer Fellowships] \nHAI is offering a fully funded summer fellowship for graduate students to gain hands-on experience in AI policy across D.C., from Congress to think tanks, applying their technical expertise to shape responsible technology policy.\n### [AI Policy Working Group] \nA unique opportunity for**Stanford graduate students and researchers**to bridge the gap between cutting-edge AI research and pressing public policy challenges.\n### [Student Policy Opportunities] \nLearn more about opportunities for Stanford students.\n### [Contact Us] \nEmail**hai-policy@stanford.edu**for general inquiries about upcoming policy research publications, training programs, briefings, or other policy-related work.\n### [Subscribe] \nSign up to receive the latest insights and opportunities as we explore the intersection of policy, society, and AI.\nResponse to Request\n![] \n#### [Response to OSTP&#x27;s Request for Information on Accelerating the American Scientific Enterprise] \n[Rishi Bommasani,] [John Etchemendy,] [Surya Ganguli,] [Daniel E. Ho,] [Guido Imbens,] [James Landay,] [Fei-Fei Li,] [Russell Wald] \n[Sciences (Social, Health, Biological, Physical)] [Regulation, Policy, Governance] Quick ReadDec 26\nStanford scholars respond to a federal RFI on scientific discovery, calling for the government to support a new “team science” academic research model for AI-enabled discovery.\nIssue Brief\n![] \n#### [Beyond DeepSeek: China&#x27;s Diverse Open-Weight AI Ecosystem and Its Policy Implications] \n[Caroline Meinhardt,] [Sabina Nong,] [Graham Webster,] [Tatsunori Hashimoto,] [Christopher Manning] \n[Foundation Models] [International Affairs, International Security, International Development] Deep DiveDec 16\nAlmost one year after the “DeepSeek moment,” this brief analyzes China’s diverse open-model ecosystem and examines the policy implications of their widespread global diffusion.\nResponse to Request\n![] \n#### [Response to FDA&#x27;s Request for Comment on AI-Enabled Medical Devices] \n[Desmond C. Ong,] [Jared Moore,] [Nicole Martinez-Martin,] [Caroline Meinhardt,] [Eric Lin,] [William Agnew] \n[Healthcare] [Regulation, Policy, Governance] Quick ReadDec 02\nStanford scholars respond to a federal RFC on evaluating AI-enabled medical devices, recommending policy interventions to help mitigate the harms of AI-powered chatbots used as therapists.",
    "length": 11280,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Generative AI in Higher Education: Seeing ChatGPT Through Universities' Policies, Resources, and Guidelines",
    "url": "https://scale.stanford.edu/ai/repository/generative-ai-higher-education-seeing-chatgpt-through-universities-policies-resources",
    "text": "Generative AI in Higher Education: Seeing ChatGPT Through Universities&#039; Policies, Resources, and Guidelines | SCALE Initiative\n[Skip to main content] \n[![Stanford University]] \n## Search and Filter\nWhat is the application?\nTeaching –Instructional Materials[**] \nTeaching –Assessment and Feedback[**] \nTeaching –Professional Learning[**] \nLearning –Student Support[**] \nCommunicating / Social Tools[**] \nOrganizing[**] \nAnalyzing[**] \nWho is the user?\nStudent[**] \nParent/Caregiver[**] \nEducator[**] \nSchool Leader[**] \nOthers[**] \nWhich age?\n0-3 years\nElementary (PK5)\nMiddle School (6-8)\nHigh School (9-12)\nPost-Secondary\nAdult\nWhy use AI?\nEfficiency[**] \nOutcomes –Literacy[**] \nOutcomes –Numeracy[**] \nOutcomes –Other Academic[**] \nOutcomes –Differentiation[**] \nOutcomes –Social Emotional[**] \nOutcomes –Durable Skills[**] \nReimagined Schooling[**] \nOther[**] \nStudy design\nDescriptive –Implementation and Use[**] \nDescriptive –Product Development[**] \nImpact –Randomized Controlled Trial[**] \nImpact –Quasi-experimental[**] \nSystematic Review[**] \nTechnical –Computational\nQuantitative –Others\n## Submit a research study\nContribute to the repository:\n[Add a paper] \n# Generative AI in Higher Education: Seeing ChatGPT Through Universities&#039; Policies, Resources, and Guidelines\nAuthors\nHui Wang,\nAnh Dang,\nZihao Wu,\nSon Mac\nDate\n07/2024\nPublisher\narXiv\nLink\n[https://arxiv.org/pdf/2312.05235] \nThe advancements in Generative Artificial Intelligence (GenAI) provide opportunities to enrich educational experiences, but also raise concerns about academic integrity. Many educators have expressed anxiety and hesitation in integrating GenAI in their teaching practices, and are in needs of recommendations and guidance from their institutions that can support them to incorporate GenAI in their classrooms effectively. In order to respond to higher educators' needs, this study aims to explore how universities and educators respond and adapt to the development of GenAI in their academic contexts by analyzing academic policies and guidelines established by top-ranked U.S. universities regarding the use of GenAI, especially ChatGPT. Data sources include academic policies, statements, guidelines, and relevant resources provided by the top 100 universities in the U.S. Results show that the majority of these universities adopt an open but cautious approach towards GenAI. Primary concerns lie in ethical usage, accuracy, and data privacy. Most universities actively respond and provide diverse types of resources, such as syllabus templates, workshops, shared articles, and one-on-one consultations focusing on a range of topics: general technical introduction, ethical concerns, pedagogical applications, preventive strategies, data privacy, limitations, and detective tools. The findings provide four practical pedagogical implications for educators in teaching practices: accept its presence, align its use with learning objectives, evolve curriculum to prevent misuse, and adopt multifaceted evaluation strategies rather than relying on AI detectors. Two recommendations are suggested for educators in policy making: establish discipline-specific policies and guidelines, and manage sensitive information carefully.\nWhat is the application?\n[Teaching –Instructional Materials],\n[Teaching –Assessment and Feedback],\n[Teaching –Professional Learning],\n[Learning –Student Support] \nWho is the user?\n[Student],\n[Educator] \nWho age?\n[Post-Secondary] \nWhy use AI?\n[Outcomes –Literacy],\n[Outcomes –Durable Skills],\n[Other] \nStudy design\n[Descriptive –Implementation and Use],\n[Quantitative –Others] \n[![Stanford University]] \n* [Stanford Home] \n* [Maps &amp; Directions] \n* [Search Stanford] \n* [Emergency Info] \n* [Privacy] \n* [Copyright] \n* [Trademarks] \n* [Non-Discrimination] \n* [Accessibility] \n©Stanford University. Stanford, California 94305.",
    "length": 3860,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Generative AI for Education Hub",
    "url": "https://scale.stanford.edu/ai",
    "text": "Generative AI for Education Hub\n[Skip to main content] \n[![Stanford University]] \n[![AI Hub for Education]] ### **Moving from AI Promise to Learning Impact**\n**AI Hub for Education**delivers trusted research, insights, and tools for K12 education leaders to leverage generative AI to benefit students, schools, and learning.\n![] \n## The Challenge\nGenerative AI is transforming education at lightning speed, with schools and districts nationwide grappling with how to harness its power responsibly and effectively. With new technology, a fast changing market, and limited data on the effectiveness of new tools, it is difficult for K-12 system leaders to make informed decisions about AI implementation. Superintendents, state K-12 leaders, and policymakers face unprecedented challenges: eliminating bias, privacy concerns, and adapting learning models to these new capacities - all while ensuring that students, teachers, and schools benefit.\n****\n## Our Solution\nThe Generative AI for Education Hub is the trusted source for superintendents and state/federal K-12 leaders on what works for leveraging generative AI to benefit students, schools, and learning. Schools need clear and actionable guidance to make the right decisions. We partner with school districts, policymakers, and ed tech innovators to leverage data, research effects, and bridge the gap between AI’s potential and real-world impact. From practical tools for next semester to an aspirational vision of learning reimagined supported by genAI, we help education leaders with pragmatic answers to solve their most pressing problems.\n## What We Offer\n* **\n### Cutting-Edge Research\nWe explore the most promising uses of AI in education, backed by rigorous, real-world studies.\n* **\n### Practical Tools and Frameworks\nFrom typologies to policy briefs, our tools help decision-makers quickly assess and implement AI solutions.\n* **\n### Expert Guidance\nOur team of leading researchers and education experts provide insights that help you stay ahead of the curve.\n[![Stanford University]] \n* [Stanford Home] \n* [Maps &amp; Directions] \n* [Search Stanford] \n* [Emergency Info] \n* [Privacy] \n* [Copyright] \n* [Trademarks] \n* [Non-Discrimination] \n* [Accessibility] \n©Stanford University. Stanford, California 94305.",
    "length": 2276,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Stanford AI Playground FAQs | University IT",
    "url": "https://uit.stanford.edu/aiplayground/faqs",
    "text": "Stanford AI Playground FAQs | University IT[Skip to main content] \n[![Stanford] University IT] Main Menu\nTopic Menu\n# Stanford AI Playground FAQs\n* [AI (General)] \n* [About the AI Playground] \n* [Using the AI Playground] \n* [Troubleshooting Common Errors] \n* [Data Privacy and Security] \nHave questions about the Stanford AI Playground? You’re in the right place. Here, we’ve compiled answers to the most common questions about the platform. If your question isn't answered here or in the[AI Playground Quick Start Guide], please[request help] from the team.\n## AI (General)\nWhat is GenAI?\nGenerative Artificial Intelligence, also known as GenAI, is a type of AI can generating original text, visual, and audio content by learning patterns from provided samples of human generated data.\nExplore[overall GenAI frequently asked questions].\nWhat is an LLM?\nLLM stands for large language model, and is a type of Generative AI which specializes in text-based content and natural language. By using LLMs to enhance the interface between the person requesting content and the generative AI tool creating the content, LLMs can help generate more specific text, images, and custom code in response to prompts.\nExplore[overall GenAI frequently asked questions].\nWhat is a token? What is tokenization?\nLarge language models (LLMs) will break down your prompts, as well as the responses they generate, into smaller chunks known as tokens. This tokenization makes the data more manageable for the LLMs and assist it in processing your data.\nThere are many methods of tokenization, and this process can vary between models. Some models may break your prompts down into individual words, subwords, or even single characters. This can change how your data is interpreted by the LLMs and is one of the many factors which can lead to receiving different answers to the same prompt.\nPlease note that there is a limit to how many tokens any AI model can handle at one time. To learn more about this context limit, see the next FAQ itme.\nWhat are the context limits of LLMs?\nYou are likely used to referring to data in regards to file size such as bytes, megabytes (MBs), or gigabytes (GBs). However, the limits for LLMs are generally measured in the number of tokens needed instead of file size. This is generally referred to as the \"context limit\" and this can vary model to model.\nTokens and context limits do not translate directly to file size. This is partially due to variance in the method of tokenization across models. A very rough rule of thumb for translating file size into context limit, is that every four English characters translates to roughly one token. That means that each token might average around 4 bytes for typical English language text. Most models have a context size between 32,000 and 200,000 tokens. Even models with a 1,000,000 token context limit would fail to process the entirety of a 5MB text file.\nIt is important to note that the context limit of a model isn't noting how much it can intake or output at one time. Instead, this limit refers to the total length of all tokens that it can keep track of across an entire conversation.\nWhat are hallucinations and why do they happen?\nHallucinations are when AI models generate output that is factually incorrect or entirely made up, even if it sounds correct. Hallucinations are an inherent limitation of large language models (LLMs) and cannot be fully eliminated at this time.\nWhile hallucinations are often seen as a weakness, they can also be a strength. Hallucinations sometimes provide necessary creative or innovative responses that can spark new ideas in research or problem-solving. While this can aid in creative pursuits, and even research, hallucinations are just another reason it is critically important to verify the output of AI models.\nWhat is the estimated energy cost of using GenAI?\nEach GenAI model and tool uses energy differently. While the AI landscape continues to evolve, research and reporting currently indicate these energy costs:\n![Generating one image with GenAI with a commercially-available large language model (LLM) uses as much energy as fully charging a smartphone. One text-based query to a GenAI LLM uses less than 1% of the energy required to charge a smartphone or approximately as much electricity as powering one LED light bulb for about 20 minutes. One typical Google search query Uses about 1/10 of the energy as a text-based query with the ChatGPT LLM. AI-powered Google search has a higher estimated energy cost than a text-based ChatGPT prompt. ] \nFor sources and additional reading, visit:\n* [A bottle of water per email: the hidden environmental costs of using AI chatbots - Washington Post] \n* [Tech firms conceal water and power demands of AI computing - Los Angeles Times] \n* [Making an image with generative AI uses as much energy as charging your phone - MIT Technology Review] \nWhere can I learn more about the university's guidance on the use of GenAI tools?\nUniversity Communications published[AI Guidelines for Marketing and Communications]. The guidelines promote responsible, ethical, and legally compliant practices that align with the university’s mission and values.\nAdditionally, University IT has compiled a list of several university specific resources on AI. You can view the complete list of these resources at[GenAI Topics and Services List].\nIs there a list of Stanford approved AI tools?\nThe Responsible AI at Stanford webpage hosts the[GenAI Tool Evaluation Matrix]. This matrix contains a list of AI tools, what they do, their availability, and their status at the university. Please refer to that resource for more information about what tools are approved for use with Stanford data.\nAm I allowed to use DeepSeek and is it safe?\n**A secure, local version of DeepSeek is available within the AI Playground.**As a reminder, high-risk data in attachments or prompts is*not*approved for use within any AI Playground model.\nOutside of the AI Playground, please refrain from using models like DeepSeek-R1, hosted by the non-US company DeepSeek, for any Stanford business. This includes connecting to DeepSeek APIs over the internet or using the DeepSeek mobile application to process confidential data, such as Protected Health Information (PHI) or Personally Identifiable Information (PII).Currently, there is no enforceable contract in place between Stanford and DeepSeek that meets the risk management standards for HIPAA compliance and cybersecurity safeguards. As a result, using non-US DeepSeek models poses unacceptable risks to data security and regulatory compliance.\n## About the AI Playground\nWhat are the benefits of the Stanford AI Playground?\nThe AI Playground is designed to let you learn by doing. With this environment, you can practice optimizing work-related tasks (not using high-risk data), such as: content generation, administrative tasks, coding and debugging, analyzing data, and more.\nThe Playground is protected by Stanford's single sign on system and all information shared with the AI Playground stays within the Stanford environment.\nCan the AI Playground generate images, charts, graphs, etc. like ChatGPT and Anthropic?\nYes. The AI Playground is capable of many advanced features like image generation, creating charts and graphs based on provided data, as well as writing and rendering programs on screen. Below are a few screenshots of these features in action.\nImage generation:\n![Image generation screenshot] \nCreating a chart from a provided data set:\n![Graph/chart creation screenshot] \nBuilding a dashboard from example and attached data:\n![Dashbaord rendering screenshot] \nIf the AI Playground states that it cannot directly render or run code, you may need to enable the Artifacts feature. Please view the related FAQ item in the Troubleshooting Common Errors section of the FAQ.\nWhat is coming next with the AI Playground? Is there a roadmap?\n![Playground Roadmap Update Dec 2025] \nThe team has many exciting new features planned for the future. Some of these possible new features include:\n* Admin Guide AI assistant agent (released Mar. 20, 2025)\n* [AI API Gateway] provides direct API access to models (released Apr. 7, 2025)\n* Faculty Handbook and DoResearch AI agents (released Jul. 04, 2025)\n* Stanford AI helper agent to answer questions about GenAI, the Playground, and Stanford AI guidelines\n* Stanford prompt library\n* Saved prompt sharing\n* Ability to save files within saved prompts\n* Text to SQL agents\n* Conversational memory about the user\n* AI agents integrated with SU enterprise systems\n* Document, slide deck, spreadsheet creation\n* AI agents for other Stanford services\nDue to the speed at which GenAI tools are developing, it can be difficult to maintain a list of models on the roadmap. Below is a list of some models we are reviewing for a possible future release within the AI Playground:\n* DeepSeek R1 model (released Feb. 13, 2025)\n* Scholar AI Research Assistant (released Apr. 17, 2025)\n* Sora\n* Whisper\n* DeepResearch models\n* Perplexity\n* Vision models\nPlease note that not all new AI features can be added to the Playground. The roadmap is subject to change, and it is possible that items on the roadmap may not be able to be implemented.\nFor a complete list of past updates, please view the[AI Playground Release Notes] page.\nI have a great idea for the Playground. How do I make a suggestion for the roadmap?\nThe AI Playground's roadmap is largely driven by feedback from the Stanford community. If you would like to make a suggestion, please use the[Feedback Form] linked here or at the bottom of the AI Playground itself.\nThe Feedback Form should not be used to report technical issues or ask questions. You will not receive a response from submitting suggestions to the Feedback Form.\nHow do I request an API key for direct access to the models in the AI Playground?\nDirect API access to the models in the AI Playground was released on April 7, 2025.\nLearn more about this exciting new feature at the[AI API Gateway service page] and request the creation of a new API key via the[Add AI API Gateway Key] form.\nDoes the Playground support Model Context Protocol (MCP) or custom agents?\nNot presently. While the open source software used to power the Playground does support MCP, it is only being used for some of the integration features. The UIT team is not able to make agent configurable MCP customizations available to Stanford users at this time.\nYou can build your own AI tool using the[AI API Gateway]. The AI API Gateway allows you to create your own AI tools and integrations using the same models available in the AI Playground. Learn more about this exciting new feature at the[AI API Gateway service page] and request the creation of a new API key via the[Add AI API Gateway Key form].\nHow often is the AI Playground updated? How often are new models released?\nThe UIT team wants to make sure the Playground is a safe and useful space for the Stanford community. As a result, the AI Playground is updated regularly. This includes feature updates, new models, platform updates, security enhancements, and performance improvements.\nThe release of new AI models is largely driven by feedback from the Stanford community. For more information on our plans for the future, check the FAQ entry above titled, \"Is there a roadmap? What is coming next with the AI Playground?\"\nFor information about past updates, please review the[AI Playground Release Notes] page. That page contains a list of every major update to the AI Playground since its release in the summer of 2024.\nHow often is the information used in the Stanford specific agents updated? (e.g. Admin Guide, DoResearch Handbook, etc.)\nThe information in the Stanford specific agents is updated monthly. This includes the agents related to the Admin Guide, DoResearch Handbook, and Faculty Handbook.\nDoes the AI Playground have conversation sharing or team collaboration capabilities?\nYou are able to share your conversations with other Stanford users. The AI Playground team is also working on a feature which will allow you to share your saved prompts with other Stanford users.\nMore advanced collaboration capabilities are not available within the AI Playground at this time.\nIs the Stanford AI Playground a custom software or a product?\nThe AI Playground is built on the opensource LibreChat platform, with a flexible infrastructure behind the scenes that allows the UIT team room to change and grow the Playground over time.\nDoes UIT limit the available context windows of models available in the AI Playground?\nNo. UIT does not restrict or impose any limits on context length. Each model in the Playground runs with the full context window supported by that model specifically. This means you can take advantage of the larger capacities available in modern models without any restrictions.\nWho can access the AI Playground?\nThe AI Playground is accessible to all Stanford faculty,staff, students, Postdocs, Visiting Scholars, and sponsored affiliates. The Playground works with both full and base SUNet IDs.\nPlease note:Based on feedback received duringthis pilot phase,the pilot duration could change or conclude based on what we learn.\nTo access the AI Playground, a SUNet ID must be a member of one of the following groups:\n**stanford:faculty**\n→stanford:faculty-affiliate\n→stanford:faculty-emeritus\n→stanford:faculty-onleave\n→stanford:faculty-otherteaching\n**stanford:staff**\n→stanford:staff-academic\n→stanford:staff-emeritus\n→stanford:staff-onleave\n→stanford:staff-otherteaching\n**stanford:staff-affiliate**\n**stanford:staff-casual**\n**stanford:staff-retired**\n**stanford:staff-temporary**\n**stanford:staffcasual**\n**stanford:stafftemp**\n**stanford:student**\n→stanford:student-onleave\n**stanford:student-ndo**\n**stanford:student-postdoc**\n**stanford:affiliate:visitscholarvs**\n**stanford:affiliate:visitscholarvt**\n**stanford:affiliate-sponsored**\nPlease note that students, postdocs, and visiting scholars do not have access to image generation models at this time.\nWhat is the Carbon Footprint of the Stanford AI Playground in particular?\nThe UIT team takes the matter of sustainability seriously. In addition to the technical factors weighed in selecting the best location for this service, the team evaluated different cloud infrastructure options for their carbon footprint. The AI Playground's cloud infrastructure is hosted in regions that are rated \"Low Carbon\". You can learn more about this rating here:[Carbon free energy for Google Cloud regions].\nWhere can I learn even more about AI and the Stanford AI Playground in particular?\nThe AI Playground team is a very small group focused on the development of the AI Playground. We have worked with several other groups around campus to provide options for individuals and teams who might need additional assistance getting started with Artificial Intelligence:\nGenAI Topics and Services List:[https://uit.stanford.edu/ai/topics-services-list] \nUIT Tech Training custom consultations:[https://uit.stanford.edu/service/techtraining/custom] \nAI Demystified trainings focused on AI:[https://uit.stanford.edu/service/techtraining/ai-demystified] \nPrompting and AI resources on the GenAI site:[https://uit.stanford.edu/ai/overview] \nStanford AI Tinkery hands on assistance:[https://ai-tinkery.stanford.edu/] \nSU Library guide to AI in Academic Research: [https://guides.library.stanford.edu/c.php?g=1472003] \nAdditionally, the UIT Tech Training team offers an interactive and beginner-friendly course titled \"AI Playground 101: Practice and Exploration\". This class is a three hour long introduction to key GenAI concepts, effective prompt engineering, responsible AI use, and it explores the AI Playground through instructor led hands-on activities. The instructor for the class,[Joshua Barnett], helped lead the development of the AI Playground and many other UIT initiatives on AI. Learn more about and sign up for this course at:[Stanford AI Playground 101: Practice and Exploration Class Overview].\n## Using the AI Playground\nThere are so many models available, how do I know which one is best for my needs?\nEach vendors and their specific LLMs have different strengths and weaknesses. By providing access to different models, the UIT hopes to provide a plethora of options to the campus community.\nThe Quick Start Guide section titled \"[3. Find and explore LLMs] \" lists out the specifics strengths of each model. Likewise, the section titled \"[5. Integrate Azure Assistants and agents] \" breifly explains the purpose of each available agent.\nPlease note that there isn’t always a single best model for every task. In fact, many models can perform well depending on how the prompt is written, the complexity of the request, and the format of the input. Often, two or more models may excel at the same task in slightly different ways. The AI Playground is designed to encourage hands on exploration to learn which ones work best for your needs.\nThe chart below gives some recommendations for which models may excel at performing certain tasks.\n|**Example Task**|**Recommended Models**|\nSummarize a document or PDF|gpt-5.2, claude-4.5-sonnet|\nAnswer questions about a long research paper or book|claude-4.5-sonnet, gemini-2.5-pro, gpt-4o|\nAnalysis academic writing|claude-4.5-sonnet, GPT o1|\nAuthor a creative story, poem, or world building content|gpt-5.2, claude-4.5-sonnet|\nWrite or reply to a friendly email|gemini-2.5-pro, gpt-5.2|\nRespond to a grant or research communication|claude-4.5-sonnet, GPT o1|\nGenerate marketing materials (emails and social media)|gpt-5.2, claude-4.5-sonnet|\nSummarize and format meeting notes or transcripts|gpt-5-mini, claude-3-haiku|\nWrite or debug code from scratch|DeepSeek-V3, claude-4.5-sonnet, gpt-4.1|\nGenerate images for a website or presentation|DALL-E-3, Imagen-3, Gemini-2.5 Flash Image|\nCreate a graph or chart based on uploaded data|gpt-5.2, cclaude-4.5-sonnet|\nCreate simple HTML/CSS/JS code|DeepSeek-V3, claude-4.5-sonnet, gpt-4.1|\nTranslate or work with multi-lingual content|gemini-2.5-pro, llama-4, gpt-5.2|\nUnderstand idioms or nuanced language in non-English texts|claude-4.5-sonnet, gemini-2.5-pro|\nProcess structured data (tables, CSVs, JSON)|claude-4.5-sonnet, gpt-5gpt-5.2, Azure Assistant Data/Code Analyst|\nExtract key info or fill out a form based on a document|gpt-5-mini, claude-3-haiku|\nCreate a lesson plan or instructional handout|gpt-5.2, claude-4.5-sonnet|\nGet a fast answer to a general knowledge question|gpt-5-mini, claude-3-haiku, gemini-2.0-flash-001|\nBrainstorm ideas for projects, titles, campaigns, etc.|gpt-5.2, claude-4.5-sonnet|\nSummarize web content from a live website URL|gpt-5.2, claude-4.5-sonnet|\nProvide an executive summary of a large document or presentation|gpt-5.2, claude-4.5-sonnet|\nSolve a tough math problem or show symbolic derivation|Wolfram agent, gpt-5.2, claude-4.5-sonnet|\nWork on competitive programming challenges or puzzles|gpt-5.2, DeepSeek-V3, claude-4.5-sonnet|\nGenerate technical documentation or API instructions|gpt-5.2, claude-4.5-sonnet, llama-4|\nDraft a policy, guideline, or official statement|gpt-5.2, claude-4.5-sonnet|\nSummarize a PDF with images and graphs|gpt-5.2, claude-4.5-sonnet, gemini-2.5-pro|\nOptimize existing code for performance/readability|gpt-5.2, DeepSeek-V3|\nWrite FAQs or help create documentation|gpt-5.2, claude-4.5-sonnet|\nWrite test cases and documentation for code|gpt-5.2, DeepSeek-V3, llama-4|\nPerform sentiment analysis or tone review of text|claude-4.5-sonnet, gemini-2.5-pro|\nDraft interview questions and evaluation criteria|gpt-5.2, claude-4.5-sonnet|\nGenerate charts and graphs for a data-driven report|gpt-5.2, claude-4.5-sonnet|\nSimulate a debate or write arguments|gpt-5.2, claude-4.5-sonnet, DeepSeek-V3|\nPerform a SWOT analysis for a project, team, or initiative|gpt-5.2, claude-4.5-sonnet|\nCompare and contrast two research papers or proposals|gpt-5.2, claude-4.5-sonnet|\nAnalyze a dataset to find errors or anomalies|claude-4.5-sonnet, Azure Assistant Data/Code Analyst|\nHelp draft project plan and timeline with milestones|gpt-5.2, claude-4.5-sonnet|\nWrite a marketing tagline, slogan, or branding message|gpt-5.2, claude-4.5-sonnet|\nWrite SQL queries to extract information from a database|gpt-5.2, claude-4.5-sonnet|\nCreate a chatbot that responds to questions quickly|gpt-5-mini, claude-3-haiku, gemini-2.0-flash-lite-001|\nCheck a document for inconsistencies|gpt-5.2, claude-4.5-sonnet|\nBrainstorm counterarguments to a position or claim|gpt-5.2, claude-4.5-sonnet|\nExtract citations or bibliographic data from a paper|claude-4.5-sonnet, gemini-2.5-pro|\nWrite a quick update for a channel on Slack or Teams|gpt-5-mini, gemini-2.5-Pro|\nNatural language summary of images/visuals (like a dashboard)|gpt-5.2, claude-4.5-sonnet, gemini-2.5-pro|\nSimulate a tutoring or Q&amp;A session|gpt-5.2, claude-4.5-sonnet|\nDiscuss pros and cons of adopting a new tool or service|gpt-5.2, gemini-2.5-Pro|\nGenerate potential risk scenarios for a simulation plan|gpt-5.2, claude-4.5-sonnet|\nQuestions about the Stanford Admin Guide|Admin Guide Search agent|\nQuestions about the FinGate system|Financial Info Navigator agent|\nQuestions about the Playground and AI in general|AI Helper agent|\nAre there any limitations on the devices, OS, or browsers I can use to access the AI Playground?\nThere are no device or OS specific limitations for accessing the AI Playground. Any device capable of running an up to date, modern browser can use the AI Playground.\nIs Cardinal Key required to access the AI Playground?\nNo. You should be able to access the AI Playground from any internet capable device with an up to date, modern browser.\nWhy does the AI Playground sometimes provide different responses to the same prompts?\nLLMs perform unique processes during the generation of content. As a result, even when using the same prompt, the random sampling that occurs during content generation can lead to different outputs. You may try adjusting the temperature in the configuration options for the selected LLM. This will help reduce the amount of \"randomness\" and \"creativity\" in the model's responses. The models selected also make a difference. Each model has different strengths and weaknesses, which can lead to different results. We encourage you to test out various models to find the ones that work best for your specific use cases.\nIs the AI Playground always accurate? Why is information provided by the AI Playground sometimes incorrect?\nThe current nature of Generative AI is such that these tools can make mistakes. Always verify the information given to you by the AI Playground, or any other AI tools. These tools will sometimes generate incorrect information, and relay in such a way that implies the answer is correct. Double check any information generated by AI before sharing or taking action.\nWhat file types are supported by the AI Playground?\nThe AI Playground works best with the following file types:\n**PDF files**\n→.pdf files (normal, unsecured files only)\n**Text files**\n→.txt files\n**Comma Separated Values files**\n→.csv files\n**Microsoft Excel files**\n→.xls files\n→.xlsx files\n**Microsoft Word files**\n→.doc files\n→.docx files\n**Image files**\n→.png files\n→.jpg files\nWhy do the models sometimes have difficulty formatting text when responding to my prompt?\nIssues with poor formatting in responses can occur for several reasons: context size, interference settings, bad training data, lack of specific instructions, and input constraints.You may also notice that some models, like Meta's Llama 3.1, are more prone to this issue than other models. If you encounter issues with poorly formatted responses you can try to:\n1) reword your prompt, asking the model to format its response to make the output more readable.\n2) switch to a different model and try your prompt again.\nCan I use AI to fully automate critical or sensitive tasks?\nIt's important to remember that AI tools can make mistakes. While these technologies are powerful for augmenting decision making, they should not be used as a total replacement for human judgment. There should always be a human in the loop.\nHuman Oversight is the first of the Guiding Principles outlined in the[Report of the AI at Stanford Advisory Committee]. AI systems can generate helpful outputs and perform rudimentary tasks, but these outputs should be carefully reviewed by a human before being acted upon or shared.\nPlease see the related FAQ items below for additional guidance on commercial use of AI generated materials, citation of AI generated content, and the potential copyright considerations of AI generated works.\nCan I generate images and text to use commercially?\nCurrently, the AI Playground is not intended to create works for commercial use. UIT also believes in transparency when it comes to AI. Though it isn’t directly prohibited, UIT strongly recommends that you cite whenever you use content generated by any AI tool, especially when shared publicly. This includes text, pictures, code, video, audio, etc. This applies to materials which are wholly generated or extensively altered by AI Playground or any other AI tool.\nIf you do decide that you want to use something generated by the AI Playground commercially, UIT recommends that you review and follow each particular model's published policies when sharing content generated by the AI Playground. Remember that you are responsible for the content you generate.\nWe suggest these resources for further learning and guidance:\n* [Responsible AI at Stanford - UIT] \n* [Report of the AI at Stanford Advisory Committee] \n* [OpenAI's Business Terms] \n* [Artificial Intelligence Teaching Guide - Stanford Teaching Commons] \n* [Generative AI Policy Guidance - Office of Community Standards] \n* [Reexamining \"Fair Use\" in the Age of AI - Stanford HAI] \n* [Stanford Law School publications on \"Generative AI\" and \"copyright\"] \nCan you provide some examples of how to cite when content is generated by AI?\nUIT recommends citing materials which were generated or extensively rewritten by any AI technology. Below are just some examples of what this could look like. Your use case is not required to look like or be worded exactly like the examples below.\n**Example 1**: AI generated content in documents\n![Example of citation in documents. ] \n**Example 2**: AI generated images and content in presentations\n![Example of citation in presentations. ] \n**Example 3**: AI generated content in code repositories\n![Example of citation in code. ] \nA more detailed[guide for AI in Academic Research] is provided by the Stanford Libraries. This guide offers researchers a practical framework for evaluating, applying, and citing Large Language Models (LLMs) in text-based research.nbsp;\nWhat are the copyright implications when using the AI Playground?\nThe relationship between GenAI and copyright law is complex, relatively new, and evolving. In reality, many popular LLMs and GenAI tools have been trained on copyright materials, which is then hard to disentangle from the results they produce.\nUIT recommends that you follow each particular model's published policies when sharing content generated by the AI Playground. UIT also recommends that you cite publicly shared material which were generated or extensively rewritten by the AI Playground. Remember that you are responsible for the content you generate.\nWe suggest these resources for further learning and guidance:\n* [Responsible AI at Stanford - UIT] \n* [Report of the AI at Stanford Advisory Committee] \n* [Artificial Intelligence Teaching Guide - Stanford Teaching Commons] \n* [Generative AI Policy Guidance - Office of Community Standards] \n* [Reexamining \"Fair Use\" in the Age of AI - Stanford HAI] \n* [Stanford Law School publications on \"Generative AI\" and \"copyright\"] \nCan I transfer my previous conversations from a paid ChatGPT account to the Stanford AI Playground?\nThere are two available methods of importing data into the Playground.\nTo move individual conversations, you can copy and paste the content of the conversation into a document and upload that to the Playground along with a prompt asking the Playground to review and continue the uploaded conversation. This gives you flexibility to organize the material however you like and avoids potential technical issues with formatting or file size.\nTo move your entire history, you can export your ChatGPT data into a JSON file (by going to your profile icon, selecting Data Controls, and clicking on Export Data) and then import this JSON file into the AI Playground (by going to your profile icon, selecting Settings, then Data Controls, and clicking on Import). This feature is provided to the Stanford community, but not supported. This is because extensive ChatGPT histories may not be uploaded in a single file, and would have to be broken up by someone who understands how to parse JSON files.\nPlease also note that importing data only replicates past conversations in a static way. This method will not transfer custom GPTs, files, plugin data, etc.\nIs there a visually clean way to export individual conversations from the AI Playground?\nThe best way to export individual conversations for sharing, publication, or other materials is to share a link to the conversation, open the shared link, and print that chat to a PDF. This method will preserve unique formatting such as LaTeX, HTML, etc. and present the conversation in an easily readable format.\nWhen printing shared links to PDF, some text is clipped at the tops and bottoms of pages. How do I fix this?\nThis is a known issue when printing to PDF from Firefox, and is not limited to the Playground. As a work around, please try printing to PDF from a different browser like Google Chrome. If the problem persists, please try adjusting the print settings. this issue can often be resolved by altering the scale, margins, or page size.\nDoes the AI Playground compress or resize uploaded images before analyzing them?\nYes. In order to maintain system stability and performance, large images uploaded to the AI Playground are automatically compressed and resized. This optimization helps prevents errors caused by high resolution files, but it also means the uploaded image may have lower resolution when reviewed by the AI model. If your project requires full resolution image analysis or precise measurements, you can use the AI API Gateway to build a custom tool. The API Gateway does not compress images like the AI Playground, allowing you to work directly with the original image data.\nWhat happened to ScholarAI? Why don't I see ScholarAI in the list of agents any longer?\nScholarAI was removed from the AI Playground in late 2025. ScholarAI was purchased by a third party and that third party was no longer able to support the security and privacy requirements outlined in the university agreement.\nCan I add the AI Playground to my phone or computer as an app? Does the Playground support Progressive Web App (PWA)?\nYes. You can enjoy faster, easier access to the Stanford AI Playground by installing it as a Progressive Web App on your computer or mobile device. UIT has created instructions for Windows, Mac, and iOS.\nPlease review the[Install Stanford AI Playground Like an App] page for detailed instructions. ## Troubleshooting Common Errors in the AI Playground\nWhat should I do when receiving an error message stating \"Error processing file\" or \"An error occurred\" appears?\nIf you encounter an error message that reads \"Error processing file\" or \"An error occurred while uploading the file\", this may mean that your attachment is using too many tokens. Try breaking your attachment into smaller chunks instead of trying to upload the entire file at once.You may also check that the file you are uploading is one of the supported file types for attachments. Look for the FAQ item titled, \"What file types are supported by the AI Playground?\" for more information on supported file types.\nWhat should I do if I encounter a message stating \"Error connecting to server\"?\nIf you encounter an error message that reads, \"Error connecting to server, try refreshing the page,\" this means that your request may be using too many tokens. Try adjusting your prompt to break your request into smaller chunks instead of processing the entire request at once. If you are uploading an attachment, try removing some pages or columns/rows from the file.\nWhat should I do if I encounter a message stating \"You have hit the message limit\"?\nTo ensure the stability and security of the AI Playground, UIT limits the frequency of specific actions. These limits are crucial in preventing both intentional abuse and accidental overuse. If you encounter an error message saying you have reached a message limit, simply take a break and slow down.\nYou can learn more about the exact rate limits in the Stanford Knowledge Base article[AI Playground Rate Limit Mechanisms].\nHow do I resolve the error reading, \"Only 2 messages at a time. Please allow any other responses to complete before sending another message...\"?\nTo ensure the stability and security of the AI Playground, UIT limits the frequency of specific actions. These limits are crucial in preventing both intentional abuse and accidental overuse. If you encounter an error message saying \"Only 2 messages at a time\", then please allow any other responses to complete before sending another message, or wait one minute before proceeding.\nYou can learn more about the exact rate limits in the Stanford Knowledge Base article[AI Playground Rate Limit Mechanisms].\nWhen attempting to login, it gets stuck in a loop and takes me back to the login page. How do I fix this?\nIf you experience a loop asking you to sign in over and over, you may not be a member of the appropriate groups to access the AI Playground.\nIn the FAQ section above, there is an item titled \"Who can access the AI Playground?\" Expand that FAQ item for an up to date list of all the groups which currently have access. Your account must be in one of the groups listed in order to access the AI Playground.\nI uploaded a file but the AI Playground says it can't find it. What should I do?\nYour upload may be timing out. If this happens try waiting for ten minutes, refreshing the page and trying again. You may also check that the file you are uploading is one of the supported file types for attachments. Look for the FAQ item titled, \"What file types are supported by the AI Playground?\" for more information on supported file types.\nIf the problem persists, try breaking your attachment into smaller chunks. If it is a 20 page word document, try breaking it into three or four smaller documents. If it is a spreadsheet with thousands of cells, try breaking it into three or four smaller spreadsheets.\nYou can also[open a support ticket] with the error received, links to the conversations in which the errors occurred, and copies of the attachments.\nThe AI Playground says cannot directly render or run code in the chat interface. How do I fix that?\nIf the AI Playground states it is unable to render or display charts, graphs, applications, etc., then you may need to enable the Artifacts feature in Settings. To do this, click your name in the bottom left corner, and then click Settings. Click on the Beta features tab, and then make sure the \"Toggle Artifacts UI\" option is enabled.\n(See screenshot below for what it should look like when enabled.) ![Enable the Artifacts feature. ] \nPlease note, the Artifacts feature works best with OpenAI and Anthropic models.\nWhen accessing a shared conversation link, an error message stating \"Shared link not found\" is shown.\nFor security purposes, the university ISO team requires users be authenticated before viewing shared links. If you see an error message stating \"Shared link not found\", please make sure you are logged into the AI Playground first, before clicking the link.\nWhat do I do when the conversation displays an error reading, \"Something went wrong. Here's the specific error message we encountered...\"?\nYou may encounter an error reading “Something went wrong. Here’s the specific error message we encountered: An error occurred while processing your request. Please contact the Admin.” This issue does appear to be a browser issue impacting a very small number of Playground users.\nThe best method is to fork the conversation into a new branch. This will allow you to continue the conversation in a new chat. Go to the last good response in the current conversation and select the fork button. The fork button should be the fourth button under the AI's response.\nTo prevent this error again in the future, you could create a custom preset in the parameters menu to disable the option to \"Resend Files\". To do this, open the right hand sidebar, expand the Parameters option, disable the toggle for \"Resend Files\", and save this setting as a preset. Then you can select this preset when continuing that conversation in the forked branch.\nLastly, you can try clearing the cache and history of your browser or trying a different browser before continuing the forked conversation.\nHow do I fix errors stating \"The latest message token count is too long...\"?\nYou might see an error message stating, \"The latest message token count is too long, exceeding the token limit, or your token limit parameters are misconfigured, adversely affecting the context window. Please shorten your message, adjust the max context size from the conversation parameters, or fork the conversation to continue.\"\nThis is likely an issue with the browser you are using to view the AI Playground. Simply clear your browser cache, try using the AI Playground in a different browser, or log into the Playground using an incognito/private browser. When sending a prompt to GPT 5, it is showing an error stating, \"Error connecting to server, try refreshing the page.\" What is going on?\nChatGPT 5 takes more time to respond than older models. This can cause issues where the response time for GPT 5 is exceeding the defined timeout of AI Playground. This timeout setting is important, and protects overall system stability to ensure a good experience for everyone using the Playground. Running large AI models requires significant computing power, and if a single request runs too long, it ties up system resources for everyone else. That can slow down or even block other users’ sessions.\nThis timeout issue is especially prevalent in prompts containing complex mathematic equations. To help work around this, we suggest you lower the Reasoning Effort slider to Low and reduce the temperature setting to around 0.40 when using complex prompts with GPT 5. If this is successful for your needs, you could even save a custom preset for GPT 5 to reuse whenever encountering this error.\nIt can also help to adjust your prompt. You could try breaking your prompt into chunks and running them sequentially. Alternatively you could try adding guardrails or constraints to your prompt, preventing GPT 5 from taking too much time processing. Used in conjunction with the above custom parameters, you should be able to avoid timeout errors with GPT 5.\n![FAQ-Error-GPT5-Custom_Parameters] \nWhy am I seeing an error reading, \"com\\_error\\_invalid\\_request\\_error\"?\nThis is a known error in the opensource software used to power the AI Playground. It can occur when a user has defined custom parameters that are not valid with the model you have selected. This conflict could be happening with multiple defined parameters or even with the vendor's API rules for the model.\nCommon culprits are for this error are setting an incorrect number of specified tokens or an invalid combination of custom parameters. Try tweaking your defined parameters and if the problem persists, click the \"Reset Model Parameters\" button at the bottom of the Parameters menu.\nWhy does Imagen-3 and Gemini 2.5 Flash Image sometimes say it cannot generate images at the moment and to try again later?\nThis usually appears when a prompt has triggered Google’s content filters, not because the system is unavailable or unable to generate images. If you encounter this, try rephrasing or altering your prompt.\nPlease note that prompts which attempt to generate pictures of people are more likely to trigger Google's filters.\nGemini 2.5 Flash Image sometimes states that there is a \"temporary issue\" or that it cannot generate images. How do I fix this?\nThis error is incredibly misleading, and being said by the model itself. There is no technical error in the Playground preventing the model from generating the image. This is not unique to the AI Playground. Below you can see screenshots from October 2025, showing the same behavior in the commercial versions of Gemini and Chat GPT.\nThe best workaround is to simply try again. You can simply reprompt the model within the same conversation or even click the edit prompt button and resubmit it without any edits. Sometimes this error happens because the model cannot make sense of the prompt or something in the prompt triggered one of the vendor's content filters. Change your prompt to be less vague and add clear commands to generate an image.\n![Gemini image generation error]![GPT image generation error] \nGemini 2.5 Flash Image is not showing the images it generated. How can I see the images?\nThere seems to be an intermittent issue with Gemini 2.5 Flash Image, where it does not always display the generated image. You can simply try responding with another prompt stating, \"Please show the image, you did not show it.\"\nIf that does not resolve the problem, please try to enable Artifacts. (You can do this by clicking your name in the lower left corner, then Settings. Click the Beta Features tab and click the top toggle for Artifacts UI.) Some users have reported this is helpful, although it should not be required for Gemini 2.5 Flash Image.\nSpecialty formatted data appears cut off or truncated on the right side of the screen. How do I see the entire response?\nThis issue may occur when the models is producing specially formatted materials such as mathematic notation or snippers of code. To see the entire response, please try increasing the size of the browser window (maximizing) or closing the right sidebar where the Saved Prompts, Model Parameters, and other configuration menu items are displayed.\n## Data Privacy and Security in the AI Playground\nWhat is the Information Release page that appears when I first log in? What happens with this information?\nThe Information Release is part of the university’s authentication system and is used solely for logging into the platform. This information will not leave university systems and is only shared at the time of logging in. This release is part of a new check added to the university's authentication process for some applications when signing in for the first time. The information is solely for authentication purposes and is only shared at the time of logging into the platform. The shared information is limited to what is displayed on screen below (i.e., name, affiliation, email, user ID, and the workgroup allowing access). You can select among three options for how frequently you are prompted to approve that information be shared: every time you log into this application, each time something changes in one of the data fields listed, or this time only. This information is only shared between the user directory and the platform running the AI Playground. Both systems are maintained by Stanford University IT, keeping the data within the university's environment.\nCan anyone else at Stanford access the content I share with or generate via the AI Playground?\nNo. Information shared with the AI Playground is restricted to your account and not accessible by other people using the AI Playground.\nAs with other university services, a small number of people within University IT (UIT) are able to access information shared with the AI Playground, but only do so if required. See the next FAQ for more information on those circumstances.\nWhat is the purpose of the Temporary Chat feature?\nThe Temporary chat feature helps keep you chat history clean and focused. You can use it for sensitive topics, quick experiments, or anything you don’t need to permanently save.\nThese chats are excluded from your personal search results, cannot be bookmarked, and are automatically deleted from the database after 30 days.\nHow long are files which were uploaded to or generated by the AI Playground kept?\nFiles uploaded to or generated by the Playground are kept for one day. After which time they are deleted from the system.\nWhat happens to conversations that are deleted?\nIt can depend.\nPer ISO requirements, normal conversations are retained by the system indefinitely, even if they were deleted them from the conversation history.\nThe UIT team is working with ISO on a long term file retention policy which will remove files uploaded to or created by the Playground after a six to 12 month period. This is not yet in place at this time.\nAny conversations that use the Temporary Chat feature are completely deleted from the database after 30 days.\nIs the data entered into the Playground used to train new AI models?\nNo. Data entered into the Playground is not used to train models. UIT does not perform any custom training or fine tuning of the models available in the AI Playground. The data you share with the AI Playground is for your use only and remains private during your interaction with the service. Any text or files you upload will not be used to alter or train the LLM in any way.\nAs with other university services, a small number of people within University IT (UIT) are able to access information shared with the AI Playground, but only do so if required. See the next FAQ for more information on those circumstances.\nDoes UIT review the content entered into or generated by the AI Playground?\nNo. The entire UIT team wants to make sure the AI Playground is a trusted space for the entire campus community. While we are building out more robust reporting capabilities, the focus of those reports are on usage trends (such asactive users, top users, total number of conversations, etc.) and not specific conversations. In rare circumstances, as a result of investigations, subpoenas, or lawsuits, the university may be required to review data stored in university systems or provide it to third parties. You can learn more about the appropriate use of Stanford compute systems and these situations in the[Stanford Admin Guide].\nCan I request conversations with the AI Playground be exempt from logging?\nNo. If this is important to you, it is suggested that you use the Temporary Chat feature. Any conversations that use the Temporary Chat feature are completely deleted from the database after 30 days. Per ISO requirements, UIT logs information sent to through the AI API Gateway within the Stanford environment. UIT does not monitor this information, but in rare circumstances, as a result of investigations, subpoenas, or lawsuits, the university may be required to review data stored in university systems or provide it to third parties. You can learn more about the appropriate use of Stanford compute systems and these situations in the[Stanford Admin Guide].\nIs the AI Playground FERPA compliant?\nYes. Based on current[risk classifications], the AI Playground is approved for use with medium-risk data, like FERPA data.\nPlease note, that the use of any AI models with Stanford data may require a fully completed and approved Data Risk Assessment (DRA). More information on that can be found on the[GenAI Tool Evaluation Matrix].\nWhat are the cautions against entering high-risk data into the AI Playground?\nThe AI Playground is not currently approved for high-risk data, PHI, or PII data. The UIT team is currently working with the Information Security Office (ISO) and the University Privacy Office (UPO) on completing one of the most thorough security reviews to date and hope to see this approval in the 2025 calendar year.\nIt is worth noting that all AI models and underlying technologies used by the AI Playground are covered under a Business Associate Agreement (BAA).\nPlease note, that the use of any AI models with High Risk data may require a fully completed and approved Data Risk Assessment (DRA). More information on that can be found on the[GenAI Tool Evaluation Matrix].\nDoes anyone outside of Stanford have access to the information I share withthe AI Playground?\nNo.Where possible, UIT is working with vendors to ensure that the information you upload will not be retained by the vendor or alter the LLM in any way. Microsoft has committed to refrain from retaining any data shared with the OpenAI GPT and DeepSeek models. Google has committed to refrain from retaining any data shared with the Gemini, Anthropic, and Meta models, except for data saved for abuse monitoring purposes.\n## More options and support\n### **\nThe AI Playground\nLaunch the AI Playground to continue exploring.\n[Open the AI Playground »] \n### **\nAI Playground community\nShare and learn from the Stanford community on the AI Playground the[#ai-playground] Slack channel.\n[Join AI Playground conversation »] \n### **\nFeedback\nTo share questions, suggestions, or thoughts about the AI Playground, reach out and let us know what's on your mind.\n[Share feedback »] \n[![Stanford University]] \n* [Stanford Home] \n* [Maps & Directions] \n* [Search Stanford] \n* [Emergency Info] \n* [Terms of Use] \n* [Privacy] \n* [Copyright] \n* [Trademarks] \n* [Non-Discrimination] \n* [Accessibility] \n©CopyrightStanford University.Stanford,California94305.",
    "length": 49664,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Teaching in the AI Era - Teaching and Learning Hub",
    "url": "https://tlhub.stanford.edu/docs/teaching-in-the-ai-era",
    "text": "Teaching in the AI Era - Teaching and Learning Hub\n[Skip to content] \nStanford University\n**\nFor immediate technical assistance during class time, call the GSB classroom support hotline: 1(650)736-3342\nOr submit a[support ticket] to Stanford Service-Now\n[![Teaching and Learning Hub, Stanford GSB]] \n[**MenuClose] \n# Teaching in the AI Era\n![] \n# Engaging Students\n9\n* **[Pre-recording Videos to Share with Your Students] \n* **[Handout: Zoom Instructions for Remote Guest Speakers] \n* **[Faculty Tips: Teaching with Presence and Connection] \n* **[Tools and Technology] \n* **[Asynchronous Course Discussions] \n* **[Conducting Role-Plays] \n* **[Using Polls in the GSB Classroom] \n* **[Building Classroom Community] \n* **[Leading Effective Discussions] \n![] \n# Feedback\n1\n* **[Collecting Mid-quarter Student Feedback] \n![] \n# Resources for Remote Teaching\n1\n* **[Teaching in the Virtual Classroom] \n![] \n# Open Discourse\n6\n* **[Teaching During an Election Season] \n* **[Handling Sensitive Topics] \n* **[Writing Content Notices for Sensitive Content] \n* **[GSB Students on Open Discourse: May 24, 2023 Panel Highlights] \n* **[Engaging with Difference in Course Content] \n* **[GSB Faculty on Open Discourse and Participation: April 13, 2022 Panel Highlights] \n![betterdocs-category-icon] \n# Teaching and AI\n6\n* **[GSB Student Voices Panel on Exploring AI: Event Highlights] \n* **[Faculty Showcase on AI and Teaching] \n* **[Starting Small with AI in the Classroom] \n* **[Teaching in the AI Era] \n* **[Course Policies on Generative AI Use] \n* **[Quick Start Guide to AI and Teaching] \n[] \n* [Teaching and AI] \n# Teaching in the AI Era\n* updated onOctober 3, 2025\nTable of contents\n* [Generative AI Course Policies] \n* [Coursework in the AI Era] \n* [Using AI Tools] \n* [AI Tools and Academic Honesty] \n* [Getting Started with AI] \n* [Support and Resources] \n**Find top tips and quarterly updates in our[Quick Start Guide to AI and Teaching].**\nThe recent growth in generative artificial intelligence (AI) tools has quickly changed the education landscape. Below are some frequently asked questions and suggestions from the TLHub to help you be on the forefront of these tools and their impact on your course. Please note that these are emerging tools, and we will continue to provide new and updated resources as we develop our guidance on the intersection of teaching and AI.\n## Generative AI Course Policies\n****What is the GSB’s policy on student AI use for coursework?\n**MBA/MSx courses: Instructors may not ban student use of AI tools for take-home coursework**, including assignments and exams.**Instructors may choose whether to allow student use of AI tools for in-class work**, including exams. Except for in-class exams, students should be allowed to use AI tools in cases where they are able to use the internet and electronic devices. In-class assignments for which electronic devices are prohibited inherently also prohibit use of the internet, AI tools, tablets, laptops, calculators, etc.\n**PhD/Undergraduate courses:**Follow the[Generative AI Policy Guidance] from Stanford&#8217;s Office of Community Standards.\nNote that:\n* Students are expected to cite all resources, including generative AI tools, used in the preparation of academic work.\n* It is recommended that students use secure, Stanford-approved tools such as the[Stanford AI Playground] or Google[Gemini] and[NotebookLM] (secure only when logged in with SUNet ID).\n* Students are expected to follow UIT’s guidelines on[Responsible AI at Stanford], which limits the use of sensitive data with third-party AI tools, including for take-home coursework.\nWondering how to plan your assignments accordingly? See the FAQs on[Coursework in the AI Era].\n****What should my course policy on AI use include?\nIn your course policy on AI use, you may want to consider the following:\n* Explain**whether students may use AI tools in class**, and under what circumstances, alongside a rationale for your policy.\n* Highlight any**sensitive and/or proprietary data that students will handle**in your course and restrictions on how these materials may be used with AI tools. This should include whether or not students may input instructor-created course materials (e.g., slide decks, assignment prompts) into commercial AI tools. [Not sure what to recommend? See &#8220;What are Stanford&#8217;s data security guidelines for AI use?&#8221; below.]\n* Describe**how students should cite or report AI use**in your course. [See &#8220;How can I ask students to cite or report their AI use?&#8221; below.]\nFor syllabus statement templates and more tips, see our resource on[Course Policies on Generative AI Use].\nState your policy in your**course syllabus**and the**[Course Policies and Norms form] **, and then follow up in class with your students.\n****How can I ask students to cite or report their AI use?\nGSB students are expected to**cite all resources, including generative AI tools**, used in the preparation of academic work.\nNevertheless, many students are hesitant to report AI use. It can help to ask all students to report*how*they used AI (instead of*whether*they used AI). Asking students to regularly report their AI use can help normalize transparent AI use.\nHere are some suggestions for**how students can acknowledge AI use**:\n* Submit chat logs associated with an assignment or exam\n* Include statements with coursework to disclose how AI was used\n> Sample statement: Claude was used to generate data visualizations and an initial written draft. Substantial revisions were made by the author. The ideas discussed in this report are those of the author alone.\n* Reflect on how they used AI for the assignment and what was or wasn’t helpful for them\n* Cite AI use according to standards in your field or industry\nFind more in our resource on[Course Policies on Generative AI Use].\n****What are Stanford's data security guidelines for AI use?\nWhen using any AI tool for Stanford-related work,**follow****UIT’s guidelines on**[**Responsible AI at Stanford**].\n**When using third-party commercial tools:**If you (or your students) use third-party generative AI tools that Stanford does not support (such as[ChatGPT] or[Claude]), note that such tools should be used only for[low risk data] that can be made public. For example, when using these tools, Stanford users should avoid inputting materials containing students’ personal information and proprietary or copyrighted materials (which may include case studies, course assignments, data sets, and more).\n**When using Stanford-approved tools:**Faculty, staff, and students may use the[Stanford AI Playground], Google[Gemini], and[NotebookLM] (when logged in with SUNet ID). These secure tools are approved for use with[low and moderate risk data].This includes most Stanford course materials.Learn more about UIT&#8217;s[data risk classifications] and[generative AI at Stanford].\nSee also our FAQs on[Using AI Tools] below.\n## Coursework in the AI Era\n****What are some guiding principles around AI and teaching?\nAI tools have been evolving so quickly that it’s difficult to pin down best practices for teaching and learning. But a few top ideas have emerged:\n* **Authentic work today often includes using generative AI tools.**Today’s students will go on to careers in which AI technology will be commonly used, and educators want students to leave the classroom prepared for that work.\n* Leading institutions aim to**educate students for the long-term**, providing them with lasting principles and problem-solving capabilities. Developing this knowledge and these skills may not always rely on or require the use of AI tools.\n* Students want to know the**rationale behind coursework and AI-use recommendations**. Connect your choices or recommendations to the learning goals of the course, industry standards (e.g., publication standards), or common industry cases (e.g., needing to give thoughtful/persuasive/clear input on the spot during board meetings).\nUltimately, how AI tools are best used (or not used) in your course will**depend on your course goals**. In some cases students may be able to use AI tools to reach new heights, while at other times students may use AI tools to shortcut their learning.\nNot sure where to start?**Test your assignments with AI tools regularly.**AI tools are evolving at a rapid pace, and the nature of what an AI tool can produce for your coursework may change every quarter that you teach.\n****How can I best assess student learning in the AI era?\n**Consider your course goals**to inform how to guide students on activities, assignments, and exams. For example:\n* What do you want your students to be able to do or understand by the end of the course?\n* How will students demonstrate their skills or understanding to you?\n* Is it important that students are able to demonstrate those skills or knowledge without using AI tools (or the internet)? Why or why not?\nIf it is important that students demonstrate skills or knowledge***without using AI tools***, consider the following:\n* **In-class work without AI:**Conduct an in-class activity or assessment during which students do not use AI tools (e.g., exam, presentation, role-play, or discussion).\n* **Take-home work with in-class checks:**Connect take-home assignments (that may use AI tools) to in-class activities (without using AI tools). For example:\n* Pair projects with an in-class presentation or poster session in which students must discuss and defend their approaches.\n* Before assigning a take-home essay, have students work individually or in small groups develop hypotheses, arguments, and central points they plan to build on.\n* Have students prepare study questions of first-order thinking at home and then go deeper with questions of second-order thinking during in-class discussion.\n* Conduct a short quiz at the start of class based on the problem set students recently submitted.\n* **Take-home work with recommendations:**Provide students with clear instructions of how to complete a take-home assignment and explain why the method you recommend is valuable for their learning. This will not prohibit AI use, but can guide students’ approach to learning a skill or information.\nIf***AI use could support students***in demonstrating certain skills or knowledge, let students know. Make sure students know what your expectations and standards are for an assignment or exam. If you would like them to use AI tools (or other supports) in a particular way, provide students with clear instructions.\n**Looking for more?**[Contact] the Teaching and Learning Hub to discuss ideas for assessing student learning in your course.\n****How can I promote students’ deep learning and thinking in the AI era?\nAs AI tool use becomes more prevalent, how can we help students avoid becoming over-reliant on AI tools? Here are five tips to promote students’ deep learning and thinking in the AI era:\n**1. Celebrate struggle.**Genuine learning is strenuous rather than effortless. When asking students to struggle with a task or concept without the help of AI, explain why this struggle is productive and what real-world context this challenging work is preparing students for.\n**2. Engage authentically with AI.**\n* Lead a discussion about affordances and challenges of AI in your field.\n* Ask a guest (or students) to discuss their work experiences with AI or AI tools.\n* Challenge students to experiment with how they might use AI to &#8216;level up&#8217; their work.\n* Challenge students to reflect on situations in which they shouldn’t rely on AI.\n**3. Slow down to emphasize process over product.**\n* Break assignments into parts.\n* Ask students to complete AI-assisted work step-by-step.\n* Incorporate periodic reflection points into coursework, through writing or small group discussion.\n* Ask students to discuss how they approached their work, not just what conclusion they reached.\n**4. Provide in-class demonstrations and practice.**Students are more likely to execute skills or process content in a specific way (whether with or without AI tools) if the instructor has demonstrated this in class. If this is important in your course, demonstrate the skill or process in class at least once, then give students a chance to practice and ask questions.\n**5. Start with human ideas.**Provide opportunities for students to think deeply about a problem, before leaning on AI and other supports as they complete coursework.\n* Give students 5 to 10 minutes in class to brainstorm approaches or ideas before starting work on an essay, project, or problem.\n* Arrange peer review feedback sessions on drafts of written work or presentations.\n* Ask students to reflect on connections between theory or frameworks and in-class experiential learning tasks or discussions.\n* Ask students to apply course concepts to an issue of their choice that they are passionate about or that is related to their work experiences.\n****What are some examples of how GSB students have used AI tools in their coursework?\nView[survey data] to learn more about how GSB instructors and students have been using AI in the classroom (SUNet login required).\nMost GSB students use AI tools regularly, including with workflows that combine AI tools for different parts of a project or learning routine.**GSB students have reported exploring or engaging with AI tools**for tasks like:\n* Summarizing material (developing key takeaways, asking questions about a reading, compiling meeting notes, etc.)\n* Research (finding relevant news articles, digesting academic scholarship, etc.)\n* Brainstorming and ideation (developing potential talking points, brainstorming alternative approaches to a problem, etc.)\n* Writing and editing (refining writing mechanics, drafting paragraphs from an outline, etc.)\n* Studying, reviewing, and getting feedback (checking answers, getting alternative explanations, creating study materials, turning long-form articles into podcasts, etc.)\n* Performing creative tasks (generating image, video, or data visualizations; writing a children’s story, etc.)\n* Drafting and troubleshooting code (e.g., Claude, GitHub CoPilot)\n**Encourage students to critically evaluate all AI-generated content**, no matter how they use the tools, and refer students to your AI use policy to let them know how to report their AI use.\nFor more, review the[event highlights] or[watch the recording] of Student Voices: Exploring AI in the GSB Classroom.\n****What are some examples of how GSB instructors have used AI tools in their teaching?\nView[survey data] to learn more about how GSB instructors and students have been using AI in the classroom (SUNet login required).\nFind examples of how GSB faculty have integrated AI tools into their courses in our**[Faculty Showcase on AI and Teaching] **, featuring the following faculty:\n* Rachel Konrad, Lecturer in Management\n* Dan Iancu, Associate Professor of Operations, Information &amp; Technology\n* Mark Voorsanger, Lecturer in Management\n****What are some small-scale strategies or activities for exploring AI in the classroom?\nView our resource on[Starting Small with AI in the Classroom], in which we offer tips on:\n* How to test your course assignments with AI tools\n* Reframing existing coursework to incorporate AI use thoughtfully\n* Short in-class activities to bring AI into the classroom in 10 minutes or less\n* And more!\n## Using AI Tools\n****What AI tools can students use?\n**AI Playground:**Students have access to the[Stanford AI Playground], a platform for safely interacting with multiple advanced and widely used AI models, including ChatGPT and Claude. This is a secure tool for Stanford users and may be used with[low and medium risk data].\n**Google Gemini and NotebookLM:**Students have access to Google[Gemini] and[NotebookLM]. These tools are secure for Stanford users when logged in with SUNet ID and may be used with[low and medium risk data].\n**Nectir AI:**This secure and customizable chatbot tool that integrates directly into Canvas, is now available (GSB only). Visit this[Canvas site] for how-to guides and to interact with sample bots, or[contact us] to learn more.\n**Commercial tools:**Students may also use commercially available AI tools for their coursework, provided they follow the guidelines in Stanford IT’s resource on[Responsible AI at Stanford].\nAdditional resources and tips:\n* For**sample syllabus statements to guide students&#8217; use of AI tools**, see our resource on[Course Policies on Generative AI Use].\n* If you would like to use a tool in your course that is not currently approved, see the[**adoption process for new technologies**] for GSB courses.\n****What is the Stanford AI Playground?\nThe[Stanford AI Playground] is a generative AI platform for Stanford students, faculty, and staff. Access the platform using your Stanford credentials and learn more in the[AI Playground Quick Start Guide].\nTop features and tips for using the AI Playground:\n* **Access multiple advanced and widely used AI models, including ChatGPT 5 and Claude 4 Sonnet.**Switch between AI models using the dropdown menus at the top of the screen.\n* **The tool is safe and secure to use.**You can use the tool with[low and medium risk data]. Information you share with the AI Playground is not accessible by other people using the tool and is not saved by the vendor or used to train the AI models.\n* **Test your assignments using the AI Playground regularly.**AI tools are evolving at a rapid pace, and the quality of what an AI tool can produce for your assignments or coursework may change every quarter that you teach.\n****What materials can I use with or input into Stanford-approved AI tools when teaching?\nThe[Stanford AI Playground], Google[Gemini], and[NotebookLM] are safe to use with course and student materials containing[low and moderate risk data]. Note that Gemini and NotebookLM are secure only when logged in with SUNet ID.\n✅**Instructor-generated materials.**Stanford-approved tools may be used with materials such as:\n* Exams\n* Assignments\n* Lecture notes\n* Slides\n⚠️**Student-generated materials**, including completed assignments and other coursework or student survey and evaluation responses. These materials have restrictions on how they may be used with Stanford-approved tools:\n* Instructors may input student content for uses such as generating takeaways about student learning or highlighting gaps in understanding, but Stanford discourages using AI for grading at this time.\n⚠️**Published and proprietary materials**, including case studies and data sets. These materials have restrictions on how they may be used with Stanford-approved tools:\n* Some published and proprietary materials, including some library materials or research data, may not be used with AI tools. Please consult with the[Business Library] for specific usage guidelines.\n* Any use of published or proprietary materials in the classroom, including when using AI tools, should align with fair use principles. Please consult with[Stanford’s Copyright and Fair Use Center] for further information on specific classroom use cases.\n❌**High-risk data.**The AI Playground, Google Gemini, and NotebookLM are not safe for any data classified as high risk according to[Stanford UIT’s risk classifications].\n****Can I still use ChatGPT or other AI tools?\n**Stanford community members may use third-party AI tools**that are not Stanford supported, such as ChatGPT and Claude,**according to UIT’s guidelines on[Responsible AI at Stanford] **, but only for low risk data that can be made public. When using these tools, faculty and staff should avoid inputting materials containing students’ personal information and proprietary or copyrighted materials (which may include case studies, course assignments, data sets, and more), or any information that may be classified as moderate or high risk according to[Stanford UIT’s risk classifications].\n## AI Tools and Academic Honesty\n****Can I use AI detection software to detect student AI use?\nUnder the honor code,**faculty may use AI detection software if they provide students with “clear, advance notice,”**as in the Office of Community Standard’s[Tips for Faculty &amp; TAs].\nIf you are considering using AI detection software, consider the following guidelines:\n* **AI detection software cannot be used as a deterrent for student AI use in take-home coursework**, since GSB instructors may not ban student AI use for take-home coursework.\n* **Stanford does not have approved general licenses for plagiarism monitoring tools**(a common example is[Turnitin]). We recommend faculty**use AI detector tools with a high degree of caution**, if they choose to use such software, because AI detector tools can vary widely in their accuracy and can produce both false negatives and false positives. Users can also bypass detection tools by revising AI-generated text and testing their work in AI detector tools themselves to check detection likelihood.\n* Each AI detection tool differs in its approach to privacy and how inputted material may be shared with third parties. If you choose to use an AI detection tool,**avoid inputting information that should not be made public, according to UIT’s guidelines for[Responsible AI at Stanford] **. This includes personal, sensitive, confidential, or proprietary information that may be contained within students’ coursework, including student names.\n****How can I identify student work that has used AI-generated content without citation?\nYou may ask your students to cite some or all of their use of AI-generated content. (See &#8220;How can I ask students to cite or report their AI use?&#8221; above, as well as our resource on[Course Policies on Generative AI Use].)\nIf you are concerned about student use of AI-generated content that is not properly cited, consider the following:\n* **Test your course assignments using one or several AI tools.**Use a secure tool such as the[Stanford AI Playground], Google[Gemini], or[NotebookLM] (secure only when logged in with SUNet ID). This will provide the clearest sense of how an AI-generated output may compare to high-quality student work in your course. Note that such tools do not generally produce the same output each time a question is posed.\n* Some**common strategies for detecting plagiarism**may be useful for identifying improperly cited use of AI tools. These include looking for generic or repetitive language, large shifts in writing voice or style, improper and missing citations or facts, or if a student can’t explain how they arrived at an answer or made choices in producing an assignment.\n* **AI detection software is available, but has significant limitations.**See above for more.\n****What else can I do to deter students from using AI tools in ways that might undermine their learning?\nThere are some methods you can use to minimize students using learning shortcuts when completing assignments. For design practices that help improve learning and encourage deep student engagement, see the FAQs on[Coursework in the AI Era].\n## Getting Started with AI\n****What are generative AI tools?\nGenerative AI is a**type of artificial intelligence technology that can generate new content**based on patterns and information gathered from sets of sample materials. Here’s what else to know about this technology:\n* The**most well-known generative AI tools are ‘chatbots’**(such as the[Stanford AI Playground]; and commercial tools, e.g.,[ChatGPT],[Microsoft Copilot],[Google Gemini], or[Claude]). These are AI-based text generation tools that users interact with using a chat interface. Users type a question or prompt, and the chatbot responds with a cohesive and creative written answer. The chatbot can then refine or adjust its responses based on how the user continues to interact with the tool, such as by continuing to pose questions or asking for changes to the chatbot’s output.\n* **Other generative AI tools produce many different outputs**such as images (e.g.,[DALL-E] or[Midjourney]), code (e.g.,[GitHub CoPilot],[Cursor]),slides (e.g.,[Gamma],[Canva]), or internet search results (e.g.,[Perplexity]).\n* AI tools use ‘training data’ (sample materials or data) to inform the content they generate. The**quality of AI-generated content can vary widely**, in part based on the quality or focus of the training data. For example, a tool trained on large amounts of internet content may be able to respond to a prompt in many different and creative ways, but the tool’s output may also reflect the varying accuracy or biases embedded within the sample internet content. In addition, AI tools sometimes prioritize completing or responding to a user’s prompt over accuracy of information, and can sometimes make up information entirely (known as ‘hallucinating’).\n* **For definitions of common AI terms and additional explanations of how AI chatbots work**, see Stanford Teaching Commons’ resource on[defining AI and chatbots].\n****How can I learn more about AI tools?\n* **Stanford UIT’s[AI Playground] and[GenAI Overview] **provides a safe, secure, and Stanford-approved tool to test various AI models with tips and resources available.\n* **Stanford Teaching Commons’[Artificial Intelligence Teaching Guide] **provides a step by step guide to generative AI tools in the higher education context, including exercises to experiment with AI tools.\n## Support and Resources\n### TLHub Support\nWe can offer one-on-one consultations, answers to questions via email, and even a short presentation with Q&amp;A at your next faculty meeting. If you have questions about the impact of generative AI tools on your course and assignments,**[reach out] to discuss your concerns**or specific use case with us.\n### Stanford-approved AI Tools\n* [AI Playground], Stanford UIT&#8217;s secure generative AI platform for faculty, staff, and students\n* Google[Gemini] and[NotebookLM], secure and Stanford-approved tools for faculty, staff, and students (when logged in with SUNet ID)\n* Nectir AI, a secure chatbot available to GSB faculty (limited pilot only in AY2025-26).[Watch the demo] or[contact the TLHub] to learn more.\n### GSB Resources\n* [Quick Start Guide to AI and Teaching] \n* [Course Policies on Generative AI Use] \n* [Faculty Showcase on AI and Teaching] resource, and the[event recording] \n* [Faculty Showcase on Exploring AI in the GSB Classroom] (event recording)\n* [Student Voices panel on Exploring AI in the GSB Classroom] (event recording)\n* [Teaching with AI —A Guide for AI Use in Assignments] (workshop recording)\n* [Starting Small with AI in the Classroom] \n### Stanford Resources\n* [GenAI Overview], Stanford UIT\n* [Responsible AI at Stanford], Stanford UIT\n* [AI Meets Education at Stanford] (AIMES), Stanford Center for Teaching and Learning\n* [Artificial Intelligence Teaching Guide], Stanford Teaching Commons\n* [Artificial Intelligence teaching resources topic], Stanford Teaching Commons\n* [AI Index Report], Stanford’s Human-Centered Artificial Intelligence Lab\n### Acknowledgements\nThis article draws from the[Artificial Intelligence Teaching Guide], Stanford Teaching Commons.\n**\n### Important Note:\nTechnology is changing at a rapid pace. While we make every attempt to ensure our content is updated to reflect changes to the interface and functionality, we can only guarantee the accuracy of the content on this resource page when it was written or recorded. Please be sure to check the software developer's website for the latest updates and release notes for the most up to date information. If you have questions or concerns, or need additional support, please contact us.\n[![Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License logo]] \n[Unless otherwise noted, you may copy and redistribute this resource in its unmodified form under the terms of the Creative Commons BY-NC-ND 4.0 International license (attribution, non-commercial, no-derivatives).] \n[**]",
    "length": 27918,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "AI Data Stewardship Framework | Stanford Law School",
    "url": "https://law.stanford.edu/2023/03/09/a-data-stewardship-framework-for-generative-ai",
    "text": "AI Data Stewardship Framework - CodeX - Stanford Law School\nlogo-footerlogo-fulllogo-stanford-universitylogomenu-closemenuClose IconIcon with an X to denote closing.Play IconPlay icon in a circular border.\n[Skip to main content] \n![Sls logo] \n# AI Data Stewardship Framework\n* March 9, 2023\n* By\n* Eran Kahana\n* [Subscribe] \n* [**Share on Twitter] \n* [**Share on Facebook] \nData is the lifeblood of generative AI applications. And while these applications depend on access to enormous amounts of it, that is not enough. At this point we can see the relationship between quality and quantity: Data quality and quantity are equally important; scarcity in one inevitably destabilizes the other. The dynamics of this relationship becomes most evident by the performance of these applications, they are ultimately only as good as the data they train on. There are various ways of defining the optimal data set, one that exhibits sufficient quality and quantity. Here I call it simply &#8220;high quality&#8221; data.  It is obvious, therefore, that having and maintaining policies and procedures that are specifically designed to ensure***high quality***data is continuously provided is critical. I refer to this overall effort as the**AI Data Stewardship Framework (AI-DSF)**.\nTo remain relevant, the AI-DSF is periodically updated. The**Use Cases for the AI-DSF**section offers examples of when reference to the framework makes sense. Finally, the**Notes and Discussion**section is reserved for taking a closer look at the various controls and at practical ways in which they can come into play.\n**Who is this for?**The AI-DSF can be used by data brokers, data consumers, companies that build generative AI applications (and their boards), AI auditors, regulators (e.g., FTC), courts, and lawmakers. It is not limited to the use of data sourced from public domains and can be a valuable reference for enterprise applications that, for example, use Generative AI.\nThe AI-DSF structure is composed of: Basic Controls, Foundational Controls, and Organizational Controls. All of these controls are internal (the organization) and external (supply chain) facing. Supply chain members are subject to a meet-or-exceed standard that corresponds to the organization&#8217;s policies.\n**Basic Controls****&#8211; Represent the baseline which every organization should have.**\n* **Data Governance**&#8211; Policies, processes, procedures, and practices comply with relevant legal and regulatory requirements as well as relevant standards, best practices, and industry guidelines; data licensing requirements are routinely reviewed, updated, and enforced throughout relevant functions of the organization; all Basic and Foundational Control functions are coordinated and aligned with legal, regulatory, and contractual requirements; application design, development and life cycle is aligned with data dimension principles (&#8220;data dimension&#8221; is discussed in more detail below); relevant principles of the[AI Life Cycle Core Principles] are integrated into organizational policies, processes, procedures, and practices.\n* **Data Inventory**&#8211; Incudes control for data type &#8211; public, expert, or synthetic; Sets purpose, scope, roles, and responsibilities for data collection, creation, use, and retention; Identifies: (i) categories of external data sources (e.g., service providers, partners, customers); (ii) categories of internal data sources (e.g., employees, prospective employees, contractors); (iii) how data set diversity and sufficiency is established and monitored; and (iv) the data storage environment &#8211; geographic location, internal, cloud, third party; Data supply chain management practices are periodically reviewed and updated to comply with relevant laws, regulations, best practices, and organizational risk tolerance; Addresses data life cycle, including compliance with legal and other variables; Data licensing requirements are reviewed and complied with prior to dataset ingestion.\n* **Continuous Data Vulnerability Management**&#8211; Data quality baseline is set and regularly monitored; Pre-training and post-training procedures are executed, documented, and measured. Proven anomaly detection tools are continuously used; Detected data anomalies are analyzed to determine cause, vector, and assess operational and legal impact (necessary for the Data Incident Response Plan); Corrective action is implemented and documented within established reasonable timeframes; Data deletion and data unlearning methodologies are readily available and implementable; Implements policies and procedures to regularly test for alignment with enabling explainability (XAI); Policies and procedures are designed with reference to well-accepted standards.\n* **Data Incident Response Plan**&#8211; A formal structure for planning and implementing an effective response to a detected data vulnerability.\n* **Secure Configuration for Data**&#8211; Aligns with internal (e.g., IT policy) and external (e.g., supply chain management) policies and procedures; Cybersecurity environment is maintained in accordance with well-defined and accepted standards.\n* **Maintenance, Monitoring, and Analysis of Data**&#8211; Roles and responsibilities are identified and formalized for human oversight; Acceptable Use policies are in place and enforced; A lessons-learned approach is implemented for analyzing vulnerabilities, threats, and incidents of compromise; Periodic use of Data Protection Impact Assessment (DPIA).\n* ***Note**:*Look for vendor documentation that details how data is managed throughout its life cycle. It should consist of at least a basic set of controls that comprise a set of essential measures. They describe: (i) the steps the organization follows to comply with applicable laws, regulations, industry standards, and best practices; (ii) its data licensing practices; (iii) how the first two tasks are reflected in actual contractual practices; (iv) how the application design and development activities are carried out to ensure proper data handling; and (v) which AI core principles (e.g., transparency, accountability, safety, security, and ethics) are embedded into the organization&#8217;s policies, procedures, and processes. If the vendor&#8217;s answer is in the affirmative, make sure to look for examples that demonstrate adherence to the framework is systemic, not a one-off display effort.\n**Foundational Controls****&#8211; The next step up from the basic controls –can be useful for organizations that identify they have a higher risk profile.**\n* **Data Storage Protections**&#8211; Physical environment monitoring is consistent with well established best practices and/or standards, such as NIST SP 800-53, Cybersecurity Framework, ISO/IEC 27001.\n* **Data Threat Defenses**&#8211; Implements and maintains processes and procedures to identify and mitigate threats from internal (e.g., employees) and external threats (e.g., hackers); Incorporates threat and vulnerability information from information sharing sources; Reference and periodic updates made in reference to ISO 27001, 27014, and NIST AI 100-2e2025 Adversarial Machine Learning: A Taxonomy and Terminology of Attacks and Mitigations.\n* **Data Provenance Protections**&#8211; Employs blockchain-based security mechanisms; Where alternative or compensating security methods are used, sufficient documentation is available to demonstrate reasonableness of selection; Implements guardrails to protect against use of unlicensed, unverified, and unintended data sets; Implements protection against use of unlicensed output data; Implements protection against using outdated data; Implements safeguards against data poisoning; Data licensing practices are regularly reviewed to ensure alignment with intended use.\n* **Secure Configuration for all Data Sources**&#8211; All computing and data storage assets are aligned with security configurations in accordance with the organization&#8217;s IT policy.\n* **Data Sources Boundary Defense**&#8211; Data flow is continuously monitored; Only pre-approved external data sources are permitted access and use.\n* **Controlled Access to Data Sources**&#8211; Access is restricted to pre-approved data sources.\n* **Audit and Control**&#8211; Internal and external (supply chain) periodic audit of all Foundational Controls; findings are regularly provided to senior management and board of directors; Identified vulnerabilities are promptly dealt with and documented.\n**Organizational Controls****&#8211; Focused on people and processes.**\n* **Implement Data Stewardship Program**&#8211; The program has formal, documented support from senior management and is subject to periodic review and audit by the board of directors and outside auditors.\n* **Data Incident Response Management**&#8211; Data incidents are managed through a formal, documented process that is reviewed by relevant members of senior management.\n* **Fuzzing Tests and other Red Team Exercise**&#8211; Vulnerabilities in the Basic and Foundational Controls are periodically tested, documented and reported to senior management.\n**Purpose**\nThe AI-DSF aims to formalize the generation, supply, protection and use of high quality data for training algorithms. Doing something once or a handful of times will not yield a meaningful, positive result, especially when it comes to the complex undertaking detailed in this framework. This is underscored by the term &#8220;formalize.&#8221; It shows the importance of rendering all the stewardship tasks as something that is repeatable and measurable.  Underlying all of this effort is effective governance. Senior leadership buy-in, including the board of directors is essential and a central characteristic of the framework&#8217;s formal nature.\n&#8220;High quality&#8221; is a complex attribute in that it contains numerous variables, or &#8220;dimensions.&#8221; MIT researchers Richard Wang and Lisa Guarascio list 20 data dimensions: Believability, value added, relevancy, accuracy, interpretability, ease of understanding, accessibility, objectivity, timeliness, completeness, traceability, reputation, representational consistency, cost effectiveness, ease of operation, variety of data &amp; data sources, concise, access security, appropriate amount of data, and flexibility. (*See*, Dimensions of Data Quality: Toward Quality Data by Design 1991.) I don&#8217;t consider this an exhaustive list and it may even benefit from other changes, such as adding &#8220;integrity,&#8221; &#8220;resilient,&#8221; and &#8220;hygienic.&#8221; But it is an important reference tool for determining what exactly we need to pay attention to in this framework.\n**Use Cases for the AI-DSF**\n1. The AI-DSF can be used by developers in the development gating process. Among the inherent benefits here is that it can help simplify compliance with some of the developer&#8217;s contractual obligations. For instance, a typical provision requires that the developer represent and warrant that the application is in compliance with all applicable law and regulations. While this is a very broad requirement, the AI-DSF can help meet part of that obligation insofar as it relates to the qualities of the data that impact privacy, copyright, etc.\n2. Service level agreements (SLA): An application developer could employ it for determining how to better draft the data supply agreement and the SLA with the data broker. This helps ensure the data delivery is relevant and responsive to the needs of the algorithm. Why is this important? Consider, for example, a scenario where the data broker delivered data that is accurate, concise, and free of data poisoning, but it was not timely delivered. That&#8217;s a huge problem. Training an algorithm on data that is outdated can be devastating and so appropriate contractual provisions are vital to mitigate that risk.\n3. Legal rights: Licensees typically rely on a representation and warranty by the licensor as to the quality of the data: the rights in the data (e.g., having an appropriate license), provenance, etc. The AI-DSF provides a useful framework for ensuring the data provided is high quality, not tainted with legal problems such as infringement or other undesirable variables such as offensive, inaccurate, or controversial data.\n4. Guide service agreement negotiations: Subscribers in a service agreement can use the AI-DSF to contractually obligate the provider to maintain select policies and procedures that ensure data integrity. For example, add a requirement to implement and maintain protection against using outdated data.\n5. Compliance with privacy law: The privacy landscape is increasingly complicated. The absence of a federal law in the U.S., has driven more and more states to put privacy laws on the books. This is also a hot topic for regulatory bodies (e.g., the FTC) and for their part they remain vigilant and periodically sharpen their and guidance and enforcement efforts. On the international front, regulations such as the General Data Protection Regulation (GDPR) create additional layers of complexity that must be complied with. In this setting, adhering to the AI-DSF becomes a useful guide for helping mitigate the risk inherent in using data that might contain private information.\n**Notes and Discussion**\n1. The AI-DSF design references in its design: NIST Cybersecurity Framework; NIST Privacy Framework; NIST AI Risk Management Framework; CIS-20 Cybersecurity Controls; Information Commissioner&#8217;s Office AI Auditing Framework (draft guidance for consultation).\n2. **Data Governance**&#8211; There are two driving principles here: (1) enable and (2) demonstrate compliance with applicable legal, regulatory, and best practices. The latter can be of particular importance in the organization&#8217;s routine operations, such as responding to an RFP, complying with investor and other stakeholder demands.\n3. Blocking the ingestion of malicious content is one of the functions to be executed in the**Basic**and**Foundational**controls. Under the former, this comes into play primarily as part of the*Data Inventory*task. And under the latter this process takes on a larger role, which we can see in every one of the subtasks. The*Data Storage Protections*task prevents attackers from gaining access and tampering with the datasets.*Data Threat Defenses*deals with processes that identity and stop malicious content threats, regardless of origin.*Data Provenance Protections*is in charge of continuous data origin and integrity checking. The*Secure Configuration for Data Sources*hardens the underlying systems against malicious content attacks. The*Data Sources Boundary Defense*and*Controlled Access to Data Sources*work in unison, forming a security perimeter that monitors all data flows and permits access only to white-listed sources. Finally, the*Audit and Control task*functions as the verification layer, which ensures the system&#8217;s defenses are consistently up to the task.\n4. Blockchain-based security mechanisms are one way to secure data provenance. Where an organization chooses a different method to secure data provenance, it will likely be important to ensure that there is sufficient documentation that explains the rationale for its use. Ultimately, the question will be whether the selected method is reasonable considering all the surrounding circumstances that led to its selection and implementation.\n5. A DPIA is an essential tool for demonstrating that the organization is using legally reasonable means for risk assessment of data used by AI.\n6. The OECD Framework for the Classification of AI Systems addresses data and input. It has &#8220;Collection&#8221; criteria and &#8220;Rights &amp; Identifiability&#8221; criteria. Missing from this framework is reference to the Basic Controls of the AI-DSF.\n7. The**Data Incident Response Plan**guides the response so as to minimize the potential for degraded data materially impacting the AI application(s). A data incident can be defined in a number of ways. For example: Any unauthorized instance in which there is an attempt, whether successful or not, to access the data. Limiting the inquiry to whether or not there was access, as opposed to whether data was tampered with, is but one response option. Selecting it may be reasonable depending on the incident&#8217;s surrounding circumstances and the severity level that is assigned. Severity levels are typically assigned from level one to three. Level one is the most minor and used where the incident can be dealt with internally and has no impact on the organization&#8217;s operation. A level three is the most severe. Regulatory agencies and/or law enforcement need to be involved, multiple stakeholders are affected, and there is a material risk to the ongoing normal operation of the business. A Data Incident Response Plan can be a subset of the organization&#8217;s overall incident response plan or be a separate document. However it is implemented, it is important to confirm there are no gaps with other relevant policies, such as the IT policy.\n8. Supply chain members are subject to a meet-or-exceed standard that corresponds to the organization&#8217;s policies. This requires identifying specific requirements in the contract with the supplier. For example, if the organization uses SHA-256 for encryption, the supplier must either use the same or a better protocol.\n9. Pre-approved data sources are those that are subject to contractual requirements with the data provider.\n10. Timely detecting data vulnerability plays an essential part in properly managing the organization&#8217;s risk. For example, demonstrating that your organization regularly looks for and effectively responds to indications of compromise can be valuable for minimizing/eliminating liability in the event of a regulatory inquiry.\n11. **Data Provenance Protections**&#8211;  A key function is protection against use of unlicensed output data which is where an LLM is trained on the output of another LLM. While this practice may have some engineering upside in the form of cutting the amount of training time, the potential legal downside may be significant. From a copyright perspective, as long as such training falls under fair use, the question of liability for infringement can be set aside. But things get murkier from a contractual perspective, where the output data is protected by terms of use. In this setting, training on output data without permission can trigger a breach of contract claim.\n12. OpenAI has until April 30 to comply with Italian regulator data privacy requirements. It is uncertain (to say the least) that OpenAI will be able to comply. France, Germany, Ireland, and Canada are also looking into OpenAI&#8217;s data collection and use practices. And there&#8217;s more trouble brewing with the European Data Protection Board (EDPB) also setting its sights on OpenAI. All developers of generative AI apps need to pay close attention to these developments. An important takeaway here is for developers to make sure they comply with the AI Data Stewardship Framework or something similar. It may be the only way to satisfy data privacy legal requirements.***Update 5-2-2023***:[TechCrunch reports] that ChatGPT is back in Italy. OpenAI made changes to the way it presents its service to Italian users, including an age verification pop-up and alerting the user to a privacy policy with links to a help center. Will the Italy experience guide the way out of other EU/EDPB challenges to generative AI? Maybe. What is also of interest is how this all ties in with another topic I first wrote about in 2011 under the title*[Maximizing Representative Efficacy &#8211; Part I] *and with its*[Part II] *which came out shortly thereafter. Now,*Part II*is relatively more relevant to this discussion, but Part I sets the stage, building on an excellent law review by Richard Craswell,*Taking Information Seriously: Misrepresentation and Nondisclosure in Contract Law and Elsewhere.*Part II is particularly relevant here because it describes the role of AI apps in helping users understand terms of service. Considering the latest seismic changes in public awareness and apprehension of AI, the role of these apps (the good ones anyway) will be vital for making these services more accessible and compliant with what will likely be rigorous regulatory and enforcement efforts.\n13. **Data Incident Response Plan**&#8211; How does the organization respond and contain an external-based attack vector that pollutes its LLM data set? The manner of response is dependent and guided by the degree of model adherence to AI Life Cycle Core Principle variables such as Resilience. For example, in a model with a high degree of Resilience, the organization may have more time to alert its end user base of the incident and minimize the degree of harm. This can be an important way to maximize the ability to comply with service level agreements and other contractual obligations.\n14. Techniques such as Word Prediction Accuracy (WPA) provide an example of operationalizing part of the**Maintenance, Monitoring, and Analysis of Data function**. See https://lilt.com/research.\n15. The**Continuous Data Vulnerability Management**function calls for the availability and implementation of reliable data deletion methodology such as is provided by Machine Unlearning (MU). For more on MU, see[Machine Unlearning: its nature, scope, and importance for a &#8220;delete culture.&#8221;] Within this function there are also processes that deal with ongoing validation at various checkpoints during the application&#8217;s development and, if relevant, post-deployment.\n16. **Data Provenance Control**tackles challenges such as copyright infringement. When it comes to training foundation models, there are two primary focal points here: (i) Whether it is permissible to use the data for training and (ii) does the output infringe? Permissible training occurs when it is on data that is not subject to copyright, the training is carried out pursuant to a license (or an implied license such as the absence of robots.txt or other retstriction), or when the use of the data set qualifies as fair use. As for the output, it does not infringe***if***it qualifies as fair use. In this note we consider what infringement guardrails look like in the context of fair use and for this we focus on the transformative factor of fair use. We can see that the transformative variable may be much too dependent on the quality of the end user&#8217;s prompt. Now, suppose that it&#8217;s true that a sophisticated prompt (e.g., one with significant semantic texture) is more likely to generate output that qualifies as transformative than one done coming from a simple prompt (lacking significant semantic texture). If that is the case, we can use the prompt&#8217;s quality as a factor in scoring the probability of infringing output. This is accomplished by training an algorithm to measure and score the quality of a given prompt. The prompt&#8217;s score is the product of comparing the prompt to others that have similar characteristics and this is done by reference to an ontology of prompts. Again, presuming it is true that sophisticated prompts are more likely to yield non-infringing output, the prompt scoring approach can help bring us closer to spotting generative output that more likely passes the transformative test in fair use. Model providers could then put in place prompt guardrails that help the end user lower the probability of generating output with a low transformative score. This would include protection against prompts that are known or are likely to generate infringing output. Finally, model providers that opt not to use this prompt scoring approach would not benefit from a safe harbor (if and once something like that is set).\n17. **Data Provenance Protections**contains a requirement for implementing guardrails to protect against use of unintended data sets. Part of the focus here is to identify whether the AI model is using personal data unintentionally. If it is, appropriate actions need to be taken to prevent use of this model until there is assurance that the personal data has been removed. The use of personal data, even if it is unintentional, is irrelevant to the question of the need to comply with applicable laws and regulations. The GDPR, for example, has a number of articles that apply (e.g., Articles 4, 5, 6, 7, 22). Being unaware of their applicability creates unmitigated risk.\n18. Dataset diversity is a key feature of the**Data Inventory Controls**. It is important for enhancing model alignment with the**Fairness**life cycle core principle, a principle which aims to manage against unintended disparate treatment and reduce unexpected outcomes. As such, insufficient or lack of dataset diversity can be regarded as a data quality deficiency and risk which can have undesirable downstream consequences. This makes dataset diversity a key contracting checkpoint for end users. In this setting, contractually obligating the system provider to exercise best practices in controlling for dataset diversity helps the end user better manage its risk, internally (e.g., for its employees) and externally (e.g., its licensees).\n19. How were the humans selected for the Reinforcement Learning from Human Feedback (RLHF) task? This is an important question because RLHF plays an important role in teaching the AI agent how to improve its decision-making capabilities. Accordingly, this question should be asked during either the vendor selection process (RFP) or at the contract negotiation phase. A developer that can disclose its RLHF methodology is more effectively aligned with the[Transparency] core principle. Additionally, this inquiry also aligns both parties, the developer and end user, with the[Ethics] life cycle core principle, which calls for (in relevant part) the implementation of practices that manifest socially beneficial conduct.\n20. The AI-DSF enables most of the[AI life cycle core principles]. For example: Accountability, Accuracy, XAI, Fidelity, Governance, Permit, Privacy, Relevant, Security, Transparency, and Trustworthy. Adhering to the AI-DSF, or a similar data stewardship framework, therefore, can help developers and end users reduce their risk in using AI.\n21. The Federal Trade Commission (FTC) has at its disposal the remedy of algorithmic disgorgement to enforce deletion of algorithms created using data for which the developer had no rights or which is in violation of law. Such an outcome can have significantly expensive, even devastating consequences for the developer, and it can also negatively impact the developer&#8217;s end users that were using the &#8220;tainted&#8221; application, potentially even further magnifying the developer&#8217;s liability exposure. The AI-DSF helps solve this problem. Properly implemented, the AI-DSF helps the developer ensure it is not using tainted data. And apart from the benefit of not being subject to algorithmic disgorgement, the developer can adopt licensee-friendly terms and conditions with respect to infringement indemnification and defense, which can help differentiate its services from those of its competitors.\n22. Limiting training to datasets comprised of: (i) what has been licensed by the organization;  (ii) its own creations; (iii) works in the public domain, and (iv) moderated generative AI content coalesces to ensure the organization is not infringing on copyrighted works of others. The benefits of this type of practice also extend beyond the organization. In the case of AI service providers, the benefit of this risk reduction approach can also be flowed down to its end users in the form of licensee-friendly liability mitigating terms and conditions, namely the infringement indemnification.\n23. The cybersecurity requirement in the**Secure Configuration for Data**foundational control calls for a regime designed in accordance with standards. The term &#8220;standards&#8221; is loosely used. It includes ISO 27001 (one of the best known standards), NIST Cybersecurity Framework, CIS Critical Security Controls, PCI DSS, etc. The intent is to ensure that the organization is using a widely accepted data security methodology and not a one-off, informal approach. The more grounded the cybersecurity regime is, the better it aligns with various[AI Life Cycle Core Principles], such as:**Accountability**,**Accuracy**,**Big Data**,**Ethics**,**XAI**,**Fairness**,**Human Centered**,**Privacy**,**Relevant**,**Reliability**,**Resilience**,**Robust**,**Safety**, and**Security**.\n24. Developers that follow the data licensing guideline (see**Data Governance**) are in a good position to offer their licensees substantive infringement indemnification and defense protections. This practice is valuable. It can help differentiate developers from competitors that do not follow this process and rely on sweeping liability disclaimers.\n25. Governance and board oversight are critical for ensuring proper implementation of the AI-DSF. For such oversight to be effective, senior management and the relevant board members should have sufficient understanding of the various controls and functions of the AI-DSF, receive periodic reports (as required by the controls), and routinely confirm that the organization has sufficient resources to maintain the AI-DSF. Additionally, a hallmark of effective Data Governance is an operational environment that only uses data that is relevant to the authorized activity. An &#8220;authorized activity&#8221; derives from a formal agreement (contract) with another party. This is not limited to a B2B agreement and includes legally-valid consent obtained from an individual.\n26. Within the**Data Provenance Protections**controls there is an emphasis on avoiding challenging, potentially expensive, even devastating scenarios that arise from regulatory or court ordered model disgorgement. It is important to keep in mind that the destructive effect of model disgorgement can extend to the developer&#8217;s end users (subscribers). If that occurs, additional liability can pop up due to a breach of contract, subscriber loss, and reputational damage. All said, the risk of failing to adhere to the**Data Provenance Protections**is so clear and significant that it should be a subject that is regularly inquired upon by the developing company&#8217;s senior leadership. (See also Notes 20 and 24.)\n27. A key part of the**Continuous Data Vulnerability Management**control is dealing with data anomalies. Toxic data is considered a data anomaly. When it occurs, the Data Incident Plan should be referenced in the effort to determine cause, vector, and assess operational and legal impact. In addition, corrective actions measures should be taken to prevent recurrence.\n28. The AI-DSF is designed to help maintain the developer&#8217;s alignment with multiple AI Life Cycle Core Principles. It supports, for example, compliance with the**Ethics**principle by protecting against misinformation and disinformation. Protecting against both of these types of harms is part of the effort to focus on developing and maintaining AI applications that are socially beneficial.\n29. The absence of effective contracting practices with data supply chain members can degrade compliance with the requirements under the**Data Inventory Controls**. Take, for example, the use of poorly drafted service level agreements. This could be considered as a sign of misalignment with data life cycle management best practices. If that practice is identified (ideally through the periodic review requirements under this control group), a thorough investigation should be conducted into all data supply management practices (not just those relating to contracting).\n30. Statistical exploration of data used/to be used is part of the**Continuous Data Vulnerability Management**control. Policies and procedures under this control should reference well-accepted standards such as ISO/IEC 42001.\n31. Requiring end users to routinely and manually check generative AI output is important. (This can also be performed via red teaming.) It is part of the Acceptable Use policy which is required by the**Maintenance, Monitoring, and Analysis of Data**control.\n32. The AI-DSF aligns with the Fair Information Principles (FIPS). FIPS came out in the early 70’s and remains monumentally important for maintaining data privacy. It is built around seven principles that are required of entities that collect and process personal information: (1) placing limits on information use; (2) formalizing data minimization; (3) limiting disclosure of personal information; (4) collecting and using only information that is accurate, relevant, and up-to-date; (5) enabling individuals with notice, access, and correction rights; (6) building transparent data processing systems; and (7) providing security for personal information.\n33. In the NIST Privacy Framework, reference to the**Data Governance**control can (also) be found in the CT.PO-P4 subcategory of the &#8220;Control&#8221; function. Instead of using &#8220;govern,&#8221; NIST refers to this function as &#8220;data life cycle manage[ment].&#8221; The &#8220;life cycle&#8221; here can be seen as cautionary, steering organizations away from adopting a one-off approach to the tasks required under**Data Governance**.\n34. Satisfying the**Data Sources Boundary Defense**and the**Controlled Access to Data Sources**controls requires (among other things) effective data supply chain management practices. For example, a Network Access Agreement or similar contract should be in place before any access is granted. The term of the agreement should be carefully considered, with the default choice being a shorter duration and renewal of the agreement conditioned on the data supplier satisfying all security requirements. This approach helps prevent scenarios where suppliers have outdated or otherwise poorly managed access to data assets.\n35. The American Privacy Rights Act (APRA) is the latest legislative effort aimed at drafting a federal privacy law. Even if APRA doesn&#8217;t make the cut, there are still things that can be gleaned from it and so it&#8217;s worth looking into. Some parts of it may ultimately find a home in whatever the final much-talked-about federal privacy act ends up being. In APRA we can see an emphasis on data minimization and accountability. The former (unsurprisingly) is one of the key characteristics found in the[**Privacy**] life cycle core principle and the latter is in and of itself a life cycle core principle, one that contains nearly 10 characteristics. (For more on**Accountability**, see[here].) APRA makes it clear that compliance will necessitate reference to a formal data stewardship framework; a one-off attitude will not make the cut. As such, APRA makes the case for using the AI-DSF. It is also important to highlight the importance of the[**Governance**] core principle as it is vital for ensuring APRA&#8217;s emphasis on data minimization and accountability is effectively operationalized.\n36. NIST recently released two updated data protection guidelines that are relevant to the Basic Controls in the AI-DSF:*Protecting Controlled Unclassified Information in Non-Federal Systems and**Organizations*(NIST SP 800-171, Rev. 3) and its companion*Assessing Security Requirements for Controlled Unclassified Information*(NIST SP 800-171A, Rev 3). Controlled unclassified information (CUI) includes various categories of data such as, health information, intellectual property, personnel records, and general proprietary business information. (A complete list of CUI categories and their description is available at the[National Archives].) While these publications are aimed at assisting federal agencies with setting and managing contractual data protection requirements with their contractors, they contain valuable guidelines that can be used by the private sector as well. One of the useful guidelines found in SP 800-171 relates to data security requirements, which contains 17 sections. Here I highlight just Section 3.17 &#8211;*Supply Chain Risk*Management, specifically the*Discussion*section. It walks through the organization&#8217;s risk in its dependence on its supply chain and the need to protect against threats such as tampering, malware, and counterfeiting. As such, the guidance in 3.17 can serve as a useful reference for drafting the policies, processes, procedures, and practices that make up the Basic Controls.\n37. Part of the Basic Controls requires routine review, update, and enforcement of data licensing requirements. To effectively execute this, organizations need to have in place written policies that direct this activity. Absent such policies, the effort is doomed as it will be unsustainable. Alignment with the[**Governance**] core principle helps make sure that does not happen. It ensures the effort contains the hallmarks of a mature and legally reasonable approach, one that is systemic, repeatable. Everything that is the opposite of a one-off exercise. The review, update, and enforcement of these requirements is also driven by the contractual agreement the organization has with the data licensor (supplier). As such, it is also important to make sure that the routine review part of the effort is not only focused on the contract itself, but also more broadly at examining the contracting practices that produce the contracts and making adjustments as necessary.\n38. Data exfiltrated from cybersecurity breaches frequently ends up on the dark web. The data is then sold to other malicious actors and ultimately propagates throughout the internet, polluting it. Take the National Public Data breach, for example. It happened months ago, but we are only learning about it now; and it&#8217;s possibly the largest breach ever. Just how big? Well, the number varies somewhat, but so far it looks like between 2.5 and 2.9 billion records were compromised. Full names, social security numbers, email addresses, phone numbers, and mailing address. The potential for damage here is concomitant with the size of the affected population. Aside from the cybersecurity and privacy issues these breaches bring about, these incidents (and not just the big ones) are a useful reminder to take AI data stewardship seriously. The Basic Controls in the AI-DSF serve to ensure that data ingested by the AI model is controlled, is clean, unpolluted by breach-sourced data.\n39. Most AI-specific legislation in the pipeline provide individuals negatively impacted by these so-called &#8220;automated decision-making&#8221; systems with the right to correct their data. In theory, it sounds good, but that&#8217;s about it. Suppose the individual contacts the company&#8217;s customer service with this request. The likelihood they will know what to do with the request is low, more likely zero. However, if the company in question follows the AI-DSF or a similar framework, it stands a better chance of complying.\n40. Guarding against model inference and inversion attacks can be enhanced through the AI-DSF through the Basic Controls and Foundational Controls. Here, the Data Governance and Data Inventory functions in the Basic Controls and the Audit and Control in the Foundational Controls work together. They help protect against these attacks not via technical means, but through appropriate contractual provisions with the provider. It&#8217;s essentially externalizing the risk, shifting it to the provider, making it warrant that the training dataset is not vulnerable to these attacks. If the provider uses the AI-DSF, or a similar framework, they should be in a good position to warrant as much. And if they don&#8217;t use such a framework, then it begs the question: Why? How are they managing quality?\n41. The Department of Justice recently updated its Evaluation of Corporate Compliance Programs (ECCP). The ECCP helps prosecutors assess how well an organization&#8217;s compliance program works. It&#8217;s important because it influences their decisions on whether to press charges, what sentences to recommend, and the best way to resolve cases involving corporate crimes. One of the items in the latest ECCP relates to data access and this is directly on point with the AI-DSF. Specifically, &#8220;[h]ow is the company managing the quality of its data sources.&#8221; (See page[13].) Though the setting here is one dealing with criminal investigations, the logic remains sound in other settings. Failure to have and implement the AI-DSF (or a similar formal framework) is not going to be easy to explain.\n42. The FTC&#8217;s stance on data management provides additional support for implementing the AI-DSF. In its technology blog post &#8220;[Lenses of security: Preventing and mitigating digital security risks through data management, software development, and product designs for humans] &#8221; the FTC leaves no room for doubt: Failing to implement a data stewardship program is inexcusable. This is especially clear in the section titled &#8220;Security in data management.&#8221; I often share the following story in my lectures and presentations. It recounts a meeting I was attending in which the marketing team was being grilled by the CEO on a product design that they missed. The company&#8217;s primary competitor was going to eat the company&#8217;s lunch. I recall the CEO asking the head of marketing: &#8220;How did you not know this?&#8221; It&#8217;s a scary question. I urge you to think about it in the context of how your company deals with data management. Do you have a good answer?\n43. *Thomson Retuers v. Ross Intelligence*is a useful example of the importance of using (not just having) a formal data stewardship framework. In their effort to build their AI legal search application, Ross asked Thomson Reuters for a license to use its Westlaw headnotes and Key Number System. After Thomson Reuters refused, Ross hired LegalEase to generate summaries of legal issues and identify relevant cases which would then be used to train the Ross application. LegalEase generated the training data by referencing Westlaw headnotes and Ross used that to build its AI legal search application. The district court found that Ross infringed on Thomson Reuters&#8217; copyright and Ross subsequently went out of business. While this is not a generative AI case, it is the first time that a federal court addressed fair use as a defense in the context of training an AI natural language processing (NLP) application. The difference is significant, and this case may turn out to be limited to just that, NLP. But the data stewardship principles are valid here as well. By observing the AI-DSF controls and the relevant AI life cycle core principles, AI developers can minimize the likelihood that they&#8217;re infringing.\n44. A well-drafted indemnification and defense provision isn&#8217;t enough. It can create and perpetuate an illusion of safety in the indemnified party. Suppose that during their contract negotiations a deployer demanded and the developer immediately agreed to their indemnity and defense language. No negotiation. No changes. In my experience this rarely happens. That&#8217;s because indemnity and defense is an obligation that has all the makings of a heavy-duty financial undertaking. Therefore, the party called on to provide it will seek to minimize their risk profile as much as possible and this can result in some potentially extensive back and forth rounds of negotiation. When there&#8217;s no pushback, however, that should give the deployer pause. Since there are no language issues to grapple with, the next inquiry for the deployer to dive into is whether the developer actually has the ability to deliver on its indemnity and defense obligations. In this process, it&#8217;s worthwhile turning to the**Wherewithal**life cycle core principle. Among its components is the call to confirm the developer is financially sound. Since a financial audit can, but isn&#8217;t usually undertaken in this process, the next thing to look for is the developer&#8217;s insurance. In this process, it is important to determine if they have enough coverage (type and amount), that the insurance is from a financially stable provider (measured by their AM Best rating), that there&#8217;s a waiver of subrogation, and that the developer has an obligation to renew the policy at equal or better terms. Absent such a diligent approach, the indemnification and defense language they agreed to is meaningless. Now let&#8217;s bring all of this into the*Thomson Reuters*setting. Did Ross demand indemnity and defense obligations from LegalEase? If it did, did it also reference guidelines such as those found the Wherewithal core principle? Considering that it went out of business due to this legal battle, it&#8217;s a safe bet the answer is in the negative. Considering that it went out of business due to this legal battle serves as a powerful case for following the**Wherewithal**core principle.\n45. ISO/IEC 5259 deals with AI data quality and serves as a key reference for implementing the AI-DSF. As a standard, a key implementation characteristic it provides is formality. The best way to understand &#8220;formality&#8221; is to remember it is the opposite of a one-off, ad hoc effort. Formality is essential for putting in practice the all the necessary processes and procedures. It&#8217;s also essential for projecting due diligence, which helps mitigate risk. The lack of formality opens the door to tough questions such as &#8220;why didn&#8217;t the company do this again&#8221; which tend to sabotage efforts to portray the company as being diligent. Consider this also from the perspective of an acquisition. Observance of formality can help the company position itself as a more attractive candidate than one that seemingly doesn&#8217;t have its act together.\n46. Without strong controls around information security there is virtually no prospect for aligning with the AI-DSF&#8217;s goal of ensuring high-quality data. The absence of these controls should lead to the conclusion that the organization&#8217;s data is already compromised. Having evidence that this is so just reinforces what&#8217;s already known. This is, of course, an unacceptable situation and quite likely legally negligent. This is why we see in the Foundational Controls mention of ISO 27014, a standard dedicated to information security governance. This directly ties into the Governance life cycle core principle, which is mentioned frequently in the discussion above &#8212; and which bears mentioning again &#8212; is the single most important AI life cycle core principle.\n**Back to the Top",
    "length": 46866,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Generative AI for Education Hub",
    "url": "https://scale.stanford.edu/genai",
    "text": "Generative AI for Education Hub\n[Skip to main content] \n[![Stanford University]] \n[![AI Hub for Education]] ### **Moving from AI Promise to Learning Impact**\n**AI Hub for Education**delivers trusted research, insights, and tools for K12 education leaders to leverage generative AI to benefit students, schools, and learning.\n![] \n## The Challenge\nGenerative AI is transforming education at lightning speed, with schools and districts nationwide grappling with how to harness its power responsibly and effectively. With new technology, a fast changing market, and limited data on the effectiveness of new tools, it is difficult for K-12 system leaders to make informed decisions about AI implementation. Superintendents, state K-12 leaders, and policymakers face unprecedented challenges: eliminating bias, privacy concerns, and adapting learning models to these new capacities - all while ensuring that students, teachers, and schools benefit.\n****\n## Our Solution\nThe Generative AI for Education Hub is the trusted source for superintendents and state/federal K-12 leaders on what works for leveraging generative AI to benefit students, schools, and learning. Schools need clear and actionable guidance to make the right decisions. We partner with school districts, policymakers, and ed tech innovators to leverage data, research effects, and bridge the gap between AI’s potential and real-world impact. From practical tools for next semester to an aspirational vision of learning reimagined supported by genAI, we help education leaders with pragmatic answers to solve their most pressing problems.\n## What We Offer\n* **\n### Cutting-Edge Research\nWe explore the most promising uses of AI in education, backed by rigorous, real-world studies.\n* **\n### Practical Tools and Frameworks\nFrom typologies to policy briefs, our tools help decision-makers quickly assess and implement AI solutions.\n* **\n### Expert Guidance\nOur team of leading researchers and education experts provide insights that help you stay ahead of the curve.\n[![Stanford University]] \n* [Stanford Home] \n* [Maps &amp; Directions] \n* [Search Stanford] \n* [Emergency Info] \n* [Privacy] \n* [Copyright] \n* [Trademarks] \n* [Non-Discrimination] \n* [Accessibility] \n©Stanford University. Stanford, California 94305.",
    "length": 2276,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "‎ | HEAL-AI",
    "url": "https://heal-ai.stanford.edu/dax-copilot-ethical-assessment",
    "text": "‎ | HEAL-AI\n[Skip to main content] [Skip to secondary navigation] \n[Stanford University(link is external)] \n[\nStanfordHEAL-AI\nHealthcare Ethical Assessment Lab for AIHealthcare Ethical Assessment Lab for AI\nHealthcare Ethical Assessment Lab for AI\n] \n![] \n## Copilot Ethics Assessment\n# ‎Main content start\n### Tool Overview\nThis generative AI tool, nicknamed Copilot, canhelp doctors summarizepatient visits. Currently, doctors spend twice as much time on electronic medical record documentation as they do with patients, contributing to burnout. Copilot uses voice recognition and large language models to transcribe and summarize conversations during visits, potentially reducing documentation time and allowing doctors to focus more on patients. It can distinguish among speakers, understand medical terms, and organize summaries into key sections. Doctors would still need to review and edit the AI-generated summaries before they’re added to the patient’s record.\n### Report Summary\nThe use case is promising, and ethical considerations do not militate against deployment. Stakeholders, including patients, were enthusiastic about its potential to reduce documentation burden and improve the quality of physician-patient interactions and expressed confidence that the tool would be responsibly deployed. Ongoing evaluation of the tool as its use is expanded should focus on (1) better ascertainment of inaccuracies that involve risk of patient harm and whether measurable benefits are accrued to clinicians or patients; (2) potential for lower performance for patients with limited or accented English, speech impediments, complex visits, or caregivers speaking during the visit; and (3) over the long term, unintended consequences of automation bias (lack of physician editing to ensure accuracy) and de-skilling (decreasing beneficial cognitive processing through note writing, especially by trainees). More explicit guidance for obtaining informed consent would also help address an issue of concern to patients: use of data by the third-party vendor.\n### Key Issues Identified\n* All stakeholders express generalized concerns about possible bias, but even developers seem to have poor visibility into actual performance for patient subgroups, e.g.,patients with limited or accented English, speech impediments, complex visits, or caregivers speaking during the visit\n* Evaluating the extent to which tool output contains clinically significant inaccuracies will be quite challenging due to several factors, including lack of a benchmark against which to compare the quality of the tool’s summaries.\n* Clinicians have high optimism about the tool, which may heighten the riskof automation bias. They have little understanding of (or interest in) the adequacy of the training data.\n* Avoiding unintended harm by correcting inaccuracies in the draft summaries may require more human oversight than is likely to occur or is commensurate with the objective of reducing physicians’ workload.\n* It is unclear what information patients receive when asked for consent, especially concerning transmission and use of their data.\n[Download Full Report] \nBack to Top",
    "length": 3162,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Safeguarding Third-Party AI Research | Stanford HAI",
    "url": "https://hai.stanford.edu/policy/safeguarding-third-party-ai-research",
    "text": "Safeguarding Third-Party AI Research | Stanford HAI\n### Stay Up To Date\nGet the latest news, advances in research, policy work, and education program updates from HAI in your inbox weekly.\n[\n**Sign Up**For Latest News\n] \n[] \n* [] \n* [] \n* [] \n* [] \n* [] \n* [] \n###### Navigate\n* [\nAbout\n] \n* [\nEvents\n] \n* [\nCareers\n] \n* [\nSearch\n] \n###### Participate\n* [\nGet Involved\n] \n* [\nSupport HAI\n] \n* [\nContact Us\n] \n[Skip to content] \n[] \n[] \n* [] \n* [] \n* [] \n* [] \n* [] \n* [] \n[] \n[\npolicyPolicy Brief\n] # Safeguarding Third-Party AI Research\nDate\nFebruary 13, 2025\nTopics\n[\nPrivacy, Safety, Security\n] [\nRegulation, Policy, Governance\n] \n![Safeguarding third-party AI research] \n[**Read Paper**] \nabstract\nThis brief examines the barriers to independent AI evaluation and proposes safe harbors to protect good-faith third-party research.\n### Key Takeaways\n* Third-party AI research is essential to ensure that AI companies do not grade their own homework, but few companies actively protect or promote such research.\n* We found no major foundation model developers currently offer comprehensive protections for third-party evaluation. Instead, their policies often disincentivize it.\n* A safe harbor for good-faith research should be a top priority for policymakers. It enables good-faith research and increases the scale, diversity, and independence of evaluations.\n### Executive Summary\nThird-party evaluation is a cornerstone of efforts to reduce the substantial risks posed by AI systems. AI is a vast field with thousands of highly specialized experts around the world who can help stress-test the most powerful systems. But few companies empower these researchers to test their AI systems, for fear of exposing flaws in their products. AI companies often block safety research with restrictive terms of service or by suspending researchers who report flaws.\nIn our paper, “[**A Safe Harbor for AI Evaluation and Red Teaming**],” we assess the policies and practices of seven top developers of generative AI systems, finding that none offers comprehensive protections for third-party AI research. Unlike with cybersecurity, generative AI is a new field without well-established norms regarding flaw disclosure, safety standards, or mechanisms for conducting third-party research. We propose that developers adopt safe harbors to enable good-faith, adversarial testing of AI systems.\n### Introduction\nGenerative AI systems pose a wide range of potential risks, from enabling the creation of nonconsensual intimate imagery to facilitating the development of malware. Evaluating generative AI systems is crucial to understanding the technology, ensuring public accountability, and reducing these risks.\nIn July 2023, many prominent AI companies signed voluntary commitments at the White House, pledging to “incent third-party discovery and reporting of issues and vulnerabilities.” More than a year later, implementation of this commitment has been uneven. While some companies do reward researchers for finding security flaws in their AI systems, few companies strongly encourage research on safety or provide concrete protections for good-faith research practices. Instead, leading generative AI companies’ terms of service legally prohibit third-party safety and trustworthiness research, in effect threatening anyone who conducts such research with bans from their platforms or even legal action. For example, companies’ policies do not allow researchers to jailbreak AI systems like ChatGPT, Claude, or Gemini to assess potential threats to U.S. national security.\nIn March 2024, we penned an open letter signed by over 350 leading AI researchers and advocates calling for a safe harbor for third-party AI evaluation. The researchers noted that while security research on traditional software is protected by voluntary company protections (safe harbors), established vulnerability disclosure norms, and legal safeguards from the Department of Justice, AI safety and trustworthiness research lacks comparable protections.\nCompanies have continued to be opaque about key aspects of their most powerful AI systems, such as the data used to build their models. Developers of generative AI models tout the safety of their systems based on internal red teaming, but there is no way for the government or independent researchers to validate these results, as companies do not release reproducible evaluations.\nGenerative AI companies also impose barriers on their platforms that limit good-faith research. Similar issues plague social media: Companies have taken steps to prevent researchers and journalists from conducting investigations on their platforms that, together with federal legislation, have had a chilling effect on such research and worsened the spread of harmful content online. But conducting research on generative AI systems comes with additional challenges, as the content on generative AI platforms is not publicly available. Users need accounts to access AI-generated content, which can be restricted by the company that owns the platform. Many AI companies also block certain user requests and limit the functionality of their models to prevent researchers from unearthing issues related to safety or trustworthiness. The stakes are also higher for AI, which has the potential not only to turbocharge misinformation but also to provide U.S. adversaries like China and Russia with material strategic advantages.\nTo assess the state of independent evaluation for generative AI, our team of machine learning, law, and policy experts conducted a thorough review of seven major AI companies’ policies, access provisions, and related enforcement processes. We detail our experiences with evaluation of AI systems and potential barriers other third-party evaluators may face, and propose alternative practices and policies to enable broader community participation in AI evaluation.\n[**Read Paper**] \nShare\n[] [] [] \nLink copied to clipboard!\nAuthors\n* [\n![Kevin Klyman] \n###### Kevin Klyman\n] \n* [\n![Shayne Longpre] \n###### Shayne Longpre\n] \n* [\n![Sayash Kapoor] \n###### Sayash Kapoor\n] \n* [\n![Rishi Bommasani] \n###### Rishi Bommasani\n] \n* [\n![Percy Liang] \n###### Percy Liang\n] \n* [\n![Peter Henderson] \n###### Peter Henderson\n] \n### Related Publications\n##### [Response to OSTP&#x27;s Request for Information on Accelerating the American Scientific Enterprise] \n[Rishi Bommasani,] [John Etchemendy,] [Surya Ganguli,] [Daniel E. Ho,] [Guido Imbens,] [James Landay,] [Fei-Fei Li,] [Russell Wald] \nQuick ReadDec 26, 2025\nResponse to Request\n![] \nStanford scholars respond to a federal RFI on scientific discovery, calling for the government to support a new “team science” academic research model for AI-enabled discovery.\nResponse to Request\n![] \n#### [Response to OSTP&#x27;s Request for Information on Accelerating the American Scientific Enterprise] \n[Rishi Bommasani,] [John Etchemendy,] [Surya Ganguli,] [Daniel E. Ho,] [Guido Imbens,] [James Landay,] [Fei-Fei Li,] [Russell Wald] \n[Sciences (Social, Health, Biological, Physical)] [Regulation, Policy, Governance] Quick ReadDec 26\nStanford scholars respond to a federal RFI on scientific discovery, calling for the government to support a new “team science” academic research model for AI-enabled discovery.\n##### [Response to FDA&#x27;s Request for Comment on AI-Enabled Medical Devices] \n[Desmond C. Ong,] [Jared Moore,] [Nicole Martinez-Martin,] [Caroline Meinhardt,] [Eric Lin,] [William Agnew] \nQuick ReadDec 02, 2025\nResponse to Request\n![] \nStanford scholars respond to a federal RFC on evaluating AI-enabled medical devices, recommending policy interventions to help mitigate the harms of AI-powered chatbots used as therapists.\nResponse to Request\n![] \n#### [Response to FDA&#x27;s Request for Comment on AI-Enabled Medical Devices] \n[Desmond C. Ong,] [Jared Moore,] [Nicole Martinez-Martin,] [Caroline Meinhardt,] [Eric Lin,] [William Agnew] \n[Healthcare] [Regulation, Policy, Governance] Quick ReadDec 02\nStanford scholars respond to a federal RFC on evaluating AI-enabled medical devices, recommending policy interventions to help mitigate the harms of AI-powered chatbots used as therapists.\n##### [Jen King&#x27;s Testimony Before the U.S. House Committee on Energy and Commerce Oversight and Investigations Subcommittee] \n[Jennifer King] \nQuick ReadNov 18, 2025\nTestimony\n![] \nIn this testimony presented to the U.S. House Committee on Energy and Commerce’s Subcommittee on Oversights and Investigations hearing titled “Innovation with Integrity: Examining the Risks and Benefits of AI Chatbots,” Jen King shares insights on data privacy concerns connected with the use of chatbots. She highlights opportunities for congressional action to protect chatbot users from related harms.\nTestimony\n![] \n#### [Jen King&#x27;s Testimony Before the U.S. House Committee on Energy and Commerce Oversight and Investigations Subcommittee] \n[Jennifer King] \n[Privacy, Safety, Security] Quick ReadNov 18\nIn this testimony presented to the U.S. House Committee on Energy and Commerce’s Subcommittee on Oversights and Investigations hearing titled “Innovation with Integrity: Examining the Risks and Benefits of AI Chatbots,” Jen King shares insights on data privacy concerns connected with the use of chatbots. She highlights opportunities for congressional action to protect chatbot users from related harms.\n##### [Russ Altman’s Testimony Before the U.S. Senate Committee on Health, Education, Labor, and Pensions] \n[Russ Altman] \nQuick ReadOct 09, 2025\nTestimony\n![] \nIn this testimony presented to the U.S. Senate Committee on Health, Education, Labor, and Pensions hearing titled “AI’s Potential to Support Patients, Workers, Children, and Families,” Russ Altman highlights opportunities for congressional support to make AI applications for patient care and drug discovery stronger, safer, and human-centered.\nTestimony\n![] \n#### [Russ Altman’s Testimony Before the U.S. Senate Committee on Health, Education, Labor, and Pensions] \n[Russ Altman] \n[Healthcare] [Regulation, Policy, Governance] [Sciences (Social, Health, Biological, Physical)] Quick ReadOct 09\nIn this testimony presented to the U.S. Senate Committee on Health, Education, Labor, and Pensions hearing titled “AI’s Potential to Support Patients, Workers, Children, and Families,” Russ Altman highlights opportunities for congressional support to make AI applications for patient care and drug discovery stronger, safer, and human-centered.",
    "length": 10477,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "AI Teaching Strategies",
    "url": "https://ctl.stanford.edu/aimes/ai-teaching-strategies",
    "text": "AI Teaching Strategies | Center for Teaching and Learning\n[Skip to main content] [Skip to secondary navigation] \n[Stanford University(link is external)] \n[\nStanford\nCenter forTeaching and Learning\n] \nSearch this siteSubmit Search\n# AI Teaching Strategies\n## Main navigation\n[Skip Secondary Navigation] \nMain content start\nWhile the[AIMES Library of Examples] allows you to filter and explore real course policies, assignments, and other teaching artifacts, this page provides a brief overview of strategies that may be helpful when assigning, limiting, and prohibiting AI use.\n## Introduction\nGenerative AI has permeated higher education, with rapid adoption by students and with instructors wondering whether, and how, to incorporate its use in teaching and learning. Research on educational uses of generative AI is still nascent, and we do not yet have extensive, stable, or discipline-specific bodies of evidence to draw from. That said, Stanford instructors have been exploring, testing, and critiquing uses of generative AI in teaching and learning on campus. The emerging practices discussed here, synthesized from Stanford examples, are consistent with broader educational research findings.\n## Educational Approaches to AI\nInstructors have adopted a variety of approaches to use of AI in their courses. Here are three approaches that have emerged. Click to jump to an approach, or scroll down for pedagogical strategies that apply across all approaches, followed by more details about each one.\n![AI-Assigned] \nWhen you want students to learn AI skills such as prompting and verifying, practice AI tasks in a particular field or domain, and explore or critique AI.\n[Strategies When Assigning AI Use] \n![AI-Limited] \nWhen you want students to navigate AI responsibly, make and justify decisions about AI use, and disclose and cite AI use alongside other kinds of resources.\n[Strategies When Limiting AI Use] \n![AI-Prohibited] \nWhen you want students to develop skills independently and in collaboration with peers and instructors.\n[Strategies When Prohibiting AI Use] \n## Pedagogical Strategies Across Approaches to AI\n### Set and communicate a detailed and specific course policy\n* No matter what approach you take to use of AI in your course, write a clear course policy in the syllabus, with enough detail to address the specific kinds of work students will do in the course.\n* Provide examples of applying the course policy in scenarios that you anticipate and that students ask about.\n* Revisit the course policies periodically throughout the course, especially when students encounter a new form of coursework, assignment, or assessment for the first time.\n* The[CTL Syllabus Template] includes sample policies for limited and prohibited AI use, but you will likely benefit from tailoring them to your specific course.### Address transparency and motivation\n* Share with students why you have chosen your approaches to AI and how it is informed by the course learning outcomes. What skills do you want them to develop? How will those skills serve them in their studies, life, and work?\n* Revisit your reasoning behind the approach to AI throughout the course–for example, in the context of learning goals, class activities, assignments, and assessments.\n* Emphasize students’ progress on goals and skills, whether related to AI use or on tasks they complete on their own, through individual feedback and discussion with the whole class.### Emphasize alignment and process\n* Ensure that course policies are aligned with the goals and reasons you have for students to use, limit, or avoid generative AI.\n* Require that students show their processes of doing the work of the course (e.g., problem solving process, writing process, analysis process, coding process, reasoning process, prompting process, revision process) as components of assignments.\n* For major assignments such as projects, long papers, and capstones, include several stages that provide scaffolding and feedback on the way to the completed product. Logs or journals that accompany major projects and are reviewed with the instructor or TA periodically can also be effective.### Further Reading\n* Bertram Gallant, T., &amp; Rettinger, D. A. (2025). The Opposite of Cheating: Teaching for Integrity in the Age of AI. University of Oklahoma Press.[Stanford University Libraries Catalog] (login required).\n* Flaherty, C. (2025, August 29). How AI Is Changing--Not 'Killing'--College.*Inside Higher Ed.*[www.insidehighered.com/news/students/academics/2025/08/29/survey-college-students-views-ai] \n![AI-Assigned] \n## Strategies When Assigning AI Use:\nIn addition to[strategies relevant to all approaches to AI]:\n### Make AI Use Transparent and Motivational\n* Share with students why you are asking them to use AI in this course or assignment. How and why does AI enhance what is possible in this context, while still giving students practice and helping students advance their own thinking and skills?\n* Revisit and check in with students about their use of AI. Asking students to reflect, through critical thinking and metacognition, helps ensure that AI is incorporated as part of students’ own learning, not to replace it.\n* Provide a course policy that specifies how and why AI use is assigned in the course.### Provide instruction and practice on responsible use of AI\n* Guide students to go to the Stanford AI Playground. Discuss the risks of sharing sensitive data or information. Open up a conversation about ethical use of AI in ways that tap into disciplinary perspectives represented in the course.\n* Consider whether an alternative assignment or approach may be necessary in cases where a student has a strong ethical objection to using AI tools.\n* Model how you expect students to integrate AI into their work through in-class examples and guided assignments.\n* Discuss and model examples of prompting and fact-checking AI results in ways that are specific and appropriate to your discipline, course, and assignment.### Assess students’ use of AI along with other objectives\n* Articulate criteria and create a rubric that illustrates the important AI-related skills you will evaluate in students’ work.\n* Require that students show their process with using AI in the course, e.g., save and turn in full histories of prompts and outputs, how and why they chose specific generative AI tools (if given a choice).\n* Follow up on assignments in which students use AI tools, e.g., by asking students to explain their reasoning, discuss an example from their work, explain a technique they used, and articulate their approach to working with AI tools.\n* Give students feedback on their use of AI as well as other components of assignments.### Further Reading\nThe following publications highlight recent discussion about incorporating AI use in higher education courses. This list is not a comprehensive bibliography.\n* AI Pedagogy Project (N.D.). Assignments.[aipedagogy.org/assignments/] \n* Bowen, J. A. &amp; Watson, C. E. (2024).*Teaching with AI: a Practical Guide to a New Era of Human Learning*. Johns Hopkins University Press.[Stanford University Libraries Ebook] (login required).\n* Dungo, C. A. B., Beltran, Z. L. E., Declaro, B. C., Dela-Cruz, J. J. C., &amp; Viray, R. U. (2025). Students’ level of awareness on the environmental implications of generative AI.*Journal of Education in Science, Environment and Health, 11(2)*, 93-107.[doi.org/10.55549/jeseh.777] \n* Perkins, Furze, Roe &amp; MacVaugh (2024). The Al Assessment Scale.[aiassessmentscale.com/] (includes several peer-reviewed publications using the scale)\n* Yang, T., Cheon, J., Cho, MH. et al. Undergraduate students’ perspectives of generative AI ethics.*Int J Educ Technol High Educ 22*, 35 (2025).[doi.org/10.1186/s41239-025-00533-1] \n![AI-Limited] \n## Strategies When Limiting AI Use:\nIn addition to[strategies relevant to all approaches to AI]:\n### Clarify what AI uses are permitted, what uses are prohibited, and why\n* Create a specific course policy about use of AI. Where some uses are allowed and others are not, additional detail may be needed.\n* Consider providing a table or list that breaks down and compares different circumstances where AI may and may not be used.\n* Share with students why they should follow your guidance about when it is and is not appropriate to use AI in your course or assignment, and how these policies connect to course learning goals, skills that are useful in their studies, life, and work, and students future goals.\n* Be explicit about the kinds of activities AI could productively enhance and detract from in this particular academic context.\n* Consider ways in which limited use of AI may support students with learning differences.### Engage with students on AI-limited tasks\n* Model the kind of thinking, analysis, discussion, problem-solving, and other academic work that your course is designed to help students achieve.\n* Demonstrate and discuss the drawbacks of relying on forms of AI assistance that is limited in your class.\n* Check in with students about how they are experiencing the different AI use cases in their learning.### Guide students on allowed use of AI\n* Guide students on how to disclose and cite AI use. Some instructors require a disclosure with every assignment; it can also include reflection about how and why the students approached their work on the assignment and what kinds of tools were most and least helpful.\n* Be upfront about how AI could limit students’ learning, as well as where AI may assist students in deep learning, and how to tell the difference.\n* Guide students to go to the Stanford AI Playground for instances when they do use AI in the course or assignment. Discuss the risks of sharing sensitive data or information. Open up a conversation about ethical use of AI in ways that tap into disciplinary perspectives represented in the course.\n* Discuss and model examples of prompting and fact-checking AI results in ways that are specific and appropriate to your discipline, course, and the AI use cases that you allow.### Assess students fairly and accurately\n* Articulate assessment criteria and create rubrics that include appropriate disclosure and citation of AI use, alongside the important learning goals for the assignment or exam.\n* Make sure there are no hidden penalties for using AI in ways that are permitted. Some research in workplace settings has indicated an inequitable \"competence penalty,\" in which evaluators gave lower ratings when they believed work products were created with AI assistance (Acar et a. 2025).### Further Reading\nThe following publications highlight recent discussion related to limiting AI use in higher education courses. This list is not a comprehensive bibliography.\n* Acar, O. A., Gai, P. J., Tu, Y., &amp; Hou, J. (2025, August 1). Research: The Hidden Penalty of Using AI at Work.*Harvard Business Review*.[hbr.org/2025/08/research-the-hidden-penalty-of-using-ai-at-work] \n* Hsu, H. (2025). The End of the Essay.*New Yorker*, 101(19), 21-27.[Stanford University Libraries full text] (login required).\n* Kinsey, V. (2025, March 13). Teaching with AI: Resources, Fears, Reflections, Invitation. In the Moment, Stanford Teaching Writing.[teachingwriting.stanford.edu/news/teaching-ai-resources-fears-reflections-invitation] \n* Perkins, Furze, Roe &amp; MacVaugh (2024). The Al Assessment Scale.[aiassessmentscale.com] (includes several peer-reviewed publications using the scale)\n* McGee, N. J., Kozleski, E., Lemons, C. J., &amp; Hau, I. C. (2025). AI+Learning Differences: Designing a Future with No Boundaries. White Paper, Stanford Accelerator for Learning.[acceleratelearning.stanford.edu/app/uploads/2025/07/AI-Learning-Differences-Designing-a-Future-with-No-Boundaries\\_Final.pdf] \n![AI-Prohibited] \n## Strategies When Prohibiting AI Use:\nIn addition to[strategies relevant to all approaches to AI]:\n### Communicate with students\n* Discuss with students your reasons for prohibiting AI use. These could include perspectives from the course or the broader discipline, emphasis on learning objectives that can best be developed by students working independently, and objections to various aspects of generative AI development and sustainability (e.g., intellectual property used in training, environmental impacts, and more).\n* Ask students about their experience with using AI in similar courses so that you understand what will be familiar and what will be new in this academic setting.\n* Make room for discussion. While staying true to your course policies and approaches to AI, giving students a chance to ask questions and discuss their perspectives may increase their investment, motivation, and understanding.### Be explicit about the importance of learning without AI\n* Normalize cognitive effort and struggle, which are crucial for learning, but can be shortchanged by use of AI. Discuss what it is like to be unsure, confused, and to grapple with concepts in this course, and why those challenging experiences are important for learning.\n* Use feedback and discussion about students’ work to increase their engagement and accountability for the development of their own ideas and lead to strong final products without AI assistance.\n* Bring some of the important intellectual and academic work into class, where you can provide guidance and modeling. For example, have students generate and refine ideas through short, individual writing and discussion with peers and instructors in class. Doing so may help students feel more ownership and be less likely to turn to AI for when they are working on their own outside of class.### Address AI-related changes (even while not allowing its use)\n* Become familiar with where and how students may inadvertently encounter AI assistance built into software, searches, and other platforms they use for coursework. Demonstrate and discuss the drawbacks of relying on AI assistance that is prohibited in your class and advise students about how to disable AI in the applications they use. Seek support from university technology offices to learn more about AI in campus applications.\n* Recognize that incoming students may be accustomed to relying on AI assistance. If students have offloaded crucial skills and habits of mind to AI in prior learning environments, modeling and guidance may be necessary.\n* Provide a “safety valve” for students to admit to making mistakes in following the AI policy without it being catastrophic to their grade. For example, some instructors ask students to disclose AI use on every assignment, even when it is prohibited, and then discuss what went wrong or led to the inappropriate use of AI, in order to support students in planning a revised approach next time. You could limit the number of times students may use such a safety valve.### Redesign assignments and assessments\n* Follow up on assignments completed outside of class, e.g., by asking students to explain their reasoning, discuss an example from their work, or explain a technique they used. If students do not seem to understand their work, they may need to redo the assignment with your guidance on how to develop a deeper understanding, whether or not they used AI assistance.\n* Especially on lower-stakes assignments or those leading up to longer projects and papers, provide feedback that focuses on progress toward learning goals and how to improve, rather than only on getting the right answer.\n* Consider conducting assessments in class (e.g., \"blue books\" or other in-class written formats). As of fall 2025, proctored exams are only allowed as part of the[Academic Integrity Working Group Proctoring Pilot], but additional courses may request to join the pilot each quarter.\n* Help students prepare for in-class and/or proctored assessments. Remind them of the conditions they will encounter and encourage them to practice under similar conditions before the exam. Scaffold learning experiences leading up to timed, in-class writing so that a higher-stakes assessment is not the first time students experience the format.### Further Reading\nThe following publications highlight recent discussion about prohibiting AI use in higher education courses. This list is not a comprehensive bibliography.\n* Bayley, T. Maclean, K. D. S., Weidner, T. (2024). Back to the Future: Implementing Large-Scale Oral Exams.*Management Teaching Review*.[doi.org/10.1177/23792981241267744] \n* Fernandes, M., Sano-Franchini, J., McIntyre, M. (2025). What is GenAI Refusal? Refusing Generative AI in Writing Studies.[refusinggenai.wordpress.com/what-is-refusal/] \n* Fritts, M. (2025, May 23). What I Learned Serving on My University’s AI Committee: We need to embrace a more radical response.*Chronicle of Higher Education*.[www.chronicle.com/article/what-i-learned-serving-on-my-universitys-ai-committee] \n* O’Rourke, M. (2025, July 18). I Teach Creative Writing. This is What A.I. is Doing to Students.*New York Times*. Stanford access:[guides.library.stanford.edu/newspapers].\n* Petrocelli, J. V. (2025). Return of the Blue Books: Grading in the Time of Artificial Intelligence.*Change: The Magazine of Higher Learning*, 57(4), 25–28.[doi.org/10.1080/00091383.2025.2511577] \n* Shirky, C. (2025, August 26). Students Hate Them. Universities Need Them. The Only Real Solution to the A.I. Cheating Crisis.*New York Times*. Stanford access: Stanford access:[guides.library.stanford.edu/newspapers].\n## Last updated:\nSeptember 2, 2025\n## [AIMES Library of Examples] \n![AIMES, AI Meets Education at Stanford] \n## [AI Meets Education at Stanford] \nBack to Top",
    "length": 17591,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Stanford CRFM",
    "url": "https://crfm.stanford.edu/2024/04/08/aups.html",
    "text": "Stanford CRFM\n[![]] \n## Acceptable Use Policies for Foundation Models\n#### **Author:**[Kevin Klyman] \n> > *> > Considerations for Policymakers and Developers *> > Policymakers hoping to regulate foundation models have focused on preventing specific objectionable uses of AI systems, such as the creation of bioweapons, deepfakes, and child sexual abuse material. Effectively blocking these uses can be difficult in the case of foundation models as they are general-purpose technologies that in principle can be used to generate any type of content. Nevertheless, foundation model developers have been proactive in this area, adopting broad acceptable use policies that prohibit many dangerous uses that developers select themselves as part of their terms of service or model licenses.\nAs part of the[2023 Foundation Model Transparency Index], researchers at the Stanford Center for Research on Foundation Models cataloged the acceptable use policies of 10 leading foundation model developers. All 10 companies publicly disclose the permitted, restricted, and prohibited uses of their models, but there is little additional information available about these policies or how they are implemented. Only 3 of 10 leading foundation model developers disclose how they enforce their acceptable use policy, while only 2 of 10 give any justification to users when they enforce the policy.\nThis post provides background on acceptable use policies for foundation models, a preliminary analysis of 30 developers’ acceptable use policies, and a discussion of policy considerations related to developers’ attempts to restrict the use of their foundation models.\n## Background\n**What is an acceptable use policy?**\nAcceptable use policies are common across digital technologies. Providers of public access computers, websites, and social media platforms have long[adopted] acceptable use policies that articulate how their terms of service restrict what users can and cannot do with their products and services. While enforcement of these policies is[uneven], restrictions on specific uses of digital technologies are widespread.\nIn the context of foundation model development, an acceptable use policy is a policy from a model developer that determines how a foundation model can or cannot be used.[1] Acceptable use policies restrict the use of foundation models by detailing the types of content users are prohibited from generating as well as domains of prohibited use. Developers make these restrictions binding by including acceptable use policies in terms of service agreements or in licenses for their foundation models.[2] \nAcceptable use policies typically aim to prevent users from generating content that may violate the law or otherwise cause harm. They accomplish this by listing specific subcategories of violative content and authorizing model developers to punish users who generate such content by, for example, limiting the number of queries users can issue or banning a user’s account.\nAcceptable use policies relate to how foundation models are built in important ways. For example, developers frequently filter training data in order to remove content that would violate acceptable use policies. OpenAI’s technical[report] for GPT-4 states “We reduced the prevalence of certain kinds of content that violate our usage policies (such as inappropriate erotic content) in our pre-training dataset, and fine-tuned the model to refuse certain instructions such as direct requests for illicit advice.”[3] \nIn addition, many developers state that the purpose of reinforcement learning from human feedback (RHLF) is to make their foundation models less likely to generate outputs that would violate their acceptable use policies. Meta’s technical[report] for Llama 2 notes that the risks RLHF was intended to mitigate include “illicit and criminal activities (e.g., terrorism, theft, human trafficking); hateful and harmful activities (e.g., defamation, self-harm, eating disorders, discrimination); and unqualified advice (e.g., medical advice, financial advice, legal advice),” which correspond closely to the acceptable use policy in Llama 2’s license. Anthropic’s[model card] for Claude 3 similarly says “We developed refusals evaluations to help test the helpfulness aspect of Claude models, measuring where the model unhelpfully refuses to answer a harmless prompt, i.e. where it incorrectly categorizes a prompt as unsafe (violating our [Acceptable Use Policy]) and therefore refuses to answer.”\n**Distinguishing acceptable use policies from other mechanisms to control model use:**\nAcceptable use policies are not the only means at a developer’s disposal to restrict the use of its models. Several other policy-related mechanisms that developers implement to restrict model use include:\n* **Model Behavior Policy**: Model behavior policies determine what a model can or cannot do. While acceptable use policies apply to user behavior, model behavior policies apply to the behavior of the model itself.[4] A model behavior policy is one way of effectively embedding the acceptable use policy into the model; methods for imposing a model behavior policy include using RLHF to cause the model to be more likely to refuse violative prompts or employing a safety classifier to filter violative model outputs. Model behavior policies generally go well beyond the acceptable use policy in terms of their impact on the model; for example, many developers fine-tune their models to produce more polite responses, though they do not prohibit users from generating impolite responses.\n* **Model Card**: Model cards, which are published alongside machine learning models when they are released, provide essential information about models[such as] their intended uses and out-of-scope uses. However, model cards are not enforceable contracts, and they are not generally referenced in model licenses or developers’ terms of service; as a result, these out-of-scope uses do not rise to the same level as prohibited uses in an acceptable use policy.[5] \n* **API Policy**: Policies relating to application programming interfaces (APIs) determine acceptable user behavior in relation to a company’s API. These policies generally apply to more than one foundation model and differ from acceptable use policies in that they attempt to shape the actions a user takes via an API, not the content that a user elicits from a specific model.[6] Enforcement of API policies differs from enforcement when users are making individual queries in a playground environment or a user interface for a chatbot—companies must consider the risk of mass generation of violative content as well as the creation of services built on top of their models that violate their policies.\n* **Third party contracts**: Foundation model developers frequently partner with other firms to disseminate foundation models. These include cloud service providers (e.g., Amazon Web Services, Microsoft Azure, Google Cloud Platform), platform providers (e.g., Scale AI, Nvidia), database providers (e.g., Salesforce, Oracle), and model distributors (e.g., Together, Quora). Custom contracts with third party providers of a developer’s foundation models often include use restrictions, but the extent to which companies’ acceptable use policies are altered via these partnership agreements is unclear.[7] \n⠀**Emerging norms among closed foundation model developers on acceptable use policies:**\nAlthough generative AI is a nascent industry, norms have begun to emerge around use restrictions for foundation models. In 2022, OpenAI, Cohere, and AI21 Labs[wrote] in their “Best practices for deploying language models” that organizations should “Publish usage guidelines and terms of use of LLMs in a way that prohibits material harm to individuals, communities, and society such as through spam, fraud, or astroturfing.”[8] As part of the November 2023 UK AI Safety Summit, Amazon, Anthropic, Google Deepmind, Inflection, Meta, Microsoft, and OpenAI[shared] their policies related to “preventing and monitoring model misuse,” many of which referenced their acceptable use policies.\n**Limitations of acceptable use policies for foundation models:**\nAs currently implemented, acceptable use policies for foundation models can have serious limitations. Recent[research] from Longpre et al. has shown that of seven major foundation model developers with acceptable use policies, none provide comprehensive exemptions for researchers. As a result, acceptable use policies can act as a disincentive for independent researchers who red team models for issues related to safety and trustworthiness, which led over 350 researchers and advocates to sign an open[letter] calling for a safe harbor for this type of research.\n![] *A summary of how foundation model developers distribute access to their models, safeguard researcher access, and provide transparency regarding enforcement of their acceptable use policies from Longpre et al.*\nThe enforceability of acceptable use policies, especially for open foundation model developers, is a major limitation on how effective they are at restricting use. For example, the legality of enforcement actions based on the use restrictions contained in[Responsible AI Licenses] has not been tested, and in the current ecosystem there are[few] people who will be paid to enforce the terms of these licenses.[9] Even if enforcement were simple, it may require developers to monitor users closely, which could facilitate privacy violations in the absence of adequate data protection, or increase the number of false refusals, reducing the helpfulness of foundation models.[10] \nOn the other hand, use restrictions in model licenses can have a dissuasive effect regardless of enforcement.[11] Most companies and individual users are not bad actors and may adhere to a clear acceptable use policy despite gaps in enforcement. Acceptable use policies can play a helpful role in limiting undesirable uses of foundation models, though they are not without costs and limitations.\n**Government policies related to acceptable use policies for foundation models:**\nGovernments have taken an interest in acceptable use policies, which are a salient effort by foundation model developers to “self-regulate.” The European Union’s AI Act[requires] that all providers of general-purpose AI models disclose the “acceptable use policies [that are] applicable” to both the EU’s AI Office and other firms that integrate the general-purpose AI model into their own AI systems.[12] China’s Interim Measures for the Management of Generative Artificial Intelligence Services, which were[adopted] in July 2023, go a step further by requiring that providers of generative AI services act to prevent users from “using generative AI services to engage in illegal activities…including [by issuing] warnings, limiting functions, and suspending or concluding the provision of services.”[13] In the US, the White House’s[Voluntary AI Commitments], which 16 leading AI companies have adopted, include a provision that companies will publicly report “domains of appropriate and inappropriate use” as well as any limitations of the model that affect these domains.[14] \nNeither the AI Act nor the White House Voluntary Commitments require that companies enforce their acceptable use policies or restrict any particular uses. While China’s Interim Measures for the Management of Generative Artificial Intelligence Services do not detail restrictions on content that is not outright illegal, regulatory[guidance] issued in February 2024 by China’s National Cybersecurity Standardisation Technical Committee specifies many types of prohibited uses such as subverting state power, harming national security, and disinformation.[15] \n## Acceptable Use Policies for Foundation Models in Practice\n![] \n**An overview of foundation model developers’ acceptable use policies:**\nFigure 2 details the acceptable use policies for 30 foundation model developers, including (i) the title of the acceptable use policy, (ii) whether it is applied to a specific foundation model, (iii) whether it is included in a model’s license, the company’s terms of service, or a standalone document, (iv) the developers’ flagship model series (and its modality), (v) the location of the developer’s headquarters, (vi) whether the weights of the developer’s flagship model series are openly available, and (vii) a reference to the public document that includes the acceptable use policy.[16] \nDevelopers use different policy documents to restrict model use. These different types of documents include: standalone acceptable use policies for all of their foundation models (e.g., Google, Stability AI), use restrictions that are built into a general model license (e.g., AI2), use restrictions that are part of a custom model license (e.g., BigScience, Meta, TII), or provisions in an organization’s terms of service that apply to all services including foundation models (e.g., MidJourney, Perplexity, Eleven Labs). Developers release little information about how they enforce their acceptable use policies, making it difficult to assess whether one of these approaches is more viable.\nFoundation model developers that have acceptable use policies are heterogenous along multiple axes. In terms of model release, 12 of the developers openly release the model weights for their flagship model series, while 18 do not. These models have a variety of different output modalities, with 19 language models, 5 multimodal models, 3 image models, 2 video models and 1 audio model. And in terms of where the developers are headquartered, 19 are based in the United States while the remainder include organizations are based in Canada, China, France, Germany, Israel, and the UAE. The different legal jurisdictions in which these organizations operate and make their models available helps explain some of the differences in the substance of their policies as some governments have requirements related to acceptable use policies and specific types of unacceptable uses.\n**Content-based prohibitions[17] **\n![] \nAcceptable use policies commonly prohibit users from employing foundation models to generate content that is explicit (e.g., violence, pornography), fraudulent (scams, spam), abusive (harassment, hate speech), deceptive (disinformation, impersonation), or otherwise harmful (malware, privacy infringements).[18] Many developers have granular restrictions related to these types of prohibited content (e.g., Anthropic, Cohere, OpenAI), whereas others have broad restrictions without much elaboration (e.g., AI21 Labs, Midjourney, Writer). Another group of developers has fairly minimal acceptable use policies, such as open developers like the Technology Innovation Institute, which prohibits use of Falcon 180B to harm minors, disseminate disinformation, or harass others; by contrast, Stability AI, another open developer, has over 45 itemized prohibited uses.[19] \nAcceptable use policies also generally prohibit users from generating content that violates the law, impedes the model developer’s operations, or is not accompanied by adequate disclosure that it is machine-generated. These catch-all prohibitions cover unenumerated content that may be problematic, and make acceptable use policies more malleable and comprehensive by linking them to laws that may change. Importantly, content-based restrictions generally apply only to user prompts that attempt to have a model generate this type of content –models will classify the toxicity of this type of content if asked, but they will not generate it.\n![] \n![] \n![] \nWhile the bulk of these prohibited uses are common across acceptable use policies, there are also a handful of edge cases.[20] (See the underlying data in the[GitHub].) Political content, such as using foundation models for campaigning or lobbying, is explicitly prohibited by OpenAI, Anthropic, Cohere, Midjourney, and others, whereas Google, Meta, and Eleven Labs have no such prohibition.[21] Eating disorder-related content, such as prompts related to anorexia, is explicitly prohibited by Character, Cohere, Meta, and Mistral, but not by other developers. Weapons-related content is explicitly prohibited by the Allen Institute for AI, Anthropic, Meta, Mistral, OpenAI, and Stability, but not by other developers. And while some open developers such as Adept, DeepSeek, and Together broadly prohibit some types of sexual content, others like Meta and Mistral prohibit content related to prostitution or sexual violence.[22] \nSeveral other prohibited uses that stand out include:\n* Undermining the interests of the state: Baidu and DeepSeek, two of three model developers based in China in Figure 2, state in their acceptable use policies that users must not generate content “endangering national security, leaking state secrets, subverting state power, overthrowing the socialist system, and undermining national unity…damaging the honor and interests of the state…undermining the state’s religious policy”. 01.ai, the other Chinese developer, also includes a prohibition against “harming national security”. These restrictions directly parallel China’s regulatory[guidance] on Basic Safety Requirements for Generative Artificial Intelligence Services.\n* Password trafficking: Eleven Labs, the only foundation model developer specializing in audio in Figure 2, prohibits users from using its models to “trick or mislead us or other users, especially in an attempt to learn sensitive account information, for example user passwords”. This may help address common[concerns] regarding the use of voice cloning for scams.\n* Misinformation: The extent to which companies restrict users’ ability to generate inaccurate content varies widely. While some companies’ usage policies include wholesale bans on misinformation (e.g., AI21 Labs, Inflection), others have more lenient restrictions that apply only to verifiable disinformation with the intent to cause harm (e.g., TII).\n![] \n**Restrictions on types of end use:**\nIn addition to content-based restrictions, acceptable use policies for foundation models often restrict the types of activities that users can engage in when using their models. Several notable examples include:\n* Model scraping: Anthropic’s Acceptable Use Policy states that it does not allow users to “Utilize prompts and results to train an AI model (e.g., ‘model scraping’)”. Developers such as Adobe, Aleph Alpha, and Perplexity similarly prohibit the use of outputs of their models for training other foundation models.[23] \n* Scaling distribution of AI-generated content: AI21 Labs’ Usage Guidelines state that “No content generated by AI21 Studio will be posted automatically (without human intervention) to any public website or platform where it may be viewed by an audience greater than 100 people.”\n* Hosting the model with an API: The Technology Innovation Institute’s[license] for Falcon 180B, which incorporates its acceptable use policy, prohibits users from hosting the model with an API.[24] \n⠀**Restrictions on industry-specific uses:**\nA number of acceptable use policies include restrictions that prevent firms in certain industries from making use of the related foundation models.[25] Nevertheless, companies in these industries may succeed in using foundation models by negotiating permissive contracts with the developer. This creates an information asymmetry where the company and its clients are aware of the domains in which foundation models are being used, while regulators and the public may be led to believe that model use is based solely on the publicly disclosed acceptable use policy.\nCommon examples include restrictions on the following industries:\n* Weapons manufacturers: Several acceptable use policies prohibit the use of models for the development of weapons, with some developers’ policies restricting many different specific types of weapons. Although the use of these foundation models by weapons manufacturers would violate the developers’ publicly stated acceptable use policy, it is possible that developers negotiate custom contracts with weapons manufacturers.\n* Legal, medical, and financial advice: 8 of the 30 acceptable use policies prohibit users from leveraging foundation models for use in highly-regulated industries, such as providing legal, medical, and financial advice. This prohibition applies not only to lawyers, doctors, and financial advisers, but also to the many organizations that provide informal advice in these fields.\n* Surveillance: The Allen Institute for AI, Anthropic, Amazon, Google, and OpenAI prohibit use of their models for surveillance to some extent. For instance, Google prohibits use of its models for “tracking or monitoring people without their consent” while AI2 singles out “military surveillance.” This would theoretically prevent spyware companies and defense and intelligence contractors respectively from making use of such foundation models.\n⠀**Foundation models without acceptable use policies:**\nThere are tens of foundation model developers besides the 30 described above and many do not have acceptable use policies for their models.[26] \nSome open foundation model developers do not use acceptable use policies because their models are intended for research purposes only—if they were to adopt use restrictions, it could deter researchers from conducting safety research through red teaming, adversarial attacks, or other means (in the absence of an exemption for research). Other models intended for research may lack acceptable use policies on the basis that they present less severe risks of misuse, whether because they have less significant capabilities or fewer users.[27] Non-commercial models such as these are frequently distributed using licenses without use restrictions such as Apache 2.0 or Creative Commons Attribution-NonCommercial licenses. While a model may not include any use restrictions for noncommercial users, commercial users may have to agree to custom use restrictions in their contracts with the model developer.\nModels for commercial use may not include acceptable use policies for several reasons.[28] In some cases, models are not intended for commercial use without further fine tuning or other safety guardrails, leading developers to offer the model as is, leaving downstream developers to restrict uses (e.g., Databricks’ MPT-30B). Other developers release their models without complete documentation, whether because they intend to release an acceptable use policy at a later point, which could be part of[staged release], or due to under-documentation in the rush to release a model.\nNevertheless, foundation models without acceptable use policies may be covered by other kinds of restrictions. Alibaba Cloud restricts companies with over 100 million users from making use of Qwen-VL through its license, which also bans model scraping. Restrictions on who can use a foundation model may have a significant effect on how it is used even in the absence of binding restrictions on certain categories of use.\n## Discussion:\nGovernments have attempted to spur[self-regulation] in the foundation model ecosystem through voluntary codes of conduct.[29] The G7 and member states such as the US and Canada have issued codes of conduct and received pledges from leading foundation model developers. The G7’s code of conduct[states] that foundation model developers should “take appropriate measures… [to] mitigate risks across the AI lifecycle” including risks related to bias, disinformation, privacy, and cyber. Canada’s code of conduct[requires] developers to create “safeguards against malicious use” while the White House’s “Voluntary AI Commitments”[include] a commitment to “prioritize research on societal risks posed by AI systems” such as risks to children.\nHowever, these codes of conduct stop short of recommending that foundation model developers enact binding restrictions on risky use cases. Foundation model developers have gone beyond voluntary commitments, shielding themselves against legal[liability] for misuse of their models. In this way, acceptable use policies are a significant form of self-regulation.\nStill, there is little publicly available information about how acceptable use policies are actually enforced. Although companies make the prohibited uses of their models clear, it is often unclear how they enforce their policies against users. Just as the 2023 Foundation Model Transparency Index found that 10 leading foundation model developers share little information about how they enforce, justify, and provide appeals processes for acceptable use policy violations, this work finds that other foundation model developers provide little or no information about these matters. This lack of transparency is different from other digital technologies; social media companies, for instance,[regularly] [release] transparency reports that provide[details] about how they enforce their acceptable use policies and other provisions in their terms of service. Without information about how acceptable use policies are enforced, it is not obvious that they are actually being implemented or effective in limiting dangerous uses. Companies are moving quickly to deploy their models and may in practice[invest little] in establishing and maintaining the trust and safety teams required to enforce their policies to limit risky uses.\n![] *A screenshot from Meta’s 2023[transparency report] regarding takedowns on Instagram in line with EU Digital Services Act requirements.*\nAnother issue in gauging the enforcement of acceptable use policies is the way in which they propagate across the foundation model ecosystems. Companies’ publicly available policies often do not specify precisely how their acceptable use policies propagate to third parties such as cloud service providers, and whether third parties have obligations to enforce the original acceptable use policy. In addition to developers, cloud service providers act as deployers of foundation models that were not developed in-house. Cloud service providers have their own acceptable use policies that do not align perfectly with external developers’ acceptable use policies, and it is not clear that a cloud provider would have adequate expertise to restrict the uses of the foundation models in accordance with acceptable use policies that are more stringent than that of the cloud provider.[30] \nAcceptable use policies may impose obligations on users that they are ill equipped to follow. Acceptable use policies are a means of distributing responsibility for harm to users and downstream developers; however, these other actors may not be well suited to prevent certain uses of a developer’s model.[31] Restricting users from generating self-harm related content, for instance, only works for some types of users—for example, vulnerable users who are turning to a language model for advice may trigger acceptable use policies by openly sharing their mental health issues.\nQuestions regarding the enforceability of acceptable use policies extend beyond users who cannot self-enforce. While many acceptable use policies are reasonably comprehensive, the degree to which developers can enforce their policies differs across context and jurisdiction. Enforcement of acceptable use policies differs based on how models are deployed, whether through an API, a user interface operated by the developer, or locally. Strict enforcement is near impossible when models are deployed locally, whereas it resembles content moderation for social media platforms when models are deployed via an API.\nNevertheless, many open foundation model developers attempt to restrict the use of their models to some degree. Twelve of the developers examined that have acceptable use policies openly release their model weights (and other assets in some cases), but do so using licenses or terms of service that block certain unacceptable uses. Although[open foundation models] are frequently referred to as “open-source” in popular media, truly open-source software or machine learning models[cannot] have use restrictions by definition.\nWhile[many] [previous] [works] [have] taxonomized the[risks] [stemming] from foundation models, this post assesses how companies taxonomize risk on the basis of their own policies. Future research directions include assessing the enforcement of acceptable use policies and comparing acceptable use policies to model behavior policies, toxicity classifiers, responsible scaling policies, and existing AI regulations.\n## Key takeaways for policymakers:\n**1.****Developers’ acceptable use policies have important differences**\nAcceptable use policies differ across developers in terms of restrictions on content, types of end use, and industry-level restrictions. These differences have been underexplored and may have important implications for the safety of foundation model deployment.\n* There are several gray areas where some companies include content-based restrictions and others do not. Content related to politics, eating disorders, sex, and medical advice are among the areas where some companies have harsh prohibitions and others are silent.\n* While some companies’ policies prevent their models from being used by the weapons or surveillance industry, others have few industry-related restrictions, and others release only noncommercial models with no policies limiting use.\n⠀**2.****Acceptable use policies help shape the foundation model market**\nAcceptable use policies alter the foundation model market by affecting who can use a model and for what purpose.\n* Foundation model developers advertise use restrictions or lack thereof as a comparative advantage relative to other companies. For example, Mistral[says] its models are “customizable at will,” allowing users to adapt its models more fully, whereas OpenAI[states] it provides safety to users and the ecosystem through “post-deployment monitoring for patterns of misuse”.\n* Foundation model developers employ acceptable use policies in order to prevent other companies from making use of their services, stealing their intellectual property, or using their products to develop a competitor.[32] For example, companies ban firms and other users from using their models to train other AI models, restricting the supply of datasets of model outputs and concentrating the market for models that are trained on model outputs.[33] \n* Acceptable use policies help determine what industries can make use of developers’ foundation models. For example, policies that prohibit the use of models for weapons production may block the arms industry from making use of those foundation models.\n* Acceptable use policies determine the types of uses that are legitimate for foundation models (e.g., no automated decision systems). Even industries that are allowed to make use of models may not be able to do so for common applications.\n⠀**3.****Policy proposals to restrict the use of foundation models should take acceptable use policies into account**\nMany AI regulatory proposals do not acknowledge that companies already limit the use of their foundation models with acceptable use policies. Policymakers should craft regulations that contend with existing restrictions in these acceptable use policies, including by potentially reinforcing them or by helping to close gaps in enforcement regimes.\n* Among the most common policy proposals related to foundation models are those that propose to restrict specific uses. Policies such as liability for model developers and export controls on model weights are primarily intended to block undesirable uses of foundation models.\n* These policies should account for the fact that foundation model developers already have policies in place that prevent such risky uses. For example, every foundation model developer restricts unlawful uses, a broad restriction that governments could encourage developers to enforce in specific ways.\n* Enforcement of acceptable use policies (and companies’ other policies) is piecemeal and quite difficult. Policymakers should encourage and assist in enforcement of existing policies in addition to imposing new requirements.## Acknowledgments\nI thank Ahmed Ahmed, Rishi Bommasani, Peter Cihon, Carlos Muñoz Ferrandis, Peter Henderson, Aspen Hopkins, Sayash Kapoor, Percy Liang, Shayne Longpre, Aviya Skowron, Betty Xiong, and Yi Zeng for discussions on this topic. A special thanks to Loredana Fattorini for her assistance with the graphics.\n```\n`@misc{klyman2024aups, author ={Kevin Klyman}, title ={Acceptable Use Policies for Foundation Models: Considerations for Policymakers and Developers}, howpublished ={Stanford Center for Research on Foundation Models},\nmonth = apr,\nyear = 2024, url ={https://crfm.stanford.edu/2024/04/08/aups.html}}`\n```\n1. Acceptable use policies are given different names by developers, with some labeling them as prohibited use policies, use restrictions, usage policies, or usage guidelines. In this post, acceptable use policy is used as a catch-all term for any policy from a foundation model developer that restricts use of a foundation model. Use restrictions imposed by deployers of the foundation model,[model hubs], and other actors in the AI[supply chain] are highly relevant to these issues but are outside the scope of this post.[&#8617;] \n2. While many companies choose to release acceptable use policies as standalone documents, they are then invoked in the terms of service.[&#8617;] \n3. OpenAI’s[Usage Policies] state “We also work to make our models safer and more useful, by training them to refuse harmful instructions and reduce their tendency to produce harmful content.”[&#8617;] \n4. Developers often refer to model behavior policies as “model policies.” For example, Google’s technical[report] for Gemini states “We have developed a set of model safety policies for Gemini models to steer development and evaluation. The model policy definitions act as a standardized criteria and prioritization schema for responsible development and define the categories against which we measure launch readiness. Google products that use Gemini models, like our conversational AI service Gemini and Cloud Vertex API, further implement our standard product policy framework which is based on Google’s extensive experience with harm mitigation and rigorous research. These policies take product use cases into account –for example, providing additional safety coverage for users under 18.”[&#8617;] \n5. In order to make use-based licensing more stringent, developers might adopt model licenses that prohibit uses that are deemed out-of-scope by the model card.[&#8617;] \n6. There is some overlap between API policies and acceptable use policies; some companies apply acceptable use policies to all of their services (including their APIs) whereas others have separate policies for models, systems, products, and APIs. Many companies do not offer APIs and so do not have API policies.[&#8617;] \n7. As one example, users of Amazon Bedrock must agree to an end-user license agreement in order to gain access to each foundation model on the platform. These agreements are custom terms of service agreements for the use of the foundation model via Amazon Bedrock and often reference the developer’s acceptable use policy (they can be found on the Model Access page).[&#8617;] \n8. The full section[reads], “Prohibit misuse: Publish usage guidelines and terms of use of LLMs in a way that prohibits material harm to individuals, communities, and society such as through spam, fraud, or astroturfing. Usage guidelines should also specify domains where LLM use requires extra scrutiny and prohibit high-risk use-cases that aren’t appropriate, such as classifying people based on protected characteristics. Build systems and infrastructure to enforce usage guidelines. This may include rate limits, content filtering, application approval prior to production access, monitoring for anomalous activity, and other mitigations.”[&#8617;] \n9. There is a[growing] [academic] [literature] and[wider] [discussion] in the AI research community regarding the purpose,[prevalence], and enforceability of Responsible AI Licenses for open model developers. Many of the open foundation model developers discussed in this post draw on Responsible AI Licenses with identical or near-identical terms, including BigCode, BigScience, and Databricks. (Character AI’s terms of service also[state] that “your use of the Services may be subject to license and use restrictions set forth in the CreativeML Open RAIL-M License,” while Google’s prohibited use[policy] for its Gemma models bears some resemblance to OpenRAIL licenses.) Closed foundation model developers have also drawn on each other’s policies, with many using language similar to OpenAI’s usage policy (which was among the first such policies).[&#8617;] \n10. There are also a wide variety of use cases for foundation models that could be beneficial but are prohibited by major developers’ acceptable use policies. For example, organizations that seek to use foundation models to engage in harm reduction regarding prohibited categories of harm would not be able to do so under such acceptable use policies.[&#8617;] \n11. Firms may be more risk averse and enforcement of acceptable use policies against corporate users can be more straightforward. Moreover, there are benefits to Responsible AI Licenses that should be considered alongside enforcement challenges. For example, BigCode’s OpenRAIL-M[license] requires that downstream developers share existing model cards and add additional high-quality documentation to reflect changes they make to the original model, which could help improve transparency in the AI ecosystem.[&#8617;] \n12. See Annex IXa(1) 1.b, and Annex IXb 1.b of the[EU AI Act].[&#8617;] \n13. The definition of providers of generative AI services includes foundation model developers providing their models through an API or other means. See Article 14 and Article 22(2) of the[Interim Measures] for the Management of Generative Artificial Intelligence Services.[&#8617;] \n14. The Hiroshima Process[International Code of Conduct] for Organizations Developing Advanced AI Systems, which was published by the G7, also requires that companies publicly report domains of appropriate and inappropriate use. The signatories to the US voluntary commitments are Amazon, Anthropic, Google, Inflection, Meta, Microsoft, and OpenAI, which signed in July 2023, and Adobe, Cohere, IBM, Nvidia, Palantir, Salesforce, Scale AI, and Stability AI, which signed in September 2023.[&#8617;] \n15. See Appendix A on Main Safety Risks of Corpora and Generated Content in China’s National Cybersecurity Standardisation Technical Committee’s[Basic Safety Requirements] for Generative Artificial Intelligence Services.[&#8617;] \n16. (1) Note: not all of these policies are directly comparable or comprehensive. Some companies have many additional provisions in their terms of service for various products, platforms, and services that may include additional use restrictions. For example, the “Meta AIs[Terms of Service] ” covers use of Meta’s foundation models on its own platforms; DeepSeek has a[license] for its flagship model with use restrictions; and Google has thousands of different terms of service agreements. Companies with general acceptable use policies often also have model-specific acceptable use policies with additional use restrictions, though these are outside the scope of this post. (2) The acceptable use policies cited here are (i) specifically applied to foundation models by the developer (i.e. they are invoked for a specific foundation model, the contract that includes them cites their applicability to the developer’s AI models, or the licensor’s primary product is a foundation model so a plaintext reading of a contract that applies to all products and services is sufficiently tied to a foundation model), (ii) name prohibited types of content (i.e. acceptable use policies that are content neutral are excluded), and (iii) are incorporated into the license or terms of service to make them binding. (3) Luminous’ model weights are open for customers, but not the general public. (4) AWS’ Responsible AI Policy points directly to AWS’ acceptable use policy in the text, so I consider both for the purposes of this post; the same is true of Mistal’s terms of use and its terms of service for Le Chat. (5) As a whole, acceptable use policies change with some frequency—as one indication of this, multiple policies changed significantly over the course of writing this post.[&#8617;] \n17. For a full list of content-based prohibitions in companies’ acceptable use policies, see the[GitHub]. The repo also has details on how figures 3-6 were compiled.[&#8617;] \n18. Use restrictions in a developer’s acceptable use policy vary by the modality of its models. For example, acceptable use policies for image models have less need to restrict the generation of malware than those for language models.[&#8617;] \n19. Minimalist acceptable use policies are not necessarily less forceful. The Allen Institute for AI, for example, is the only developer whose acceptable use[policy] singles out uses related to military surveillance and has strong terms regarding “‘real time’ remote biometric processing or identification systems in publicly accessible spaces for the purpose of law enforcement”.[&#8617;] \n20. Note: this analysis is based on uses that are referenced explicitly in developers’ acceptable use policies—there are many similar prohibited uses that are described with slightly different language and so are not accounted for here. This exercise is, to a significant degree, an effort to capture the language that developers use to taxonomize and describe unacceptable uses.[&#8617;] \n21. Stability AI strikes a middle ground here,[prohibiting] “Generating or facilitating large-scale political advertisements, propaganda, or influence campaigns”.[&#8617;] \n22. Meta does[have] a narrow prohibition on “the illegal distribution of information or materials to minors, including obscene materials,” as does Stability AI. However, the lack of broader restrictions on sexual content implies that downstream developers that fine tune Llama 2 to produce sexual chatbots may not be violating its acceptable use policy. AI pornography sites like CrushOn (which[claims] to use Llama 13B Uncensored –a fine-tuned version of Llama, not Llama 2) and SpicyChat (which[claims] to use Pygmalion 7B, another fine-tuned version of Llama) may be within their rights to use Llama 2 in the future for use cases that other developers prohibit.[&#8617;] \n23. OpenAI’s[Business Terms] do not allow other businesses “to develop any artificial intelligence models that compete with our products and services,” which reportedly led it to[suspend] ByteDance’s account. There have also been other[reports] of model developers enforcing related terms on other companies.[&#8617;] \n24. Section 9.3 of the Falcon 180B[license] reads “you are not licensed to use the Work or Derivative Work under this license for Hosting Use. Where You wish to make Hosting Use of Falcon 180B or any Work or Derivative Work, You must apply to TII for permission to make Hosting Use of that Work in writing via the Hosting Application Address, providing such information as may be required” where Hosting Use “means any use of the Work or a Derivative Work to offer shared instances or managed services based on the Work, any Derivative Work (including fine-tuned versions of a Work or Derivative Work) to third party users in an inference or finetuning API form.”[&#8617;] \n25. References in acceptable use policies to unsolicited advertising, gambling, prostitution, and multi-level marketing schemes also function as restrictions on specific industries.[&#8617;] \n26. Stanford CRFM’s[Ecosystem Graphs] documents over 75 different developers of foundation models.[&#8617;] \n27. Another view is that acceptable use policies are inappropriate for foundation models and should instead apply to AI systems that incorporate foundation models.[&#8617;] \n28. Note: Phi-2 was[originally released] under a Microsoft Research license, not an MIT license. Phi-2’s page on Hugging Face also[features] the Microsoft Open Source Code of Conduct.[&#8617;] \n29. Voluntary codes of conduct from the G7 and U.S. explicitly call out foundation models, while Canada refers to generative AI systems with general-purpose capabilities. The G7[states] “the Hiroshima Process International Code of Conduct for Organizations Developing Advanced AI Systems aims to promote safe, secure, and trustworthy AI worldwide and will provide voluntary guidance for actions by organizations developing the most advanced AI systems, including the most advanced foundation models and generative AI systems”. The White House[states] “The following is a list of commitments that companies are making to promote the safe, secure, and transparent development and use of generative AI (foundation) model technology.”[&#8617;] \n30. Model hosting platforms such as Hugging Face and GitHub also have[policies] regarding the types of content they will host that[may overlap] with developers’ acceptable use policies.[&#8617;] \n31. Many developers’ terms of service make clear that users are fully responsible for how they use the developer’s foundation models. For example, Reka AI’s[terms] read “You are solely responsible for your use of your Outputs created through the Services, and you assume all risks associated with your use of your Outputs, including any potential copyright infringement claims from third parties”.[&#8617;] \n32. Foundation model developers may limit use of their models in order to ensure that use restrictions on some training datasets (e.g., related to copyright, trademark, and protected health data) are honored.[&#8617;] \n33. User restrictions have a significant effect here as well. Companies that restrict competitors (or companies with a large number of users) from making use of their models have taken a major step in restricting model use.[&#8617;]",
    "length": 46215,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Research in Action | SCALE Initiative",
    "url": "https://scale.stanford.edu/research-in-action",
    "text": "Research in Action | SCALE Initiative\n[Skip to main content] \n[![Stanford University]] \n# Research in Action\n[![How K-12 Educators Are Actually Engaging with AI: Early Findings from an EdTech Platform]] \n##### [How K-12 Educators Are Actually Engaging with AI: Early Findings from an EdTech Platform] \nResearch Brief -08/05/2025\nSince the release of user-facing generative artificial intelligence (AI) tools like ChatGPT in 2022, AI adoption has quickly permeated many sectors, including education. New EdTech platforms are constantly emerging, often designed to help teachers with tasks ranging from lesson planning to worksheet generation to assessment grading. The emergence of these new technologies spark both excitement and concern, not only in teachers and students, but also among parents and policymakers. Yet, we know little about what tools teachers are using, how much they are using these tools, and what implications this use has for student learning.\n[![How is ChatGPT impacting schools, really? Stanford researchers aim to find out]] \n##### [How is ChatGPT impacting schools, really? Stanford researchers aim to find out] \nNews and Announcements -07/29/2025\nA new collaboration between Stanford’s SCALE and OpenAI, the creator of ChatGPT, strives to better understand how students and teachers use the popular AI platform and how it impacts learning\nEducation is one of the fastest-growing use cases of AI products. Students log on for writing assistance, brainstorming, image creation, and more. Teachers tap into tools like attendance trackers, get curriculum support to design learning materials, and much more.\nYet despite the rapid growth –and potential –a substantial gap remains in knowledge about the efficacy of these tools to support learning.\nA new research project from the[Generative AI for Education Hub] at SCALE, an initiative of the Stanford Accelerator for Learning, aims to help fill that gap by studying how ChatGPT is used in K-12 education. In particular, the research will examine how secondary level teachers and students use ChatGPT.\n[![The Impact of High-Impact Tutoring on Student Attendance: Evidence from a State Initiative]] \n##### [The Impact of High-Impact Tutoring on Student Attendance: Evidence from a State Initiative] \nResearch Brief -07/01/2025\nThis study provides compelling evidence that tutoring can do more than boost test scores; it can actually get students back in the classroom. On average, students were 1.2 percentage points less likely to be absent on days when they were scheduled to receive tutoring, suggesting that they are motivated to participate in tutoring. This impact was even greater for middle schoolers and students who’d missed more than 30% of school days the prior year. The study also found that the design matters: tutoring only improved attendance when it combined at least two evidence-based features like small groups, frequent sessions, and in-school delivery.\n[![Stanford University]] \n* [Stanford Home] \n* [Maps &amp; Directions] \n* [Search Stanford] \n* [Emergency Info] \n* [Privacy] \n* [Copyright] \n* [Trademarks] \n* [Non-Discrimination] \n* [Accessibility] \n©Stanford University. Stanford, California 94305.",
    "length": 3197,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Syllabus Statements for Generative AI Usage | Stanford Law School",
    "url": "https://law.stanford.edu/robert-crown-law-library/edtech/syllabus-statements-for-generative-ai-usage",
    "text": "Syllabus Statements for Generative AI Usage - EdTech Hub - Stanford Law School\nlogo-footerlogo-fulllogo-stanford-universitylogomenu-closemenuClose IconIcon with an X to denote closing.Play IconPlay icon in a circular border.\n[Skip to main content] \n![Sls logo] \n# SLS Login\n## To view this page, please log in.\nThe page you seek is accessible only to visitors logged in with a valid SLS account. To view it, please log in or register for an account with SLS.\n[Login with your SUNet ID] \nIf you’re lost or have reached this page in error, our apologies. Please use the top-left navigational menu or the links in the footer below to find your way through the site.\n**Still can’t find what you’re looking for?**\n**Back to the Top",
    "length": 726,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "New Responsible AI Guide for Stanford Offers Current Best Practices",
    "url": "https://uit.stanford.edu/news/new-responsible-ai-guide-stanford-offers-current-best-practices",
    "text": "[Skip to main content] \n\n# New Responsible AI Guide for Stanford Offers Current Best Practices\n\nBrush up on key IT security and privacy considerations around AI\n\nOctober 17, 2023\n\nAs you might already know, generative artificial intelligence (AI) is a fast-growing field with the potential to affect our everyday lives.\n\nSometimes called GenAI, “generative AI” is the term for systems built using algorithms that can generate text, images, videos, audio, and 3D models in response to prompts. Popular examples of generative AI include ChatGPT and Google Bard.\n\nWith the rise of GenAI, we all have an opportunity—and responsibility—to work together to shape our way forward. That’s why security and privacy experts at Stanford, including the Information Security Office (ISO), are offering considerations for generative AI with a new site: **[Responsible AI at Stanford (responsibleai.stanford.edu)] **.\n\nThe guidance on the new Responsible AI at Stanford site was created in collaboration with multiple units at Stanford. It also aligns to our university [privacy policies] and [IT security standards].\n\n## What you’ll find with the Responsible AI site\n\nThe new Responsible AI at Stanford site provides our community guidance based on emerging best practices for our security and privacy context today.\n\nThe site is _not_ exhaustive. But it _is_ a synthesis of current best practices, and is designed to provide signposts for our work and decision-making.\n\nThe new site, developed through cross-disciplinary collaboration across Stanford, includes:\n\n- [Safety measures] \n- [Risk factors] \n- [Guidance] \n- [Resources and help] \n\nThe Responsible AI site helps you get to know the current GenAI landscape, so that you can innovate, build, and explore more responsibly.\n\n## What’s next\n\nSoon, the Responsible AI at Stanford site will also include more opportunities for engagement and discovery, with connections to specific bodies of research and work at Stanford.\n\nExplore the [Responsible AI site] to consider how _you_ might move forward in today’s new AI landscape.\n\n[Share Feedback] \n\n_DISCLAIMER: UIT News is accurate on the publication date. We do not update information in past news items. We do make every effort to keep our service information pages up-to-date. Please search our service pages at [uit.stanford.edu/search]._\n\n## What to read next:\n\n[Initiatives and Projects] \n\n## What to Know About Upcoming Google Storage Limits\n\n[Learn more about What to Know About Upcoming Google Storage Limits] \n\n[Information Security] \n\n## AI Playground: The Safer AI Platform for the Stanford Community\n\n[Learn more about AI Playground: The Safer AI Platform for the Stanford Community] \n\n[Special Reports] \n\n## Introducing the COLT: Cellular-on-Light Truck\n\n[Learn more about Introducing the COLT: Cellular-on-Light Truck] \n\n[Go to Newsroom] \n\n- [Stanford Home] \n- [Maps & Directions] \n- [Search Stanford] \n- [Emergency Info] \n\n- [Terms of Use] \n- [Privacy] \n- [Copyright] \n- [Trademarks] \n- [Non-Discrimination] \n- [Accessibility] \n\n©CopyrightStanford University.\nStanford,\nCalifornia94305.",
    "length": 3091,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "AI and Your Learning: A Guide for Students",
    "url": "https://ctl.stanford.edu/aimes/ai-learning-guide-students",
    "text": "AI and Your Learning: A Guide for Students | Center for Teaching and Learning\n[Skip to main content] [Skip to secondary navigation] \n[Stanford University(link is external)] \n[\nStanford\nCenter forTeaching and Learning\n] \nSearch this siteSubmit Search\n# AI and Your Learning: A Guide for Students\n## Main navigation\n[Skip Secondary Navigation] \nMain content start\nIn this guide, we provide information and guidelines to help you make informed decisions about navigating AI tools.\nGenerative AI tools, such as ChatGPT, Claude, and Gemini, have evolved quickly over the past few years and become increasingly widespread in their use. While these tools can be exciting, they can also raise questions and concerns. Whether you’re using these tools already or might want to explore them further, making decisions about AI tools as a student can be complicated. Perhaps you’ve wondered:*Can I use AI? Should I use AI? How might using AI tools impact my learning?*\n## Can I Use Generative AI for Assignments or Research?\nAI policies vary dramatically from one class to the next, often because courses have distinct learning goals. Before you start using AI as a learning or research tool, make sure to**review your instructor’s course policy**. It is important for you to know if AI is allowed in your course and if so, how.Which ways of using AI are permissible and which are not?\n*> Before you start using AI as a learning or research tool, make sure to **> review your instructor’s course policy.\n***\nYou also need to understand that[Stanford’s Generative AI Policy Guidance] treats any use of generative AI as analogous to receiving help from another person, and using generative AI to substantially complete an assignment is always prohibited. Your default approach should be to always*assume AI use is not allowed unless otherwise indicated in the syllabus or assignment*, and to disclose any use of generative AI for help with your assignments. In addition, your instructor might have a specific way they want you to document how and when you utilized AI tools.**If you are ever unclear about if, how, and when AI is appropriate, please consult with your instructor**.\nOn a final note, sometimes you may be using AI-related tools for research. In that case, clarify the rules for using AI in your lab, department, discipline, and/or the specific journal that you might be submitting to. They may have specific AI-related policies that you are expected to follow.\nIn summary, when in doubt:**ask and document!**\n## How Might the Design of AI Tools Influence My Learning?\nPerhaps you’re thinking about using a tool such as ChatGPT to quiz yourself on course concepts, clarify points of confusion, or get feedback on your writing. While this may seem no different from the ways in which you might learn and interact with a tutor or friend, learning with AI is a bit more complicated. Knowing how AI tools work can help you understand their strengths and limitations and ultimately make informed decisions about when and how you use them to support your learning.\nOn a basic level, generative AI tools such as ChatGPT, Claude, and Google Gemini are called “generative” because they produce text, images, audio, or other content in response to a prompt. Through a complex process of machine learning, which involves training the AI model on vast amounts of data and then fine-tuning the model, an AI tool is then able to generate responses that resemble human-generated language. If you ask ChatGPT a question, it is able to respond not because it “knows” the answer to your question, but because it has identified patterns and relationships within the training data and can reassemble language from that data to produce an answer.\nThe generative nature of AI tools offers a number of strengths: the tools are easy to use, they can take on a number of roles, and they provide novel ways to learn and create. However, the underlying structure of AI tools can also lead to problems. Because of the way AI tools “guess” statistically plausible information, AI tools will sometimes provide responses that are incorrect, which are called “hallucinations.”\nAdditionally, researchers have documented a variety of biases that can exist in training data (e.g. lack of geographical and population diversity) and note that the algorithms that are then applied to this data can further exacerbate these biases (Mehrabi et al., 2021). Research has also shown that AI tools have a tendency to misattribute sources, or in some cases, create a citation for a source that doesn’t exist (Jaźwińska &amp; Chandrasekar, 2025).\nFinally, it’s important to note that while the experience of interacting with an AI tool can feel life-like and similar to talking with a human, these tools have serious limitations in their ability to provide holistic support. Whether you’re facing challenges in your learning or in your broader life as a student, it’s important to connect with human support resources such as tutors, academic coaches, advisors, and counselors. Given these limitations, being able to think critically about the output from AI tools is essential, as not everything they generate will be accurate or useful.\n## Can AI Tools Help Me Meet My Learning Goals?\nA frequently cited quote from Herbert Simon, one of the founders of the field of cognitive science, describes how learning takes place: “Learning results from what the student does and thinks and only from what the student does and thinks. The teacher can advance learning only by influencing what the student does to learn” (cited in Ambrose, 2023, p.1). By extension, the specific strategies you use while studying, because they shape what you do and think about during the learning process, also play an important role in whether your efforts at learning are successful.\nStrategies that involve thinking deeply about the material, such as comparing and contrasting concepts with each other, explaining in one’s own words, and self-quizzing tend to be more effective than superficial strategies such as highlighting or rereading material (Dunlosky, et. al., 2013).\nThe relationship between how you use generative AI and your learning depends on how you use this technology. Does a specific use of generative AI facilitate or take away an opportunity for deeper learning? The answer to this question may not be straightforward. For example, effective active reading strategies include previewing the reading and asking yourself questions that might be answered in the text (McGuire, 2018). While generative AI could be used to help with these strategies by generating a summary and questions, does it take away an opportunity for you to engage directly with the text by previewing it yourself? And do you risk introducing hallucinations into any summaries or questions generated by AI? If you use AI as a study aid, what's important is that you’re using effective strategies to facilitate your learning and not delegating your learning process to an AI tool.\nThe following framework suggests questions for you to consider when using generative AI as a learning tool.\n## Questions to Consider When Using Generative AI for Learning\n### Assignments and Research\n* How do I know whether I’m allowed to use AI in my assignment or research?\n* How am I allowed to use AI in my assignment or research, if at all?\n* How should I document my use of AI?\n* Am I required to disclose my use of AI?### Accuracy and Bias\n* How will I evaluate the accuracy of AI output?\n* In what ways can I monitor AI output for bias?\n* How will I identify which sources are contributing to AI output and whether they are being used accurately? How will I properly attribute sources?### AI and Your Learning\n* Does using generative AI take away an opportunity for me to engage more deeply with the material? If so, are there alternative learning strategies I could use?\nIf you’re not sure, a[CTL academic coach] can help you find new approaches to studying.\n* Alternatively, does using generative AI deepen my engagement with the material or otherwise help me reach my learning goals? If so, how?\n* In addition to the considerations described above, what other factors are important to me as I make decisions about using AI tools? (e.g., the environment, data privacy).\n## Additional AI Resources\n* [Responsible AI at Stanford]: Stanford UIT’s guide to using AI tools and models while keeping your and Stanford's data safe.\n* [OCS Generative AI Policy Guide]: Stanford Office of Community Standards' guidance on the honor code implications of generative AI tools.\n* [Stanford's AI Playground]: Stanford University hosted platform that allows users to test out multiple AI models in a safe, university-supported environment.\n* [Student Guide to Artificial Intelligence]: Guide published by Elon University providing an overview of key considerations for student use of AI, including skill-building, ethics, academic integrity, and more.## References\nDunlosky, J., Rawson, K., Marsh, E., Nathan, M., and Willingham, D. (2013). Improving Students’ Learning With Effective Learning Techniques: Promising Directions From Cognitive and Educational Psychology.*Psychological Science in the Public Interest*, 14(1), 4-58.[https://journals.sagepub.com/doi/full/10.1177/1529100612453266] \nJaźwińska, K. &amp; Chandrasekar, A. (2025, March 6). AI search has a citation problem.*Columbia Journalism Review*.[https://www.cjr.org/tow\\_center/we-compared-eight-ai-search-engines-theyre-all-bad-at-citing-news.php] \nLovett, M., Bridges, M., DiPietro, M., Ambrose, S., &amp; Norman, M. (2023).*How Learning Works: 8 Research-Based Principles for Smart Teaching*. (2nd ed.). Jossey-Bass.\nMcGuire, S. (2018).*Teach Yourself How to Learn: Strategies You Can Use to Ace Any Course at Any Level*. Stylus Publishing.\nMehrabi, N., Morstatter, F., Saxena, N., Lerman, K., &amp; Galstyan, A. (2021). A survey on bias and fairness in machine learning.*ACM Computing Surveys (CSUR)*, 54(6), 1-35.\n[More Tips, Tools, and Resources for Student Learning] \nBack to Top",
    "length": 10046,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Stanford CRFM",
    "url": "https://crfm.stanford.edu/policy.html",
    "text": "Stanford CRFM\n[![]] \n## 2024\n#### [The AI Executive Order through the lens of the AI Index] [[Paper]] \nJonathan Xue, Rishi Bommasani\nPolicy Analysis 2024\n*Areas:*US AI EO\n#### [AI Policy and Governance Working Group Response to the US NTIA Request for Comment on Open Foundation Models] [[Paper]] \nAlondra Nelson, Andrew Trask, Ben Garfinkel, Christine Custis, Dan Hendrycks, Deep Ganguli, Dorothy Chou, Helen Toner, Irene Solaiman, Jaan Tallinn, Janet Haven, Marc Aidinoff, Marietje Schaake, Matthew Salganik, Miranda Bogen, Nathan Lambert, Rishi Bommasani, Sébastien Krier, Solon Barocas, Stephanie Ifayemi, Suresh Venkatasubramanian, William Isaac, Zoë Brammer\nUS NTIA Request for Comment on Open Foundation Models 2024\n*Areas:*Openness\n#### [Stanford-Princeton led Response to the US NTIA Request for Comment on Open Foundation Models] [[Paper]] \nAlondra Nelson, Arvind Narayanan, Caroline Meinhardt, Daniel E. Ho, Daniel Zhang, Dawn Song, Inioluwa Deborah Raji, Kevin Klyman, Marietje Schaake, Mihir Kshirsagar, Percy Liang, Peter Henderson, Rishi Bommasani, Rohini Kosoglu, Rumman Chowdhury, Sayash Kapoor, Seth Lazar, Shayne Longpre, Stefano Maffulli, Stella Biderman, Victor Storchan\nUS NTIA Request for Comment on Open Foundation Models 2024\n*Areas:*Openness\n#### [A Safe Harbor for AI Evaluation and Red Teaming] [[Paper]] \nKevin Klyman, Shayne Longpre, Sayash Kapoor, Arvind Narayanan, Aleksandra Korolova, Peter Henderson\nUS Copyright Office Request for Comment on Exemptions to Permit Circumvention of Access Controls on Copyrighted Works 2024\n*Areas:*Safe Harbor\n#### [The AI Regulatory Alignment Problem] [[Paper]] \nNeel Guha\\*, Christie M. Lawrence\\* , Lindsey A. Gailmard, Kit T. Rodolfa, Faiz Surani, Rishi Bommasani, Inioluwa Deborah Raji, Mariano-Florentino Cuéllar, Colleen Honigsberg, Percy Liang, Daniel E. Ho\nGeorge Washington Law Review 2024\n*Areas:*Regulatory methods\n## 2023\n#### [Do Foundation Model Providers Comply with the Draft EU AI Act?] [[Paper]] \nRishi Bommasani, Kevin Klyman, Daniel Zhang, Marietje Schaake, Percy Liang\nPolicy Analysis 2023\n*Areas:*TransparencyEU AI Act\n#### [Stanford-Princeton Response to the US NTIA Request for Comment on AI Accountability] [[Paper]] \nRishi Bommasani, Sayash Kapoor, Daniel Zhang, Arvind Narayanan, Percy Liang\nUS NTIA Request for Comment on AI Accountability 2023\n*Areas:*EvaluationTransparencyOpenness\n#### [Improving Transparency in AI Language Models: A Holistic Evaluation] [[Paper]] \nRishi Bommasani, Daniel Zhang, Tony Lee, Percy Liang\nPolicy Brief 2023\n*Areas:*EvaluationTransparency\n#### [Considerations for Governing Open Foundation Models] [[Paper]] \nRishi Bommasani, Sayash Kapoor, Kevin Klyman, Shayne Longpre, Ashwin Ramaswami, Daniel Zhang, Marietje Schaake, Daniel E. Ho, Arvind Narayanan, Percy Liang\nPolicy Brief 2023\n*Areas:*Openness\n#### [Towards Compromise: A Concrete Two-tier Proposal for Foundation Models in the EU AI Act] [[Paper]] \nRishi Bommasani, Tatsunori Hashimoto, Daniel E. Ho, Marietje Schaake, Percy Liang\nPolicy Analysis 2023\n*Areas:*EU AI Act\n#### [Decoding the White House AI Executive Order’s Achievements] [[Paper]] \nRishi Bommasani, Christie M. Lawrence, Lindsey A. Gailmard, Caroline Meinhardt, Daniel Zhang, Peter Henderson, Russell Wald, Daniel E. Ho\nPolicy Analysis 2023\n*Areas:*US AI EO\n#### [Drawing Lines: Tiers for Foundation Models] [[Paper]] \nRishi Bommasani\nPolicy Brief 2023\n*Areas:*TiersEU AI Actregulatory methods\n#### [By the Numbers: Tracking The AI Executive Order] [[Paper]] \nCaroline Meinhardt, Christie M. Lawrence, Lindsey A. Gailmard, Daniel Zhang, Rishi Bommasani, Rohini Kosoglu, Peter Henderson, Russell Wald, Daniel E. Ho\nPolicy Analysis 2023\n*Areas:*US AI EO\n#### [What the Executive Order means for Openness in AI] [[Paper]] \nArvind Narayanan, Sayash Kapoor, Rishi Bommasani\nPolicy Analysis 2023\n*Areas:*US AI EO",
    "length": 3857,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Response to OSTP’s Request for Information on the Development of an AI Action Plan | Stanford HAI",
    "url": "https://hai.stanford.edu/policy/response-to-ostps-request-for-information-on-the-development-of-an-ai-action-plan",
    "text": "Response to OSTP’s Request for Information on the Development of an AI Action Plan | Stanford HAI\n### Stay Up To Date\nGet the latest news, advances in research, policy work, and education program updates from HAI in your inbox weekly.\n[\n**Sign Up**For Latest News\n] \n[] \n* [] \n* [] \n* [] \n* [] \n* [] \n* [] \n###### Navigate\n* [\nAbout\n] \n* [\nEvents\n] \n* [\nCareers\n] \n* [\nSearch\n] \n###### Participate\n* [\nGet Involved\n] \n* [\nSupport HAI\n] \n* [\nContact Us\n] \n[Skip to content] \n[] \n[] \n* [] \n* [] \n* [] \n* [] \n* [] \n* [] \n[] \n[\npolicyResponse to Request\n] # Response to OSTP’s Request for Information on the Development of an AI Action Plan\nDate\nMarch 17, 2025\nTopics\n[\nRegulation, Policy, Governance\n] \n![] \n[**Read Paper**] \nabstract\nStanford scholars respond to a federal RFI on the development of an AI Action Plan, urging policymakers to promote open and scientific innovation, craft evidence-based AI policy, and empower government leaders.\nIn collaboration with\n[\n![] \n] \n[\n![] \n] \n### Executive Summary\nIn this response to a request for information issued by the National Science Foundation’s Networking and Information Technology Research and Development National Coordination Office (on behalf of the Office of Science and Technology Policy) regarding the development of an AI Action Plan, scholars from Stanford HAI, the Center for Research on Foundation Models (CRFM), and the Regulation, Evaluation, and Governance Lab (RegLab) urge policymakers to prioritize four areas of policy action:\n1. Promote open innovation as a strategic advantage for U.S. competitiveness;\n2. Maintain U.S. AI leadership by promoting scientific innovation;\n3. Craft evidence-based AI policy that protects Americans without stifling innovation;\n4. Empower government leaders with resources and technical expertise to ensure a “whole-of-government” approach to AI governance.\n[**Read Paper**] \nShare\n[] [] [] \nLink copied to clipboard!\nAuthors\n* [\n![headshot] \n###### Caroline Meinhardt\n] \n* [\n![Daniel Zhang] \n###### Daniel Zhang\n] \n* [\n![Rishi Bommasani] \n###### Rishi Bommasani\n] \n* [\n![Jennifer King] \n###### Jennifer King\n] \n* [\n![Russell Wald headshot] \n###### Russell Wald\n] \n* [\n![Percy Liang] \n###### Percy Liang\n] \n* [\n![Dan Ho headshot] \n###### Daniel E. Ho\n] \nRelated\n* ###### [Inside Trump’s Ambitious AI Action Plan] \n[Caroline Meinhardt,] [Daniel Zhang,] [Jennifer King,] [Andy Haupt,] [Elena Cryst] \nJul 24\nnews\n![Donald Trump] \nThe White House favors market-driven growth and light governance, signaling a departure from the previous administration.\n### Related Publications\n##### [Response to FDA&#x27;s Request for Comment on AI-Enabled Medical Devices] \n[Desmond C. Ong,] [Jared Moore,] [Nicole Martinez-Martin,] [Caroline Meinhardt,] [Eric Lin,] [William Agnew] \nQuick ReadDec 02, 2025\nResponse to Request\n![] \nStanford scholars respond to a federal RFC on evaluating AI-enabled medical devices, recommending policy interventions to help mitigate the harms of AI-powered chatbots used as therapists.\nResponse to Request\n![] \n#### [Response to FDA&#x27;s Request for Comment on AI-Enabled Medical Devices] \n[Desmond C. Ong,] [Jared Moore,] [Nicole Martinez-Martin,] [Caroline Meinhardt,] [Eric Lin,] [William Agnew] \n[Healthcare] [Regulation, Policy, Governance] Quick ReadDec 02\nStanford scholars respond to a federal RFC on evaluating AI-enabled medical devices, recommending policy interventions to help mitigate the harms of AI-powered chatbots used as therapists.\n##### [Russ Altman’s Testimony Before the U.S. Senate Committee on Health, Education, Labor, and Pensions] \n[Russ Altman] \nQuick ReadOct 09, 2025\nTestimony\n![] \nIn this testimony presented to the U.S. Senate Committee on Health, Education, Labor, and Pensions hearing titled “AI’s Potential to Support Patients, Workers, Children, and Families,” Russ Altman highlights opportunities for congressional support to make AI applications for patient care and drug discovery stronger, safer, and human-centered.\nTestimony\n![] \n#### [Russ Altman’s Testimony Before the U.S. Senate Committee on Health, Education, Labor, and Pensions] \n[Russ Altman] \n[Healthcare] [Regulation, Policy, Governance] [Sciences (Social, Health, Biological, Physical)] Quick ReadOct 09\nIn this testimony presented to the U.S. Senate Committee on Health, Education, Labor, and Pensions hearing titled “AI’s Potential to Support Patients, Workers, Children, and Families,” Russ Altman highlights opportunities for congressional support to make AI applications for patient care and drug discovery stronger, safer, and human-centered.\n##### [Michelle M. Mello&#x27;s Testimony Before the U.S. House Committee on Energy and Commerce Health Subcommittee] \n[Michelle Mello] \nQuick ReadSep 02, 2025\nTestimony\n![] \nIn this testimony presented to the U.S. House Committee on Energy and Commerce’s Subcommittee on Health hearing titled “Examining Opportunities to Advance American Health Care through the Use of Artificial Intelligence Technologies,” Michelle M. Mello calls for policy changes that will promote effective integration of AI tools into healthcare by strengthening trust.\nTestimony\n![] \n#### [Michelle M. Mello&#x27;s Testimony Before the U.S. House Committee on Energy and Commerce Health Subcommittee] \n[Michelle Mello] \n[Healthcare] [Regulation, Policy, Governance] Quick ReadSep 02\nIn this testimony presented to the U.S. House Committee on Energy and Commerce’s Subcommittee on Health hearing titled “Examining Opportunities to Advance American Health Care through the Use of Artificial Intelligence Technologies,” Michelle M. Mello calls for policy changes that will promote effective integration of AI tools into healthcare by strengthening trust.\n##### [Response to the Department of Education’s Request for Information on AI in Education] \n[Victor R. Lee,] [Vanessa Parli,] [Isabelle Hau,] [Patrick Hynes,] [Daniel Zhang] \nQuick ReadAug 20, 2025\nResponse to Request\n![] \nStanford scholars respond to a federal RFI on advancing AI in education, urging policymakers to anchor their approach in proven research.\nResponse to Request\n![] \n#### [Response to the Department of Education’s Request for Information on AI in Education] \n[Victor R. Lee,] [Vanessa Parli,] [Isabelle Hau,] [Patrick Hynes,] [Daniel Zhang] \n[Education, Skills] [Regulation, Policy, Governance] Quick ReadAug 20\nStanford scholars respond to a federal RFI on advancing AI in education, urging policymakers to anchor their approach in proven research.",
    "length": 6472,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "AI Meets Education at Stanford",
    "url": "https://aimes.stanford.edu/",
    "text": "AI Meets Education at Stanford | Center for Teaching and Learning\n[Skip to main content] [Skip to secondary navigation] \n[Stanford University(link is external)] \n[\nStanford\nCenter forTeaching and Learning\n] \nSearch this siteSubmit Search\n![AIMES, AI Meets Education at Stanford] \n# AI Meets Education at Stanford\n## Main navigation\n[Skip Secondary Navigation] \nMain content start\nAI Meets Education at Stanford (AIMES) is a VPUE effort to catalyze and support critical engagement with generative AI in Stanford teaching and learning contexts, coordinated by the Center for Teaching and Learning.\nAIMES fosters discussions among faculty, instructors, and students, and lower barriers for Stanford educators to engage productively and teach effectively in a world where generative AI appears to be here to stay. This effort is focused specifically on teaching and learning at Stanford University, complementing projects that advance AI research and policy in the broader world by bringing insights home to Stanford teaching and learning.\nAs a collection of activities and resources, AIMES sparks engagement fueled by the kinds of deep critical thinking and discourse that are hallmarks of a Stanford education. As the homophone “aims” suggests, AIMES takes academic goals as the starting point for considering, rejecting, integrating, or leveraging generative AI affordances in Stanford teaching and learning. The purpose of AIMES is to amplify a range of approaches to AI in undergraduate education and how they function ethically, pedagogically, practically, ecologically, and disciplinarily within the university context.\n![] \n## AIMES Library of Examples\nExamples of how Stanford instructors have responded to generative AI in their classes, including a range of teaching artifacts. These examples are meant to spark ideas and showcase a variety of approaches to teaching in a university setting where generative AI exists.\n[Explore the AIMES Library] \n![] \n## AI Teaching Strategies\nAn overview of strategies that may be helpful when assigning, limiting, and prohibiting AI use.\n[Discover AI Teaching Strategies] \n![four connected spheres under the heading AI literacy framework containing text headings that read &quot;functional&quot;, &quot;ethical&quot;, &quot;rhetorical&quot; and &quot;pedagogical&quot;; underneath is a line of text reading &quot;Human-centered values: why do we engage with AI?&quot;] \n## Teaching Commons Artificial Intelligence Teaching Guide\nLearn how to positively influence the dialogue around AI in education, in your own courses and at Stanford. Modules cover AI literacy, pedagogical uses, implications for your course, creating your course policy, and integrating AI into assignments, as well as a DIY workshop kit.\n[Explore the AI Teaching Guide] \n![] \n## Critical AI Literacy for Instructors Canvas Resource\nThis self-paced resource is for instructors who are new to generative AI (genAI) technology. It provides the foundational knowledge and skills to begin critically and effectively navigating genAI in teaching and learning contexts.\n[Learn more and enroll in Critical AI Literacy for Instructors] \n![] \n## AI and Your Learning: A Guide for Students\nThis guide from CTL provides information and guidelines to help students make informed decisions about navigating AI tools.\n[AI and Your Learning] \n![] \n## Academic Technology Tools List\nThe new Academic Technology Tools List is an interactive guide to Stanford supported academic technologies. You can browse or filter the list by integration level, tool type, and**AI capabilities**.\n[Browse the Academic Technology Tools List] \n![] \n## Share Input: Thoughtful Approaches to AI in Education\nShare examples of how Stanford instructors are engaging with generative AI in their course. Whether it's a creative use of AI tools, strategies to help students navigate AI, or methods that encourage students to think critically about when and why not to use AI, we want to hear it. Your input will help CTL facilitate community discussions and share diverse practices and perspectives.\n[Share Thoughtful Approaches to AI] \n![a group of pf three at a table looking thoughtfully at and discussing papers and notes] \n## Request a Consultation or Workshop\nFor one-to-one advice on creating your course AI policy or revising assignments in light of generative AI, to try out generative AI approaches at the CTL Academic Technology Solutions Lab, or to request a customized workshop on generative AI in your department or academic community at Stanford, contact us.\n[Request a CTL Consultation or Workshop] \n![View of the front of Stanford Campus] \n## Share Feedback on AIMES\nWe welcome your feedback on all aspects of AI Meets Education at Stanford. Please share your input via this anonymous[feedback form], or email AIMES co-leads,[Michele Elam], Senior Associate Vice Provost for UndergraduateEducation, WilliamRobertson Coe Professor of Humanities and Senior Fellow at the Institute for Human-Centered AI, and[Cassandra Volpe Horii], Associate Vice Provost for Education and Director, CTL\n[Share Feedback on AIMES] \n## **AIMES Approaches**\n* Enact proposed principles from the[January 9, 2025 Report of the AI at Stanford Advisory Committee] (AISAC):\n* human oversight\n* human alignment\n* human professionalism\n* ethical and safe use\n* privacy, security, and confidentiality\n* data quality and control\n* AI golden rule.\n* Build coherent and reusable resources when investing in the creation of new resources. Make them as evergreen and reusable as possible, while planning ahead for sustainable refresh cycles in this rapidly developing domain.\n* Emphasize a campus culture of reflection, curiosity, and experimentation to promote conversation and experimentation with AI, within boundaries that are openly discussed.## AIMES Goals\n* Address education-specific needs and recommendations from the[AISAC 2025 report], such as developing and sharing “frameworks and worked-out examples to help instructors think through… aspects of pedagogy impacted by AI” and facilitating “setting[s] where community members can experiment with AI tools.”\n* Support Stanford academic communities to more readily discover, create, and share discipline-specific AI approaches.\n* Integrate AI approaches with known evidence and with the university’s core mission related to teaching and learning: for example, metacognition, scaffolding, transparency, purposes of a liberal education, preparation for citizenship and discovery.\n* Connect across initiatives and groups working on AI and AI-adjacent issues in teaching and learning at Stanford.## Coming Soon\nAIMES team members at CTL, along with partners in VPUE, schools, and departments, are preparing the following new resources and opportunities:\n* Discipline-based and departmental community conversations sharing and discussing specific examples and practices within relevant shared educational contexts\n* Forums for discussion about generative AI across instructor, TA, and student roles## Additional Stanford Resources\nThe AIMES team in CTL and VPUE is committed to coordinating and collaborating with colleagues across the university, each working on distinct and connected aspects of generative AI. This network of experts and endeavors provides a resource-rich environment for Stanford instructors, researchers, and learners, including the following:\n* University IT provides resources on[Responsible AI at Stanford] and the[Stanford AI Playground]. The AI Playground is a user-friendly platform, built on open-source technologies, that allows Stanford faculty, staff, students, postdocs, and visiting scholars to safely try various AI models.\n* The Office of Community Standards provides[guidance on generative AI] with respect to the Honor Code.\n* The AI at Stanford Advisory Committee published a[report on January 9, 2025].\n* The[Stanford Accelerator for Learning] addresses research and policy on various facets of digital learning, including generative AI, and hosts the[AI Tinkery], “a collaborative space for educators to learn and make with generative AI.”\n* The[Stanford Institute for Human-Centered Artificial Intelligence (HAI)] is an interdisciplinary institute that advances AI research, education, policy, and practice.## Questions?\nFor questions and suggestions related to AIMES, please contact co-leads:\n* Cassandra Volpe Horii, associate vice provost for education and director of the Center for Teaching and Learning, at[cvhorii@stanford.edu] \n* Michele Elam, senior associate vice provost for undergraduate education, William Robertson Coe Professor of Humanities, and senior fellow at the Institute for Human-Centered AI\nBack to Top",
    "length": 8679,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Regulating Under Uncertainty: Governance Options for Generative AI",
    "url": "https://cyber.fsi.stanford.edu/content/regulating-under-uncertainty-governance-options-generative-ai",
    "text": "Regulating Under Uncertainty: Governance Options for Generative AI | FSI\n[Stanford University] \n[![Logo for the Cyber Policy Center]] [![Logo for the Cyber Policy Center]] \nThe Cyber Policy Center is part of the[Freeman Spogli Institute for International Studies].\nMenuClose\nSearch\n# Regulating Under Uncertainty: Governance Options for Generative AI\n![illustration of swooping lines intersecting] \nREGULATING UNDER UNCERTAINTY: Governance Options for Generative AI\nby Florence G&#039;sell\n* [DOWNLOAD FULL REPORT] \nImage\n![abstract image of lines swirling] \n## **About the Report**\n### Although innovation in AI has occurred for many decades, the two years since the release of ChatGPT have been marked by an exponential rise in development and attention to the technology. Unsurprisingly, governmental policy and regulation has lagged behind the fast pace of technological development. Nevertheless, a wealth of laws, both proposed and enacted, have emerged around the world.\n### The title of this report –“Regulating Under Uncertainty: Governance Options for Generative AI” –seeks to convey the unprecedented position of governments as they confront the regulatory challenges AI poses. Regulation is both urgently needed and unpredictable. It also may be counterproductive, if not done well. However, governments cannot wait until they have perfect and complete information before they act, because doing so may be too late to ensure that the trajectory of technological development does not lead to existential or unacceptable risks. The goal of this report is to present all of the options that are “on the table” now with the hope that all stakeholders can begin to establish best practices through aggressive information sharing. The risks and benefits of AI will be felt across the entire world. It is critical that the different proposals emerging are assembled in one place so that policy proponents can learn from one another and move ahead in a cooperative fashion.\n### **Please note: This document is the revised second edition of the report, updated as of September 2024**.\n![project liberty logo] \n### *We are greatly indebted to the*[*Project Liberty Institute*] *****for their support of the Program on Governance of Emerging Technologies, which made this report possible*.\nExecutive Summary\n[Executive Summary] \nExecutive Summary\nThe revolution underway in the development of artificial intelligence promises to transform the economy and all social systems. It is difficult to think of an area of life that will not be affected in some way by AI, if the claims of the most ardent of AI cheerleaders prove true. Although innovation in AI has occurred for many decades, the two years since the release of ChatGPT have been marked by an exponential rise in development and attention to the technology. Unsurprisingly, governmental policy and regulation has lagged behind the fast pace of technological development. Nevertheless, a wealth of laws, both proposed and enacted, have emerged around the world. The purpose of this report is to canvas and analyze the existing array of proposals for governance of generative AI.\n[DOWNLOAD EXECUTIVE SUMMARY] \nCHAPTER ONE | Introduction\n[CHAPTER ONE | Introduction] \nCHAPTER ONE | Introduction\nEffective regulation of emerging technologies inevitably presents legislators with a set of difficult choices. If they act aggressively to mitigate all hypothetical risks, they might inhibit the development of the technology. If they act too conservatively at the outset, they might miss the chance to steer the industry toward the safe development of the technology and away from foreseeable harms. These choices must also be made at a time when the knowledge and expertise about the technology resides mostly in the private sector. As a result, governments often do not have the necessary expertise to design and enforce a new regulatory regime.\nProposed and existing government regulation occurs along a continuum, from a laissez faire model, that mostly characterizes the United States, to a more command-and-control model characteristic of traditional forms of regulation, with China at the extreme opposite pole from the U.S. In the middle are different degrees of co-regulation, such as that prevalent in the European Union, in which governments exist in a dialogic relationship with companies to respond incrementally to new developments and discovered harms from the technology.\n[DOWNLOAD CHAPTER 1] \nCHAPTER 2 | Generative AI: The technology and supply chain\n[CHAPTER 2 | Generative AI: The technology and supply chain] \nCHAPTER 2 | Generative AI: The technology and supply chain\nDespite its success and widespread use, the term “generative AI” encompasses sophisticated technology and a complex, often opaque supply chain. Therefore, it is essential to clarify the nature of generative AI and its technical characteristics. This chapter begins by defining generative AI, followed by a brief overview of the main stages in developing a generative AI model. Finally, it highlights the key characteristics of the current supply chain.\n[DOWNLOAD CHAPTER 2] \nCHAPTER 3 | Challenges and risks of generative AI\n[CHAPTER 3 | Challenges and risks of generative AI] \nCHAPTER 3 | Challenges and risks of generative AI\nAlthough no consensus has yet emerged as to the transformational potential of the current crop of generative AI, few dispute its widespread economic and social impact. Of course, some of these benefits are also viewed as risks or costs. AI’s transformation of the economy, like all previous technological transformations, will come with massive job displacement. And while AI may help find ways to mitigate climate change and energy shortages, the building of massive data centers and the training of new models promises to significantly increase energy demands in the short term due to the buildout of AI technology.\nDisinformation threats from synthetic imagery, an explosion in virtual child pornography, scams using voice mimicking technology to defraud unsuspecting victims, discrimination and bias in AI algorithms, and a host of problems due to vulnerabilities to jailbreaking and inaccurate responses provided by chatbots are just a few of the problems already presented by existing generative AI tools. As these tools are rolled out for use in law enforcement, criminal justice, judicial process, employment, education, healthcare, and any number of other domains, both the malfunctioning of the systems and their abuse by bad actors cautions against overreliance on AI and wholesale replacement of human oversight of these processes.\nThose who warn of the societal impacts from AI are alarmed by the potential for catastrophic harms —from novel viruses and new weapons to uncontrollable AI agents and cyberattacks. Early in the immediate aftermath of ChatGPT, leading AI scientists and business leaders called on governments to begin addressing collectively the problems of new existential risks posed by AI. However, experts remain divided on the plausibility of “lossof control” scenarios, where a highly intelligent “rogue AI” could surpass human oversight and potentially spiral out of control. And critics of that concern over future existential risks suggest focus should be placed instead on the immediate and tangible risks posed by generative AI that require government action.\n[DOWNLOAD CHAPTER 3] \nCHAPTER 4 | Industry initiatives\n[CHAPTER 4 | Industry initiatives] \nCHAPTER 4 | Industry initiatives\nThe increasing public attention and evolving risks associated with generative AI have spurred AI companies to develop practices that mitigate risks while harnessing economic potential. It would be an overstatement to claim that individual measures by AI developers constitute industry-wide self-regulation, yet these initiatives may contribute to the creation of self-regulatory instruments. These emerging standards and practices are widely discussed and collaboratively refined within the AI community, often becoming recognized as best practices.\nThis chapter begins by offering a general overview of the practices commonly adopted by companies developing generative AI models and systems to address current risks and challenges. It then explores the collective initiatives within the industry that resemble self-regulation.\n[DOWNLOAD CHAPTER 4] \nCHAPTER 5 | Regulatory Initiatives\n[CHAPTER 5 | Regulatory Initiatives] \nCHAPTER 5 | Regulatory Initiatives\nGovernment regulation of some kind is inescapable, if only to clarify how existing laws will apply to the newest technologies. The most significant and comprehensive piece of legislation passed thus far with respect to AI is the EU’s AI Act. The AI Act regulates AI systems based on risk levels and use cases, particularly in sensitive sectors. The Act categorizes risks based on the “intended” use of AI systems and classifies them into four risk categories: unacceptable risk, high risk, limited or “transparency” risk, and minimal or no risk. Depending on the risk level involved in the application, different legal requirements will apply. During the negotiation process over the AI Act, provisions were added to regulate general-purpose (i.e., foundation) models, shifting the focus from specific use cases to the technology itself. General-purpose AI models posing systemic risks must comply with additional obligations related to cybersecurity, red teaming, risk mitigation, incident reporting, and model evaluation.\nAlthough the United States may be characterized as having a “hands-off” regulatory approach, either through the courts or through legislation, some regulation will be necessary. For now, most of the action in the US has occurred either in the executive branch or in the states. In addition to securing voluntary commitments from the major AI companies, the Biden administration issued Executive Order 14110, which outlines eight guiding principles and policy priorities for federal agencies and authorities. The absence of a comprehensive legal framework has prompted some individual states to enact their own AI-related legislation, addressing issues such as deepfakes and algorithmic discrimination. Perhaps the most significant proposal, California's pending “Safe and Secure Innovation for Frontier Artificial Intelligence Models Act,” would impose significant compliance obligations on AI developers.\nThis chapter examines the AI regulatory approaches of other countries. China has implemented a series of laws that place significant restrictions on AI development. Other countries, such as Brazil and Canada, are actively engaged in formulating regulatory frameworks for AI. In contrast, countries like Japan and India initially refrained from enacting specific AI legislation but are now gradually progressing towards its adoption. The United Kingdom, while recognizing the risks and challenges associated with AI, particularly its most advanced models, has thus far chosen not to introduce specific AI legislation, though this stance may be reconsidered in the future.\n[DOWNLOAD CHAPTER 5] \nCHAPTER 6 | International initiatives and negotiations\n[CHAPTER 6 | International initiatives and negotiations] \nCHAPTER 6 | International initiatives and negotiations\nGiven the geopolitical implications of the race to control this new and powerful technology and the ease with which the technology will transcend the boundaries of a given regulator, it should come as no surprise that most major international organizations have proposed or are drafting new initiatives related to AI. The G7’s*Hiroshima AI Process*has resulted in non-binding yet influential frameworks, such as its*Guiding Principles*and*Code of Conduct*for AI. The G20 has also published its own*G20 AI Principles*, and the EU-US Trade and Technology Council has released collaborative AI projects. The five-nation BRICS group has formed an AI Study Group to foster innovation and establish AI governance standards, in alignment with China’s*Global AI Governance**Initiative*.\nIn May 2024, the Council of Europe adopted the first international AI treaty, the*Framework Convention on Artificial Intelligence and Human Rights, Democracy, and the Rule of Law*, which requires ratification by at least five states to enter into force. The United Nations established a High-Level Advisory Body on AI and adopted its first AI resolution. UNESCO provided ethical guidelines and global guidance on AI use in education and research. Additionally, the African Union published various policy documents to guide AI development in Africa. Most of these international initiatives have been strongly influenced by the work of the OECD’s “Recommendation on Artificial Intelligence.”\n[DOWNLOAD CHAPTER 6] \nCHAPTER 7 | Final conclusions\n[CHAPTER 7 | Final conclusions] \nCHAPTER 7 | Final conclusions\nSeveral high-level principles and observations emerge from this exploration of the different national and international initiatives related to AI:\n\\**Regulation of the technology or its applications*. Sector specific laws allow AI regulation to work incrementally and adapt laws in narrow ways to the changes made by AI. However, due to the uncertain reach and implications of AI technology and the development of general-purpose AI models, regulating the technology itself is particularly important. \\**The importance of transparency and auditing*. Because the impacts of generative AI are unknown, transparency in the development of this technology is critical. To fully understand the implications of foundation models and generative AI applications, both developers and external third parties must rigorously test them prior to deployment.\n\\**The importance of enforcement*. Enforcement will prove as important, if not more so, than legislation. This will require governments to hire AI talent and coordinate with companies and civil society to provide continuous guidance on how the rules on the books apply to new and emerging contexts.\n\\**The relative power of the public and private sectors*. The need to collect vast amounts of data, the scarcity of chips, and the high costs of computation have concentrated the resources required to develop and train the most powerful models in the hands of a few private companies. To “democratize” the production of AI may require massive public investment to ensure that other actors will be able produce the most cutting edge AI models.\n\\**The promise and risks of open models*. Open-source models might create a competitive environment, quite different from that of social media and search engines, which have been controlled by a few oligopolistic actors. However, once these models are released, they can be used and fine-tuned by bad actors for all kinds of intended and unintended purposes. Moreover, once they are “out the door,” there is little that the companies or governments can do to control their impact. Government regulation in this space must address the relative risks and benefits posed by open-source models.\n[DOWNLOAD CHAPTER 7] \n## **Acknowledgements and Contributors**\nThis work would not exist without the dedicated efforts of the members of the Governance of Emerging Technologies program at the Cyber Policy Center, directed by[**Florence G’sell**] and managed by[**Ben Rosenthal**.] We are greatly indebted to the**Project Liberty Institute**for their support of the Program on Governance of Emerging Technologies, which made this report possible.\nThis report has benefited from the significant inputs of several key contributors.[**Elliot Stewart**] focused on technology, meticulously examining the practices of major AI companies, a task made even more challenging by their constantly evolving practices and policies.[**Chris Suhler**] and[**Ashok Ayyar**], assisted by[**Nikta Shahbaz**], scrutinized the ongoing strategy of the U.S. federal administration as well as the regulations adopted and proposed at both the federal and state levels. Professor[**Jiaying Jiang**] **,**[**Jasmine Shao**], and[**Sabina Nong**] joined their efforts to provide a comprehensive and precise overview of the Chinese framework.[**Zeke Gillman**] analyzed the frameworks of Canada, South Korea, Singapore, the UK, Israel, Saudi Arabia, and the Emirates, and also studied ongoing international initiatives.[**Arpit Gupta**] examined the legal framework of India and the collective practices of AI companies.[**Tally Smitas**] analyzed the legislation currently being adopted in Brazil.[**Ryoko Matsumoto**] provided an overview of Japan's framework. Professor[**Keeheon Lee**] offered significant insights into South Korea's regulatory strategy, while**Nathan Levit**and[**Maya Rodriguez**] finalized the presentation of the South Korean and Singaporean frameworks.\nIt is also appropriate to express gratitude to[**Sanna Ali**],[**Tamian Derivry**], and[**Luca Lefevre**] for their research efforts and assistance on various topics throughout the drafting of this report.\nThis report has also been enriched by the valuable feedback and comments of numerous colleagues who dedicated their time to review it and offer substantial and relevant suggestions. In particular, Professor**Jingwen Wang**and Professor**Xinyu Fu**provided invaluable insights on generative AI technology, while[**Dave Willner**] shared his expertise on the generative AI industry.\nThis work has also benefited from the precious comments of[**Nate Persily**] **, Carlos Escapa,**[**Sarah Cen**] **, David Shao, Jiankun Ni, Sijia Liu, Mick Li, Chun Yu Hong, Hiroki Habuka, Shayne Longpre, Rishi Bommasani, Kevin Klyman, Dan Ho,**[**Mark Lemley**] **,**[**Daphne Keller**] **, Presley Warner, Suzanne Marton, Jerrold Soh, Conor Chapman**and****[**Samidh Chakrabarti**].\nInvaluable technical assistance was provided by copy editors**Lisa Keen**and**Eden Beck**, and designer**Michi Turner**.\n**Nick Amador, Zeke Gillman, Nathan Levit, Nate Low, Jasmine Shao, Nikta Shahbaz**, and**Harith Khawaja**completed the Bluebooking.\nFinally, all the students of the Spring 2023**Governance and Regulation of Emerging Technologies Policy Practicum**should be thanked for their inspiring work, which served as the starting point for this policy report:**Taylor Applegate, Sindy Braun, Alexis Dye, Drew Edwards, Mary Rose Fetter, Dakota Foster, Christopher Giles, August Gweon, Caroline Hunsicker, Poramin Insom, Crys Jain, Harith Khawaja, Atsushi Kono, Ashley Denise Leon, Zahavah Levine, Miranda Lin Li, Katherine McCreery, Caroline Meinhardt, David Mollenkamp, Ilari Papa Kate Reinmuth, Gabriela A. Romero, Greg D. Schwartz, Sade Snowden- Akintunde, Elliot Stahr, Silva Stewart, Christine Strauss, Chris Suhler, Kiran Wattamwar,**and**Ashton E. Woods**..\n[Florence G&#039;sell]![Florence G&#039;Sell Headshot] \n## [Florence G&#039;sell] \n## AUTHOR | FLORENCE G&#039;SELL\nFlorence G’sell is a visiting professor of private law at the Cyber Policy Center, where she leads the Program on Governance of Emerging Technologies. She also holds the Digital, Governance, and Sovereignty Chair at Sciences Po (France) and is a professor of private law at the University of Lorraine (currently on leave).\n[FULL BIO]",
    "length": 19148,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "",
    "url": "https://hai.stanford.edu/sites/default/files/2021-11/Building-national-ai-research-resource.pdf",
    "text": "Building a \nNational \nAI Research \nResource: \nA Blueprint for \nthe National \nResearch Cloud\nDaniel E. Ho\nJennifer King\nRussell C. Wald\nChristopher Wan\nWHITE PAPER\nOCTOBER 2021\nPrincipal Authors \nDaniel E. Ho, J.D., Ph.D., is the William Benjamin Scott and Luna M. Scott Professor of Law, \nProfessor of Political Science, and Senior Fellow at the Stanford Institute for Economic Policy \nResearch at Stanford University. He directs the Regulation, Evaluation, and Governance Lab \n(RegLab) at Stanford, and is a Faculty Fellow at the Center for Advanced Study in the Behavioral \nSciences and Associate Director of the Stanford Institute for Human-Centered Artificial \nIntelligence (HAI). He received his J.D. from Yale Law School and Ph.D. from Harvard University \nand clerked for Judge Stephen F. Williams on the U.S. Court of Appeals for the District of \nColumbia Circuit. \nJennifer King, Ph.D., is the Privacy and Data Policy Fellow at the Stanford HAI. She completed \nher doctorate in information management and systems (information science) at the University of \nCalifornia, Berkeley School of Information. Prior to joining HAI, she was the Director of Consumer \nPrivacy at the Center for Internet and Society at Stanford Law School from 2018 to 2020.\nRussell C. Wald is the Director of Policy for the Stanford HAI, leading the team that advances \nHAI’s engagement with governments and civil society organizations. Since 2013, he has held \nvarious government affair roles representing Stanford University. He is a Term Member with the \nCouncil on Foreign Relations, Visiting Fellow with the National Security Institute at George Mason \nUniversity, and a Partner with the Truman National Security Project. He is a graduate of UCLA.\nChristopher Wan is a JD/MBA candidate at Stanford University and was teaching assistant for \nthe Stanford Policy Practicum: Creating a National Research Cloud. He also serves as a research \nassistant for the Stanford HAI and as an investor at Bessemer Venture Partners. He received his \nB.S. in computer science from Yale University and worked as a software engineer at Facebook and \nas a venture investor at In-Q-Tel and Tusk Ventures.\nThe Stanford Institute for Human-Centered Artificial Intelligence\nCordura Hall, 210 Panama Street, Stanford, CA 94305-4101\nOctober 2021, V1.0\n2\nContributors \nMany dedicated individuals contributed to this White Paper. To acknowledge these contributions, \nwe list here the contributors for each chapter and section. \nExecutive Summary and Introduction \nDaniel E. Ho, Tina Huang, Jennifer King, Marisa Lowe, Diego Núñez, Russell Wald, Christopher Wan, \nDaniel Zhang \n \nThe Theory for a National Research Cloud \nNathan Calvin, Shushman Choudhury, Tina Huang, Daniel E. Ho, Kanishka Narayan, Diego Núñez, \nFrieda Rong, Russell Wald, Christopher Wan \nEligibility, Allocation, and Infrastructure for Computing\nDaniel E. Ho, Krithika Iyer, Tyler Robbins, Jasmine Shao, Russell Wald, Daniel Zhang \nSecuring Data Access\nNathan Calvin, Shushman Choudhury, Daniel E. Ho, Ananya Karthik, Jennifer King, Christopher Wan \nOrganizational Design \nSabina Beleuz, Drew Edwards, Daniel E. Ho, Jennifer King, Christopher Wan \nData Privacy Compliance \nSimran Arora, Neel Guha, Jennifer King, Sahaana Suri, Christopher Wan, Sadiki Wiltshire \nTechnical Privacy and Virtual Data Safe Rooms\nNeel Guha, Jennifer King, Christopher Wan \nSafeguards for Ethical Research \nDaniel E. Ho, Jennifer King, Diego Núñez, Russell Wald, Daniel Zhang \nManaging Cybersecurity Risks \nNeel Guha, Diego Núñez, Frieda Rong, Russell Wald \nIntellectual Property \nSabina Beleuz, Daniel E. Ho, Ananya Karthik, Diego Núñez, Christopher Wan \nCase Studies \nDaniel E. Ho, Krithika Iyer, Jennifer King, Marisa Lowe, Kanishka Narayan, Tyler Robbins \nWe also would like to thank Jeanina Casusi, Celia Clark, Shana Lynch, Kaci Peel, Stacy Peña, \nMike Sellitto, Eun Sze, and Michi Turner for their help in preparing this White Paper. \n3\nExternal Participants \nErik Brynjolfsson \nStanford University \n \nIsabella Chu \nPopulation Health \nSciences \nStanford University \n \nJack Clark \nAnthropic \nJohn Etchemendy \nStanford University \n \nFei-Fei Li \nStanford University \nMarc Groman \nGroman Consulting \nGroup LLC \n \nEric Horvitz \nMicrosoft \nSara Jordan \nThe Future of \nPrivacy Forum \nVince Kellen \nUC San Diego, \nCloudBank \n \nEd Lazowska \nUniversity of \nWashington \nNaomi Lefkovitz \nNational Institute of \nStandards and \nTechnology (NIST) \nBrenda Leong \nThe Future of \nPrivacy Forum \nAmy O’Hara \nGeorgetown Federal \nStatistical Research \nData Center \nGeorgetown University \n \nWade Shen \nActuate Innovation \n \nSuzanne Talon \nCompute Canada \n \nLee Tiedrich \nCovington & Burling LLP \n \nEvan White \nCalifornia Policy Lab \nUC Berkeley \nIn our process, we also engaged many civil society leaders and advocates who have expressed many \nperspectives about building a National Research Cloud. We have incorporated their feedback where \npossible and are grateful for their shared thoughts and for helping us shape a better White Paper.\nGuest Lecturers and Interviewees \nWe relied on extraordinary outside expert reviewers for feedback and guidance. We are grateful to \nLeisel Bogan, Jack Clark, John Etchemendy, Mark Krass, Marietje Schaake, and Christine Tsang for \ntheir thoughtful review of the full White Paper, and thank Isabella Chu, Kathleen Creel, Luciana Herman, \nSara Jordan, Vince Kellan, Brenda Leong, Ruth Marinshaw, Amy O’Hara, and Lisa Ouellette for their \nsubject expertise on specific chapters. \nReviewers \n4\nTaka Ariga \nGovernment Accountability \nOffice \nKathy Baxter \nSalesforce \nLeisel Bogan \nBelfer Center \nHarvard University \nJeffrey Brown \nIBM \nMiles Brundage \nOpen AI \nL. Jean Camp \nUniversity of Indiana \nat Bloomington \nDakota Cary \nCenter for Security and \nEmerging Technology\nGeorgetown University \nShikai Chern \nVeritas Technologies \nIsabella Chu \nPopulation Health Sciences \nStanford University \nJack Clark \nAnthropic \nMeaghan English \nPatrick J. McGovern \nFoundation \nCyrus Hodes \nThe Future Society \nSara Jordan \nThe Future of Privacy Forum \nVince Kellen \nUC San Diego, CloudBank \nMichael Kratsios \nScale AI \nSamantha Lai \nBrookings Institution \nBrenda Leong \nThe Future of Privacy Forum \nRuth Marinshaw \nStanford Research \nComputing Center \nJoshua Meltzer \nBrookings Institution \nSam Mulopulous \nU.S. Senate \nDewey Murdick \nCenter for Security and \nEmerging Technology \nGeorgetown University \nHodan Omaar \nCenter for Data Innovation \nCalton Pu \nGeorgia Tech \nAsad Ramzanali \nU.S. House of \nRepresentatives \nDavid Robinson \nUpturn \nSaiph Savage \nNortheastern University \nMichael Sellitto \nStanford HAI \nIshan Sharma \nFederation of \nAmerican Scientists \nBrittany Smith \nData and Society \nJohn Smith \nIBM \nBrittany Smith \nData and Society \nVictor Storchan \nJP Morgan Chase \nKeith Strier \nAI Compute Task \nForce Organisation \nfor Economic \nCo-operation and \nDevelopment (OECD) \nLee Tiedrich \nCovington & Burling LLP \n \nEvan White \nCalifornia Policy Lab \nUC Berkeley \nOn August 5, 2021, the co-authors hosted a feedback session to hear from a variety of stakeholders \nin academia, civil society, government, and industry. We are thankful for the time and helpful \nadvice participants offered. Workshop attendee affiliations are listed for identification purposes \nonly. Individuals from Microsoft and AI Now also attended the workshop but did not want to be \npersonally identified. \nWorkshop Participants \n5\nDisclosures\nStanford University actively engaged with Congress and lobbied for the National Artificial Intelligence \nResearch Resource Task Force Act. Co-author Russell Wald built a coalition of academic, civil society, \nand industry stakeholders and lobbied Congress to pass the National Artificial Intelligence Research \nResource Task Force Act. \nHAI Co-Director Fei-Fei Li, who served as a guest lecturer in the class, was an early supporter of a \ntask force to study the National Research Cloud. Dr. Li has been appointed to serve as a member of \nthe National Artificial Intelligence Research Resource (NAIRR) Task Force. \nCo-author Daniel Ho directs the Stanford RegLab, which has received compute support from HAI’s \ncloud credit program (AWS and GCP), Microsoft’s AI for Earth Azure compute credit grant program, \nand Google’s Cloud credit grant for COVID-19 research. \nCo-author Jennifer King received unrestricted gift funding for research from Mozilla, Facebook, \nand Accenture in her previous role at the Center for Internet and Society. \nThe Stanford Institute for Human-Centered Artificial Intelligence (HAI) receives financial and cloud \ncomputing support from A121 Labs, Amazon Web Services, Google, IBM, Microsoft, and OpenAI. \nAbout HAI\nAbout the SLS Policy Lab\nStanford University’s Institute for Human-Centered Artificial Intelligence (HAI) applies rigorous \nanalysis and research to pressing policy questions on artificial intelligence. A pillar of HAI is to inform \npolicymakers, industry leaders, and civil society by disseminating scholarship to a wide audience. \nHAI is a nonpartisan research institute, representing a range of voices. The views expressed in this \nWhite Paper reflect the views of the authors. \nThe Policy Lab at Stanford Law School offers students an immersive experience in finding solutions \nto some of the world’s most pressing issues under the direction of Stanford faculty and researchers. \nDirected by former SLS Dean Paul Brest, the Policy Lab reflects the school’s belief that systematic \nexamination of societal problems, informed by rigorous research, can generate solutions to society’s \nmost challenging public problems.\nAcademic Independence\nThis White Paper was developed independently by the research team. While we solicited feedback from \na wide range of stakeholders, no HAI donors, corporations, or other stakeholders had any involvement \nwith the research and production of this White Paper. Per HAI policy, “Donors cannot dictate research \ntopics pursued by HAI researchers” nor “control permission to publish research results.” For more \ninformation, please see HAI’s policy: https://hai.stanford.edu/about/fundraising-policy. \n6\nBuilding a National AI Research Resource: \nA Blueprint for the National Research Cloud\n7\nTable of Contents\nEXECUTIVE SUMMARY: Creating a National Research Cloud 9\nINTRODUCTION 15\nCHAPTER 1: A Theory for a National Research Cloud 17\nCHAPTER 2: Eligibility, Allocation, and Infrastructure for Computing 22\nCHAPTER 3: Securing Data Access 35\nCHAPTER 4: Organizational Design 48\nCHAPTER 5: Data Privacy Compliance 53\nCHAPTER 6: Technical Privacy and Virtual Data Safe Rooms 61\nCHAPTER 7: Safeguards for Ethical Research 66\nCHAPTER 8: Managing Cybersecurity Risks 70\nCHAPTER 9: Intellectual Property 76\nGLOSSARY OF ACRONYMS 82\nAPPENDIX 84\nENDNOTES 90\nBuilding a National AI Research Resource: \nA Blueprint for the National Research Cloud\n8\nCOMPUTE MODELS\nNSF CloudBank 27\nNSF XSEDE 29\nFugaku 32\nCompute Canada 34\nDATA MODELS\nColeridge Initiative 42\nStanford Population Health Sciences 43\nThe Evidence Act 46\nORGANIZATIONAL MODELS\nScience and Technology Policy Institute 50\nAlberta Data Partnerships 51\nOTHER MODELS\nAdministrative Data Research UK 58\nCalifornia Policy Lab 64\nCase Studies\nBuilding a National AI Research Resource: \nA Blueprint for the National Research Cloud\n9\nArtificial intelligence (AI) appears poised to transform the economy across sectors ranging from healthcare and finance \nto retail and education. What some have coined the “Fourth Industrial Revolution”1 is driven by three key trends: greater \navailability of data, increases in computing power, and improvements to algorithm design. First, increasingly large amounts \nof data have fueled the ability for computers to learn, such as by training an algorithmic language model on all of Wikipedia.2\nSecond, better computational capacity (often termed “compute”) and compute capability have enabled researchers to build \nmodels that were unimaginable merely 10 years ago, sometimes spanning billions of parameters (an exponential increase \nin scope from previous models).3 Third, basic innovations in algorithms are helping scientists to drive forward AI, such as the \nreinforcement learning techniques that enabled a computer to defeat the world champion in the board game Go.4\nHistorically, partnerships between government(s), universities, and industries have anchored the U.S. innovation \necosystem. The federal government played a critical role in subsidizing basic research, enabling universities to undertake \nhigh-risk research that can take decades to commercialize. This approach catalyzed radar technology, the internet, and \nGPS devices. As the economists Ben Jones and Larry Summers put it, “[e]ven under very conservative assumptions, it is \ndifficult to find an average return below $4 per $1 spent” on innovation, and the social returns might be closer to $20 for \nevery dollar spent.5 Industry in turn, scales and commercializes applications. \nCHALLENGES TO THE AI INNOVATION ECOSYSTEM\nYet this innovation ecosystem faces serious potential challenges. Computing power has become critical for the \nadvancement of AI, but the high cost of compute has placed cutting-edge AI research in a position accessible only to key \nindustry players and a handful of elite universities.6 Access to data—the raw ingredients used to train most AI models—is \nincreasingly limited to the private sector and large platforms7, since government data sources remain largely inaccessible \nto the AI research community.8 As the National Security Commission on AI (NSCAI) has determined, “[t]he consolidation \nof the AI industry threatens U.S. technological competitiveness.”9\nFour interrelated challenges illustrate this finding: First, we are seeing a significant brain drain of researchers \ndeparting universities.10 In 2011, AI Ph.D.s were roughly as likely to go into industry as academia.11 Ten years later, two\u0002thirds of AI Ph.D.s go into industry, and less than one quarter go into academia.12 Second, these trends indicate that \nmany university researchers struggle to engage in cutting-edge science, draining the field of the diverse set of research \nvoices that it needs. Third, the fundamental research that would guarantee the United States stays at the helm of AI \ninnovation is being crowded out. By one estimate, 82 percent of algorithms used today originated from federally funded \nnonprofits and universities, but “U.S. leadership has faded in recent decades.”13 Fourth, government agencies have faced \nchallenges in building compute infrastructure,14 and there are societal benefits to reducing the cost of core governance \nfunctions and improving government’s internal capacity to develop, test, and hold AI systems accountable.15 In short, \na growing imbalance in AI innovation tilts toward industry, leaving academic and noncommercial research behind. \nGiven the long-standing role of academic and non-commercial research in innovation, this shift has substantial negative \nconsequences for the American research ecosystem. \nExecutive Summary: \nCreating a National Research Cloud\nBuilding a National AI Research Resource: \nA Blueprint for the National Research Cloud\n10\nTHE NATIONAL AI RESEARCH \nRESOURCE TASK FORCE ACT\nResponding to these challenges, Congress enacted \nthe National AI Research Resource Task Force Act as \npart of the National Defense Authorization Act (NDAA) \nin January 2021.16 The Act forms part of the National \nArtificial Intelligence Initiative, which identifies further \nsteps to increase research investments, set technical \nstandards, and build a stronger AI workforce. The Act \ncreated a Task Force—the composition of which was \nannounced on June 10, 202117—to study and plan for \nthe implementation of a “National Artificial Intelligence \nResearch Resource” (NAIRR), namely “a system that \nprovides researchers and students across scientific \nfields and disciplines with access to compute resources, co-located with publicly available, artificial intelligence-ready \ngovernment and non-government data sets.”18 This research resource has also been referred to as the National Research \nCloud (NRC) and was strongly endorsed by the NSCAI, which wrote that the NRC “will strengthen the foundation of \nAmerican AI innovation by supporting more equitable growth of the field, expanding AI expertise across the country, and \napplying AI to a broader range of fields.”19\nWhile other initiatives have sought to improve access to compute or data in isolation,20 the NRC will generate distinct \npositive externalities by integrating compute and data, the two bottlenecks for high-quality AI research. Specifically, the \nNRC will provide affordable access to high-end computational resources, large-scale government datasets in a secure \ncloud environment, and the necessary expertise to benefit from this resource through a close partnership between \nacademia, government, and industry. By expanding access to these critical resources in AI research, the NRC will support \nbasic scientific AI research, the democratization of AI innovation, and the promotion of U.S. leadership in AI. \nTHEMES\nStanford Law School’s Policy Lab program convened a multidisciplinary research team of graduate students, staff, \nand faculty drawn from Stanford’s business, law, and engineering schools to study the feasibility of and considerations for \ndesigning the NRC. Over the past six months, this group studied existing models for compute resources and government \ndata, interviewed a wide range of government, computer science, and policy experts, and examined the technical, business, \nlegal, and policy requirements. This White Paper was commissioned by Stanford’s Institute for Human-Centered Artificial \nIntelligence (HAI), which originated the proposal for the NRC in partnership with 21 other research universities.21\nThroughout our research, we observed three primary themes that cut across all areas of our investigation. We have \nintegrated these themes into each section of our White Paper and drawn on them to explain our findings. \n• Complementarity between compute and data. As we evaluated the existing computing and data-sharing ecosystems, \none of the systemic challenges we observed was a decoupling of compute resources from data infrastructures. \nThe NRC directs more resources \ntoward AI development in the public \ninterest and helps ensure long-term \nleadership by the United States in the \nfield by supporting the kind of pure, \nbasic research that the private sector \ncannot undertake alone.\nBuilding a National AI Research Resource: \nA Blueprint for the National Research Cloud\n11\nHigh-performance computing can be useless \nwithout data, and a major impediment to data \nsharing—particularly for high-value government \ndata—lies in requirements for a secure, privacy\u0002protecting computing environment. \n• Rebalancing AI research toward long-term, \nacademic, and noncommercial research. Presently, \nAI innovation is disproportionately dependent on \nthe private sector. Public investment in basic AI \ninfrastructure can both support innovation in the \npublic interest and complement private innovation \nefforts. The NRC directs more resources toward \nAI development in the public interest and helps \nensure long-term leadership by the United States in the field by supporting the kind of pure, basic research that the \nprivate sector cannot undertake alone.\n• Coordinating short-term and long-term approaches to creating the NRC. Our research considers many near-term \npathways for standing up a working version of the NRC by spelling out how to work within existing constraints. We \nalso identify the structural, legal, and policy challenges to be addressed in the long term for executing the full vision \nof the NRC. \nWe summarize our main recommendations here.\nCOMPUTE MODEL\n• The “Make or Buy” Decision. The main policy choice will be whether to build public computing infrastructure or \npurchase services from existing commercial cloud providers. \n° It is well-established that, based solely on hardware costs, it is more cost-effective to own infrastructure when \ncomputing demand is close to continuous.22 The government also has experience building high-performance \ncomputing clusters, typically built by contractors and operated by national laboratories.23 The National Science \nFoundation (NSF) has also supported many supercomputing initiatives at academic institutions.24\n° The main countervailing concerns are that existing commercial cloud providers have software stacks and usability \nthat AI researchers have widely adopted and may consider to be a more user-friendly platform. Commercial cloud \nproviders offer a way to expand capacity expeditiously, although scale and availability will still be constrained by \nthe availability of current graphics processing unit (GPU) computing resources. \n° We recommend a dual investment strategy: \n■ First, the compute model of the NRC can be quickly launched by subsidizing and negotiating cloud \ncomputing for AI researchers with existing vendors, expanding on existing initiatives like the NSF’s \nCloudBank project.25\n■ Second, the NRC should invest in a pilot for public infrastructure to assess the ability to provide similar \nresources in the long run. Such publicly owned infrastructure would still be built under contract or \nOne of the systemic challenges \n[to basic AI research is] a decoupling \nof compute resources from data \ninfrastructures. . . . [A] secure, \nprivacy-protecting computing \nenvironment [will be critical].\nBuilding a National AI Research Resource: \nA Blueprint for the National Research Cloud\n12\ngrant, but could be operated much like national laboratories (e.g., Sandia National Laboratories, \nOak Ridge National Laboratory) that own sophisticated supercomputing facilities or academic \nsupercomputing facilities. \n• Researcher Eligibility. While some have argued the NRC should be open for commercial access, for the purposes of \nthis White Paper, we adhered to the spirit of the legislation forming the NAIRR Task Force and only reviewed the use of \nan NRC for academic and nonprofit AI research. We recommend that the NRC eligibility start with academics who hold \n“Principal Investigator” (PI) status (i.e., most faculty) at U.S. colleges and universities, as well as “Affiliated Government \nAgencies” willing to contribute previously unreleased, high-value datasets to the NRC in return for subsidized compute \nresources. PI status should be interpreted expansively to encompass all fields of AI application. Students working with \nPIs should presumptively gain access to the NRC. Scaling the NRC to meet the demand of all students in the United \nStates may be challenging, but we also recommend the creation of educational programs as part of the new resource \nto help train the next generation of AI researchers. \n• Mechanism. In order to keep the award processing costs down, we recommend a base level of compute access to \nmeet the majority of researcher computing needs. Base-level access avoids high overhead for grant administration \nand may meet the compute demands for the supermajority of researchers. For researchers with exceptional needs, \nwe recommend a streamlined grant process for additional compute access. \nDATA ACCESS MODEL\n• Focus on Government Data. We focus our recommendations for data provision/access to government data \nbecause: (1) there are already a wide range of platforms for sharing private data,26 and (2) distribution by the NRC \nof private datasets would raise a tangle of thorny IP issues. We recommend that researchers be allowed to compute \non any datasets they themselves contribute, provided they certify they have the rights to that data, and the use of \nsuch data is for academic research purposes.\n• Tiered Access. We recommend a tiered access model: By default, researchers will gain access to government data \nthat is already public; researchers can then apply through a streamlined process to gain access at higher security \nlevels on a project-specific basis. It will be critical for the NRC to ultimately displace the current fragmented, agency\u0002by-agency relational approach. By providing secure virtual environments and harmonizing security standards (e.g., \nFederal Risk and Authorization Management Program (FedRAMP)27), the NRC can collaborate with proposals for a \nNational Secure Data Service28 to provide a model for accelerating AI research, while protecting data privacy and \nprioritizing data security.\n• Agency Incentives. To incentivize federal agencies to share data with the NRC and improve the state of public \nsector technology, we recommend the NRC permit federal agency staff to use the NRC’s compute resources. In \nkeeping with the practices of existing data-sharing programs, such as the Coleridge Initiative,29 we also recommend \nthat the NRC provide training and support to work with agencies to modernize and harmonize their data standards.\n• Strategic Investment for Data Sources. In the short term, we recommend that the NRC focus its efforts on making \navailable non-sensitive, low- to moderate-risk government datasets, rather than sensitive government data (e.g., \ndata about individuals) or data from the private sector, due to data privacy and intellectual property concerns. \nResearchers can still use NRC compute resources on private data but should rely on existing mechanisms to acquire \ndata for their own private buckets on the NRC. For example, images taken from Earth observation satellites, such as \nBuilding a National AI Research Resource: \nA Blueprint for the National Research Cloud\n13\nLandsat imagery, provide a promising low-risk, high-reward government dataset, as making such satellite imagery \nfreely available to researchers has generated an estimated $3-4 billion in annual economic benefits, particularly \nwhen combined with high-performance computing.30 Agencies such as the National Oceanic and Atmospheric \nAdministration, the U.S. Geological Survey, the Census Bureau, the Administrative Office of the U.S. Courts, and \nthe Bureau of Labor Statistics, for instance, also have rich datasets that can more readily be deployed. In the long \nrun, access to high-risk datasets, such as those owned by the Internal Revenue Service (IRS) and the Department of \nVeterans Affairs (VA), will depend on the tiered access model. \nORGANIZATIONAL FORM\nWhere to institutionally locate the NRC poses a tradeoff between ease of coordination to obtain compute and ease \nof data access. For instance, locating the NRC within a single agency would make coordination with compute providers \neasier, but would make data access across agencies more difficult, absent further statutory authority. Many efforts to \nmake data access to government data easier, most notably the Foundations for Evidence-Based Policymaking Act of \n2018, have proven to be among the most daunting challenges of government modernization.31 Building on those insights, \nwe ultimately recommend that the NRC be instituted as a Federally Funded Research and Development Center (FFRDC) \nin the short run, and a public-private partnership (PPP) in the long run.\n• FFRDC. FFRDCs at Affiliated Government Agencies would reduce the significant costs of securing data from those \nhost agencies. This approach will also cohere with the greater reliance on commercial cloud credits in the short run, \nmaking compute and data coordination less central. In the long run, however, streamlined coordination between \ndata and compute may be more difficult with FFRDCs hosted at specific agencies when (1) the NRC moves away \nfrom commercial cloud credits and toward its own high-performance computing cluster, and (2) a greater number \nof interagency datasets become available. \n• PPP. In the long run, we recommend the creation of a PPP model, governed by officers from Affiliated Government \nAgencies, academic researchers, and representatives from the technology sector, which can house both compute \nand data resources. \nADDITIONAL CONSIDERATIONS\n• Data Privacy. As an initial matter, an NRC where sensitive or individually identifiable administrative data from \nmultiple agencies are used to build and train AI models will face challenges from the Privacy Act of 1974.32 The Act is \nintended to put a check on interagency data-sharing and disclosure of sensitive data without consent. \n° In order to avoid conflicts with nonconsensual interagency data-sharing, we recommend that the NRC should \nnot be instituted as its own federal agency, nor should federal agency staff be allowed access to interagency \ndata. \n° To avoid conflicts with the Act’s “no disclosure without consent” requirement, any data released to the NRC \nmust not be individually identifiable. Despite these constraints, the majority of AI research will likely fall under \nthe Act’s statistical research exception, contingent on proposals aligning with an agency’s core purpose. \n° Given concerns about the potential privacy risks, federal agencies may desire to share data, contingent on \nthe use of technical privacy measures (e.g., differential privacy). While useful in many instances, technical \nBuilding a National AI Research Resource: \nA Blueprint for the National Research Cloud\n14\napproaches are no panacea and should not \nsubstitute for data access policies. \n° The NRC should explore the design of virtual \n“data safe rooms” that enable researchers to \naccess data in a secure, monitored, and cloud\u0002based environment.\n° Additional legislative interventions could \nalso facilitate data-sharing with the NRC (e.g., \nrequiring IT modernization to include data\u0002sharing plans with the NRC).\n• Ethics. Rapid innovation in AI research raises a \nhost of potential ethical challenges. Given the \nscope of the NRC, it will not be feasible to review \nevery single research proposal for potential ethical \nviolations, particularly since ethical standards are still in flux. The NRC should adopt a twofold approach. \n° First, for default PI access to base-level data and compute, the NRC should establish an ex-post review process \nfor allegations of ethical research violations. Access may be revoked when research is shown to manifestly and \nseriously violate ethical standards. We emphasize that the high standard for a violation should be informed \nby the academic speech implications and potential political consequences of government involvement in \nadministering the NRC and determining academic research directions. \n° Second, for applications requesting access to restricted datasets or resources beyond default compute, which \nwill necessarily undergo some review, researchers should be required to provide an ethics impact statement. \nOne of the advantages of beginning with PIs is that university faculty are accountable under existing IRBs for \nhuman subjects research, as well as to the tenets of peer review. \n° We urge non-NRC parties (e.g., universities) to explore a range of measures to address ethical concerns in AI \ncompute (e.g., an ethics review process33 or embedding ethicists in projects34).\n• Security. We recommend that the NRC take the lead in setting security classifications and protocols, in part to \ncounteract a balkanized security system across federal agencies that would stymie the ability to host datasets. The \nNRC should use dedicated security staff to work with Affiliated Government Agencies and university representatives \nto harmonize and modernize agency security standards. \n• Intellectual Property (IP). While the evidence on optimal IP incentives for innovation is mixed, we recommend \nthat the NRC adopt the same approach to allocating patent rights, copyrights, and data rights to NRC users \nthat apply to federal funding agreements. The NRC should additionally consider conditions for requiring NRC \nresearchers to disclose or share their research outputs under an open-access license.\n• Human Resources. Given its ambition, significant human resources—from systems engineers to data officers, and \nfrom grants administrators to privacy, ethics, and cybersecurity staff—will be necessary to make the NRC a success.\nGiven its ambition, significant \nhuman resources—from systems \nengineers to data officers, and \nfrom grants administrators to \nprivacy, ethics, and cybersecurity \nstaff—will be necessary to make \nthe NRC a success.\nBuilding a National AI Research Resource: \nA Blueprint for the National Research Cloud\n15\nIntroduction\nIn March 2020, Stanford’s Institute for Human-Centered Artificial Intelligence (HAI) published an open letter, co\u0002signed by the presidents and provosts of 22 top universities in the country, to the president of the United States and \nU.S. Congress urging adoption of a National Research Cloud (NRC).1 The NRC proposal aims to close a significant gap in \naccess to computing and data that, proponents argue, has distorted the long-term trajectory of artificial intelligence (AI) \nresearch.2 Without access to such critical resources, AI research may be dominated by short-term commercial interests \nand undermine the historical innovation ecosystem where basic, fundamental, and noncommercial research have laid \nthe foundations for applications that may be decades away, not yet marketable, or promote the public interest.\nIn January 2021, Congress enacted the National Artificial Intelligence Research Resource Task Force Act (NAIRR), \nconstituting a task force to consider the design of the NRC.3\n The task force was announced in June of this year and \nincludes one of the original proponents of the NRC and co-director of HAI (Fei-Fei Li).4\nThis White Paper is the culmination of a two-quarter, independent policy practicum at Stanford Law School’s \nPolicy Lab program, which was co-taught by three of us (Ho, King, Wald) and a teaching assistant (Wan) and brought \ntogether law, business, and engineering students to contemplate key design dimensions of the NRC. We interviewed \nand convened a wide range of stakeholders, including privacy attorneys, cloud computing technologists, government \ndata experts, cybersecurity professionals, potential users, and public interest groups. Students researched governing \nlegal provisions, policy options, and avenues for the institutional design of the NRC. The practicum team worked \nindependently to shape its recommendations.\nThe proposal for an NRC is an ambitious one, and this White Paper covers a lot of ground. We begin with the \nfundamental question—why build the NRC (Chapter 1)?—and spell out what we view as a cogent theory of impact. \nWe then cover who should have access to the NRC (Chapter 2), what comprises the NRC (Chapter 2), how access to \nrestricted data may (or may not) be granted (Chapter 3), and where the NRC should be located (Chapter 4). We spend \nextensive time on the data access portion (Chapters 3, 5, and 6), due to the complexities of government data-sharing \nunder the Privacy Act of 1974.5\n As we note in those chapters, the data portion of the NRC is complementary to long\u0002standing efforts to enable greater research access to administrative data under, for instance, the Foundations for \nEvidence-Based Policymaking Act of 20186 and the National Secure Data Service Act proposal.7 Such sharing must be \ncarried out securely and in a privacy-protecting fashion. We also consider questions of ethical standards (Chapter 7), \ncybersecurity (Chapter 8), and intellectual property (Chapter 9) that inform the design of the NRC. \nWe recognize the complexity of the enterprise and that there are many questions not answered herein. The \ncontemplated scale of the NRC may be to AI what the Human Genome Project was to genomics (or what particle \naccelerators were to physics): public investment for ambitious, noncommercial fundamental scientific research to ensure \nthe long-term flourishing of a critical area of innovation for the United States. There are many areas where we wish we \nhad had the opportunity to engage in more extensive research. We hope this White Paper nonetheless, will provide a \nuseful contribution for the NAIRR Task Force, Congress, the White House, and all those interested in the AI innovation \necosystem. \nBuilding a National AI Research Resource: \nA Blueprint for the National Research Cloud\n16\nWe owe gratitude to the many people who contributed time, feedback, and insights. Most importantly, we thank \nthe extraordinary students who shaped this White Paper: Simran Arora, Sabina Beleuz, Nathan Calvin, Shushman \nChoudhury, Drew Edwards, Neel Guha, Krithika Iyer, Ananya Karthik, Kanishka Narayan, Tyler Robbins, Frieda Rong, \nJasmine Shao, and Sadiki Wiltshire. We benefited from too many individuals to name, but special thanks go to Taka \nAriga, Kathy Baxter, Miles Brundage, Jean Camp, Shikai Chern, Bella Chu, Jack Clark, Kathleen Creel, John Etchemendy, \nDeep Ganguli, Eric Horvitz, Sara Jordan, Vince Kellen, Mark Krass, Sebastien Krier, Ed Lazowska, Brenda Leong, Fei-Fei Li, \nRuth Marinshaw, Michelle Mello, Amy O’Hara, Hodan Omaar, Saiph Savage, Marietje Schaake, Mike Sellitto, Wade Shen, \nKeith Strier, Suzanne Talon, Lee Tiedrich, Christine Tsang, and Evan White for helpful insights and feedback. HAI staff \nand research assistants who were essential in helping us during the final stages of editing and compiling the White Paper \ninclude Tina Huang, Marisa Lowe, Diego Núñez, and Daniel Zhang. \nAs we spell out in this White Paper, the NRC is an idea worth taking seriously. It is worth being clear, however, what \nit would and would not solve. The NRC would enable much greater access to—and in that sense, democratize—forms \nof AI and AI research that have increased in computational demands, but it would not categorically prevent or shift \nthe centralization of power within the tech industry. The NRC would shift the attention of current AI efforts into more \npublic and socially driven dimensions by providing access to previously restricted government datasets, addressing \nlongstanding efforts to improve access to high-value public sector data, but it would not create a system to prevent all \nunethical uses of AI. The NRC would facilitate audits of large-scale models, datasets, and AI systems for privacy violations \nand bias, but it would not be tantamount to a regulatory requirement for fairness assessments and accountability. It \nis neither a tool of antitrust nor a certification body for ethical algorithms, which are areas worth taking seriously in \nindependent policy proposals.8\n These broader considerations, however, do play into key areas of design and have very \nmuch informed our recommendations below on the design of the NRC. \nWhile it alone cannot solve all that ails AI, the NRC promises to take a major affirmative step forward.\nA Blueprint for the National Research Cloud 17\nCHAPTER 1\nThis chapter articulates a theory of impact for the NRC. In conventional policy \nanalytic terms,1 what problem (or market failure) does the NRC address? From \none perspective, AI innovation is vibrant in the United States, with major advances \noccurring in language, vision, and structured data and applications developing across \nall sectors. Yet from another perspective, current commercialization of past innovation \nmasks systematic underinvestment in basic, noncommercial AI research that could \nensure the long-term health of technological innovation in this country. \nChapter 1: \nThe Theory for a \nNational Research Cloud\nKEY \nTAKEAWAYS\n The federal government \nwill play a central role in \nshaping, coordinating, and \nenabling the development \nof AI.\n AI research and \ndevelopment is \nincreasingly dependent \non access to large-scale \ncompute and data, causing \nmigration of AI talent from \nthe academic to private \nsector and limiting the \nrange of voices able to \ncontribute to AI research.\n Noncommercial and basic \nAI research is critical to the \nlong-term health of the \ninnovation ecosystem. \n An NRC that provides data \nand compute access will \nhelp to promote the long\u0002term national health of the \nAI ecosystem and mitigate \nthe risks of widening \ninequalities in the nation’s \nAI landscape.\nCurrent commercialization of past innovation masks \nsystematic underinvestment in basic, noncommercial \nAI research that could ensure the long-term health of \ntechnological innovation in this country. \nOur case for the NRC is grounded in both efficiency and distributive rationales. \nFirst, the NRC may yield positive externalities, particularly over time, by supporting \ninvestments in basic research that may be commercialized decades later. Second, it \nmay help to level the playing field by broadening researcher access to both compute \nand data, ensuring that AI research is feasible for not just the most elite academic \ninstitutions or large technology firms. Given the scale of economic transformation AI \nis posited to initiate over the next few decades, the stakes are potentially significant. \nWhile the largest private interests like platform technology companies and certain elite \nacademic institutions continue to design, develop, and deploy AI systems that can be \nreadily commercialized, a different story is playing out for the public sector and the vast \nmajority of academic institutions, which lack access to core inputs of AI research. The \nrising costs associated with carrying out research and development are exacerbating the \ndisconnect between current winners and losers in the AI space.\nThis chapter proceeds in three parts. First, we survey the current landscape of \nAI research. Second, we articulate shifting trends in AI research and the academic\u0002industry balance. Third, we spell out the risks of federal inaction and the benefits to an \ninvestment strategy that couples data and compute resources. \nA Blueprint for the National Research Cloud 18\nCHAPTER 1\nTHE AI RESEARCH LANDSCAPE\nThe field of AI research, as we consider it in this White \nPaper, is broadly construed. It includes not only academics \nwho identify themselves as researchers in artificial intelligence \nor machine learning, but also the broader community of \nresearchers who use applied AI in their work, as well as those \nwho examine its impacts on society and the environment. \nMany believe, consistent with the legislation calling for \nthe NAIRR Task Force, that AI will have a dramatic impact \non society. Nine of the world’s 10 current largest companies \nby market capitalization are technology companies that \nplace AI at the core of their business models.2 Recent figures \nfrom the AI Index demonstrate the growing amount of \ninvestment AI companies have drawn. The most recent 2021 \niteration of the Index details how global private investment \nin AI has grown by 40 percent since 2019 to a total of $67.9 \nbillion, with the United States alone accounting for over \n$23.6 billion.3 While multiple private sector predictions \nof the economic impact of AI emphasize the potential for \nAI to drive significant economic growth through a strong \nincrease in labor productivity, others worry about the pace \nof structural change in the labor market and economic \ndislocation for workers automated out of their jobs or \nimpacted by the gig economy.4\nSuch impacts are expected across domains. AI holds \nsubstantial promise to transform healthcare and scientific \nresearch: AI-related progress in the field of protein folding \nis poised to dramatically expedite vaccine development \nand pharmaceutical drug development.5 The integration \nof AI-related systems into agriculture may improve \ncrop yields through targeted use of pesticides and soil \nmonitoring.6\n And national security experts have identified \nAI as a key driver of novel defense capabilities,7 including \ncyberwarfare and intelligence collection. \nMany countries have recognized the significance \nof AI as a driver of progress in economic, scientific, and \nnational security, releasing national plans coordinating \ninvestment for continued progress in AI.8 China’s national \nplan announced billions of dollars in funding aimed at \nmaking the country the global leader in AI by 2030.9 The \nJapanese government partnered with Fujitsu to build \nthe world’s fastest supercomputer (Fugaku).10 Compute \nCanada has similarly provided research computing access \nto academics across the country. The U.K.’s national high\u0002end computing resource, HECToR, was launched in 2007 at \na cost of $118 million and used by nearly 2,500 researchers \nfrom more than 250 separate organizations who produced \nover 800 academic publications.11\nThe U.S. government initially presented a more \ndecentralized approach, providing support for AI \ndevelopment through National Science Foundation \ngrants and defense spending, but refrained from releasing \na unified national plan to coordinate resources across \ngovernment, private industry, and universities.12 The \ncreation of a National AI Initiative Office,13 the updating \nof the National Strategic Computing Initiative,14 and the \nrelease of the National Security Commission on Artificial \nIntelligence’s (NSCAI) final report15 introduced a more \ncomprehensive and coordinated approach. Within the \nUnited States, the closest model to the NRC may be the \nCOVID-19 HPC consortium, which quickly provisioned \ncompute of 50K GPUs and 6.8 million cores for close to \n100 projects across 43 academic, industry, and federal \ngovernment consortium members united by the common \ngoal of combating the COVID-19 pandemic.16\nHistorically, partnerships between government, \nuniversities, and industry have anchored the U.S. \ninnovation ecosystem. The federal government played \ncritical roles in subsidizing basic research, enabling \nuniversities to undertake high-risk research that can take \ndecades to commercialize. This approach catalyzed radar \ntechnology,17 the internet, 18 and GPS devices.19 This history \ninformed the NSCAI’s recommendation for substantial \nnew investments in AI R&D by establishing a national AI \nresearch infrastructure that democratizes access to the \nresources that fuel AI. Many policymakers believe that \nsubstantial investment will be needed over the next \nseveral years to support these efforts, while returns on \nsuch investments could potentially transform America’s \neconomy, society, and national security.20 \nTo be sure, some may challenge the theory of impact. \nFirst, some studies dispute the premise that AI will be \neconomically transformative. Some economists argue that \nA Blueprint for the National Research Cloud 19\nCHAPTER 1\nmany of the optimistic assessments fail to consider how \nconstrained the uptake of AI innovation may be due to AI’s \ninability to change essential yet hard-to-improve tasks.21\nOthers similarly critique the evidence for a fourth industrial \nrevolution.22 Second, some suggest that the provisioning \nof the NRC may strengthen the position of large platform \ntechnology companies (which of course provokes debates \nover antitrust in the technology sector23), as the NRC may \nbe hard to launch without some involvement of hardware \nor cloud providers in the procurement process. Third, some \nwould argue that the NRC would generate large negative \nexternalities in the form of energy footprints. For instance, \none study found that the amount of energy needed to train \nGPT-3, a leading natural language processing (NLP) model, \nrequired the greenhouse emissions equivalent of 552.1 \ntons of carbon dioxide,24 approximately 35 times the yearly \nemissions of an average American.25 Expanding access to \ncompute without appropriate controls may contribute \nto wasteful computing.26 Finally, some critics argue that \nany advances in AI are inherently too risky for further \ninvestment,27 given widely documented risks of bias,28\nunintended consequences,29 and harm.30\nWe are cognizant of these critiques and take them \nseriously. This White Paper proceeds on the operative \npremise animating the NRC legislation: that it will be \nimportant for the country to maintain leadership in AI— \nincluding rigorous interrogation of its uses, limits, and \npromises—and that this requires supporting access to \ncompute and data. Public investment in AI research for \nnoncommercial purposes may help to address some of \nthe issues of social harm we see presently in commercial \ncontexts31, as well as contribute to shifting the broader \nfocus of the field toward technology developed in the \npublic interest by the public sector and civil society, \nincluding academia. The preceding considerations, \nhowever, have shaped our views in key respects, such as \nthe sequential investment strategy, given the uncertainty \nof AI’s potential; the serious consideration of publicly \nowned infrastructure; the provisions for ethical review \nof compute and data access; and, most importantly, \nthe enablement of independent academic inquiry into \nthe potential harms of AI systems. The NRC is not an \nendorsement of blind and naïve AI adoption across the \nboard; it is a mechanism to ensure that a greater range of \nvoices will have access to the basic elements of AI research. \nSHIF TING SOURCES OF \nAI RESEARCH\nWe now articulate how and why AI research has \nmigrated away from basic, long-term research into \ncommercial, short-term applications. \nFirst, many current advances fueled by large-scale \nmodels are costly to train, relative to the size of typical \nacademic budgets. For example, the estimated cost \nof training Alphabet subsidiary DeepMind’s AlphaGo \nZero algorithm, capable of beating the human world \nchampion of the game Go, was more than $25 million.32\nFor reference, the total annual 2019 budget for Carnegie \nMellon University’s Robotics Institute, one of the premier \nacademic research institutions in the nation, was $90 \nmillion.33 A white paper from the Bipartisan Policy Center34\nand the Center for a New American Security noted that the \nFY2020 budget for non-defense AI R&D announced by the \nWhite House was $973 million. In contrast, the combined \nspending on R&D in 2018 by five of the major technology \nplatform companies was $80 billion. In sum, research \nuniversities cannot keep pace with private sector resources \nfor compute. This is not to say that large-scale compute is \nnecessary for all academic AI research, or that academic \nresearch is in competition with industry research, but it \ndoes illustrate why certain sectors of AI research are no \nlonger accessible to the academic researcher. \nThe NRC is not an endorsement \nof blind and naïve AI adoption \nacross the board; it is a mechanism \nto ensure that a greater range of \nvoices will have access to the basic \nelements of AI research. \nA Blueprint for the National Research Cloud 20\nCHAPTER 1\nSecond, the academic-industry divide masks \nsignificant disparities between academic institutions. \nUsing the QS World University Rankings since 2012, \nFortune 500 technology companies and the top 50 \nuniversities have published five times more papers \nannually per AI conference than universities ranked \nbetween 200 and 500.35 Private firms also collaborate \nsix times more with top 50 universities than with those \nranked between 301 and 500.36 This internal compute \ndivide across universities poses significant challenges \nfor who is at the table. \nThird, basic AI research has lost human capital.37\nWhen this is combined with decreased access to \ncompute and data in the academy, the prospect of \nconducting basic research at universities becomes \nless attractive. Top talent in AI now commands private \nsector salaries far in excess of academic salaries.38 The \ndeparture of AI faculty from American universities has \nled to what some analysts have dubbed the AI Brain \nDrain: While AI Ph.D.s in 2011 were roughly as likely to \ngo into industry as academia, two- thirds of AI Ph.D.s \nnow go into industry and less than a quarter go into \nacademia.39 One study suggests that the departure of AI \nfaculty also has a negative effect on startup formation \nby students.40\nFourth, as large-scale AI research migrates to industry, \nthe focus of research inevitably shifts. While academic \nresearchers in AI may lack access to the volume of data \nneeded to train AI models,41 large-platform companies \nhave access to vast datasets, including those about or \ncreated by their customers. This data divide in turn distorts \nAI research toward applications that are focused on private \nprofit, rather than public benefit.42 Put more colorfully by \nJeff Hammerbacher, “The best minds of my generation are \nthinking about how to make people click ads.”43 The NRC \ncan play a key role in unlocking access to public sector \ndata, which may help to reorient the focus of AI research \naway from private sector datasets.44\nThe hollowing out of academic AI capacity can be \nseen in OpenAI’s analysis of the relationship between \ncompute and 15 relatively well-known “breakthroughs” \nin AI between 2012 and 2018.45 Although the analysis was \nmeant to emphasize the role of computing power, it also \nillustrates an emerging gap between private sector and \nacademic contributions over time. Of the 15 developments \nexamined, 11 were achieved by private companies while \nonly four came from academic institutions. Furthermore, \nthis imbalance increases over time: Though private sector \nresearch has continued accelerating since 2012, academic \noutput has stagnated. The last of the major compute\u0002intensive breakthroughs in OpenAI’s analysis stemming \nfrom academia was Oxford’s 2014 release of its VGG image\u0002recognition program; NYU’s work on Convolutional Neural \nNetworks dates back to 2013. From 2015 to 2018, all eight \nbreakthroughs included in OpenAI’s analysis came out of \nprivate companies. Taken together, this leads observers \nto argue that academic researchers are increasingly \nunable to compete at the frontier of AI research.46 While \nacademic researchers have continued to make important \ncontributions in AI, these are increasingly restricted to \nless compute-intensive problems. With fewer compute\u0002intensive academic breakthroughs, AI innovations have \nfocused on private interests (e.g., online advertising) as \nopposed to long-term, noncommercial benefits. To be \nsure, the private sector has, of course, been central to AI \nresearch, but the concern is about the long-term balance \nof the AI innovation ecosystem. \nWhile AI Ph.D.s in 2011 were \nroughly as likely to go into \nindustry as academia, two-thirds \nof AI Ph.D.s now go into \nindustry and less than a \nquarter go into academia.\nA Blueprint for the National Research Cloud 21\nCHAPTER 1\nSCOPING FEDERAL INTERVENTION \nIN DATA AND COMPUTE\nHow can we achieve a more balanced approach \ntoward research and development? We first consider the \nrisks of federal inaction and discuss some of the unique \nadvantages of addressing data and compute together. \nRisks of Federal Inaction\nThe risks of federal inaction are twofold. First, basic \nAI research that has to date paved the way for advances \nin AI and machine learning will slow. According to a recent \nstudy, approximately 82 percent of the algorithms used \ntoday originated from nonprofit groups and universities \nsupported by government spending.47 Even when industry \nresearch is successful, it is typically product-focused \nor incremental, harder to reproduce, and may not be \npublished or open-sourced. An interesting case lies in \nrecent breakthroughs in protein folding. In late 2020, \nthe Alphabet subsidiary DeepMind announced that it \nhad developed a program called AlphaFold, an AI-driven \nsystem capable of accurately predicting the structure of \na vast number of proteins, using only the sequence of \nnucleotides contained in its DNA. Whether out of concern \nfor the privatization or to accelerate adoption of related \nsystems, a consortium of academics, led by scientists at \nthe University of Washington, developed an open source \ncompetitor called RoseTTaFold.48 DeepMind did make \nAlphaFold available to a broad audience, but the concerns \nillustrate the risks of science posed by exclusively private \nAI research, reminiscent of the race to sequence the human \ngenome, where public investment in the Human Genome \nProject preempted concerns about a private firm patenting \nthe human genome.49\nSecond, federal inaction could widen significant \ninequalities in the AI landscape. Without increased \naccess to computing, education, and training, large parts \nof the economy may be unable to adapt—whether in \nfinancial services, healthcare, education, or government. \nDiversifying the range of AI research may also promote \nprogress and productivity. One study suggests that the \ndiversity of AI research trajectories—that is, the specific \nquestions, topics, and problems researchers choose to \ninvestigate—has become more constrained in recent years \nand that private sector AI research is less diverse than \nacademic research.50 Smaller academic groups with lower \nprivate sector collaboration appear to bolster the diversity \nof AI research.51 From the standpoint of underdeveloped \navenues of research, such as ethics and accountability in \nAI, increasing the range of research topics and methods \nin the field raises the likelihood of finding breakthroughs \nthat make additional progress in the long term possible.52\nRecent evidence suggests that between 2005 and 2017, \njust five metro areas in the U.S. accounted for 90 percent \nof the growth in innovation sector jobs.53 According to \nStanford economist Erik Brynjolfsson, the likely impact \nof geographic concentration is “there are a whole lot \nof people—hundreds of millions in the U.S. and billions \nworldwide—who could be innovating and who are not \nbecause they do not have access to basic computer \nscience skills, or infrastructure, or capital, or even culture \nand incentives to do so.”54 AI technologies can be hard to \ndiagnose and interpret and be prone to substantial bias.55\nBroadening the set of voices that can interrogate such \nsystems will be critical to an inclusive and equitable future. \nIn sum, federal investment in public AI infrastructure \nmay promote a more equitable distribution of \nparticipation in and gains to AI innovation broadly, bolster \nU.S. competitiveness, and support fundamental research \ninto noncommercial and public sector applications.\nA Blueprint for the National Research Cloud 22\nCHAPTER 2\nThis chapter discusses eligibility, resource allocation, and computing \ninfrastructure for the NRC: Who should get access to what and how? \nFirst, when determining who should get access, it is critical to bear in mind \nthe broad goals of the NRC. As discussed in Chapter 1, there is a large resource gap \nin academia as compared to private industry. In the interest of supporting basic \nresearch and democratizing the field, this section will focus on identifying a target \ngroup for eligibility. As we articulate below, we refrain from considering expansion \nto a broader set of commercial, nonacademic parties because of the NRC’s focus on \nlong-term, fundamental scientific research. One of the narrowest approaches would \nbe a specialty faculty model that would target researchers engaged in core AI work. \nBut, the difficulties with defining AI and the rapidly expanding domains in which AI is \nbeing applied make this model too constrained to realize the full impact of the NRC. \nInstead we recommend tracking the most common criterion for federal research \nfunding and advocate that eligibility hinge on “Principal Investigator” (PI) status at \nU.S. universities.1\n One of the tradeoffs is that PIs may be less diverse than a broader \nsegment of researchers,2 so a longer-term expansion could consider moving beyond \nthis group. While the NRC aims to train the next generation of AI researchers, we \ncaution that an immediate expansion to all graduate and undergraduate students \nwould pose considerable challenges in scaling. Therefore, we recommend that \nstudents primarily gain access by participation in faculty-sponsored AI research, \ninstead of blanket student access, and that they gain training through the creation of \neducational programs. \nSecond, we discuss three models for allocating computing credit: development \nof a new grant process, delegating block compute grants to universities for internal \nallocation among faculty, or universal access. Each of these models trades off the \nease of administration against tailoring for specific NRC goals. We recommend an \napproach used by other national research clouds—namely a hybrid approach of \nuniversal default access for the majority of researchers, with a grant process for \nexcess computing beyond the default allocation. Such an approach would keep \nadministrative costs low for the vast majority of researchers, while enabling tailoring \nthrough a competitive grant process for the highest-need users. \nChapter 2: \nEligibility, Allocation, \nand Infrastructure \nfor Computing\nKEY \nTAKEAWAYS\n Researcher eligibility \nfor NRC access should \nbegin with “Principal \nInvestigator” status at U.S. \nuniversities.\n The NRC should adopt \na hybrid approach of \nuniversal default access for \nthe majority of researchers \nand a grant process when \nrequests for compute or \ndata exceed base levels.\n The NRC should adopt a \ndual investment strategy \nby developing programs \nfor expanding access to \nexisting cloud services \nand piloting the ability to \nprovide publicly owned \nresources. \nA Blueprint for the National Research Cloud 23\nCHAPTER 2\nThird, we consider the “make-or-buy” decision for \nthe NRC. One option would be for the NRC to provide \nresearch grants for the use of commercial cloud services \nthat many researchers already rely on (the “buy” decision). \nAlternatively, the NRC could create and provision access \nto a publicly high-performance computing cluster (the \n“make” decision). It is well-established that, based \nsolely on hardware costs, it is more cost-effective to \nown infrastructure when computing demand is close to \ncontinuous. On the other hand, existing commercial cloud \nproviders have developed highly usable software stacks \nthat AI researchers have widely adopted. Commercial \ncloud providers offer a way to quickly expand capacity. \nWe hence recommend a dual investment strategy to (a) \nquickly launch the NRC by subsidizing and negotiating \ncloud computing for AI researchers with existing vendors, \nexpanding on existing initiatives like the National Science \nFoundation’s CloudBank project; and (b) invest in a pilot \nfor public infrastructure to assess the ability to provide \nsimilar resources in the long run. Such publicly owned \ninfrastructure would likely be built under contract or grant, \nbut could be operated much like national laboratories \nthat own sophisticated supercomputing facilities, as is the \ncase with other national research resources (e.g., Compute \nCanada, Japan’s Fugaku).\nOur recommendations are informed by a series of \ncase studies that are presented throughout this chapter, \nas well as through the remainder of the White Paper. \nTable 1 summarizes how existing models compare on the \nthree key design decisions. At the outset, we note that few \nexisting initiatives have attempted to provide compute \npower at the scale of the NRC. At the same time, we view \nthe NRC as complementary to more traditional areas of \nscientific computing.3\nELIGIBILITY ALLOCATION OWNERSHIP\nExisting \nProgram\nPI \nOnly\nAny \nFaculty Students\nExisting \nGrant \nProcess\nUniversity \nAllocation\nNew \nProcess\nDefault \nAccess \nw/Tiers\nPrivate Public\nCloudBank X X X X\nStanford \nHAI-AWS \nCloud Program\nX X X\nStanford \nSherlock Cluster X X X\nGoogle Colab X X X X\nCompute \nCanada X X X\nFugaku X X X\nXSEDE X X X X\nDOE INCITE X X X\nTable 1: Key design differences between computing case studies. “Other faculty” indicates an eligibility set for faculty other than PI status \n(e.g., requiring Stanford affiliation for the Sherlock cluster) and “new process” is used to indicate the creation of any process other than those \ncurrently listed (e.g., Fugaku is currently soliciting proposals with research facilities). \nA Blueprint for the National Research Cloud 24\nCHAPTER 2\nEligibility\nThe first task is identifying which researchers should \nbe eligible for the NRC. Chapter 1 discussed the need \nto support AI innovation in universities. Therefore, this \nsection will scope eligibility within academia by analyzing \nthe access-resource trade-offs in alignment with the NRC \ngoals.\nAt the outset, we note that we do not analyze eligibility \nin depth beyond academic researchers. The legislation \nconstituting the NRC task force specifically contemplates \n“access to computing resources for researchers across the \ncountry.”4 The NRC is defined as “a system that provides \nresearchers and students across scientific fields and \ndisciplines with access to compute resources.”5 The most \nnatural interpretation of this language suggests a core \nfocus on scientific and academic research.6\nIntroducing commercial access to the NRC, particularly \nfor under-resourced firms such as small businesses \nand startups, may very well benefit the U.S. innovation \necosystem. But the challenges of incorporating commercial \naccess to the NRC are enormously complex. First, including \nsoftware developers at startup companies as “researchers” \nwithin the meaning of the NDAA would raise a wide \nrange of boundary questions that the NRC may be poorly \nequipped to adjudicate. According to the Small Business \nAdministration (SBA), there are over 31 million small \nbusinesses in the United States.7\n Over 627,000 businesses \nopen each year.8 Should all such businesses be eligible to \ncompute on the NRC? How would one avoid gaming (e.g., \nstrategic subsidiaries/spinoffs) eligibility? And, how would \nthis advance the scientific mission of the NRC? Second, \nwhile potentially valuable, it is less clear how the inclusion \nof startups and small businesses meets the theory of impact \nof the NRC. As currently construed, the concern animating \nthe NRC lies in the importance of long-term, noncommercial \nfundamental research that can ensure AI leadership for \ndecades to come. Commercialization is not the element \nof the AI innovation ecosystem that faces the structural \nchallenges articulated in Chapter 1. Finally, scaling the NRC \nto allow meaningful commercial access would pose serious \npractical challenges. Because the Task Force must also \nconsider the feasibility of the NRC, we have not considered \nin depth a conception that would extend the term \n“researcher” to encompass large portions of the commercial \nprivate sector. Expansion to non-academic, nonprofit \norganizations may be a more reasonable consideration, \nas the objective of some entities (e.g., not-for-profit \ninvestigative journalism, civil society organizations) may be \ncloser to the core of the NRC’s mission of empowering long\u0002term beneficial research that cannot currently occur.9\n In the \nlong term, the NRC should consider the trade-offs to such an \nexpansion. \nEven if the NRC adopts a broader computing model \ndown the road, we believe that focusing on academic \nresearchers is an important starting point as it illuminates \nsome of the main operational considerations for NRC \naccess. \nSPECIALT Y FACULT Y MODEL \nOne of the narrowest approaches to NRC eligibility \nwould be to restrict it to faculty engaged in AI research. \nUnder this approach, policymakers would direct \ncomputing resources exclusively toward faculty working \non identifiable AI projects, which often need large amounts \nof compute power. A benefit of this approach is that \nresearchers’ familiarity with the infrastructure would likely \nmean that fewer funds would be devoted to cloud service \ntraining for novice users. \nYet the set of self-identified core AI faculty are few \nand concentrated in a small number of universities, \nwhich are already more likely to gain access to large-scale \ncomputing. Limiting access to core AI faculty would hence \nundermine the mission of democratizing AI research. In \naddition, the application of AI is expanding rapidly across \ndomains. Interdisciplinary research deploying AI in new \ndomains will be vital for maintaining American leadership \nin AI, as well as for animating basic research questions. \nRestricting eligibility to core AI faculty (however defined) \ncould jeopardize the ability of researchers from all \nacademic disciplines (e.g., in the physical sciences, social \nsciences, and humanities) to contribute to realizing AI’s full \npotential.\nA Blueprint for the National Research Cloud 25\nCHAPTER 2\nGENERAL FACULTY MODEL\nA more natural starting point for NRC eligibility is with \nPrincipal Investigators (PIs) at U.S. colleges and universities, \nthe most commonly deployed criterion for federal grants. \nRequirements for PI status are set by individual universities \nand include a broad range of researchers certified by their \nuniversity as qualified to lead large research projects.10\nWhile PI certification may vary from institution to institution, \nan important baseline criterion of PI status is that the \nresearcher is subject to their institution’s training and \ncertification processes, which in turn clarify a researcher’s \nresponsibilities regarding the management and execution \nof their research proposals. Existing programs for allocating \ncomputing power typically set eligibility based on PI \nstatus as it ensures the researcher has the infrastructure \nto carry out a large-scale research project. CloudBank, an \nNSF program that distributes funds for commercial cloud \ncomputing resources, awards grants to PIs, who may \ndistribute funds to other researchers and students on the \nproject.11 Compute Canada allows all faculty granted PI \nstatus by their university to automatically receive a preset \namount of computing credits and apply for further credit \nas needed. The PI may then sponsor others to access the \ncredit.12\nWe recognize that PI status does not include all \nuniversity-affiliated researchers. In 2013, of the over \n200,000 self-identified academic researchers, just under \n60,000 were employed in a role other than full-time \nfaculty, a position that may not be eligible for PI status.13\nFrom 1973 to 2013, the percentage of full-time faculty \namong engineering doctorate holders decreased by 2 \npercent, while the percentage of “other” academic jobs \n(including research associates) increased by 12 percent.14\nBut, the reliance on PI status would not prevent PIs from \nallocating access to non-PI status researchers on a project, \nand administrative ability weighs strongly in favor of \nconsistency with current grant eligibility criteria. \nSTUDENTS \nShould graduate and undergraduate students be \nable to access the NRC? One of the principal challenges \nhere lies in scale and administrability. One estimate \nis that there are nearly 20 million college students \nin the U.S.15 Second, PI-oriented eligibility does not \npreclude university students from accessing resources \nto undertake AI research under the direction of PIs. The \nCompute Canada model, for example, restricts eligibility \nto faculty, but allows faculty to sponsor collaborators, \nincluding any student researcher. An access model for the \nNRC that allows PIs to sponsor students provides further \nresearch and training opportunities for students. Third, a \nnumber of existing cloud services already provide limited \naccess to computing credits for educational purposes. \nGoogle Colaboratory, for instance, provides free, but not \nreliably guaranteed, access to cloud services.16 Amazon \nWeb Services provides up to $35 of AWS credits for free \nto all university faculty and students. Despite existing \nresources, students may need more resources. The \nGoogle subsidiary and online community Kaggle, for \nexample, provides 30 hours of GPU access per week for \nfree and found that 15 percent of users exceeded the \nlimit.17\nWhile the exact scope of student computing power \nneeds is unclear, we recommend funding an educational \nresource once researcher needs and resource limitations \nhave been gauged. Currently, the NSF’s CloudBank is \npiloting a Community & Education Resource to earmark \na small set of credits for educational purposes.18 This \nresource allows a university professor to request a small \nnumber of credits for student coursework or small-scale \nresearch. \nRegardless of which eligibility model the NRC adopts, \nthere will also be a significant need for support staff, \ntraining documentation, and educational materials so \nresearchers can effectively make use of the compute \nand data resources (see Appendix D). The reason some \nstudents and researchers may not take advantage of all \navailable cloud credits could, for instance, stem from \nthe difficulty in using cloud platforms. If the NRC serves \nacademics from a range of disciplines, this question of \nhuman capital will be especially relevant to serve different \nmodels of research. A robust training program for users of \nthe NRC will ensure ease of use and encourage appropriate \nutilization of the cloud.\nA Blueprint for the National Research Cloud 26\nCHAPTER 2\nResource Allocation \nModels\nWe now consider three resource allocation models: (1) a \nnew grant process; (2) block grant allocation to universities; \nand (3) universal—but potentially tiered—access. \nNRC GRANT PROCESS\nEstablishing a new grant process for compute access \nwould have one main advantage. The program could \nbe built specifically for the purpose of AI research, with \nreviewers who are familiar with AI concepts, practices, and \ntrends. Such a process might therefore enable improved \nallocation decisions and provide the NRC with greater \ncontrol over its investments. \nThat said, establishing a peer-review process for \nall applications would be resource intensive, requiring \nthe establishment of a grant administration program \nakin to those at the National Science Foundation (NSF) \nor the National Institutes of Health (NIH). For instance, \nto implement peer review required for the merit review \nprocess, the NSF annually needs a community large \nenough to conduct nearly 240,000 reviews per year.19 Since \nthe contemplated reach is broad, we are mindful of adding \na significant service burden for faculty conversant in AI \nfor every application for compute access. Peer review for \ncompute access would require significant overhead and \ndelays in compute allocation. \nUNIVERSITY ACCESS\nTo reduce administrative costs, an alternative scheme \nwould be to allocate credits to universities based on the \nnumber of eligible researchers. The NRC could allocate \nresources to universities as block grants, and in turn, rely \non the university to distribute computing access. (For \nexample, the NRC could purchase significant amounts of \ncompute from cloud providers, create virtual credits that \nare convertible into appropriate cloud resources, and \ndelegate allocation to universities.) This approach would \nhave the advantage of tapping into the universities’ local \nexpertise for reviewing and distributing resources. It would, \nhowever, lead to a highly decentralized process, providing \nlittle oversight to understand the distribution of usage, and \ngive the NRC little control over resource allocation. While \nwe do not recommend this route as the principal allocation \nscheme, we do believe that some allocation to university\u0002based IT support teams may be warranted to support \nresearchers in using the NRC. XSEDE’s “Campus Champions” \nprogram, for instance, provides university employees access \nto the system to support the computational transition.20\nUNIVERSAL ACCESS\nThe last potential model would provision universal \naccess to base-level compute to all eligible PIs. The closest \nmodel is Compute Canada’s national research cloud, \nwhich provides base-level compute access to all faculty \nin Canada. This would significantly reduce administrative \noverhead, both for an institution running the review \nprocess, and academics seeking NRC access. The primary \ndownside is that base-level compute may be insufficient \nfor specialized needs.\nWe recommend combining a universal baseline model \nwith a grant process for compute needs beyond base-level \naccess. The reduced complexity in administering a universal \nbaseline access compute model makes it an attractive \noption for the NRC in allocating compute resources, \nespecially with respect to the NRC’s goal of opening access \nto compute resources.21 XSEDE, for instance, uses a similar \nmodel of streamlined “Startup Allocations” (issued for \none-year terms, typically within two weeks of application) \nand “Research Allocations” for more significant compute \nrequests. Compute Canada provides access to 15 percent \nof PIs to increased compute capacity based on a merit \ncompetition. A critical question will, of course, be the level \nof baseline computing that will determine overall costs, \nphysical space requirements, and the like. To benchmark \nthis, we recommend an in-depth study of the anticipated \ncomputing needs, based on existing academic computing \ncenters.22\nThe grant process for additional compute could \ntake multiple forms; for example, while one could allow \nindividual PIs to apply directly to the NRC for excess \nA Blueprint for the National Research Cloud 27\nCHAPTER 2\ncompute, the NRC could also allocate “blocks” of resources \nat the university level and allow universities to oversee \ntheir administration. In any case, due to the size of such \nrequests, grant reviews should be conducted on a merit \nbasis and administered by a combination of NRC staff and \nan external advisory board of university faculty. In 2021, \nCompute Canada, for instance, completed its review of 650 \nresearch submissions in about five months, with only 80 \nvolunteer reviewers from Canadian academic institutions \nto assess the scientific merit of the proposal.23 In order \nto avoid conflicts of interest, we strongly recommend \nagainst the participation of any faculty or private sector \nadvisers who have conflicts of interest with any vendors \nthat provide services to the NRC. Ideally, proposal review \nshould be independent, blinded, and based on scientific \nmerit to the extent possible.\nKEY TAKEAWAYS\n Built into existing \ngrant process: \nResearchers eligible \nfor certain existing \nNSF grants can \nsimply request \naccess to CloudBank \nthrough the same \ngrant application.\n Single point of entry \nfor compute access:\nThe CloudBank \nportal provides a \nsingle point of entry \nfor researchers to \naccess funds to \nuse on whichever \ncommercial cloud \nprovider they prefer.\n Cost reduction: No \noverhead costs are \nassociated with using \nCloudBank.\n Student access:\nLimited funds are set \naside for grants to \nstudents and classes.\nCASE STUDY: CLOUDBANK\nIn 2018, the National Science Foundation’s (NSF) Directorate for Computer and \nInformation Science and Engineering (CISE) created the Cloud Access Solicitation to \nprovide funding for AI-related research endeavors.Initially created to meet the needs \nof the NSF funding recipients to access public clouds, CloudBank is an interesting \ncase study for exploring resource allocation models. Accessible through a portal, \nCloudBank aids researchers in using cloud resources fully by facilitating the process \nof “managing costs, translating and upgrading computing environments to the cloud, \nand learning about cloud-based technologies.”24\nCloudBank is a collaboration project established via an NSF Cooperative \nAgreement with the San Diego Supercomputer Center (SDSC) and the Information \nTechnology Services Division at UC San Diego, the University of Washington \neScience Institute, and UC Berkeley’s Division of Data Science and Information.25\nEach of these institutions handles an area, according to its comparative advantage.26\nFor example, SDSC is responsible for building the online portal, and UC San Diego is \nin charge of managing the accounts of the users.27\nCloudBank also aims to reduce the cost of cloud computing: It uses both \nthe ongoing discounts with cloud providers from the University of California and \nthe discounts that come with bulk cloud purchase from the cloud procurement \nconsulting firm Strategic Blue, which regularly partners with the likes of AWS, \nMicrosoft, and Google.28 Furthermore, there is no overhead cost associated with \nthe cloud allocations through CloudBank, since the terms of the NSF cooperative \nagreement prohibit indirect costs.29 With these cost-saving mechanisms, researchers \ncan afford more computing capacities from a variety of major cloud vendors. \nBy requesting the use of CloudBank during their application to the selected \nNSF projects,30 researchers can gain access not only to various advanced hardware \nresources, but also to a variety of services to make the process more supported and \nmonitored.31 CloudBank also gives research community members access to its \neducation and training information.32 \nA Blueprint for the National Research Cloud 28\nCHAPTER 2\nComputing \nInfrastructure \nCloud computing environments connect local \ncomputing devices such as desktop computers to large, \ntypically geographically distributed servers containing \nphysical hardware. This hardware, in turn, is responsible for \nstoring data and performing computation over computer \nnetworks—all of which is mediated through a collection \nof software services. This model centralizes the usual \noperational management for those using the network and \nprovides adjustable units of computation and data storage \nto allow for fluctuations in demand. Users interact with the \ncloud by launching virtual connections to the server—cloud \ninstances—and running containerized processes remotely. \nThese operations are managed by the cloud and available \nfor monitoring through dashboards. Cloud computing \nmay be serviced through on-premises clusters, via external \nvendors, or some combination thereof, and accessed over \nnetworks with varying security and connectivity, from \ninternet-accessible to air-gapped regions.\nThe infrastructure to the NRC could be developed \nwith two general approaches: (1) the NRC could use \ncommercial cloud platforms as its infrastructure backbone; \nor (2) the federal government could engage a contractor \nto build a high-performance computing (HPC) public \nfacility specifically for the NRC. This section addresses \nsome advantages and disadvantages of both. (We provide \nan estimated cost comparison of these two approaches \nin Appendix A.) The two approaches discussed here are \nnot mutually exclusive, and we ultimately recommend \na hybrid investment strategy. In the short run, the NRC \nshould scale up cloud credit programs (similar to NSF’s \nCloudBank program) to provide both streamlined base\u0002level access and merit review for applications going beyond \nbase-level access. In the long run, the NRC should invest \nin a pilot to develop public computing infrastructure. Even \nwith public infrastructure, it will be critical to meet “burst \ndemand” (to expand resources when compute demand \npeaks). The success of the initial investments should guide \nthe prospective model as to whether to rely on publicly \nor privately owned infrastructure in the longer term. We \nnote that in order to scale successfully to either resource \nwill require building institutional capacity at academic \ninstitutions. \nCOMMERCIAL CLOUD\nThe greatest advantage of using commercial cloud \nservices for the NRC is that significant infrastructure \nalready exists.33 Under this model, the NRC would simply \nsubsidize credits for using commercial cloud services \n(similar to NSF’s CloudBank program). Thus, rather \nthan spending years building new computing resources, \npolicymakers could launch the NRC soon after they \ndetermine the program’s administrative details. (We note, \nhowever, that there may still be significant GPU shortages \nin the short run; with the contemplated scale of the NRC, \nsignificant infrastructure would need to be built.) Since \nmany researchers already use commercial cloud services \nfor their AI research, the transition into the NRC program \ncould be relatively seamless. Furthermore, commercial \ncloud platforms offer the NRC greater flexibility to change \nthe size and scope of the program. Commercial cloud \nplatforms charge for the amount of compute actually \nused.34 Thus, the size of the NRC could expand or retract \nin line with shifting demand. In contrast, a dedicated HPC \nsystem would have a set amount of hardware that costs \nthe same, no matter how effectively it’s being used.\nWorking directly with commercial cloud providers also \noffers several advantages for the NRC. The commercial \ncloud services market is highly competitive and features \nnumerous providers capable of meeting the NRC’s needs. \nThe NRC would have the option of using one provider or \nmultiple providers. If opting to use just one provider, the \ngovernment’s bargaining power may be at its strongest \nin helping to drive down prices for the NRC. Alternatively, \nusing multiple providers gives the NRC greater flexibility in \navailable services and hardware. Either way, policymakers \nwould have the opportunity to negotiate contracts and \nprices with commercial cloud providers every few years, \nwhich will be critical to cost containment.35 The NRC would \nalso not be locked into using the same provider or set of \nproviders for the duration of the program. Rather, NRC \nstaff could reevaluate which commercial cloud provider’s \ninfrastructure would best meet the NRC’s needs at the \nstart of each new contract.\nA Blueprint for the National Research Cloud 29\nCHAPTER 2\nKEY TAKEAWAYS\n Federally-funded \ninfrastructure: \nXSEDE is an NSF\u0002funded initiative \nthat integrates and \ncoordinates shared \nsupercomputing and \ndata analysis resources \nwith researchers.\n Tiered access to \ncompute: For baseline \naccess to compute, \nXSEDE leverages a \nfast, low-hurdle review \nprocess. For access \nbeyond the baseline, \nXSEDE has its own \nresource allocations \ncommittee that \nreviews applications \nevery quarter.\n “Campus Champions \nProgram:” XSEDE \npartners with \nemployees and \naffiliates at colleges, \nuniversities, and \nresearch institutions \nto help researchers \nget access to compute \nresources.\n Collaboration: XSEDE \ncollaborates with \nthe private sector in \nacquiring, operating, \nand managing \ncompute resources.\nCASE STUDY: XSEDE\nThe Extreme Science and Engineering Discovery \nEnvironment (XSEDE) is an NSF-funded organization that \nintegrates and coordinates the sharing of advanced digital \nservices such as supercomputers and high-end visualization \nand data analysis resources.36 XSEDE is a collaborative \npartnership of 19 institutions, or “Service Providers,” many \nof which are nonprofits or supercomputing centers at \nuniversities and provide computing facilities for XSEDE \nresearchers.37 XSEDE supports work from a wide variety \nof fields, including the physical sciences, life sciences, \nengineering, social sciences, the humanities, and the \narts.38 XSEDE allocations are available to any researcher \nor educator at a U.S. academic, nonprofit research, or \neducational institution, not including students.39 However, \nresearchers can share their allocations by establishing user \naccounts with other collaborators, including students.40\nResearchers have two different paths to requesting \nallocations: Startup Allocation and Research Allocation. \nStartup Allocations apportion XSEDE resources for small\u0002scale computational activities.41 Startup Allocations are one \nof the fastest ways to gain access to and start using XSEDE \nresources, as requests are typically reviewed and awarded \nwithin two weeks.42 Startup Allocation requests also \nrequire minimal documentation: the project’s abstract and \nthe researchers’ curriculum vitae (CV).43 Startup Allocations \ntypically last for one year, but requests supported by merit\u0002reviewed grants can ask for allocations that last up to three \nyears. Researchers can also submit renewal requests if their \nwork needs ongoing low-level resources.44\nFor research needs that go beyond the computational \nlimits under a Startup Allocation, researchers must \nsubmit a Research Allocation request.45 XSEDE strongly \nencourages its users to request a Startup Allocation prior \nto requesting a Research Allocation, in order to obtain \nbenchmark results and more accurately document their \nresearch needs in the Research Allocation.46 Research \nAllocation requests must include a host of documents, \nsuch as a resource-use plan, a progress report, code \nperformance calculations, CVs, and references.47\nRequests are accepted and reviewed quarterly by the \nXSEDE Resource Allocations Committee (XRAC), which \nassesses the proposals’ appropriateness of methodology, \nappropriateness of research plan, efficient use of \nresources, and intellectual merit.48\nXSEDE abides by a “one-project rule,” whereby each \nresearcher only has one XSEDE allocation for their research \nactivities.49 For instance, if a researcher has several grants \nthat require computational support, those lines of work \nshould be combined into a \nsingle allocation request. This \nminimizes the effort required \nby the researcher to submit \nrequests and reduces the \noverhead in reviewing those \nrequests.\nXSEDE also uses \na “Campus Champion \nProgram” to streamline \naccess to resources.50 The \nCampus Champion Program \nis a group of over 700 \nCampus Champions who \nare employees or affiliates \nat over 300 U.S. colleges, \nuniversities, and research\u0002focused institutions.51\nThese Campus Champions \nfacilitate and support use of \nXSEDE-allocated resources \nby researchers, educators, \nand students on their \ncampuses. For instance, the \nCampus Champions host \nawareness sessions and \ntraining workshops for their \ninstitutions’ researchers while \nalso capturing information \non problems and challenges \nthat need to be addressed by \nXSEDE resource owners.52\nFinally, XSEDE \nwelcomes collaboration \nopportunities with other \nmembers of the research \nand scientific community.53\nFor example, XSEDE assists \nother organizations in \nacquiring and operating \ncomputing resources and \nhelps to allocate and manage \naccess to those resources. \nRecently, XSEDE worked \nwith academics and private \nindustry to form the COVID-19 \nHigh Performance Computing Consortium, which \nprovides researchers with powerful computing resources \nto better understand COVID-19 and develop treatments \nto address infections.54 \nA Blueprint for the National Research Cloud 30\nCHAPTER 2\nCommercial cloud platforms also provide other \nadvantages to the NRC. The labor of managing, \nmaintaining, and upgrading the hardware behind the \nNRC would be handled by private parties that already \nhave expertise in running cloud services at scale and \nhave invested billions of dollars into doing it. This \narrangement allows researchers access to a greater \nvariety of hardware that is constantly being expanded \nand upgraded.55 With a strong economic incentive to keep \nimproving cloud offerings, commercial cloud services \noffer an assortment of instance types—i.e., the various \npermutations and combinations of GPU/CPU, memory, \nstorage, and networking specifications that constitute a \ncompute instance—with different hardware at a range of \nprice points. Thus, researchers would have the flexibility \nto choose both what hardware would best fit the needs of \ntheir projects and how best to allocate their limited cloud \ncredits. Researchers could also have access to cutting-edge \ntechnology specially designed for AI research, such as \nchips optimized for training and inference, developed and \nexclusively used by commercial cloud providers.\nUsing commercial cloud services for the NRC comes \nwith significant tradeoffs, however. While the initial costs \nof subsidizing cloud credits might be less than building \npublic infrastructure, many studies show that relying on \ncommercial cloud services would likely be much more \nexpensive in the long term.56 For example, a study of \nPurdue University’s Community Cluster Program shows \nthat the amortized cost of its on-premises cluster over five \nyears is 2.73 times cheaper than using AWS, 3.24 times \ncheaper than using Azure, and 5.54 times cheaper than \nusing Google Cloud.57 A similar study at Indiana University \nestimates that the total investment into its locally owned \nsupercomputer, Big Red II, is about $10.1 million, while the \ntotal cost of a three-year reservation on AWS about $24.9 \nmillion.58 Cost comparisons in other studies are even more \ndramatic. For instance, a study of the Advanced Research \nComputing clusters at Virginia Tech shows that the five\u0002year cost for its on-premises cloud is about $15.5 million, \nwhile the five-year cost for reserved AWS instances using \nthe same workloads would be about $136.3 million.59\nWhat explains these cost disparities? Estimates \ncomparing commercial cloud services to a dedicated HPC \ncluster show that commercial cloud services are more \nexpensive per compute cycle.60 At least in part, this is due \nto the fact that commercial services are optimized for \ncommercial applications. Compute Canada, for example, \nfound that building their own infrastructure was cheaper \nthan using commercial services, because they did not \nhave the same core use needs as commercial customers, a \ntradeoff that gained their system more computing power \nat the expense of availability.61 Although the analysis was \npublished in 2016, Compute Canada’s own benchmarking \nof costs concluded:\nCurrently, it is far more cost effective for the \nCompute Canada federation to procure and \noperate in-house cyberinfrastructure than to \noutsource to commercial cloud providers. . . . \nCloud-based costs ranged from 4x to 10x more \nthan the cost of owning and operating our own \nclusters. Some components were dramatically \nmore expensive, notably persistent storage which \nwas 40x the cost of Compute Canada’s storage.62\nUltimately, the cost difference between commercial \ncloud services and HPC systems depends on how often \nand how efficiently the HPC system is used. We provide a \ncost calculation that updates Compute Canada’s below, \narriving at cost differentials of comparable magnitude. \nCommercial cloud instances with comparable hardware \nunder constant usage, even with substantial discounts, \nWhile the initial costs of subsidizing \ncloud credits might be less than \nbuilding public infrastructure, \nmany studies show that relying on \ncommercial cloud services would \nlikely be much more expensive in the \nlong term.\nA Blueprint for the National Research Cloud 31\nCHAPTER 2\nwould be significantly more expensive over time for \nthe NRC than a dedicated HPC system. Bringing the \ncost of commercial cloud services under that of an HPC \nsystem would require policymakers to either negotiate \nexceptionally high discounts with commercial cloud \nproviders or make major sacrifices in hardware speed or \noverall scale of the NRC. A similar cost calculation is also \nwhat led Stanford University to simultaneously invest in \nboth on-premises hardware and a commercial cloud-based \nsolution for its Population Health Sciences initiative (see \nbox case study in Chapter 3). The most common practice \nacross NSF centers, such as the XSEDE initiative (see box \ncase study below), is also to build infrastructure instead \nof relying on commercial cloud credits, due to these cost \nconsiderations. \nFinally, relying on the commercial cloud may raise \nquestions about industry consolidation. There are two \nmain answers to this question. One is that building a \ndedicated, publicly owned HPC clusters would require \npurchasing sophisticated hardware from existing industry \nplayers, which also exist in concentrated industries. In \nother words, it is difficult to imagine no involvement \nof private industry under either option. Another major \nconstraint lies in time: A fully mature, public infrastructure \nNRC could not be stood up overnight. Moreover, a publicly \nowned cloud would still likely require a major technology \ncompany to build the infrastructure under contract, as is \nthe case for National Labs, or using a grant, as is the case \nfor XSEDE.\nPUBLIC INFRASTRUCTURE\nBuilding a new HPC cluster would be a bespoke \nsolution, tailored to fit the NRC’s specific compute needs. \nThis approach would be relatively well-explored territory \nfor the federal government.63 The U.S. Department of \nEnergy (DOE) and the U.S. Department of Defense (DOD) \nalready regularly contract with a handful of companies \nto build HPC clusters every few years.64 The DOE itself \nalready uses two of the three fastest HPC clusters in the \nworld and recently funded the development of two new \nsupercomputers that, when completed, will be the world’s \nfastest by a significant margin.65 The National Science \nFoundation commonly issues grants for the construction \nof high-performance computing infrastructure.66 Given this \nfamiliarity, policymakers would have reasonable estimates \nfor how much a new HPC cluster for the NRC would cost \nand would already have relationships with the companies \nthat would submit bids for the contract.\nThe hardware cost for such compute scale \nare, of course, substantial.67 For example, the IBM \nsupercomputer used at Oak Ridge National Laboratory \n(ORNL)—known as “Summit”—cost $200 million.68 At \nthe time of its completion in 2018, Summit was the \nfastest supercomputer in the world and, as of 2020, \nis still the second fastest.69 Frontier, the new Cray \nsupercomputer being built at ORNL in 2021, cost $500 \nmillion. When completed, it is anticipated to be the fastest \nsupercomputer in the world at “up to 50 times” faster \nthan Summit.70 Nonetheless, these large up-front costs \ncould come with the benefit of computing infrastructure \nspecifically designed for AI research and the NRC’s needs. \nSuch a system would be more efficient in cost per cycle \nover the long term than subsidizing commercial cloud \nservices. The NRC could also expand and upgrade multiple \nclusters over time to meet the changing needs and scope \nof the program.\nIn addition, a dedicated cluster for the NRC has the \nadvantage of giving the federal government greater control \nover computational resources (e.g., reducing uncertainty \nover the products and platforms, such as the sudden \ndeprecation of required APIs). This level of control over \nthe hardware also allows policymakers greater flexibility \nwith NRC operations. Taking the public infrastructure \napproach (i.e., “making” not “buying”) comes with several \nsignificant trade-offs to weigh against the policy goals \nof the NRC. First, building a new HPC cluster would take \nabout two years, in addition to the time it takes to solicit \nand evaluate proposals from potential contractors.71 If \nthe NRC hopes to quickly stimulate and help democratize \nAI research in the U.S., such a timeline for the program \nwould not be ideal, given how quickly AI discoveries \nadvance. Of course, contracting with cloud vendors or \nissuing grants for the construction of supercomputers \nwould also require a process. Yet, building a cluster could \nraise more challenging contracting issues, such as budget \noverruns and project delays.72 Contractors’ experience with \nbuilding this type of hardware may help mitigate some \nof these concerns, as well as their self-interest in being \nA Blueprint for the National Research Cloud 32\nCHAPTER 2\nconsidered for future government contracts. But the risks \nare nonetheless still present. \nSecond, the usability and the feature set of the software \nstack for public infrastructure is by no means proven. One \nof the most common hurdles to researcher adoption of \ncloud computing lies in the usability of systems,73 and \npublic infrastructure has less of a track record of easing that \nonboarding path at the contemplated scale. This is why we \nrecommend a pilot to assess whether a national HPC center \ncan be administered in a way to ensure the ease of cloud \ntransition and software stack that researchers have become \naccustomed with private providers. \nThird, policymakers would also need to account for \ncosts of maintaining and administering the system.74\nThey would need to find facilities to house and manage \nthe hardware and to account for the high energy costs \nof running an HPC cluster, as well as disaster prevention \nand recovery cost.75 These costs are significant. In 2021, \nthe Oak Ridge Leadership Computing Facility requested \n$225 million to operate all of its systems.76 The Argonne \nLeadership Computing Facility, in turn, requested $155 \nmillion.77 Furthermore, the lifecycle of DOE HPC systems \nhas traditionally been about seven years, after which new \nsystems are built and old ones decommissioned.78 While \nit is uncertain what the lifespan of newer systems will be, \nthis seven-year figure would lead us to argue that the NRC \nshould expect to either upgrade its systems or build new \nones with some degree of regularity. \nLast, giving the federal government greater control \nover the computing resources would not immediately \nmake the NRC safe from attacks.79 As with using \ncommercial cloud infrastructure, security will primarily be \ncontingent on the NRC’s implemented data access model.80\nWe discuss security issues in depth in Chapter 8.\nKEY TAKEAWAYS\n Significant compute \npower: Fugaku \nwas the fastest \nsupercomputer in \n2020.\n New application \nprocess for \ncompute power:\nApplications were \nsolicited to test out \nthe supercomputer \non a host of tasks \nand have control \nover who received \ncompute power. \nCASE STUDY: FUGAKU\nIn 2014, Japan’s Ministry of Education, Culture, Sports, Science and \nTechnology launched a public-private partnership between the government\u0002funded Riken Institute, the Research Organization for Information Science and \nTechnology (RIST), and Fujitsu to create the supercomputer successor to the K \ncomputer that supports a wide range of scientific and societal applications.81 The \nresult was Fugaku, which was named the world’s fastest supercomputer in 2020.82\nThe technical aim of Fugaku was to be 100 times faster than the previous \nK computer, with a performance of 442 petaFLOPS in the TOP500’s FP64 high \nperformance LINPACK benchmark.83 It currently runs 2.9 times faster than the \nnext fastest system (IBM Summit)84 and is composed of slightly over 150,000 \nconnected CPUs, with each CPU using ARM-licensed computer chips.85 Despite \nhaving around 1.9 times more parts than its K computer predecessor, Fugaku \nwas finished in three fewer months.86 The six-year budget for Fugaku was around \n$1 billion.87\nRIST solicited proposals for usage through the “Program for Promoting \nResearch on the Supercomputer Fugaku.” Under the program, Fugaku has already \nbeen used to study the effect of masks and respiratory droplets in order to inform \nJapanese policy during the COVID-19 pandemic.88 For FY 2021, 74 public and \nindustrial projects were selected for full-scale access to Fugaku.89 Currently, RIST \nis still requesting proposals that fall under specific categories of usage, and any \ninterested researcher may apply.90\nA Blueprint for the National Research Cloud 33\nCHAPTER 2\nCOST COMPARISON\nTo conclude this chapter, we provide a rough cost \ncomparison between a leading commercial cloud \nservice and a dedicated government HPC system (IBM \nSummit) (see Appendix A for details). We refer the reader \nto substantial work that has been published on the \neconomics of cloud computing for a fuller analysis, much \nof which emphasizes the variance in computing demand.91\nBuilding standalone public infrastructure is projected \nto be less expensive than implementing the NRC through \na vendor contracting arrangement over five years. At a \n10 percent discount on standard rates over five years, \nand under constant usage, AWS’s more powerful cloud\u0002computing option (known as P3 instances) could cost 7.5 \ntimes as much as Summit’s total estimated costs, using \ncomparable hardware. We use a 10 percent discount that \nwas negotiated by a major research university with a \ncommercial cloud provider. In contrast, the government \nwould need to negotiate an 88 percent discount for AWS \nto be cost-competitive with a dedicated HPC cluster in the \nlong run. Even in a scenario where NRC usage fluctuates \ndramatically, commercial cloud computing could cost 2.8 \ntimes Summit’s estimated cost. (While variability in usage \nfactors heavily into these estimates, the use of schedulers \ncan contribute to a leveling out of demand.92) \nThese cost estimates have important limitations. First, \ngovernment may be able to negotiate the cost down. We \nhave used as a benchmark one major university’s enterprise \nagreement with AWS, which provides a 10 percent discount, \nrelative to market rates. But, unless the negotiated discount \nis orders of larger magnitude, the commercial cloud will \nremain significantly more expensive. Second, these cost \nestimates primarily focus on computing.93 As Compute \nCanada’s analysis showed, the cost difference in storage \nwas even greater. Third, the use of commercial rates is \nlikely more favorable to cloud vendors, as government \nsecurity standards typically increase rates due to \nregulatory requirements. For instance, a “data sovereignty” \nrequirement for data and hardware to reside within the \nUnited States, or private cloud requirements for certain \nagency datasets, may increase the cost of commercial cloud \ncomputing significantly. Fourth, this simple cost comparison \nis static, and does not reflect changes in hardware costs \nand pricing structures that are likely to occur over a five\u0002year period under rapidly changing market conditions. \nBut, if the NRC in fact scales, systems would be procured \nincrementally over time, upgrading available resources \nand providing options at different price points, similar to \ncurrent commercial options. Last, as noted above, these \ncost estimates take into account maintenance as budgeted \nfor the Summit, but may not take into account all such \nnon-hardware costs, which is why we recommend a pilot \nto explore the ability to open up government computing \nfacilities to NRC users. \nIn short, we offer this simple comparison to highlight \nsome of the salient cost considerations to the make-or-buy \ndecision, which arrives at a very similar conclusion to the \nanalysis done by Compute Canada. \nA Blueprint for the National Research Cloud 34\nCHAPTER 2\nCASE STUDY: COMPUTE CANADA\nCompute Canada formed in 2006 as a partnership between Canada’s regional \nacademic HPC organizations to share infrastructure across Canada.94 The \norganization’s stated mission is to “enable excellence in research and innovation \nfor the benefit of Canada by effectively, efficiently, and sustainably deploying a \nstate-of-the-art advanced research computing network supported by world-class \nexpertise.”95\nCompute Canada’s infrastructure includes five HPC systems that are hosted \nat research universities across Canada.96 From 2015-2019, Compute Canada \nused about $125 million (CAD) in funding to build four of these systems.97 They \nalso investigated using commercial cloud resources instead of building these \nnew systems.98 However, they ultimately concluded that relying on commercial \ncloud providers would be significantly more expensive and could not provide \nthe desired latency for large-scale, data-intensive research.99 In 2018, Compute \nCanada requested $61 million (CAD) to fund its operations, budgeting $41 million \n(CAD) for operating its HPC systems and $20 million (CAD) on support, training, \nand outreach.100 Demand for Compute Canada’s HPC resources far exceeds the \ninfrastructure’s current capacity and is expected to keep growing.101 In 2018, \nCompute Canada estimated they would need about $90 million (CAD) per year \nover five years to invest in expanding infrastructure to the point where it could \nmeet projected demands.102\nAbout 16,000 researchers from all scientific disciplines use Compute Canada’s \ninfrastructure to support their work.103 Compute Canada distributes its resources \nin two ways. First, Principal Investigators and sponsored users may request a \nscheduler-unprioritized resource allocation for their research group.104 Compute \nCanada finds that many research groups can meet their compute needs this way.105\nAlternatively, researchers who need more or prioritized resources may submit a \nproject proposal to the annual “Research Allocation Competitions.”106 Submitted \nproposals go through a scientific peer review and a technical staff review to \nrate their merits.107 Scientific review examines the scientific excellence and \nfeasibility of the specific research project, the appropriateness of the resources \nrequested to achieve the project’s objectives, and the likelihood that the resources \nrequested will be efficiently used.108 This review is conducted on a volunteer \nbasis by 80 discipline-specific experts from Canadian academic institutions.109\nTechnical review is conducted by Compute Canada staff itself, who verify the \naccuracy of the computational resources needed for each project, based on the \ntechnical requirements outlined in the application, and makes recommendations \nabout which resources should be allocated to meet the project’s needs.110 In \n2021, Compute Canada received 651 applications to the Research Allocation \nCompetition and fully reviewed all applications in the span of five months.111\nKEY TAKEAWAYS\n Default access \nwith tiers: All \nPIs are eligible \nfor access to \na scheduler\u0002unprioritized \ncompute resource \nallocation with \nan application \nprocess built in for \nrequesting more. \nMost researchers \nfind the default \nallocation sufficient \nfor their needs.\n Widely used \nand increasing \ndemand: Compute \nCanada’s \ninfrastructure \nis widely used \nacross academic \ndisciplines, with \ndemand constantly \nexceeding \nresources. \nCompute Canada \nintends to \ninvest heavily in \ninfrastructure to \nmeet increasing \ndemands.\nA Blueprint for the National Research Cloud 35\nCHAPTER 3\nAfter compute resources, the next critical design decision for the NRC is how to \nboth store and provide its users access to datasets: the “data access” goal of the NRC. \nIndeed, as articulated in the original NRC call to action, government agencies should \n“redouble their efforts to make more and better quality data available for public \nresearch at no cost,” as it will “fuel” unique breakthroughs in research.1 Investigating \nsome of the most socially meaningful problems hinges on large but inaccessible \ndatasets in the public sector. From climate data housed by the National Oceanic and \nAtmospheric Administration (NOAA), health data from the country’s largest integrated \nhealthcare system in the Department of Veterans Affairs (VA), or employment data \nin the Department of Labor (DOL), such data could fuel both fundamental research \nusing AI and refocus efforts away from consumer-focused projects (e.g., optimizing \nadvertising) to more socially pressing topics (e.g., climate change). \nAs noted in the congressional charge, facilitating broad data access is a crucial \npillar of the NRC. Importantly, as we discuss below, we limit the scope of our \nrecommendations to facilitating access to public sector government data, which as \na condition of accessing government administrative data, NRC researchers should \nonly use for academic research purposes. NRC users should also be able to compute \non any private dataset available to them. There are available mechanisms for sharing \nsuch datasets, but we identify the NRC’s major challenge as providing access to \npreviously unavailable government data. \nGovernment data is intentionally decentralized. By design of the Privacy Act of \n1974, there is no centralized repository for U.S. government data or a core method \nfor linking data across government agencies.2\n The result is a sprawling, decentralized \ndata infrastructure with widely varying levels of funding, expertise, application of \nstandards, and access and sharing of policies. Thus, the NRC will have to develop a \nunified data strategy that can work with a wide range of agencies, unevenly adopted \nsecurity standards, and within existing data privacy legislation.\nPrevious efforts have sought to improve access to and sharing of federal data, \nboth between agencies and with external researchers, but there are still significant \nbarriers to enabling AI research access of the kind that the NRC demands.3\n By linking \ndata governance policies with access to compute, building on existing successful \nmodels, and working with agencies to create interoperable systems that satisfy \nsecurity and privacy concerns, the NRC can enable increased access to data that will \naid AI researchers in answering pressing scientific and social questions and increase \nAI innovation.4\nChapter 3: \nSecuring Data Access\nKEY \nTAKEAWAYS\n The NRC should adopt a tiered \nmodel for access to and storage \nof federal agency datasets. \nTiers should correspond to the \nsensitivity of the data.\n The NRC can help to harmonize \nthe fragmented federal data\u0002sharing landscape.\n The NRC should consider \nincentivizing agency \nparticipation by granting \nagencies that contribute data \nthe right to use NRC compute \nresources. \n The NRC should strategically \nsequence data acquisition \nby focusing first on low- to \nmoderate-risk datasets that are \ncurrently inaccessible.\n Due to legal constraints and \nmany outside options, the \nNRC should focus its efforts \non streamlining access \nto government datasets. \nResearchers should still \nbe permitted to use NRC \ncompute resources on \nprivate datasets, as long as \nresearchers certify they have \nrights to use such data.\nA Blueprint for the National Research Cloud 36\nCHAPTER 3\nWe will first explain why the NRC should focus its efforts \non facilitating federal government data sharing rather than \nprivate sector data sharing. We then examine how and why \nthe status quo for federal data sharing fails to realize the \nmassive potential of government data. While the concept \nof centralizing disparate data sources to unlock research \ninsights is not new,5 there are unique challenges for doing so \nwithin the context of the NRC. We will also discuss the key \nelements of our proposed model: (1) the use of FedRAMP as \na system for categorizing datasets based on their sensitivity, \nand for modifying access to them through tiered credentials \nfor NRC users; (2) promotion of interagency standardization \nand harmonization efforts to modernize data-sharing \npractices; and (3) strategic considerations regarding how \nto sequence efforts in streamlining access to particular \ndatasets. \nThe case studies included throughout this White \nPaper were chosen as exemplars of successful data\u0002sharing initiatives6\n and to illustrate the range of available \ndesign decisions. While each case study provides a unique \nglimpse into different approaches, some common themes \nemerge. First, many of the data-sharing entities we studied \nnot only have a single point of entry for researchers to \nrequest access, but also allow government agencies to \nretain some control over access requirements to their data. \nAs we discuss below, this conception of the NRC as a data \nintermediary would provide real benefits in streamlining \ndata access while still maintaining trust among agencies \nthat wish to protect their data. Second, some initiatives \nuse funding and personnel training as carrots to incentivize \nagencies to engage in data sharing. The NRC can learn \nfrom these initiatives in formulating its own set of \nincentives for agencies. \nPRIVATE DATA SHARING\nShould the NRC affirmatively facilitate private dataset \nsharing? While there are definite benefits to providing \nresearchers with access to private data,7 the NRC will \nhave its largest impact by focusing its efforts first on \nmechanisms to access and share government data. \nAs an initial matter, a variety of mechanisms for \ngeneral data sharing already exist.8 Private sector \nstakeholders, moreover, can and have often built their own \nin-house platforms to allow access to approved datasets \nwhile minimizing intellectual property concerns,9\n or \nprovide access to their application programming interfaces \n(APIs) to make open-source data more easily accessible.10\nBy focusing on providing access to public sector \ndata, notably administrative data that is traditionally \ninaccessible to most researchers,11 the NRC would play a \nunique and pertinent role for researchers across disciplines \nwithout having to deal with complex private-sector data \nconcerns or the need to incentivize participation by \nnongovernment actors. \nComplex intellectual property \nconcerns would arise from the NRC \npermitting, facilitating, or even \nrequiring private sector stakeholders \nand independent researchers to \nshare their private data freely \nalongside public sector data. \nComplex intellectual property concerns would arise \nfrom the NRC permitting, facilitating, or even requiring, \nprivate sector stakeholders and independent researchers \nto share their private data freely alongside public sector \ndata. First, this would involve complex questions regarding \nwhat licenses should be available or mandated for \nNRC users in order to encourage data sharing, despite \napprehensions of how such sharing may affect future \nprofitability and commercialization. While mandating \nan open-source (e.g., Creative Commons) license would \nbenefit researchers most by providing the broadest access \nto data and would benefit NRC administrators by removing \nsome possible IP infringement concerns, private sector \nstakeholders may feel deterred from uploading as a result. \nConversely, if users have a choice to adopt a license that \nA Blueprint for the National Research Cloud 37\nCHAPTER 3\nallows them to preserve their IP rights, private sector \nstakeholders may feel more comfortable sharing their \ndata, but this would shift some liability to users—or to the \nNRC itself—by relying on users to abide by the license. This \nwould involve an emphasis on enforcement, ranging from \nexplanations and user disclaimers to the industry standard \nof a full-blown notice-and-takedown system. \nData owners may want to prevent the uploading \nof copyrighted works by, for instance, having the NRC \nitself assess whether private data is already protected \nby copyright. Industry standards for conducting data \ndiligence, using manual or automated tools, would either \nbe very labor intensive12 or prohibitively expensive.13 Even \nif these industry standards were met, researchers may find \nan NRC data-sharing platform duplicative. \nNone of the above would prevent researchers from \nusing NRC compute resources on their own private \ndatasets. Like current cloud providers, the NRC can \nstipulate in an End User Licensing Agreement (EULA) that \nresearchers must agree they own the intellectual property \nrights on the data they are using.14 This EULA can also \nassign liability to the end user, rather than the NRC, for any \nuse of data that is encumbered by existing IP provisions. \nAdditionally, the discussion above pertains to whether \nresearchers should be required to share their private data, \nnot to whether researchers should be required to share the \noutputs of their research conducted on the NRC. The latter \npoint is discussed in Chapter 9.\nTHE CURRENT PATCHWORK \nSYSTEM FOR ACCESSING \nFEDERAL DATA\nThe NRC could play a pivotal role streamlining \naccess to government data in a system that is currently \ndecentralized.15 In some cases, agencies may simply \nlack a standardized method for sharing data.16 Due to \nperceived legal constraints, risks, or security concerns, \nagencies often have little practical incentive to share their \ndata.17 Successful examples of researchers gaining access \nto government data from individual agencies frequently \nrely on the researchers having personal relationships \nwith administrators, and a willingness on the part of the \nadministrator to push against these constraints in service \nof the research project.18 While this relationship-based \nprocess has produced some successes,19 the far more \ncommon outcome is that data is simply not shared or \naccessed by researchers.20 Indeed, one government official \nindicated that overcoming the obstacles to making certain \ngovernment data available for research was the greatest \nchallenge in a lengthy career.\nOne government official \nindicated that overcoming the \nobstacles to making certain \ngovernment data available \nfor research was the greatest \nchallenge in a lengthy career.\nAgencies typically require the recipient of the data \nto abide by a data-use agreement (DUA). These DUAs \nprescribe such limitations on data usage as the duration \nof use, the purpose of use, and guarantees on the privacy \nand security of data.21 However, DUAs suffer from a \ncentral problem: The process for negotiating DUAs is \nhighly fragmented and inconsistent across government \nagencies, drastically increasing the complexity in obtaining \napprovals for them.22 Some agencies have a designated \noffice or process to handle DUAs, but other agencies rely \non extemporaneous processes and ad hoc, quid pro quo \narrangements.23 One such example is the Research Data \nAssistance Center, a centralized unit within the Centers \nfor Medicare & Medicaid Services (CMS) dedicated to \nsupporting data access requests.24 In contrast, DUAs within \nthe Department of Housing and Urban Development and \nthe Department of Education are handled in decentralized \nbusiness units, each with different routing channels and \nlegal teams, which can confuse reviewers when multiple \nA Blueprint for the National Research Cloud 38\nCHAPTER 3\ndata requests between the same parties are routed \nsimultaneously but separately.25 Indeed, university DUA \nnegotiators in one survey complained that the process was \na game of “bureaucratic hot potato” and wondered, “Why \nisn’t there just one template for everything?”26 Ultimately, \nthe lack of standardization means that DUAs often require \nextensive review and revision, creating substantial delays.\nAgency-by-agency requirements also impede data \nsharing. These requirements can range from mandating \nthat researchers only access data at an onsite facility, using \ngovernment-authorized equipment, to capping the amount \nof computational cycles that can be used to analyze data, \nor restricting the amount of data available simultaneously.27\nThese restrictions are particularly problematic, given that \nmodern AI models can require massive amounts of data and \ncomputation to be most effective. \nBroadly, the reasons for this dysfunction range \nfrom valid concerns about security and liability to the \nmundane and prosaic. Information technology systems \nwithin some agencies operate literally decades behind the \ntechnological frontier; a 2016 report from the Government \nAccountability Office (GAO) detailed examples of these \nlegacy systems, discussing how several agencies were \ndependent on hardware and software that were no longer \nupdateable, and required specialized staff to maintain.28\nA lack of incentives, a risk-averse culture, and an agency’s \nstatutory authority also play an important role in enabling \nor obstructing data sharing.29\nWe are by no means the first observers to note \nthese problems. Advocates have been working for years \nto standardize and modernize government practices \naround data and technology.30 For example, the Federal \nData Strategy is the culmination of a multiyear effort to \npromulgate uniform data-sharing principles to address \nthe fact that the United States “lacks a robust, integrated \napproach to using data to deliver on mission, serve the \npublic, and steward resources.”31 However, substantial \nchallenges remain, particularly since the bulk of the efforts \nfocused on opening access to government data have \nnot been undertaken with the specific needs of machine \nlearning and AI in mind. \nTIERED DATA ACCESS AND STORAGE\nThe decentralized nature of government data has \ncascading implications across many aspects of the \ngovernment data ecosystem. One key area that will affect \nthe NRC is a lack of consistent storage and authentication \naccess protocols across government agencies. \nBecause many government datasets contain sensitive \ndata (e.g., high risk due to individual privacy concerns),32\na crucial component of the NRC’s data model will consist \nof a tiered storage taxonomy that distinguishes between \ndatasets based on their sensitivity and correspondingly \nrestricts access to different research groups. Interpreting \ntiered storage and access as two sides of the same coin, \nwe reference existing models that are based on dataset \nrisk levels and propose a framework for the NRC that \naims to achieve the dual goals of streamlining the process \nof enabling research access to government data while \nmaintaining privacy and security. \nFedRAMP: A tiered framework for data storage \non the cloud\nOne type of tiered storage taxonomy already exists for \nthird party government cloud services in one of the federal \ngovernment’s major cybersecurity frameworks, the Federal \nRisk and Authorization Management Program (FedRAMP).33\nEnacted in 2011, the framework was designed to govern all \nfederal agency cloud deployments, with certain exceptions \ndetailed in Chapter 8 of this White Paper. FedRAMP offers \ntwo paths for cloud services providers to receive federal \nauthorization. First, an individual agency may issue what \nis known as an authority-to-operate (ATO) to a cloud \nservice provider after the provider’s security authorization \npackage has been reviewed by the agency’s staff and \nthe agency has identified any shortcomings that need \nto be addressed.34 These types of ATOs are valid for each \nvendor across multiple agencies, as other agencies are \npermitted to reuse an initial agency’s security package \nin granting ATOs. The second option available to cloud \nservices providers is to obtain a provisional ATO from the \nFedRAMP Joint Authorization Board, which consists of \nrepresentatives from the Department of Defense (DOD), \nthe Department of Homeland Security (DHS), and the \nA Blueprint for the National Research Cloud 39\nCHAPTER 3\nGeneral Services Administration (GSA). These provisional \nATOs offer assurances to agencies that DHS, DOD, and the \nGSA have reviewed security considerations, but before \nany specific agency is allowed to use a vendor’s services, \nthat agency must issue its own ATO.28 In both the first and \nsecond cases, FedRAMP categorizes systems into low, \nmoderate, or high impact levels (see Table 2).\nBecause FedRAMP requirements apply to all federal \nagencies when federal data is collected, maintained, \nprocessed, disseminated, or disposed of on the cloud, \nthe NRC itself will need to be compliant with FedRAMP \nsecurity standards irrespective of the organizational form \nit takes.35 Every dataset brought on to the NRC would \nneed to be reviewed under FedRAMP with appropriate \naccess levels. If a cloud service has already been evaluated \nunder FedRAMP because it was used in the past to house \nfederal data, the service can inherit the same FedRAMP \ncompliance level in the NRC without an additional \nevaluation.36\nBesides classifying datasets, the other function of \nFedRAMP is to identify a comprehensive set of “controls,” \ni.e., requirements and mechanisms that the cloud service \nproviders must implement before the government dataset \ncan be housed on them.37 They are based on the National \nInstitute of Standards and Technology (NIST) Special \nPublication 800-53, which provides standards and security \nrequirements for information systems used by the federal \ngovernment.38\nThese controls range widely and include requirements \nsuch as ensuring that the organization requesting \ncertification “automatically disables inactive accounts,” \n“establishes and administers privileged user accounts \nin accordance with a role-based access scheme that \norganizes system access and privileges into roles,” \n“provides security awareness training on recognizing \nand reporting potential indicators of insider threat,” or \ndevelops regular security plans in the event of a breach.39\nRequirements get more strenuous for FedRAMP “high \nimpact” data (e.g., creating system level air-gaps to protect \nsensitive data).40\nLEVEL TYPE OF DATA IMPACT OF DATA BREACH NUMBER OF \nCONTROLS\nLow-impact risk\n- Low baseline\n- Low-impact SaaS\nData intended \nfor public use\nLimited adverse effects; preserves the safety, \nfinances, reputation, or mission of an agency 125\nModerate-impact risk\n- E.g., personally \nidentifiable information\nControlled \nunclassified data \nnot available to \nthe public\nCan damage an agency’s operations 325\nHigh-impact risk\n- E.g., law enforcement, \nhealthcare, emergency \nservices\nSensitive federal \ninformation\nCatastrophic impacts such as shutting down \nan agency’s operations, causing financial ruin, \nor threatening property or life\n421\nTable 2: FedRAMP levels are designated based on the degree of risk associated with the breach of an information system. The security baseline levels \nare based on confidentiality, availability, and integrity, as defined in Federal Information Processing Standard 199.41\nA Blueprint for the National Research Cloud 40\nCHAPTER 3\nThere can be significant costs with obtaining these \ncertifications and creating compliance plans, even if the \nunderlying technical specifications can be addressed or \nalready exist. A key issue for structuring the NRC is that the \nprincipal burdens of ensuring FedRAMP compliance should \nfall with NRC institutional staff, not originating agencies or \nindividual academic researchers. As part of the FedRAMP \ncertification process, NRC staff will have to consider how to \ngive access to PIs in compliance with FedRAMP rules, but \nthat process can and should avoid requiring originating \nagencies or individual universities to incur substantial \nexpenses associated with hiring consultants and attorneys \nto certify FedRAMP compliance.42\nWhile FedRAMP sets out common standards for \ncloud storage of government data within agencies,43\nit is an exception to an otherwise balkanized federal \ndata-sharing standards landscape,44 though it does not \nfacilitate data exchange. The NRC needs to maintain \ncompliance with not only FedRAMP requirements but \nalso the requirements of any agency it is partnering with \nfor data access.45 Advocates interested in increasing \ngovernment data availability have long fought to establish \na universal FedRAMP equivalent across different agencies \nthat provides shared standards for data sharing based on \ndata sensitivity.46 As we discuss in Chapter 8, establishing \nsuch universal, “centralized” security standards not only \nensures internal uniformity but also removes barriers to \ndata sharing.\nThe NRC’s implementation of FedRAMP standards can \nalso provide partnering agencies an important opportunity \nto reexamine their own standards and share best practices \nwith one another.47 This could involve raising or lowering \nrequirements that are out of date,48 given the current \nthreat to the environment and research needs. The NRC \ncan take inspiration from agencies’ best practices, as well \nas from FedRAMP to develop a common NRC standard for \ndetermining data to be high, moderate, or low risk, as well \nas what consequences should flow from that assessment. \nIn the later section on strategic considerations, we discuss \nhow to enable this process by incentivizing agencies to \nparticipate in the NRC and selecting datasets that present \na lower privacy and security risk. \nIn addition, given the diversity of data types and \nsources that could be stored on the platform, NRC \npolicy should ensure that standards and protections \nexist for data storage in areas where FedRAMP has \nblind spots. FedRAMP is in part animated by risks from \nmalicious actors like cybercriminals or adversarial foreign \ngovernments, but as we discuss in Chapter 6, privacy \nrisks may arise even for the intended use case of analysis \nby NRC researchers. Of particular concern are instances \nwhere disparate datasets are combined, which may allow \nnew inferences that make previously anonymous data \nindividually identifiable, even when the data itself did not \ncontain identifiable information.49 Such combinations \nmay also alter the original risk level of the data, creating \nan output that merits a higher risk classification. \nFurthermore, machine-learning models and \nrepresentations may unintentionally reveal properties of \nthe data used to train them,50 and dissemination of these \nmodels could pose privacy risks.\nThis is not a challenge unique to the NRC; the U.S. \nCensus Bureau and other government agencies engaged \nin data linkage have also had to develop means to \naddress this issue.51 One solution involves applying \nmethods of additional noise to the data (differential \nprivacy) in order to obfuscate individual data while \npreserving the data’s utility for research. We discuss it \nand other privacy-enhancing technologies in greater \ndetail in Chapter 7.\n52 However, privacy-enhancing \ntechnologies are no panacea, and depending on the \nnature of the particular dataset, the goals of ensuring \nanonymization, while also enabling researchers to access \nfine-grained data can conflict. \nThe NRC can also draw from the “Five Safes” data \nsecurity framework used by the UK Data Service,53 the \nFederal Statistical Research Data Centers Network, \nand the Coleridge Initiative, a model centered on data, \nprojects, people, access settings, and outputs.54 The \nimplementation of the 2019 Evidence Act is already \nusing a similar Five Safes framework in making \ndeterminations around data linkage.55 Through a \ncombined framework, the NRC could place different \nanonymization requirements on datasets, depending \non the circumstances of their access and the privacy \nA Blueprint for the National Research Cloud 41\nCHAPTER 3\nagreements through which they were collected. Similarly, \nthe NRC could control the dissemination scope of models, \ncode, and data, depending on the sensitivity. Theoretical \nidentifiability is less likely to be a concern when access \nand dissemination is restricted and the data is of a less \nsensitive nature or is not about individuals at all.56\nFacilitating Researcher Access with a Tiered \nAccess Model\nHow should researchers gain access to specific data \nresources? Currently, approval proceeds on an agency\u0002by-agency basis.57 Just as the value of the NRC for \nsupporting AI research will depend in part on the extent \nto which it can bring together datasets from different \nagencies, it will also depend on the extent to which it can \nstreamline the process for accessing data. One way to \nachieve this streamlining will be through a tiered access \nsystem for the NRC users, similar to FedRAMP’s tiered \nsystem for storing federal data on the cloud, where higher \ntiers would enable access to higher-risk data, subject to \nthe other requirements on compute and data use. We \ndiscuss this access system in more depth in Chapter 7.\nChapter 2 made the case that compute access should \nstart with PIs at academic institutions. This authorization \ncan also serve as the baseline, where all NRC-registered \nPIs can freely access and use low-risk datasets on the \nNRC. Additional tiers would impose more requirements, \nsuch as citizenship, security clearance, distribution \nrestrictions, or compute and system restrictions. These \naccess tiers will be similar to those used for determining \nFedRAMP classification for data storage, but while access \nand storage sensitivity may invoke similar considerations; \nthey might not necessarily be the same. \nA Blueprint for the National Research Cloud 42\nCHAPTER 3\nKEY TAKEAWAYS\n Balances providing \nconsistent restricted \ndata access with agency \nrequirements: The ADRF \nbalances building in each \nrestricted data set’s access \nand export review form in \na consistent manner into \nthe data portal with agency \nrequirements for data \naccess and export. This \nallows agencies to control \naccess to their data, while \nproviding a single point \nof entry for researchers. \nCurrently, the platform only \nsupports data consistent \nwith a FedRAMP Moderate \ncertificate. \n Standardized for both users \nand data stewards: Along \nwith each data access for \nusers, a point of contact at \nthe agency providing the \ndata is given access to the \nplatform as well. This allows \neasy access to approve and \ntrack projects, and work \nwith the ADRF on access \nrequirements. \n Five Safes’ data security \nframework: The ADRF \nensures data security by \nfocusing on five aspects—\ndata, projects, people, \nsettings of access, and \noutputs.\n Training as a core function: \nThe ADRF hosts workshops \nand trains government \nemployees and other \nresearchers on data use.\nCASE STUDY: COLERIDGE INITIATIVE \n(ADMINISTRATIVE DATA RESEARCH \nFACILITY)\nIn partnership with the Census Bureau and funding from the Office \nof Management and Budget, the Coleridge Initiative, a nonprofit \norganization, launched the Administrative Data Research Facility \n(ADRF), a secure computing platform for governmental agencies to \nshare and work with agency micro-data.58 The ADRF is available on \nthe Federal Risk and Authorization Management Program (FedRAMP) \nMarketplace and has a FedRAMP Moderate certification. Currently, the \nplatform supports over 100 datasets from 50 agencies.59\nThe ADRF provides access to agency-sponsored researchers and \nagency-affiliated researchers going through the ADRF training programs \nfor free. Over the past three years, over 500 employees from approximately \n100 agencies have gone through ADRF training programs.60\nThe ADRF provides a shared workspace for projects and the \nData Explorer, a tool to view an overview and metadata (name, field \ndescription, and data type) of available datasets on the ADRF.61 In \norder to access restricted data, users must meet review requirements \nset by the agency providing the data. In order to export data, users \nmust go through a unique “Export Review” process.62 The ADRF has a \nhighly involved default review process, requiring researchers to submit \nall code and output for the project for approval to the data steward \nand generating additional charges, if requesting export of more than \n10 files.63 The agency providing the data can also amend the default \nreview process, if it wishes to do so. \nPrior to transferring data files, the ADRF provides an application for \ndata hashing to safely transmit data.64 The ADRF also follows the “Five \nSafes” security model used by other government agencies, such as the \nUK Data Service.65\nData stewardship for the ADRF is defined in compliance with \nthe Title III of the Evidence-Based Policymaking Act of 2018.66 Once \na restricted dataset is shared with the ADRF, one person within the \nagency will be assigned the data steward for all project requests. \nFrom there, procedures are developed with the agency, in terms of \nexpectations for how the data will be protected, authorized users, and \naudit procedures for continued compliance. \nData stewards have access to an online portal in the ADRF. All \nproject requests for specific data are routed to the data steward \nthrough this proposal. Once access has been granted, the data steward \nalso has options to monitor the project for compliance. \nA Blueprint for the National Research Cloud 43\nCHAPTER 3\nThe NRC can \nemulate both the \nColeridge Initiative \nand Stanford’s \nCenter for Population \nHealth Sciences \n(PHS), for instance, \nwhich serve as data \nintermediaries, in \nfacilitating access to \ngovernment data. \nKEY TAKEAWAYS\n Mixed data architecture, yet consistent user \nexperience: PHS utilizes a mix of on-premises \nand cloud data services, but still seeks to \nprovide a consistent user experience.\n Restricted data access has a single point \nof entry: The PHS Data Portal standardizes, \ncentralizes, and simplifies data access \nrequirements and trainings, rather than pointing \nusers to a time-consuming process working with \neach data steward directly.\n Reduces costs and time associated with \nprocuring data: PHS leverages existing \nrelationships with agencies to consolidate \ndatasets in a single portal, saving researchers \nthe time and money necessary to gain access \nthrough individual agency requests.\nExisting models for researcher access to sensitive datasets can help paint a \npicture of how the NRC might maintain and monitor a tiered access system. The \nNRC can emulate both the Coleridge Initiative and Stanford’s Center for Population \nHealth Sciences (PHS),67 for instance, which serve as data intermediaries, in \nfacilitating access to government data. Indeed, these intermediaries have been \ndocumented as effective means to overcome barriers to data-sharing because they, \nat their core, negotiate and streamline relationships between data contributors and \nusers.68 For example, as a trusted intermediary, the NRC could centralize the DUA \nintake process by promulgating a universal standard form for agency DUAs.69\nFurthermore, similar to the Coleridge Initiative example, a designated \nrepresentative(s) within the agency could be assigned as the data steward for all \nproject requests for a certain restricted dataset. Any project requiring access to data \nin higher tiers could commence only after its proposal was reviewed and approved \nby a relevant representative. Because NRC access begins with PIs, researchers would \nalso have to obtain approval from their university Institutional Review Boards (IRBs), \nas needed. After project approval and NRC researcher clearance, data would be \nmade available through the NRC’s secure portal. Any violations of the terms of use \nor subject privacy could result in penalties ranging from a demotion of access tier to \nremoval of NRC privileges or professional, civil, or criminal penalties, as relevant.\nCASE STUDY: STANFORD \nCENTER FOR POPULATION \nHEALTH SCIENCES\nThe Stanford Center for Population Health \nSciences (PHS) provides a growing set of population \nhealth-related datasets and access methods to \nStanford researchers and affiliates.70 The PHS Data \nEcosystem hosts high-value datasets, data linkages \nand filters, and analytical tools to aid researchers. \nThe PHS partners with a wide range of public, \nnonprofit, and private entities to license population\u0002level datasets for university researchers, ranging from \nlow-risk, public datasets to restricted data containing \nProtected Health Information (PHI) and Personally\u0002Identifiable Information (PII), such as Medicare, \ncommercial claims such as Optum and Marketscan \ndata,71 and electronic medical records. \nA Blueprint for the National Research Cloud 44\nCHAPTER 3\nCASE STUDY: STANFORD CENTER FOR POPULATION \nHEALTH SCIENCES (CONT’D)\nIn addition to secure data storage and computational tools for researchers, PHS provides standardized \nand well-documented data access and management protocols, which increases data proprietor comfort \nwith sharing data. PHS also has full-time staff who cultivate and maintain relationships with organizations \nholding data. This allows PHS to work with these groups to centralize data hosting and provide secure \naccess to a wide array of researchers. \nThe PHS Data Portal is hosted on a third-party platform that enables data discovery, exploration, and \nclearly delineated, standardized steps for data access. The third-party platform, Redivis, utilizes a four-tier \naccess system: (1) overview of data and basic documentation; (2) metadata access, including definitions, \ndescriptions, and characteristics; (3) a 1 or 5 percent sample of the dataset; and (4) full data access.72\nIf data is classified as public, researchers can access it using specialized software, or simply download \nit directly.73 For restricted data, the portal has forms integrated to easily apply for access.74 After identifying \nthe dataset, the researcher must apply for membership in the organization hosting the data.75 An \nadministrator of the organization owning the dataset can set member and study requirements that must \nbe met, including training and institutional qualification, in order to access the data. Member applications \ncan be set to auto-approval or require administrative approval. Once access has been granted to a data \nset, researchers can manipulate the data using specialized software. Usage restrictions are also specified \nindividually on each dataset to control whether full, partial, or no output can be exported, and what review \nlevel is required for exporting. All applications for data and export are handled directly on the Data Explorer \nplatform.\nCurrently, the PHS Data Portal is primarily for Stanford faculty, staff, students, or other affiliates.76 Even \nwith affiliate status, certain commercial datasets may require further data rider agreements for access. \nNon-Stanford collaborators must complete all of the same access requirements as Stanford affiliates, plus \nany requirements imposed by their own institution. Additionally, a “data rider” agreement on the original \nDUA is frequently necessary.77 \nTo work with restricted data, the PHS provides two computing services for high-risk data: (1) Nero, \nwith both an on-premises and Google Cloud Platform (GCP) platform versions; and (2) PHS-Windows \nServer cluster.78 Both are managed by the Stanford Research Computing Center (SRCC). Both services are \nHIPAA compliant.79 Unrestricted data can be used on any of Stanford’s other computational environments \n(Sherlock, Oak) or simply downloaded to the researcher’s local machine.\nA Blueprint for the National Research Cloud 45\nCHAPTER 3\nPROMOTING INTERAGENCY \nHARMONIZATION AND ADOPTION \nOF MODERN DATA ACCESS \nSTANDARDS\nThe federal data-sharing landscape suffers from \ndivergent standards and practices, and individual \nagencies, left alone, have traditionally faced high hurdles \nto harmonizing and modernizing their data access \nstandards.80 As we have discussed, this state of affairs \npresents formidable barriers to AI R&D from a researcher \nperspective, but is also problematic both from an agency \nand societal perspective. As a report by the Administrative \nConference of the United States finds from surveying the \nuse of AI in the federal government, nearly half of agencies \nhave experimented with AI to improve decision-making \nand operational capabilities, but they often lack the \ntechnical infrastructure and data capacity to use modern \nAI techniques and tools.81 The lack of a modern, uniform \nstandard for data sharing in AI research, therefore, makes it \nharder for agencies to realize gains in accuracy, efficiency, \nand accountability, which subsequently impacts citizens \ndownstream, who are affected by agency decisions.82\nThe lack of a modern, uniform \nstandard for data sharing in \nAI research makes it harder \nfor agencies to realize gains \nin accuracy, efficiency, and \naccountability, which subsequently \nimpacts citizens downstream, who \nare affected by agency decisions.\nThe NRC can help overcome agency reluctance to \nshare data by enabling access to agencies to compute \non their own data. This would solve at least two crucial \nproblems for government agencies. First, access to the \nNRC’s collective computing resources would overcome \nsome difficulties that agencies have traditionally faced \nin setting up their own compute resources.83 Second, \nfacilitating agency access to modern data and compute \nresources would attract and build further in-house \ngovernment AI expertise.84 From a societal perspective, \nthis could increase the government’s capabilities in the \nresponsible adoption of AI, help reduce the cost of core \ngovernance functions, and increase agency efficiency, \neffectiveness, and accountability.85\nThe NRC can also learn from and align with other \ninitiatives to harmonize and modernize standards. \nThe Evidence Act—which requires agencies to appoint \nchief data and evaluation officers—is one example. The \nlegislation authorizing the creation of the NRC could \nprovide a federal mandate to encourage adoption of \nsharing best practices.86 However, as we discuss in \nChapter 5, a federal mandate alone, without any additional \naid or incentives, may not be enough to incentivize \nharmonization of data access and sharing standards.87\nThe Task Force should therefore consider bundling the \nmandate with additional benefits, such as providing \nfunding to assist agencies in expanding their technical or \nstaff capabilities in furtherance of the NRC and the national \nAI strategy. The NRC is aligned with the existing bipartisan \ncase for the National Secure Data Service (NSDS) \n(described in the case study below), a service that would \nfacilitate researcher access to data with enhanced privacy \nand transparency, recommended by the Commission on \nEvidence-Based Policymaking in 2018. Both the NRC and \nNSDS are complementary data-sharing initiatives that \nhave the potential to considerably improve public service \noperational effectiveness. We elaborate further on the \nNSDS proposal in Chapter 5. Lastly, training programs are \npromising avenues to increase NRC adoption and agency \nsupport. For example, as described in the case study \nabove, the Coleridge Initiative has hosted workshops to \ntrain over 500 employees from approximately 100 agencies \non data use over the past three years. \nA Blueprint for the National Research Cloud 46\nCHAPTER 3\nCASE STUDY: THE EVIDENCE ACT\nIn pursuit of greater, more secure access to and linkage of \ngovernment administrative data, a bipartisan Commission on Evidence\u0002Based Policymaking was set up by Congress in March 2016. The \ncommission’s final report88 included 22 recommendations for the federal \ngovernment to build infrastructure, privacy-protecting mechanisms,89\nand institutional capacity to provide secure access to public data for \nstatistical and research purposes. One recommendation was to create \na “National Secure Data Service” (NSDS) to facilitate access to data \nfor the purpose of building evidence, while maintaining privacy and \ntransparency. Through this service, the NSDS could help researchers by \ntemporarily linking existing data and providing secure access, without \nitself creating a data clearinghouse.\nThe Foundations for Evidence-Based Policymaking Act of \n201890 created some of the legislative footing for the commission’s \nrecommendations. In particular, it created new roles for chief data, \nevaluation, and statistical officials, and sought to increase access and \nlinkage of datasets previously within the scope of the Confidential \nInformation Protection and Statistical Efficiency Act (CIPSEA).91\nFinally, the 2020 Federal Data Strategy and associated Action \nPlan92 sought to put those legislative provisions into action. The \nstrategy included plans to improve data governance, to make data more \naccessible, to improve government use of data, and to boost the use and \nquality of data inventories, metadata, and data sensitivity.\nThe central remaining step envisioned by the initial Evidence-Based \nPolicymaking Commission is a National Secure Data Service (NSDS) \nmodeled on the UK’s Data Service.93 The UK’s Data Service provides \naccess to a range of public surveys, longitudinal studies, UK census \ndata, international aggregate data, business data, and qualitative \ndata. Alongside access, it provides guidance and training for data use, \ndevelops best practices and standards for privacy, and has specialized \nstaff who apply statistical control techniques to provide access to data \nthat are too detailed, sensitive, or confidential to be made available \nunder standard licenses.\nKEY TAKEAWAYS\n Priority data sharing where \nthere is a public service \noperational case: There is \nprecedent in large-scale \nadministrative data-sharing \ninitiatives justified on \ngrounds of improvements \nto public service \noperational efficiency and \neffectiveness.\n National Secure Data \nService (NSDS) initiative: \nThe NSDS is bolstered by \nbipartisan support.\n Institutional models \nthat balance external, \ninnovative talent and \ninternal, cross-agency \ninfluence: A Federally \nFunded Research and \nDevelopment Center \n(FFRDC) model, housed \nwithin an existing agency \n(NSF), may balance the \nability to bring in external \ntalent with internal agency \ninfluence.\nA Blueprint for the National Research Cloud 47\nCHAPTER 3\nSEQUENCING INVESTMENT INTO \nDATA ASSETS\nGiven the significant hurdles in negotiating data \naccess, the NRC will need to strategically sequence which \nagencies and datasets to focus on for researcher use. The \nfederal government collects petabytes of data,94 each with \nvarying degrees of restrictions or openness. In considering \nwhich datasets to prioritize, the NRC can draw from the \nexample of other data-sharing initiatives, as well as focus \non data sets in the short term that do not pose complex \nchallenges with regards to data privacy or sharing. One \nprivate sector example is Google Earth Engine, which \naggregated petabytes (approximately 1 million gigabytes) \nof satellite images and geospatial datasets, and then \nlinked that access to Google’s cloud-computing services \nto allow scientists to answer a variety of crucial research \nquestions.95 This process of aggregating complex data \nand hosting it in a friendly computing infrastructure to \nfacilitate research, demonstrates the compelling value \nof coupling compute and data. As another example, \nADR UK identifies specific areas of research that are of \npressing policy interest, such as “world of work,”96 and \nprioritizes data access for researchers working on those \ntopics. The UK Data Service offers datasets derived \nfrom survey, administrative and transaction sources, \nincluding productivity data from the Annual Respondents \nDatabase,97 innovation data from the UK Innovation \nSurvey,98 geospatial data from the Labour Force Survey,99\nUnderstanding Society,100 and sensitive data about \nchildhood development.101\nWhen prioritizing datasets and agencies for NRC \npartnership, we recommend the following criteria: \n• Data that is valuable to AI researchers, but is not \ncurrently available in a convenient form. For example, \nin a July 2019 request for comments, the Office of \nManagement and Budget (OMB) asked members \nof the public to provide input on characteristics of \nmodels that make them well-suited to AI R&D, what \ndata is currently restricted, and how liberation of \nsuch data would accelerate high-quality AI R&D.102\nIn one response, the Data Coalition argued that \ncontrolled release of private but structured indexed \ndata in data.gov would be valuable for research.103\nThe Data Coalition also urged agencies to consider \nreleasing raw, unstructured datasets, such as agency \ncall center logs, consumer inquiries and complaints, \nas well as regulatory inspection and investigative \nreports.104 Another example of data that is currently \nchallenging to access, but is a matter of public \nrecord, is electronic court records housed in a system \nby the Administrative Office of the U.S. Courts.105\n• Data housed within agencies that have statutory \nauthority to share data and/or that have previous \ndata-sharing experience. The Census Bureau, for \ninstance, has greater existing statutory interagency \nlinkage than other agencies, and has preexisting \nsubstantial in-house data analysis expertise.106\nThe U.S. Bureau of Labor Statistics has an existing \nprocess for sharing restricted datasets (in the \ncategories of employment and unemployment, \ncompensation and working conditions, and prices \nand living conditions) with researchers.107\n• Data with limited privacy implications. For example, \nagencies whose data concerns natural phenomena, \nrather than individuals, may be easier to manage \nfrom a privacy perspective—e.g., NASA, the US \nGeological Service, and the National Oceanic and \nAtmospheric Administration. Datasets like those \nhoused in NASA’s Planetary Data System,108 but that \nare not easily available to researchers, may serve \nas a valuable starting point for the NRC. Increasing \nthe availability and interoperability of datasets from \nthese agencies would advance the core mission of \nthe NRC and could be done without jeopardizing \nindividual privacy.\nA Blueprint for the National Research Cloud 48\nCHAPTER 4\nChapter 4: \nOrganizational Design\nWhat institutional form should the NRC take? Two overarching considerations \nare: (1) ease of access to data; and (2) ease of coordination with compute resources.1 \nAs we discussed in Chapter 3 and will detail in depth in Chapter 5, the federal \ndata-sharing landscape among agencies is highly fragmented, with many agencies \nreluctant to or legally constrained from sharing their data. The NRC will need to \ncoordinate between the entities supplying compute infrastructure and researchers \nthemselves. As the NRC’s goal is to provide researchers with access to government \ndata and high-performance computing power, one without the other will fall short \nof achieving the NRC’s mission.\nDrawing on extensive work in support of the Evidence Act, we recommend \nthe use of Federally Funded Research and Development Centers (FFRDCs) and \nprivate-public partnerships (PPPs) as possible organizational forms for the NRC. We \nrecommend the creation of an FFRDC at affiliated government agencies in the short \nterm, as we believe this path allows for the easiest facilitation of both the compute \ninfrastructure and access to government data. In the longer term, the establishment \nof a PPP could facilitate greater data sharing and access between the public and \nprivate sectors. Importantly, other options include creating an entirely new federal \nagency or bureau within an existing agency. While these options might simplify \ncoordination with compute resources, both pose challenges, with respect to data \naccessibility and interagency data sharing. \nFEDERALLY FUNDED RESEARCH AND \nDEVELOPMENT CENTER\nFFRDCs are quasi-governmental nonprofit corporations sponsored by a \nfederal agency but operated by contractors, including universities, other nonprofit \norganizations, and private-sector firms.2 The FFRDC model confers the benefits of \na close agency relationship, alongside independent administration, in facilitating \naccess to data. Due to their intimate subcontracting relationships with their parent \nagency, all FFRDCs benefit from data access that goes “beyond that which is \ncommon to the normal contractual relationship, to Government and supplier data, \nincluding sensitive and proprietary data.”3\nA recent report by Hart and Potok on the National Secure Data Service (NSDS) \n(see case study in Chapter 3) also supports the FFRDC model as an optimal way \nto facilitate access to and linkage of government administrative data.4\n The report \nconsidered FFRDCs, alongside such other institutional forms, as creating an entirely \nnew agency, housing the NSDS in an existing agency, and developing a university\u0002KEY \nTAKEAWAYS\n In the short term, the NRC \nshould be instituted as a \nFederally Funded Research \nand Development Center \n(FFRDC), which would reduce \nthe significant costs of \nsecuring data from federal \nagencies.\n In the longer term, a well\u0002designed, public-private \npartnership (PPP), governed \nby officers from Affiliated \nGovernment Agencies, \nacademic researchers, and \nrepresentatives from the \ntechnology sector, could \nincrease the quantity and \nquality of R&D, and reduce \nmaintenance costs. \n Instituting the NRC as a \nstandalone federal agency or \nbureau would face numerous \nchallenges, notably in securing \naccess to data housed in other \nagencies. \nA Blueprint for the National Research Cloud 49\nCHAPTER 4\nled, data-sharing service, but the report ultimately \nrecommended the FFRDC model for several reasons. An \nFFRDC can scale quickly, because it can access government \ndata and high-quality talent more easily than other \noptions.5 An FFRDC can also leverage existing government \nexpertise. The NSF, for instance, already sponsors five \nseparate FFRDCs and has extensive experience cultivating \nand maintaining networks of researchers.6\nHowever, the FFRDC model comes with a few \nlimitations. First, an FFRDC’s role is restricted to research \nand development for their sponsoring agency that “is \nclosely associated with the performance of inherently \ngovernmental functions.”7 Thus, it would be important to \nensure alignment during the contracting phase with the \nNRC’s core functions. \nSecond, the success of an FFRDC model for the NRC \nwill depend on the ability of the sponsoring agency to gain \ncooperation across the federal government to provide \ndata needed for research. One way to do this would be \nfor multiple agencies to co-sponsor the FFRDC, reducing \ncontracting friction for datasets.8 Another option would \nbe to create multiple FFRDCs housed in different agencies, \nincentivizing each of those agencies to share their data \nwith the respective FFRDC. An analogous example could \ninclude the National Labs as a network, where each \nNational Lab would be an instantiation of the NRC within \nits own relevant agency.9\nThird, multiple FFRDCs would require separate \nprocesses for compute resources. In the short term, \nthe NRC may alleviate this problem by contracting for \ncommercial cloud credits, which is likely already the short\u0002term solution for the NRC to provide compute access. As \ndiscussed earlier, private sector cloud providers already \nhave extensive experience in providing compute resources \nto the government10 and to academic institutions.11\nFamiliarity with these private cloud providers may reduce \nthe friction in allocating compute among researchers at \nmultiple FFRDCs.\nIn the longer term, the FFRDC model may not be the \nmost efficient. From a cost and sustainability perspective, \nFFRDCs have traditionally suffered from significant \noverruns, as they “operate under an inadequate, \ninconsistent patchwork of federal cost, accounting and \nauditing controls, whose deficiencies have contributed \nto the wasteful or inappropriate use of millions of federal \ndollars.”12 Another concern is that, historically, FFRDC \ninfrastructure has not been routinely updated. A 2017 \nDepartment of Energy report highlighted that FFRDC \ninfrastructure was inadequate to meet the mission.13 \nNASA’s Inspector General also highlighted that more \nthan 50 percent of the Jet Propulsion Laboratory (a NASA \nFFRDC) equipment was at least 50 years old.14 If an FFRDC \nversion of the NRC experiences these same challenges, \nwe recommend that the NRC, in the long run, switch to a \npublic-private partnership model.\nA Blueprint for the National Research Cloud 50\nCHAPTER 4\nKEY TAKEAWAYS\n Multiple agency \nco-sponsors: While \nSTPI’s primary \nsponsor is the \nNational Science \nFoundation, a \nnumber of other \nagencies also \nco-sponsor STPI, \nreducing difficulties \nin accessing data \nacross agencies.\n Expertise: While \nSTPI is staffed with \nits own employees, \nit can also tap into \nexpertise from \nthe hundreds of \nemployees at the \nInstitute for Defense \nAnalyses (IDA), the \norganization that \nmanages STPI. As \nan FFRDC, STPI can \nalso contract for \nadditional expertise \nas required.\nCASE STUDY: SCIENCE & TECHNOLOGY \nPOLICY INSTITUTE (STPI)\nSTPI is an FFRDC chartered by Congress in 1991 to provide rigorous objective \nadvice and analysis to the Office of Science and Technology Policy and other \nexecutive branch agencies.15 STPI is managed by the Institute for Defense Analyses \n(IDA), a nonprofit organization that also manages two other FFRDCs: the Systems \nand Analyses Center and the Center for Communications and Computing.16 IDA has \nno other lines of business outside the FFRDC framework.17\nSTPI’s primary federal sponsor is the National Science Foundation, but \nresearch at STPI is also co-sponsored by other federal agencies, including the \nNational Institute of Health (NIH), Department of Energy (DOE), Department of \nTransportation (DOT), Department of Defense (DOD), and Department of Health and \nHuman Services (HHS).18 Due to the “unique relationship” between an FFRDC and its \nsponsors, STPI “enjoys unusual access to highly classified and sensitive government \nand corporate proprietary information.”19\nNSF appropriations provide the majority of funding for STPI, including $4.7 \nmillion in FY 2020,20 but a limited amount of funding is also provided from other \nfederal agencies.21 STPI has approximately 40 full-time employees and has access \nto the expertise of IDA’s approximately 800 other employees.22 As an FFRDC, STPI \nmay also contract for expertise, as required for a particular project.23 The statute \nspecifying STPI’s duties also directs it to consult widely with representatives from \nprivate industry, academia, and nonprofit institutions, and to incorporate those \nviews in STPI’s work to the maximum extent practicable.24\nSTPI is also required to submit an annual report to the president on its \nactivities, in accordance with requirements prescribed by the president,25 which \nprovides additional accountability for the FFRDC. According to STPI’s 2020 report, \nSTPI worked across multiple federal agencies, supporting them on 48 separate \ntechnology policy analyses throughout 2020.26\nA PUBLIC-PRIVATE PARTNERSHIP (PPP)\nA Public-Private Partnership (PPP) would create a \npartnership between federal agencies and private-sector \norganizations to jointly house and manage data-sharing \nefforts and run compute infrastructure. Because different \nagencies and private-sector members may have different \ncontracting preferences, intellectual property goals, and \nsecurity allowances for data access, creating a data-sharing \npartnership within this patchwork framework could be \nchallenging in the immediate future. Nonetheless, PPPs \ncan provide a number of long-term benefits, as they have \nbeen used successfully as data clearinghouses to produce, \nanalyze, and share data between the public and private \nsector.27 Indeed, recognizing the benefits of the PPP model, \nthe European Union has launched a new initiative called \nthe Public Private Partnerships for Big Data that will offer \na secure environment for cross-sector collaboration and \nexperimentation using both commercial and public data.28\nIn general, PPPs for data-sharing can increase the quality \nand quantity of R&D, increase the value and efficiency \nof sharing public sector data, and reduce the long-run \ncost necessary to manage and maintain the data-sharing \ninfrastructure.29\nA Blueprint for the National Research Cloud 51\nCHAPTER 4\nKEY TAKEAWAYS\n Utilizing joint ventures: \nThe ADP PPP uses \na joint venture \nagreement to set the \nterms and conditions \nfor the partnership, \nwhereby one partner \nis the custodian for \ngovernment data, \nand the other is the \noperator that builds and \nmaintains the software \nthat facilitates data\u0002sharing.\n Revenue-sharing \nagreements: A shared \nrevenue model assures \ncontributions from, \nand realization of value \nto, each stakeholder. \nThe ADP subsequently \nreinvests its profits into \nimproving the system.\n Significant efficiencies: \nAccording to the \nADP, there are lower \ncosts to creating \nand maintaining the \nADP than under a \nconventional approach. \nCASE STUDY: ALBERTA DATA PARTNERSHIPS \n(ADP)\nFounded in 1997, the ADP PPP is designed to provide long-term \nmanagement of comprehensive digital data sets for the Alberta market.30\nThe PPP is structured as a joint venture between ADP, a nonprofit, and Altalis \nLtd. whereby the ADP is the “custodian” of government data and Altalis is \nthe “operator.”31 More specifically, geospatial data is owned by the provincial \ngovernment, but exclusive licensing arrangements are granted to ADP to \nallow for sales.32 Meanwhile, Altalis, under the direction and oversight of \nADP, builds software to securely load and distribute these provincial spatial \ndatasets to users. Altalis also provides training to end users and is responsible \nfor cleaning, updating, and standardizing datasets.33\nIn choosing its “operating partner” (i.e., Altalis) for the joint venture, the \nADP board initially issued a “Request for Information” that solicited proposals \nfrom private-sector companies whose core business was the improvement, \nmaintenance, management, and distribution of spatial data.34 The ADP board \nultimately chose Altalis, not only because it had the superior offering and \nexisting capabilities, but also because Altalis was willing to take on all the \ninvestment required, at its own risk, to build and operate the ADP system in \naccordance with ADP specifications.35\nToday, all Altalis and ADP costs are covered by the operations of the joint \nventure.36 The joint venture earns revenues through, for instance, through \ndirected project funding and data access fees from stakeholders, which \ninclude municipalities, regulatory agencies, energy, forestry, and mining \norganizations.37 Any profits from the joint venture are split roughly 80/20 \nbetween Altalis and ADP, respectively, and ADP subsequently uses its profit \nshare to reinvest in data and system improvements.38\nThe ADP PPP claims to have generated efficiencies for data sharing. For \ninstance, the ADP estimates that a traditional government-only approach to \nmaintaining and distributing datasets would have ranged between $65 million \nand $120 million cumulatively since ADP’s inception, and ADP claims to have \nprovided its users with $6.8 million in cost savings.39\nA Blueprint for the National Research Cloud 52\nCHAPTER 4\nA PPP model could reduce the friction of coordination \nbetween data and compute. One example of using a PPP \nfor compute resources is the COVID-19 High-Performance \nComputing Consortium, spearheaded by the Office of \nScience and Technology Policy, DOE, NSF, and IBM.40\nDrawing on the experience of XSEDE, the consortium \nhas 43 members from the public and private sectors that \nvolunteer free compute resources to researchers with \nCOVID-19-related research proposals.41 The voluntary \nnature of compute provisioning, in this instance, provides \nbenefits to both the researchers, who gain immediate \naccess to compute, and the consortium members, who \ncontribute to innovation and reap public relations benefits. \nWe also acknowledge that the evidence around the \nefficacy of PPPs is contested.42 Indeed, there is no one\u0002size-fits-all PPP model; PPPs differ vastly, according to the \nresponsibilities allocated between the private sector and \nthe public sector, and the success of a PPP can depend \non its structure.43 According to a RAND Report of 30 case \nstudies of successful public-private data clearinghouses, \nthese clearinghouses have widely different organizations, \naccess requirements, and strategies for managing data \nquality.44 Such decision points are crucial. For example, \nsome scholars emphasize the need for a trusted \nenvironment for the private and public sectors to handle \nprivacy and ethics violations in sensitive industries.45\nSimilarly, in the siloed federal data-sharing context, a PPP \nmust consider how to divide functions in tackling these \nadditional considerations in privacy, ethics, security, and \nintellectual property.\nTHE NRC AS A GOVERNMENT \nAGENCY\nThe NRC could also be constructed as a new \ngovernment agency or bureau. The main advantages \nto this model would be the development of a distinct \npublic-sector institution, devoted to AI compute and \ndata. The NRC could be to cloud and data what the U.S. \nDigital Service is to government information technology. \nSuch an agency would have to be established by statute \nor executive mandate. Enabling legislation could create \ndedicated, professional staff to build and develop the NRC, \nvest the NRC with authority to mandate interagency data \nsharing, and create a long-term plan that is informed by \nthe National AI Strategy. \nThere are, however, significant disadvantages to \ncreating a new agency or bureau. First, the NRC could \nlay claim to no government datasets at all, and could \nsubsequently encounter significant headwinds with having \nto negotiate with each originating agency for data, not to \nmention the constraints under the Privacy Act, discussed \nin Chapter 5. That said, enabling legislation could \nexempt the agency from the Privacy Act’s data linkage \nprohibitions and transfer litigation risk for data leakages \nto the new agency. Second, a new agency may face greater \nchallenges in recruiting top-flight talent.46 According to \nthe 2020 Survey on the Future of Government Service, a \nmajority of respondents at federal agencies agreed that \nthey often lose good candidates because of the time it \ntakes to hire, and less than half agreed that their agencies \nhave enough employees to do a quality job.47 Moreover, \nmany respondents highlighted inadequate career growth \nopportunities, inability to compete with private-sector \nsalaries, and lack of a proactive recruiting strategy as \nmajor factors contributing to an inadequately skilled \nworkforce in federal agencies.48 FFRDCs, in contrast, can be \nnegotiated with existing organizations, making the startup \ncosts potentially lower. Third, while national laboratories \nhave expertise contracting with entities to construct high\u0002performance computing facilities, it is unclear how a new \nfederal agency/office would approach such a task. It is \none thing for an entity like the U.S. Digital Service to help \ndevelop IT platforms for U.S. agencies; it is another to \nsimultaneously build a very large supercomputing facility \nand solve longstanding challenges with data access. \nFinally, it will be important to isolate the research mission \nof the NRC from political influence. To the extent that a \nnew agency might provide less isolation from changes \nin presidential administrations and politically appointed \nadministrators, this is an important consideration.\nWhile these disadvantages are considerable, \nambitious legislative action could, in fact, make a new \ngovernment agency a viable option.\nA Blueprint for the National Research Cloud 53\nCHAPTER 5\nChapter 5: Data \nPrivacy Compliance\nThe vision motivating the NRC is to support academic research in AI by opening \naccess to both compute and data resources. Federal data can fuel basic AI research \ndiscoveries and reorient efforts from commercial domains toward public and social \nones. As stated in the NRC’s original call, “Researchers could work with agencies to \ndevelop and test new methods of preserving data confidentiality and privacy, while \ngovernment data will provide the fuel for breakthroughs from healthcare to education \nto sustainability.”1\nBut is an NRC seeded with public sector data, particularly administrative data \nfrom U.S. government agencies, even possible given the legal constraints? Research \nproposals that sweep broadly across agencies for personally identifiable or otherwise \nsensitive data2 will rightly trigger concerns about potential privacy risk. The Privacy \nAct of 1974, the chief federal law governing data collected by government agencies, \nfundamentally challenges the notion of an NRC as a one-stop shop for federal data. \nIts research exceptions leave some uncertainty about open-ended research endeavors \nthat go beyond statistical research or policy evaluation supporting an agency’s core \nmission. Even if agencies deemed such research possible, researchers would be \nsubject to access constraints and the data itself may potentially require technical \nprivacy treatments.\nWe make the following recommendations regarding data privacy and the NRC. \nFirst, agencies may be able to share anonymized administrative data with the NRC \nwithin the boundaries of the Privacy Act for the purposes of AI research, based on the \nAct’s statistical research exemptions. Second, the NRC will require a staff of privacy \nprofessionals that include roles tasked with legal compliance, oversight, and technical \nexpertise. These professionals should build relationships with peers across agencies \nto facilitate data access. Third, the NRC should explore the design of virtual “data safe \nrooms” that enable researchers to access raw administrative microdata in a secure, \nmonitored, and cloud-based environment. Fourth, we recommend the NRC Task Force \nengage the policy and statistical research communities, and consider coordination \nwith proposals for a National Secure Data Service, which has grappled extensively \nwith these issues. \nThis chapter proceeds as follows. We first review the existing laws that apply to \ngovernment agencies and the restrictions they impose on data access and sharing. We \nthen describe current agency practices for sharing data with researchers and agencies \nunder the Privacy Act. Last, we assess the implications of current legal constraints on \nNRC data sharing and the most important cognate proposal to promote data sharing \nunder the Evidence Act. \nKEY \nTAKEAWAYS\n Agencies may share \nanonymized administrative \ndata with the NRC under \nthe statistical research \nexemption of the Privacy \nAct.\n An agency’s willingness and \nability to share data may \ndepend on the extent to \nwhich a proposed research \nproject aligns with an \nagency’s core purpose.\n The NRC will require a staff \nof privacy professionals for \nlegal compliance, oversight, \nand technical expertise. \n Individually identifiable \nor sensitive data will face \nobstacles to release and \nmay warrant technical \nprivacy and/or tiered access \nmeasures. \nA Blueprint for the National Research Cloud 54\nCHAPTER 5\nEven when authorized or \nmandated to share data in limited \ncircumstances, federal agencies \nare often reluctant to do so due to a \nmyriad of factors, most prominently \na lack of adoption of consistent \ndata security standards, as well \nas difficulties with measuring and \nassessing privacy risks.\nWe note at the outset that this chapter largely takes \nexisting statutory constraints as a given. At a macro level, \nhowever, the challenges in data sharing also suggest that \nan ambitious legislative intervention could overcome many \nexisting constraints, such as by statutorily (a) exempting \nthe NRC from the Privacy Act’s prohibition on data linkage; \n(b) granting the NRC the power to assume agency liabilities \nfor data breaches; (c) mandating that agencies transfer any \ndata that has been shared under a data use agreement or \nFreedom of Information Act (FOIA) request to the NRC; and \n(d) requiring IT modernization plans to include provisions \nfor data-sharing plans with the NRC.3\nTHE PRIVACY ACT\nData privacy issues are at the core of debates about \nsharing data, and the NRC will be no exception. Most data \nprivacy debates in the U.S. today focus on the consumer \ndata sector where data protection laws in the U.S. are \nlimited to nonexistent. In contrast, many U.S. government \nagencies are subject to a robust privacy law, the Privacy \nAct of 1974, that was passed in response to concerns \nabout government abuses of power.4 For nearly 50 years, \nthis legislation has been effective in its primary goal of \npreventing the U.S. government from centralizing and \nbroadly linking data about individuals across agencies. \nHowever, this approach has come at a cost, which is that \nmost government agencies are prevented from freely \nsharing and linking data across agency boundaries, \nwhich in turn hampers agency operational and research \nefforts.5 According to one government privacy expert, even \nwhen authorized or mandated to share data in limited \ncircumstances, federal agencies are often reluctant to do \nso due to a myriad of factors, most prominently a lack of \nadoption of consistent data security standards, as well as \ndifficulties with measuring and assessing privacy risks.6 To \nthat end, many agencies see promise in adopting technical \nprivacy measures, such as differential privacy, or the \ncreation of synthetic datasets as proxies for actual data, as \na necessary precursor for enabling data sharing for both \nresearch purposes and interagency goals.7 \nIn the nearly 50 years since the Privacy Act’s \npassage, there have been periodic efforts to address \nthe government’s approach to data management \nwhile preserving data privacy. Examples include the \nE-Government Act of 2002, 8 the Confidential Information \nProtection and Statistical Efficiency Act of 2002,9 and most \nrecently, the Foundations for Evidence Based Policymaking \nAct10 and the National Data Strategy.11 Most of these efforts \nhave been aimed at sharing government data for statistical \nanalysis and policy evaluation, and the scope of provisions \nmay need to be broadened to support AI research. We view \nthese efforts to be complementary: The NRC should build \non these efforts, while bringing increased attention to the \ncompute resources that enable AI development as well as \nadvanced data analysis.\nSTATUTORY CONSTRAINTS ON \nDATA SHARING\nOne vision of the NRC is for it to act as a data \nwarehouse for all government data. But that vision collides \nwith fundamental constraints from laws designed to \nhamper broad and unconstrained data sharing between \nU.S. government agencies. Lacking an overarching, \ncomprehensive privacy regime, similar to the European \nUnion’s General Data Protection Regulation (GDPR), the \nUS landscape is fragmented between a mix of sector\u0002specific consumer laws and certain government-specific \nlaws, such as the Privacy Act of 197412 and limited-scope \nfederal guidance, such as the Fair Information Practice \nA Blueprint for the National Research Cloud 55\nCHAPTER 5\nPrinciples.13 In particular, the Privacy Act, which focuses \nbroadly on data collection and usage by federal agencies, \nand restricts sharing between them, poses challenges \nto the ambitions of the NRC’s goal to make otherwise \nrestricted government datasets more widely available.\nExisting efforts, buttressed by such bills as the \nE-Government Act of 2002 and the Foundations of \nEvidence-Based Policymaking Act, have attempted to \nincrease access by researchers to government data assets. \nYet, these approaches were animated by the primary \npurposes of policy evaluation, not basic AI research. Nor \ndo they consider any ambitions on the part of agencies \nthemselves to pursue AI research and development.14\nApplication of these laws and regulations to the NRC, \nin part, hinge on three factors: (1) the institutional form of \nthe NRC, as we discuss in Chapter 4; (2) whether NRC users \ncan invoke the Privacy Act’s existing statistical research \nexception; and (3) whether researchers are accessing data \nfrom multiple federal agencies. Here we briefly discuss \nthe legal obligations of federal agencies. Even if the NRC \ndoes not take the form of a new standalone federal agency, \nagencies contributing data will remain subject to these \nconstraints. \nTHE PRIVACY ACT’S LIMITATIONS AND EXEMPTIONS\nThe Privacy Act was enacted in response to growing \nanxiety about digitization, as well as the Watergate \nscandal during the Nixon presidency. The Act was \nmotivated by concerns about the government’s ability \nto broadly collect data on citizens and centralize it into \ndigital databases, an emergent practice at the time. It \nis the primary limiting regulation for government data \nsharing, and has consequences for the NRC and, more \ndirectly, for any government agency wishing to share data \nwith the NRC. \nData Linkage\nThe Privacy Act applies to systems of records, which \nare defined as “a group of any records under the control of \nany agency from which information is retrieved by the name \nof the individual or by some identifying number, symbol, \nor other identifying particular assigned to the individual.”15\nImportantly, the Act places strict limits on “record \nmatching,” or linking between agencies, for the purposes of \nsharing information about individuals.16 Matching programs \nare only allowed when there is a written agreement in place \nbetween two agencies defining the purpose, legal authority, \nand the justification for the program; such agreements \ncan last for 18 months, with the option of renewal.17 These \nlimits were put in place in order to prevent the emergence \nof a centralized system of records that could track U.S. \ncitizens or permanent residents across multiple government \ndomains, as well as to limit the uses of data for the purposes \nit was collected. Indeed, while linkage across datasets may \nbe important for AI research,18 it could potentially enable \nabuse, surveillance, or the infringement of such rights such \nas free speech by enabling persecution across the many \nareas in which a U.S. citizen or resident interacts with the \nfederal system.19\nBecause the restriction on data linkages applies \nto linkages between agencies, the restriction applies in \ntwo particular scenarios for the NRC. First, if the NRC is \ninstituted as a federal agency, then agency data-sharing \nwith the NRC would run against the data linkages \nlimitation of the Privacy Act. Second, federal agency \nstaff access to the NRC could raise questions about \ninteragency data linkage under the Privacy Act. However, \nthe recommendation in Chapter 3 is focused on granting \nagencies streamlined access to the computing resources \non the NRC and their own agency data, not to any multi\u0002agency data hosted on the NRC. If the NRC is not designed \nas a federal agency and does not grant agency members \naccess to interagency data, the Privacy Act’s restrictions on \ndata linkages may not apply. \nWe note that this approach to data management \nis both unusual and out of step with the private sector, \nas well as AI research specifically. The ability for both \nindustry20 and researchers21 to associate multiple data \nsources and data points with a specific (anonymized) \nindividual is common practice outside of government. In \nfact, this limitation is not one that many governments22\nor U.S. states23 place on their data systems. However, \nthe Privacy Act’s restrictions on data linkage remains \nuncontested, even in the various reform efforts we discuss \nA Blueprint for the National Research Cloud 56\nCHAPTER 5\nbelow. It is worth noting that the federal government’s \nbroad bar against data linkages does incur welfare costs. \nFor example, during the COVID-19 pandemic, the inability \nto share and link public health data created difficulties \ntracking the spread and severity of the virus.24 While \nprojects like Johns Hopkins’ Coronavirus Research Center25\nand the COVID Tracking Project26 attempted to aggregate \navailable data, the lack of data integration slowed \nimportant operational and research responses.27 Other \ncountries, for instance, integrated immigration and travel \nrecords to triage cases and prevent hospital outbreaks.28\nWe acknowledge the potential for data linkage \nto tackle important societal problems without \nrecommending wholesale, unencumbered data linkage. \nBroad or unrestricted data linkage raises legitimate \nconcerns about both individual privacy and widespread \ngovernment surveillance,29 made concrete by the \ndisclosures of government whistle-blower Edward \nSnowden,30 among others. An initiative to link Federal \nAviation Administration (FAA) data with other agency \ndata for COVID-19 response, for instance, would meet \nresistance from the Privacy Act. The Task Force should \nappreciate these tensions and tradeoffs. Indeed, agencies \nview technical measures for privacy preservation \na necessary component of any government data \nstrategy, as methods such as multiparty computation or \nhomomorphic encryption (which we discuss in \nChapter 8) may allow for some forms of data linkages \nbetween agencies, without violating the Privacy Act.\nNo Disclosure Without Consent\nAnother core restriction of the Privacy Act is the \n“No Disclosure Without Consent” rule, which prohibits \ndisclosure of records to any agency or person without \nprior consent from the individual to whom the record \npertains.31 Because the NRC would disclose federal agency \ndata to researchers (i.e., to “person[s]”), this rule—unlike \nthe restriction on record linkage—is legally relevant and \nunavoidable. \nThe Privacy Act, however, contains a number of \nexceptions to this rule. Most pertinent to the NRC’s data\u0002sharing efforts are exemptions for: (1) “routine use”; (2) \nspecified agencies; and (3) statistical research. Under \nthe first exemption, the Privacy Act permits agencies to \ndisclose personally identifiable administrative data when \nsuch disclosure is among one of the “routine uses” of the \ndata.32 A dataset’s “routine use” is defined on an agency\u0002to-agency basis, and is simply a specification filed with \nthe Federal Register on the agency’s plan to use and share \nits data.33 As a result, the more broadly an agency defines \n“routine use” of its data, the more broadly that agency can \nshare its data with other agencies without disclosure.34\nWhile courts have limited how broadly an agency can \ndescribe “routine uses,”35 a large number of use cases can \nstill be covered by a short, general statement.36 Further \nresearch should be conducted on the conditions for when \ndata sharing for research purposes constitutes routine use. \nImplications for Data Sharing with Researchers\nMuch will rest on the interpretation of the “statistical \nresearch” exception, as applied to AI research. Despite \nthe Privacy Act’s constraints on data sharing, researchers \nhave conventionally been able to access data directly from \nagencies, based on the statistical research exception to \nthe Privacy Act. This exception allows disclosure of records \n“to a recipient who has provided the agency with advance \nadequate written assurance that the record will be used \nsolely as a statistical research or reporting record, and the \nrecord is to be transferred in a form that is not individually \nidentifiable.”37 Doing so requires either access to an approved \nresearch dataset, or for the researcher to negotiate an MOU \ndirectly with the agency, a role we suggest the NRC may be \nable to fill as an intermediary, acting as a negotiating partner \nto facilitate access requests between multiple researchers \nand agencies (discussed in Chapter 3). \nWhile the Privacy Act does not define “statistical \nresearch,” subsequent laws and policies have elaborated \non the definition. For example, the E-Government Act \ndefines “statistical purpose” to include the development \nof technical procedures for the description, estimation, or \nanalysis of the characteristics of groups, without identifying \nthe individuals or organizations that comprise such \ngroups.38 Meanwhile, a “nonstatistical purpose” includes \nthe use of personally identifiable information for any \nadministrative, regulatory, law enforcement, adjudicative, \nor other purpose that affects the rights, privileges or \nbenefits of any individual.39 That is, while researchers may \nA Blueprint for the National Research Cloud 57\nCHAPTER 5\nuse personally identifiable data for the broad purpose of \nanalyzing group characteristics, they cannot use such data \nfor targeted purposes to aid agencies with, for instance, \nspecific adjudicative or enforcement functions.\nThe precise meaning of “statistical purpose,” however, \nremains “obscure and the evaluation criteria may be \ndifficult to locate.”40 Yet, “statistical purpose” may well \nencompass data sharing for certain AI applications. The Act \nexplicitly designates the Bureau of Labor Statistics, Bureau \nof Economic Analysis, and the Census Bureau as statistical \nagencies that have heightened data-sharing powers for \nstatistical purposes.41 These agencies regularly use AI in \nconducting their statistical activities.42 While definitions \nof AI are themselves contested, statistical research may \nencapsulate at least some forms of machine learning and \nAI, if such research analyzes group characteristics43 and \ndoes not identify individuals. \nTo be sure, the NRC should not enable researchers or \nagencies to conduct an end run around the Privacy Act. \nTo that end, the NRC will require staff devoted to privacy \ncompliance and oversight to ensure compliance. Key \nquestions regarding individual identifiability, sensitivity of \nthe data, or the potential for linkage and reidentification \nwill need to be assessed by such staff. \nImplications for Agency Data Sharing with the NRC\nNotwithstanding the above avenues, agencies may \nnonetheless be reluctant to share data with the NRC and \nits researchers. Instances abound where federal agencies \nface constraints to sharing data, even if it is entirely legal \nor even federally mandated. For example, the Uniform \nFederal Crime Reporting Act of 1988 requires federal law \nenforcement agencies to report crime data to the FBI.44\nYet, no federal agencies appear to have shared their data \nwith the FBI under this law.45 Similarly, the Census Bureau \nis enabled by legislation that authorizes it to obtain \nadministrative data from any federal agency and requires \nit to try to obtain data from other agencies whenever \npossible.46 However, the statute does not similarly require \nthe program agencies to provide their data to the Census \nBureau. That is, although the Census Bureau is required \nto ask other agencies for data, those agencies are not \nrequired to, and often do not, provide it.47\nFailure to engage in data sharing, even in the face of \na statutory authorization, can stem from risk aversion. \nAccording to a GAO report, agencies choose not to share \ndata because they tend to be “overly cautious” in their \ninterpretation of federal privacy requirements.48 Because \nlegal provisions authorizing or mandating data sharing \nare often ambiguous,49 agencies may err on the side of \ncaution and choose not to share their data for fear of the \ndownside risk that recipient use of the data may violate \nprivacy or security standards.50 To make matters worse, \nbecause agencies need to devote significant resources \nto facilitate data sharing, they may simply choose not to \nprioritize data sharing at all. The lack of resources poses a \nsignificant problem; according to a Bipartisan Policy Center \nstudy on agency data sharing, about half of agencies cited \ninadequate funding or inability to hire appropriate staff as \ntheir “most critical” barrier to data sharing.51\nThe NRC may overcome these hurdles by clarifying \nlegal provisions, ensuring that the benefits to agencies of \ndata sharing outweigh the risks and costs, and advocating \nfor resources. For instance, O’Hara and Medalia describe \nhow the Census Bureau was able to obtain food stamp and \nwelfare data from state agencies. In the face of ambiguous \nstatutes authorizing the U.S. Department of Agriculture \n(USDA) and the U.S. Department of Health and Human \nServices (HHS) to perform data linkages across federally \nsponsored programs, states originally arrived at different \nstatutory interpretations. Some states agreed to share their \ndata only after (1) the Office of General Counsel at both the \nUSDA and HHS issued a memo clarifying that data sharing \nwith the Census Bureau for statistical purposes was legal \nand encouraged; and (2) the states were convinced that data \nsharing would enable evidence building that could help \nthem administer their programs.52\n[T]he NRC should not enable \nresearchers or agencies to \nconduct and end run around \nthe Privacy Act.\nA Blueprint for the National Research Cloud 58\nCHAPTER 5\nBroader data sharing with the NRC that combines \nmultiple agency or external data sources may be facilitated \nby the passage of additional laws requiring agencies to \nshare their data, subject to specific limitations on how \nthat data is used by the NRC. Even then, the effect of that \nrequirement is hardly a foregone conclusion. More is \nneeded by way of both clarifying the extent to which data \nsharing is permitted and providing benefits that incentivize \nagencies to share their data.\nCASE STUDY: ADMINISTRATIVE \nDATA RESEARCH UK\nAdministrative Data Research UK (ADR UK) is a new body, set up \nin July 2018, to facilitate secure, wide access to linked administrative \ndatasets from across government for the purpose of public research.53\nADR UK was set up as a central, coordinating point between four \nnational partnerships—ADR England, ADR Northern Ireland, ADR \nScotland, and ADR Wales—as well as the UK-wide national statistics \nagency, Office for National Statistics (ONS). ADR UK labels itself as \na “UK-wide strategic hub”: a central point that promotes the use of \nadministrative data for research, engages with government departments \nto facilitate secure access to data, and funds public good research that \nuses administrative data.54\nFunding for ADR UK came from a research council (Economic and \nSocial Research Council, ESRC) and was initially committed from July \n2018 to March 2022. A total of £59 million was provided.55\nADR UK serves three core functions. First, the promotion of the value \nand availability of government administrative datasets for research. ADR \nUK acts as a general advocate for the use of administrative datasets \nfrom across the British government. It also acts as a specific driver of \nresearch for public good: It has identified specific areas of research that \nare of pressing policy interest (e.g., “world of work”56), and is focusing \non creating access to linked datasets for researchers who tackle those \npriority themes.\nThe second core function is serving as a coordination point \nto encourage government data sharing, standards, and linkage of \nadministrative datasets. Especially for its research calls, ADR UK is able \nto highlight multiple datasets, often spanning different government \ndepartments’ scope areas that can be linked and used in research. \nIn doing so, ADR UK plays an important role in facilitating research.\nFinally, to ensure compliance with the Privacy Act, as \nwell as to facilitate the NRC’s role as a data intermediary, \nthe NRC will require a staff of privacy professionals that \ninclude positions tasked with legal compliance, oversight, \nand technical methods expertise. These professionals \nshould build relationships with peers across agencies to \nfacilitate data access.\nKEY TAKEAWAYS\n Proactive advocacy for data \nuse and linkage: Given the \nrange of agencies and data \nsources in government, \nhaving a single, coordinated \nvoice of advocacy for data \nuse and linkage of public \ndatasets for public good is an \nimportant function.\n Bringing external talent \ninto government data use: \nADR UK has two schemes—\nResearch Fellowships and \nMethod Development \nGrants—that target \nexceptional, external talent \nwith the intention of building \nawareness and use of public \ndatasets in cutting-edge \nresearch.\n Small grant funding to \naccelerate research methods \nthat use large datasets: \nBy putting out calls for \nresearch that answers broad \nthemes, ADR UK is able to \ncorral a range of datasets \nin answering research \nquestions and avoids a single \ndisciplinary focus.\nA Blueprint for the National Research Cloud 59\nCHAPTER 5\nThird, ADR UK has a strategic funding approach to further the use of administrative datasets in research \nthat has three categories of funding:\n• Building new research datasets: ADR UK’s Strategic Hub Fund initially solicited invitation\u0002only bids for researchers who would build new research datasets of public significance in the \ncourse of their work.57 These new, research-ready datasets are now accessible to a wide range of \nresearchers.58\n• Research Fellowship Schemes: A major funding focus now is on funding research through \ncompetitive open-bid invitations under a Research Fellowship Scheme.59 Specific researchers are \nidentified through the competition. They are accredited for secure data access and placed right \nat the heart of government (with 10 Downing Street), with access to linked datasets to answer \nquestions of public significance.60\n• Methods Development Grants: Separately, ADR UK invites research proposals that further \nmethodological progress for the use of large-scale administrative datasets, such that the wider \nsocial science community can draw on developed methods in research.61\nPrivacy and Security\nThe UK’s 2017 Digital Economy Act62 created a legal gateway for research access to secure government \ndata. Deidentified data held by a public authority in connection with the authority’s functions could be \ndisclosed for research, under the assurance that individual identities would not be specified.\nAny data shared with researchers is anonymized: Personal identifiers are removed, and checks are \nmade to protect against re-identification.63 A rigorous accreditation process—for both the researcher and \nproposed research—is undertaken to ensure public benefit. Data access primarily takes place via a secure \nphysical facility, or a secure connection to that facility, provided by ADR UK’s constituent partners.64 There is \nclose monitoring of researcher activity and outputs, and any output is checked before release.65\nFrom a researcher’s point of view, access to ADR UK datasets requires the following steps:66\n• Researcher submits proposal for project to ADR UK.\n• Project is approved by relevant panels.\n• Researcher engages in training and may take assessment (e.g., access to linked data held by ONS \nrequired accreditation to ONS’ Secure Research Service,67 and can access data either in person or, \nwhere additionally accredited, through remote connection).\n• Required data is determined by ADR UK (through one of the four regional partners, or ONS), then \ningested by the relevant data center.\n• De-identified data is made available through a secure data service (either at the ONS, or one of the \nfour regional partners).\n• Researcher conducts analysis; activity and outputs are monitored.\n• Outputs are checked for subject privacy. Research serving the public good is published.\nCASE STUDY: ADMINISTRATIVE DATA RESEARCH UK (CONT’D)\nA Blueprint for the National Research Cloud 60\nCHAPTER 5\nCOMPLEMENTARY EFFORTS TO \nIMPROVE THE FEDERAL APPROACH \nTO DATA MANAGEMENT\nThe barriers to data sharing created by the Privacy \nAct have long posed a challenge to researchers interested \nin using government data to evaluate or inform policy.68\nThe policy and statistical research communities, both \nwithin and outside the federal government, have engaged \nin admirable reform efforts to facilitate data sharing for \npolicy evaluation.69\nThe Foundations for Evidence Based Policymaking \nAct (EBPA) of 2018, which enacted reforms to improve \ndata access for evidence-based decision-making, is a key \nachievement of these efforts to date. However, several of \nthe provisions in the Act that helped to address some of \nthe barriers to data linking and sharing were not passed \nby Congress. These provisions—known collectively as \nthe National Secure Data Service (NSDS)—remain a high \npriority for facilitating further progress for sharing data \nfor research purposes. According to the nonprofit Data \nFoundation, one of the major supporters of the NSDS, its \npassage will “create the bridge across the government’s \ndecentralized data capabilities with a new entity that \njointly maximizes data access responsibilities with \nconfidentiality protections.”70\nThe NSDS is envisioned as an independent legal \nentity within the federal government that would have \nthe legal authority to acquire and use data. However, this \nauthority is currently conceived of as emanating from the \nEBPA, which focuses on using statistical data for evidence\u0002building purposes. A broader source of authority may be \nnecessary for AI research purposes under the NRC, which \nmay be distinct from agency obligations. One clear area \nof overlap is the proposal’s call for the NSDS to facilitate \nits own computing resources, which could be harmonized \nwith the compute needs of the NRC. Similar to Chapter 4’s \ndiscussion of organizational options, NSDS supporters \nidentify a fundamental need for both a reliable funding \nsource as well as thoughtful placement of the NSDS either \nwithin an existing agency or as an independent agency or \nFFRDC. The areas of common ground between the NRC \nand NSDS, as well as the expertise and momentum behind \nthe proposal, strongly suggest that the NRC engage and \ncoordinate with these efforts. \nAnother complementary initiative is the Federal Data \nStrategy (FDS), launched in 2018 by the executive branch \nand led by the OMB. FDS is a government-wide effort to \nreform how the entire federal government manages its \ndata. The plan calls out the need for “safe data linkage” \nthrough technical privacy techniques,71 and incorporates \na directive from the 2019 Executive Order on Maintaining \nAmerican Leadership in Artificial Intelligence to “[e]\nnhance access to high-quality and fully traceable federal \ndata, models, and computing resources to increase the \nvalue of such resources for AI R&D, while maintaining \nsafety, security, privacy, and confidentiality protections, \nconsistent with applicable laws and policies.”72 The FDS \ndirects OMB to “identify barriers to access and quality \nlimitations” and to “[p]rovide technical schema formats \non inventories,” with a focus on open data sources (i.e., \nnon-sensitive or individually identifying data).73 Datasets \nidentified by this process could be key candidates for \npopulating the NRC. \nWhile both the NSDS and the FDS may promote data \nsharing, these efforts are presently focused primarily on \nfurthering policy evaluation purposes. Fortunately, there \nis much overlap and complementarity between these \ninitiatives and the NRC, illustrating the broad importance \nof more effective mechanisms to share federal data \nsecurely and in a privacy-protecting way. \nA Blueprint for the National Research Cloud 61\nCHAPTER 6\nChapter 6: Technical \nPrivacy and Virtual \nData Safe Rooms\nWe now discuss the role of technical privacy methods for the NRC. In the past \nseveral decades, researchers have devised a variety of computational methods that \nenable data analysis while preserving privacy. These methods hold considerable \npromise for enabling the sharing of government data for research purposes. We note \nat the outset that technical methods are merely one mechanism to strengthen privacy \nprotections. While effective, such methods may be neither sufficient nor universally \nappropriate. The application of any particular method does not obviate the need \nto inquire into whether the data itself adheres to articulated privacy standards. The \nmethods discussed here are not “replacements” for the recommendations discussed \nearlier and never themselves justify the collection of otherwise problematic data. \nUse of data from the NRC introduces two threats to individual privacy. The \nfirst type involves accidental disclosure by agencies (agency disclosure): An agency \nuploads a dataset to the NRC which lacks sufficient privacy protection and contains \nidentifying information about an individual. A researcher—either analyzing this \ndataset alone or in conjunction with other NRC datasets—discovers this information \nand re-identifies the individual.1\n The second type involves accidental disclosure by \nresearchers (researcher disclosure). Here, a researcher releases products computed on \nrestricted NRC data (e.g., trained machine learning models, publications). However, \nthe released products lack sufficient privacy protection, and an outside consumer of \nthe research product learns sensitive information about an individual or individuals in \nthe original dataset used by the researcher.2\nWe recommend that, due to the infancy and uncertainty surrounding uses of \nprivacy-enhancing technologies, privacy should primarily be approached via access \npolicies to data. While there will be circumstances that suggest, or even mandate, \ntechnical treatments, access policies, discussed in Chapter 3, are the primary line of \ndefense: They ensure sensitive datasets are protected by controlling who can access \nthe data. We recommend a tiered access policy, with more sensitive datasets placed \nin more restricted tiers. For instance, highly restricted access data may correspond \nto individual health data from the VA, while minimally restricted access data may \ncorrespond to ocean measurements from NOAA. Proposals requesting access to highly \nrestricted data would face heightened standards of review, and researchers may \nbe limited to accessing only one restricted access dataset at a time. This approach \nmirrors current regimes where researchers undergo special training to work with \ncertain types of data.3\nKEY \nTAKEAWAYS\n Technical privacy measures \nare useful, but not substitutes \nfor securing data privacy \nthrough access policies. \n In some instances, the NRC or \nagencies may wish to make \naccess to data conditional on \nthe use of technical privacy \nmeasures.\n Contributing agencies and \nthe NRC should collaborate to \ndetermine technical privacy \nmeasures based on dataset \nsensitivity, dataset utility, and \nequity implications. \n The NRC must have technical \nprivacy staff to administer \ntechnical privacy treatments, \nas well as to support \nadversarial privacy research.\n The NRC should explore \nadopting virtual “data \nsafe rooms” that enable \nresearchers to access \nraw administrative data \nor microdata in a secure, \nmonitored, and cloud-based \nenvironment.\nA Blueprint for the National Research Cloud 62\nCHAPTER 6\nTechnical treatments are a different line of defense: \nThey significantly reduce the chances of deanonymizing a \ndataset. There are a range of technical methods that can \nenable analysis while ensuring privacy:\n• Techniques like k-anonymity and ℓ-diversity \nattempt to offer group-based anonymization by \nreducing the granularity of individual records in \ntabular data.4 While effective in simple settings and \neasy to implement, both methods are susceptible \nto attacks by adversaries who possess additional \ninformation about the individuals in the dataset.\n• One of the most popular techniques is differential \nprivacy,5\n which provides provable guarantees \non privacy, even when an adversary possesses \nadditional information about records in the dataset. \nHowever, differential privacy requires adding \nrandom amounts of statistical “noise” to data and \ncan sometimes compromise the accuracy of data \nanalyses. Although differential privacy has become \na point of contention with respect to the Census \nBureau’s new disclosure avoidance system,6\n the \ntechnique remains a powerful defense against bad \nactors seeking to take advantage of public data for \nthe purposes of re-identification.\n• Researchers have also identified other promising \nmethods. Recent work has demonstrated that \nmachine learning can be used to generate \n“synthetic” datasets, which mirror real world \ndatasets in important ways but consist of entirely \nsynthetic examples.7\n Other work has focused on \nthe incorporation of methods from cryptography, \nincluding secure multiparty computation8 and \nhomomorphic encryption.9\nMethods that obscure data introduce fundamental \ntensions with the way machine-learning researchers \ndevelop models. For example, when considering \nquestions of algorithmic fairness, in some instances \nprivacy protections can undercut the power to assess \nwhether such a technical method as differential privacy \nresults in demographic disparities, particularly for small \nsubgroups.10 Similarly, “error analysis”—the study of \nsamples over which a machine-learning model performs \npoorly—is central to how researchers improve models. It \nrequires understanding the attributes and characteristics \nof the data in order to better understand the deficiencies \nof an algorithm. Therefore, such methods as differential \nprivacy, which make raw data more opaque, will invariably \nimpede the process of error analysis. Synthetic data \ntypically captures relationships between variables only \nif those relationships have been intentionally included \nin the statistical model that generated the data,11 and \nthus, may be poorly suited to certain AI models that \ndiscover unanticipated relationships among data. \nWhile homomorphic encryption may not require similar \nassumptions on data structure, existing methods are \ncomputationally expensive.\nWhile promising, understanding and applying these \nmethods is an evolving scientific process. The NRC is \npoised to contribute to their evolution by directly \nsupporting research into their application. \nCRITERIA AND PROCESS FOR \nADOPTION\nThe NRC will contain a rich array of datasets, each \npresenting unique privacy implications over different \ntypes of data formats (e.g., individual tabular records, \nunstructured text, images). Including a dataset on the \nNRC raises a question of choice: Which technical privacy \ntreatment should be applied (e.g., k-anonymity vs. \ndifferential privacy), and how should it be applied? This \nquestion often requires technical determinations about \ndifferent algorithmic settings, but such technical choices \ncan also have important substantive consequences.12\nFirst, we recommend that these determinations are \nmade with respect to the following factors: \n• Dataset sensitivity: Different datasets will \npose privacy risks that range in type and \nmagnitude. Health records, for instance, are \nmore sensitive than weather patterns. The \nprivacy method chosen should reflect this \nsensitivity. As we discuss in Chapter 3, these \nprivacy methods should correspond and be tiered \nA Blueprint for the National Research Cloud 63\nCHAPTER 6\nto the appropriate FedRAMP classification for the \ndataset.\n• Dataset utility: As discussed above, applying \na privacy method can distort the original data, \ndiminishing the accuracy and utility of analysis. \nBecause different methods affect different levels \nof distortion, the choice of method should be \ninformed by the perceived utility of the data. \nHigh-utility datasets—where accurate analyses \nare highly important (e.g., medical diagnostic \ntools)—may necessitate methods that produce \nless distortion. \n• Equity: Certain privacy measures can \ndisproportionately impact underrepresented \nsubgroups in the data.13 In determining which \nmethod to apply, the presence of sensitive \nsubgroups and their relation to the objectives of \nthe dataset should be evaluated. \nFor any given dataset, we recommend that agencies \nproviding the data collaborate with NRC staff to identify and \nrecommend any privacy treatments. Originating agencies \nand NRC staff will possess domain and research expertise \nto make evaluations on the balance of privacy, utility, and \nequity, but agencies should consult with NRC staff and \nresearchers on the most appropriate treatments. Given the \ncost of review, such privacy treatments should be much less \nwidely considered for low-risk datasets. \nVIRTUAL DATA SAFE ROOMS \nFor individual research proposals that would be greatly \nhampered by technical privacy measures, the NRC should \nexplore the use of virtual “data-safe rooms” that enable \nresearchers to access raw administrative microdata in a \nsecure, monitored environment. Currently, the Census \nBureau implements these safe rooms in physical locations \nand moderates access to raw interagency data through \nits network of Federal Statistical Research Data Centers \n(FSRDCs). However, the NRC should not adopt the FSRDC \nmodel wholesale. Indeed, the barriers to using FSRDCs \nare high, and “only the most persistent researchers are \nsuccessful.”14 For instance, applying for access and gaining \napproval to use an FSRDC takes at least six months, requires \nobtaining “Special Sworn Status,” which involves a Level \nTwo security clearance, and is limited to applicants who \nare either U.S. citizens or have been U.S. residents for \nthree years.15 To further complicate matters, agencies \nhave different review and approval processes for research \nprojects that wish to access agency data using an FSRDC.16\nFinally, even after approval is granted, researchers can only \naccess the data in person by going to secure locations, such \nas the FSRDC itself.17 \nTo be clear, some of these restrictions are unique to \nthe Census Bureau. U.S. law provides that any Census \ndatasets that do not fully protect confidentiality may only \nbe used by Census staff.18 Researchers trying to access such \ndata therefore must go through the rigorous process of \nbecoming a sworn Census contractor. The extent to which \nthese restrictions apply to the NRC will depend on whether \nthe NRC institutionally houses itself in the Census Bureau, \nwhich we ultimately do not recommend.19 Other problems, \nhowever, such as the lack of interagency uniformity in \ngranting access to datasets is not a problem unique to \nCensus, but a common problem throughout the federal \ngovernment (see Chapter 3).\nAnother common problem—not necessarily tied to \nFSRDCs or the Census Bureau—is the use of a physical data \nroom to access raw microdata. The NRC should explore a \nvirtual safe room model, whereby researchers can remotely\naccess such microdata. For instance, in the private sector, \nthe nonpartisan and objective research organization, NORC, \nlocated at the University of Chicago, is a confidential, \nprotected environment where authorized researchers can \nsecurely store, access, and analyze sensitive microdata \nremotely.20 Some federal government agencies have also \nimplemented their own virtual data safe rooms. The Center \nfor Medicare and Medicaid Services’ Virtual Research Data \nCenter (VRDC), for instance, grants researchers direct \naccess to approved data files through a Virtual Private \nNetwork.21 In a 2019 Request for Information (RFI), the \nNational Institutes of Health also solicited input for its own \nadministrative data enclave and whether such an enclave \nshould be physical or virtual.22 As articulated in responses \nto the RFI from the American Society for Biochemistry and \nMolecular Biology and the Federation for of American \nA Blueprint for the National Research Cloud 64\nCHAPTER 6\nSocieties for Experimental Biology, a virtual enclave would \ngreatly facilitate researcher access to data and can be \ndesigned and administered in a way to preserve privacy \nand security.23\nA National Research Cloud cannot function effectively \nif access to certain datasets is ultimately tied to a National \nResearch Room. \nCASE STUDY: \nCALIFORNIA POLICY LAB \nThe California Policy Lab (CPL) is a University \nof California research institute that provides \nresearch and data support to help California state \nand local governments craft evidence-based \npublic policy.24 CPL offers a variety of services to \ngovernments, including data analysis services and \nsecure infrastructure for hosting and linking the vast \namounts of data collected by government entities.25\nThese services help bridge the gap between \nacademia and government by helping policymakers \ngain access to researchers and providing researchers \na secure way to access administrative data. CPL \naims to build trusting partnerships with government \nentities and enable them to make empirically \nsupported policy decisions.\nCPL enters data-use agreements with various \ngovernment entities around California, including, for \nexample, the California Department of Public Health \nand Los Angeles Homeless Services Authority.26\nThese agreements allow CPL to store administrative \ndata in a linkable format, promoting broad \nlongitudinal analyses across various public sector \ndomains.\nTo help manage the requirements of the various \ndata-use agreements and simplify compliance, \nCPL applies the strictest requirements for any \nindividual data across all data it stores.27 Each set of \nadministrative data is thus subject to strict technical \nrestrictions and thorough audits.28 CPL manages the \ndata in an on-premises data hub at UCLA. This data \nhub uses “virtual enclaves” modeled after air-gapped \nclean rooms typically used for sensitive government \ndata.29 Virtual enclaves are virtual machines that \nforbid any outbound connections.\nCPL creates a \nnew virtual enclave for \neach research project \nand only gives specific \nresearchers access \nto specific datasets \nfor each project.30\nResearchers can only \nwork with the data in \nthe enclave and can \nonly use tools provided \nin the environment. \nData access processes \nvary, based on the \nrequirements for the \ngovernment entities, \nand most of CPL’s data\u0002use agreements are \npurpose limited and thus \nrequire approval from \nthe relevant government \nentity before being used \nin a project.31\nGenerally, CPL helps researchers understand how \nto gain access to various types of administrative data. \nFor some datasets, CPL has formalized applications \non its website.32 CPL prescreens project proposals and \nsends promising projects to its government partners \nfor final approval. Researchers then conduct these \napproved projects on CPL’s secure infrastructure. For \nother datasets without formalized access processes, \nCPL directs researchers toward individuals within \nthe government entities.33 CPL can then take over \nmanagement of approved projects that aim to use \ndata stored on its hub under their standing data-use \nagreements. Alternatively, the government entities \nand researchers themselves may craft new data-use \nagreements for specific projects.\nKEY TAKEAWAYS\n Virtual enclaves: \nThe CPL heavily \nutilizes secure \nvirtual enclaves \nfor researchers to \naccess, work with, \nand perform data \nlinkages across \nsensitive datasets. \n Acting as an \nintermediary: CPL \nfacilitates and \nstreamlines access \nto administrative \ndata by acting as \nan intermediary \nbetween researchers \nand relevant state \nagencies.\nA Blueprint for the National Research Cloud 65\nCHAPTER 6\nIMPLICATIONS FOR THE NRC\nDEDICATED STAFF\nAs discussed above, it will be critical for the NRC to \nmaintain a dedicated professional staff who specialize in \nprivacy technologies. First, not all agencies or departments \nthat seek to place data into the NRC will have the expertise \nto both determine the privacy method that meets data \nutility expectations and data privacy demands, and apply \nit to the dataset of interest. Specialized NRC staff will be \nessential to assisting such agencies and departments. \nSecond, even where agencies and departments do \npossess the requisite expertise, NRC staff will bring a \nunique perspective from their collaborations across the \ngovernment. Where a specific department’s staff may \nonly foresee risks specific to the dataset, NRC staff will \nbe able to foresee instances where the presence of other \ndata in the NRC may raise other concerns. In fact, by \nworking with Affiliated Government Agencies and agency \nrepresentatives, the NRC staff can also help these agencies \ninternalize such benefits as helping them understand the \nfull range of privacy risks with respect to their data.34 Such \ncollaborative governance will be necessary to ensure that \nprivacy assessments consider the full implications of access \nand privacy technologies. Finally, it must not be overlooked \nthat while data management in general requires technical \nexpertise, these various privacy-enhancing technologies \nalso require very specific, highly skilled expertise. Using \nsynthetic data sets as an example, NRC staff could be asked \nto build synthetic data on an agency’s behalf, or need \nto validate the work performed at an agency to ensure \nit is done properly and well. Whatever the task, there \nare cascading effects downstream through the research \necosystem if not carefully managed and executed.\nA FOCUS ON EVALUATING AND RESEARCHING PRIVACY\u0002ENHANCING TECHNOLOGIES\nIt will be necessary to continually evaluate the \nstate of privacy protections on the NRC, either by NRC \nstaff members or by supporting privacy and security \nresearchers at academic institutions. Technical privacy \nand security research is by nature adversarial: Researchers \nadopt the posture of adversaries in order to probe the \nweaknesses of a system/dataset. In the context of the \nNRC, this will require simulating attacks as researchers \ntry to reidentify individuals within specific NRC datasets. \nThis type of research is necessary to advance the field, \nand the NRC may be specially positioned to support a \nresearch center devoted to researching privacy-enhancing \ntechnologies. Doing so would allow the research \ncommunity to build stronger privacy methods to ensure \nanonymity, identify flaws, and self-regulate an evolving \ndata ecosystem.\nA National Research Cloud cannot \nfunction effectively if access to \ncertain datasets is ultimately tied \nto a National Research Room. \nA Blueprint for the National Research Cloud 66\nCHAPTER 7\nChapter 7: Safeguards \nfor Ethical Research \nThe pace of advances in AI has sparked ample debate about the principles that \nshould govern its development and implementation. Despite the technology’s promise \nfor economic growth and social benefits, AI also poses serious ethical and societal risks. \nFor example, studies have demonstrated AI systems can propagate disinformation,1\nharm labor and employment,2 demonstrate algorithmic bias along age, gender, race, and \ndisability,3 and perpetuate systemic inequalities.4\nThis chapter considers how the NRC should ensure its resources are deployed \nresponsibly and ethically. A growing body of research on AI fairness, accountability, and \ntransparency has raised serious and legitimate questions about the values implicated by \nAI research and its impact on society.5 The NRC’s focus on increasing access to sources of \npublic data and fostering noncommercial AI research is intended to help address these \nconcerns by enabling broader opportunities for academic research. At the same time, \nbroadening access to resources is not enough to assure that academic AI research does \nnot exacerbate existing inequalities or perpetuate systematic biases. In addition, the NRC \nmust also be prepared to handle and act upon complaints of unethical research practices \nby researchers. \nWhile there is an abundance of proposed ethics frameworks for AI (see Appendix C for \nthose published by federal agencies), there is not a set of accepted principles enshrined \ninto law, like the Common Rule for human subjects research, that clearly establishes the \nboundaries for ethical research with AI.6 Lacking such guidance, a core question for the \nNRC is how to institutionalize the consideration of ethical concerns. This chapter starts by \ndiscussing two potential approaches for research proposals: ex ante review at the proposal \nstage for access to NRC resources (e.g., compute, dataset), and ex post review after \nresearch has concluded. Separately, we discuss guidance for the NRC on issues related to \nresearch practices. One of the virtues of starting with access by Principal Investigator (PI) \nstatus (Chapter 2) is that researchers will (a) often have undergone baseline training by \ntheir home institutions in research compliance, privacy, data security, and practices for \nresearch using human subjects; and (b) be subject to research standards and peer review \n(e.g., through IRB review when applicable). These mechanisms are insufficient to cover \nmany AI research projects, such as when human subjects review is deemed inapplicable. \nThus, we tailor our recommendations to the institutional design of the NRC. \nFirst, we recommend that the NRC require including an ethics impact statement for PIs \nrequesting access beyond base-level compute, or for research using restricted datasets. \nThis provides a layer of ethical review for the highest resource projects that are already \nrequired to undergo a custom application process. Second, for other categories of research \n(e.g., research conducted under base-level compute access, where no custom review is \nKEY \nTAKEAWAYS\n Researchers requesting \naccess to compute \nbeyond the default \nallocation and/or \nrestricted data (i.e., \nthose undergoing a \ncustom application \nprocess) should be \nrequired to provide an \nethics impact statement \nas part of their \napplication.\n The NRC should \nestablish a process \nto handle complaints \nabout unethical research \npractices or outputs. \n Eligibility based on \nPrincipal Investigator \nstatus will ensure \nsome review under the \nCommon Rule as well \nas through peer review, \nbut we recommend \nuniversities consider \nmore comprehensive \nmodels for assessing the \nethical implications of AI \nresearch.\nA Blueprint for the National Research Cloud 67\nCHAPTER 7\ncontemplated), we recommend that the NRC establish \na process for handling complaints that may arise out of \nunethical research practices and outputs. Third, given \nthe limitations of the prior mechanisms, we recommend \nthe exploration of a range of measures to address ethical \nconcerns in AI compute, such as the approach taken by the \nNational Institutes of Health to incentivize the embedding \nof bioethics in ongoing research. \nETHICS REVIEW MECHANISMS\nEX ANTE\nEx ante review assesses research yet to be performed.7\nFunding agencies and research councils worldwide rely on \nex ante peer reviews to evaluate the intellectual merit and \npotential societal impact of research proposals, based on \nset criteria.8 Institutional Review Boards (IRBs) commonly \nassess academic research involving human subjects prior \nto its initiation.9 However, much AI-related research may \nnot fall under IRB oversight, as the research may not use \nhuman subjects or rely on existing data (not collected by \nthe proposers) about people that is publicly available,10\nused with permission from the party that collected the \ndata, or is anonymized. Potential ethical issues may, \ntherefore, escape IRB review.11\nCreating an across-the-board ex ante ethics review \nprocess, however, would be challenging. First, as we \ndiscuss in Chapter Two, we recommend against case-by\u0002case review for all PI requests for access to NRC compute \nand data, as such a process would require substantial \nadministrative overhead. At the stage when researchers \nare simply applying for compute access, the research \nmay be so varied and early stage, that there is not much \nconcrete to review. And to the extent that every PI would \nrequire project-specific review, such a process would be \nonerous. \nSecond, ex ante review is unlikely to grapple with the \nmany ethical implications of design decisions that take \nplace after research commences.12 Research design can \nchange substantially from initial proposals as projects \nprogress. Ex ante review could identify some concerns, \nbut unlikely all.13 The nature of machine learning is \ninherently uncertain—and predictions can be challenging \nto explain—as well as highly dependent on the data used \nto build and train models.14 Ex ante proposal review alone \nmay not be sufficient to identify biased outcomes, and \nmay in fact require extensive documentation and review \nof the data used in a specific project to assess with any \nreliability.15\nThird, there are unique academic speech concerns \nabout government assessment of research. Authorizing the \ngovernment to conduct an ethics review (separate from \nIRB review under the Common Rule, which is typically \ndelegated to academic institutions) with vague standards \nmay implicate academic speech concerns, as well as \nsubject proposals to politically driven evaluation that can \nshift from administration to administration.\nIf the NRC were to create a process for ex ante \nreview of research proposals for ethical concerns, such \na board would likely need to be composed of scientific \nand ethics experts, similar to how the NSF conducts their \nprocess, though perhaps with the addition of members \nfrom civil society organizations that focus on countering \nAI harms. The NSF convenes groups of experts from \nacademia, industry, private companies, and government \nagencies as peer reviewers, led by NSF program officers \nand division directors.16 However, the scope and range \nof NRC research proposals are likely to be both broad, \nand highly interdisciplinary in nature, making ethics \nassessments challenging. \nEX POST\nEx post evaluations provide an assessment after \nresearch has concluded.17 In academia, researchers \nsubmit research results to journals or conferences for ex \npost peer review; it is at this pre-publication stage that \nethical issues not identified by ex ante processes may be \nsurfaced by reviewers or editors. In the public sector, for \nexample, the Privacy and Civil Liberties Oversight Board \n(PCLOB) conducts ex post reviews on counterterrorism \npractices by executive branch departments and agencies \nto ensure they are consistent with governing laws, \nregulations, and policies regarding privacy and civil \nliberties.18 PCLOB has also recently begun to evaluate the \nA Blueprint for the National Research Cloud 68\nCHAPTER 7\nuse of new technologies in foreign intelligence collection \nand analysis,19 and to identify legislative proposals that \nstrengthen its oversight of AI for counterterrorism.20\nRECOMMENDATIONS\nWhile we do not recommend across-the-board ex \nante review of research proposals, we do recommend that \nthe NRC establish a process to handle complaints about \nethical research practices and outputs. On that point, \nwe recommend the NRC collaborate with the Office of \nResearch Integrity (ORI) at the Department of Health and \nHuman Services to model their processes and procedures \nfor managing issues of research misconduct.21 The ORI \nhas substantial experience overseeing concerns about \nethical research practices. Parties could petition the NRC \nto revoke access when research is shown to manifestly \nviolate general ethical research standards or practices \napplicable to a researcher’s disciplinary domain. We note \nthat the NRC may want to adopt a high standard for such \na violation, given the academic speech considerations. For \nexample, federal agencies or external parties that wish to \nrevoke compute or data access from PIs would need to file \na written complaint with supporting evidence. Decisions \nto revoke access should require input from NRC executive \nleadership and legal counsel. \nFor PIs requesting access beyond base-level compute \nor for restricted datasets, we recommend requiring the \ncompletion of ethics impact statements to be submitted \nwith research proposals. A recent proposal to address \nthe lack of “widely applied professional ethical and \nsocietal review processes” in computing piloted such a \nrequirement in a grant process, requiring a description of \nthe potential social and ethical impacts and mitigation \nefforts by researchers.22 We limit this approach to \nproposals for compute access beyond default allocation \nor requests for access to restricted datasets, as the \nadministrability concerns are weaker for researchers who \nare already applying for compute or data access beyond \nthe default levels. For those applications, a review \nprocess of a specific proposal will already occur by an \nexternal review panel of experts (Chapter 2), and, much \nlike the NSF requires statements of “Broader Impacts;”23\nstatements about the ethical considerations of the work \ncould easily be included. It is important to note that \nethics impact statements would be only one component \nof NRC applications and should be weighed in \nconjunction with other application materials. In addition \nto requiring researchers to carefully think through and \ndocument the potential impacts of their own work, the \nstatements may also serve as useful documentation of \npotential negative impacts and be of use to NRC staff \nwhen determining whether to provide access to specific \ntypes of data. Such assessments may also be helpful for \njournals, conferences, or universities addressing ex post \nconcerns about ethical impacts. \nNext, we recommend that the NRC employ a \nprofessional staff devoted to ethics oversight, similar \nto what we propose regarding data privacy in Chapters \n5 and 6. In addition to staff devoted to handling legal \ncompliance issues, the NRC needs staff with specialized \ntraining in AI ethics (as well as expertise in other \nsubdomains) to provide expert internal consulting to \nNRC applicants, as well as to aid in evaluating ethics \nimpact statements. Similarly, data privacy experts can \nidentify ethical privacy issues specifically related to data, \nsuch as whether consent has been properly obtained \nand documented. To ensure that decisions are based on \nthe merits, the NRC staff overseeing these issues must \noperate independently of other federal agencies and be \ninsulated from political interference. \n[E]mbedded ethics approaches \nmay . . . identify[] and address[] \nissues as the research proceeds, \nin contrast to ex ante review, \nwhere it may be too early to spot \nan issue, and ex post review, \nwhich may be too late.\nA Blueprint for the National Research Cloud 69\nCHAPTER 7\nWe acknowledge that these ethics review mechanisms \nmay not identify all instances where researchers use \nthe NRC in a way to conduct research that raises ethical \nquestions. Few review mechanisms could, particularly \nin light of the considerable ambiguity present in \nethics standards (see Appendix C). Nonetheless, these \nmechanisms can augment key academic checkpoints (IRB \nreview and peer review) in an administrable fashion that \ndoes not raise serious concerns about academic speech. \nLastly, we recommend that non-NRC parties explore \na range of measures to address ethical concerns in AI \ncompute. These may include an ethics review process or \napproaches widely deployed in bioethics by the National \nInstitutes of Health, namely to incentivize the embedding \nof ethicists in research projects.24 Such embedded \nethics approaches may have the particular advantage \nof identifying and addressing issues as the research \nproceeds, in contrast to ex ante review, where it may be \ntoo early to spot an issue, and ex post review, which may \nbe too late. We expect this to be an active area of inquiry \nas new approaches are validated. The NRC, potentially in \nconjunction with the NSF, should consider offering funding \nfor projects that embed ethics domain experts into teams, \nin order to support this proposal.\nA Blueprint for the National Research Cloud 70\nCHAPTER 8\nChapter 8: Managing \nCybersecurity Risks \nWhile the NRC has the potential to level the \nplaying field for AI research, it will also create an \nalluring target for a vast array of bad actors. \nCybersecurity—the effort to protect \nsystems against incidents that may compromise \noperations or cause harm to relevant assets \nand parties—will be a critical focus of the NRC. \nIt will require a cybersecurity framework that \nmanages potential incidents throughout their \nlifecycle, spanning: (1) preparation; (2) detection \nand analysis; (3) containment, eradication, and \nrecovery; and (4) post-incident activity, which \ncollectively encompasses incident monitoring, \ndetection, recovery, and reporting.1\n Effective \ncybersecurity practices complement risk \nassessment based on impact, immediacy, and \nlikelihood, and will help gain the trust of users and thwart subversion and interference \nfrom foreign actors or other adversarial parties. Careful administrative design of the \nNRC with cybersecurity at the forefront will set a high standard as information systems \nbecome more central to our national infrastructure.\nIn this chapter we address these cybersecurity concerns. We first provide an \noverview of common types of vulnerabilities and attacks, and assess their relevance \nto the NRC. Next, we provide an overview of the federal government’s regulatory \nlandscape, as it pertains to cybersecurity, with a special focus on the FISMA and \nFedRAMP frameworks. Finally, we close with a discussion of the security and system \ndesign measures best suited to ensure that the integrity of the NRC is not compromised.\nMOTIVATIONS FOR POTENTIAL ATTACKS\nPossible attacks against the NRC could take a number of approaches, each of \nwhich would entail substantial consequences for the NRC.2 First, adversaries could \nlaunch an attack against the NRC with the intention of disrupting its operations or its \nability to aid research. For example, adversaries could attack the NRC’s infrastructure \ndirectly by disabling or interfering with NRC servers. As a result, researchers would be \nunable to access NRC servers or effectively utilize them. By launching such attacks, \nadversaries may throttle the NRC, thereby raising costs for the federal government.3\nAlternatively, adversaries could seek to attack specific research projects on the NRC, \nKEY \nTAKEAWAYS\n Deterring malicious actors \nfrom attacking the NRC will \nrequire more than adhering \nto current FISMA and \nFedRAMP standards.\n The NRC should centralize \nsecurity responsibilities for \ndatasets with the program’s \nstaff rather than deferring \nto originating agencies.\n Technical measures the \nNRC should investigate \ninclude confidential \nclouds, federated learning, \nand cryptography\u0002based measures such as \nhomomorphic encryption \nand secure multiparty \ncomputation.\nWhile the NRC \nhas the potential \nto level the \nplaying field for AI \nresearch, it will also \ncreate an alluring \ntarget for a vast \narray of bad actors.\nA Blueprint for the National Research Cloud 71\nCHAPTER 8\nthereby slowing the pace of that research or compromising \nthe quality of the research findings. They may also initiate \n“data-poisoning” attacks on NRC datasets, thereby \ncompromising the quality of research findings. \nSecond, bad actors could also launch cyber operations \nagainst the NRC, intending to steal computational \nresources. In this case, the purpose would not be to disrupt \nthe NRC, but to repurpose computational power toward \nillicit purposes (e.g., cryptocurrency mining).4 For instance, \nindividuals could pretend to be researchers, claiming to \nuse cloud credits for legitimate research purposes while \nactually using them for alternative ends. Individuals \ncould also infiltrate the NRC’s network, siphoning off \ncomputational resources from other projects and reducing \nthe functionality for legitimate users. \nThird, adversaries might pose a threat to the NRC out \nof a desire to steal or make use of the data and research \nproducts housed within the system. The NRC promises \nto be an attractive target because it will house data from \na range of different agencies. If an adversary wanted to \nsteal equivalent data from the agencies themselves, they \nwould need to break into each agency independently. \nHowever, the potential combination of datasets on the \nNRC, including researcher-owned datasets, may increase \nthe potential gains from accessing this information. \nAdditionally, adversaries may attempt to break into \nthe NRC in order to steal products generated by NRC \nresearchers. This could include trained machine-learning \nmodels or specific research findings.\nRelatedly, bad actors could determine that executing \nintrusions into the NRC is an effective way to target \nAffiliated Government Agencies. Because a participation \nincentive for agencies is the computing support that \nthe NRC will offer, one of the biggest cyber risks is of \nmalicious actors attempting to use the NRC to hack into \ntheir systems. For that reason, the cybersecurity risk to \nthe government may be substantial. On the other hand, \nas we discussed in Chapter 3, the NRC also presents an \nopportunity to enhance and harmonize security standards \ncompliance, as agencies move into the cloud. \nA range of other motivations may exist. Successful \noperations against the NRC, as a federal entity, would \ncarry symbolic value and capture attention. Ransomware \nattacks could result in significant payoffs. The NRC could \nalso be a target for espionage, both on the part of nation\u0002state actors seeking to acquire sensitive datasets (e.g., \nenergy grid infrastructure) and on the part of private sector \nentities looking to steal intellectual property or to monitor \nthe latest technological advances.\nIf successful, any attack could undermine the NRC. \nFor example, researchers would be deterred from using \nthe NRC and may invest their efforts in alternate private \nclouds. This could occur because researchers believe \nthe NRC would be ineffective to use (e.g., on account of \nfrequent server outages), or because they believe their \nresearch products would be inadequately protected. \nFederal agencies and departments could be deterred from \nentrusting the NRC with sensitive datasets. Federal entities \ncould risk embarrassment and face obstacles executing \ntheir policy objectives if datasets were accidentally leaked. \nIf the NRC is insufficiently secure, such entities may choose \nto avoid sharing data altogether.\nFISMA, FEDRAMP, AND EXISTING \nFEDERAL STANDARDS\nAs a federal entity, the NRC will be subject to federal \nstandards and regulations. In this section, we provide a \nhigh-level overview of the two most relevant regulations: \nthe Federal Information Systems Management Act (FISMA) \nand the Federal Risk and Authorization Management \nProgram (FedRAMP).5 FISMA traditionally applies to non\u0002cloud systems that support a single agency, whereas \nFedRAMP authorization is required for cloud systems.6\nWe finish by discussing critiques of these regulations. \nFISMA \nThe Federal Information Systems Management Act \n(FISMA) was first passed in 2002, with the purpose of \nproviding a comprehensive framework for ensuring the \neffectiveness of security controls for federal information \nsystems.7\n The law was later amended in 2014, and has \nA Blueprint for the National Research Cloud 72\nCHAPTER 8\nsince been augmented through other individual legislative \nand executive actions, and our discussion focuses on the \ncollective impact of FISMA compliance regulations.8\nFISMA applies to all federal agencies, contractors, \nor other sources that provide information security for \ninformation systems that support the operations and \nassets of the agency.9 It invests responsibility in several \ndifferent entities. First, the National Institute of Standards \nand Technology (NIST) is tasked with developing uniform \nstandards and guidelines for implementing security \ncontrols, evaluating the riskiness of different information \nsystems and other methodologies.10 Second, the Office of \nManagement and Budget (OMB) is tasked with overseeing \nagency compliance with FISMA and reporting to Congress \non the state of FISMA compliance.11 Third, the Department \nof Homeland Security is tasked with administering the \nimplementation of agency information security policies \nand practices.12 Finally, federal agencies are required \nto develop and implement a risk-based information \nsecurity program in compliance with NIST standards and \nOMB policies.13 Agencies are further required to conduct \nperiodic assessments to ensure continued efficiency and \ncost effectiveness.14\nSeveral NIST requirements are worth mentioning \nhere. Pursuant to NIST SP 800-18, agencies are required \nto identify relevant information systems falling under the \npurview of FISMA. Agencies must also categorize each of \nthese systems into a risk level, following the guidance laid \nout in FIPS 199 and NIST 800-60.15 NIST 800-53 outlines \nboth the security controls that agencies should follow \nand the manner in which agencies should conduct risk \nassessments.16 Agencies must further summarize both \nthe security requirements and implemented controls \nin “security plans,” as outlined in NIST 800-18.17 Finally, \norganization officials are required to conduct annual \nsecurity reviews in accordance with NIST 800-37. \nFEDRAMP \nIn the late 2000s, federal agencies began expressing \nsecurity concerns as a barrier to cloud computing \nadoption.18 In response, Congress passed the 2011 \nFederal Risk and Authorization Management Program \n(FedRAMP) to provide a cost-effective, risk-based approach \nfor the adoption and use of cloud services by the federal \ngovernment.19 FedRAMP approval is exempted where: \n(i) the cloud is private to the agency; (ii) the cloud is \nphysically located within a federal facility; and (iii) the \nagency is not providing cloud services from the cloud\u0002based information system to any external entities.20 Like \nFISMA, FedRAMP security requirements are governed by \nNIST standards, including NIST SP 800-53, FIPS 199, NIST \n800-37, and others.21 However, unlike FISMA, FedRAMP’s \ntwo tracks to receiving an authority-to-operate means \nthat vendors working with multiple agencies do not \nnecessarily need to undergo the full approval process with \neach agency. This means that cloud services providers and \nagencies alike are able to save significant time and money.\nCRITICISMS OF FISMA AND FEDRAMP \nThese regulations are not without fault. Most notably, \ncritics point to the fact that despite their existence, cyber \nintrusions on government infrastructure are common \nand accelerating.22 A 2019 report by the U.S. Senate \nCommittee on Homeland Security and Governmental \nAffairs investigating eight agencies noted that the \nfederal government is failing its legislative mandate \nfrom FISMA.23 The errors identified included a failure to \nprotect personally identifiable information, inadequate \nIT documentation, poor remediation of bugs, a failure \nto upgrade legacy systems, and inadequate authority \nvested in agency chief information officers.24 Reports by \nthe Government Accountability Office (GAO) have reached \nsimilar conclusions.25 In turn, some have criticized the \ngovernment’s approach to cybersecurity wholesale, \narguing it places too much emphasis on merely detecting \nintrusions.26 They argue for a framework of “zero trust,” \nwhich assumes that intruders will penetrate a network and \ninstead focus on security controls limiting the ability of \nthose intruders to navigate the network.27\nFedRAMP faces its own criticisms. A recent study \nnoted that securing authorization can be time-consuming \nand expensive—taking up to two years and costing \nmillions of dollars in some cases.28 Even though parts of \nFedRAMP are designed to be reusable across agencies, \nagencies often delay the process by imposing separate, \nA Blueprint for the National Research Cloud 73\nCHAPTER 8\nadditional requirements. A variety of reasons for these \ndeficiencies have been noted, including an understaffed \nJoint Authorization Board, a lack of trust between agencies \nwith regards to Authorization to Operate (ATOs), and an \noverly complex authorization process that leads to errors \nby agencies and Cloud Services Providers.29 Proposed \nrecommendations to address these deficiencies include \nincreased funding for FedRAMP’s Joint Authorization \nBoard, incentives to encourage reuse of ATOs, and \nmechanisms to improve the efficiency of the authorization \nprocess.30\nOn May 12, 2021, the Biden administration released \nan Executive Order (EO) on Improving the Nation’s \nCybersecurity,31 and OMB published a draft federal strategy \nfor public comment on September 7, 2021.32 Signed in the \naftermath of the breach of the software vendor SolarWinds, \nand the ransomware attack on Colonial Pipeline, the EO \npresents several new initiatives. First, it calls on the federal \ngovernment to embrace “zero-trust architecture” and \nimprove post-attack investigation processes. Second, it \nseeks to improve collaboration between the public and \nprivate sectors by improving disclosure requirements and \nestablishing a private-public Cybersecurity Safety Review \nBoard (modeled after the National Transportation Safety \nBoard). Finally, it seeks a more cohesive government-wide \napproach to cybersecurity, calling for the creation of a \nplaybook to standardize cyber response across federal \nagencies, alongside a government-wide detection and \nresponse system for attacks. \nThough it is too soon to determine whether the EO \nand the proposed strategy will be effective, it appears to \naddress deficiencies identified in the existing landscape. \nIt seeks to improve documentation and responsiveness \nto attacks and suggests a shift in cybersecurity thinking. \nIt is unclear, however, whether it will address the \nunderlying procurement issues and lack of interagency \ntrust that critics believe have hampered the effectiveness \nof FedRAMP. But given the potential for highly sensitive \ndata to be stored on the NRC, embracing a zero-trust \narchitecture at the outset is a crucial consideration for \nensuring its integrity.\nNRC SECURIT Y STANDARDS AND \nSYSTEM DESIGN MEASURES\nHere, we present recommendations on cybersecurity \npolicy for the NRC informed by the landscape of the \nexisting federal regulations and unique considerations that \na national research cloud will pose.\nPROCESS FOR RISK AND SECURITY DETERMINATIONS\nUnder the current regulatory landscape, agencies \nare responsible for determining the appropriate risk \ncategorizations and security controls for the datasets \nlocated on their servers. However, this raises a potential \nchallenge as agencies begin to share their data with the \nNRC—making it unclear who will maintain authority for \ncategorizing the risk of these datasets and determining \nappropriate security controls. \nOn the one hand, agencies themselves could continue \nto retain discretion over the security classification \nand controls for datasets they place into the NRC. In \nthis decentralized approach, much of the security \nresponsibilities assigned by FISMA would remain with \nthe agencies, irrespective of whether the data existed \non NRC servers. On the other hand, the NRC could take \nresponsibility for all security decisions. Datasets added to \nthe NRC would then be classified according to the NRC’s \nassessment of risk, and protected with controls that the \nNRC staff deems appropriate. This approach “centralizes” \nsecurity responsibilities by vesting it with the NRC after the \nonetime negotiation for each dataset.\nThough both approaches have their merits, we \nrecommend the centralized approach for several reasons. \nFirst, the centralized approach ensures internal uniformity. \nThe paradox of federal cybersecurity regulation is that \nalthough NIST has articulated a set of standards pertaining \nto risk and controls, agencies interpret these standards \ndifferently, leading to discrepancies in implementation and \nclassification across the federal government. Following \neach agency’s security classifications for data on the NRC \nwould produce unnecessarily complex and incoherent \nclassifications for a single system. This threatens \nto diminish the usability of the NRC, and the added \nA Blueprint for the National Research Cloud 74\nCHAPTER 8\ncomplexity could arguably weaken security by increasing \nthe likelihood of errors. Permitting the NRC to impose its \nown classifications allows for uniformity within the NRC \nand alignment with the access tiers suggested in \nChapter 3 of this White Paper. This approach may also \nsimplify managing security practices across a potential mix \nof cloud compute providers.\nSecond, the NRC represents a valuable opportunity \nto harmonize federal cybersecurity standards across \ndifferent agencies. The assessments and implementations \nadopted by the NRC must generalize to the full diversity of \nfederal datasets. Hence, the NRC’s practices can serve as \na template for NIST’s guidelines, which any agency is free \nto adopt. \nThird, the centralized approach will remove hurdles \nfor data sharing. Security concerns often impede agency \ndata sharing. In a scheme where agencies retain control \nover all security determinations, agencies could demand \nsecurity classifications that are excessively high or \nimpractical to implement. The centralized approach \nwould place the burden on agencies to articulate with \nspecificity why the NRC’s security policies or classification \nguidelines are inadequate for a particular dataset. \nFinally, researchers should also have a voice in \ndetermining the appropriate security controls, since a \npublic resource of this magnitude that cannot attract users \nis bound to fail. As security controls implicate usability, \nthe NRC should not opt for controls that substantially \ninhibit or disincentivize researchers from leveraging its \nresources. The NRC needs to strike the right balance \nbetween usability and security.\nTECHNICAL CONSIDERATIONS\nThe federal government already possesses a range of \ntechnical options and countermeasures to cyberattacks. \nCybersecurity threats and defenses are, of course, actively \nevolving, so we discuss these only as a starting point—\nrobust, long-term cybersecurity comes through continued \nvigilance and prioritization that recognizes the shifting \nnature of the field. \nDATA STORAGE\nData storage mechanisms should ensure proper \nprotection from outside access. Encryption can be used \nto protect sensitive data at rest, to be later unencrypted \nwhen needed. Physical isolation through air-gapped \nenvironments is another design feature that can remove \nthe possibility of wireless network interfaces from being \nused to connect the data to malicious outside threats. \nHowever, even air gapping is not a foolproof solution: There \nare ways to “jump” air gaps such as through hiding in USB \nthumb drives (which is allegedly how the Stuxnet malware \nfamously compromised Iranian nuclear centrifuges).33 More \nrecent attacks bypass the need for electronic transmission \naltogether by leveraging other signals that leak data, \nsuch as FM frequencies, audio, heat, light, and magnetic \nfields. These kinds of threats bring home the need for a \ncomprehensive and evolving approach to cybersecurity. \nNETWORKING PROTOCOLS\nData packets sent over networks are transmitted \naccording to a set of internationally standardized internet \nprotocols. Following the Open Systems Interconnection \n(OSI) model, the conceptual layers involved in computer \nnetworking can be categorized into seven dimensions: \nphysical, data link, network, transport, session, \npresentation, and application layers.34 \nRUNTIME SECURITY\nWhen considering runtime security technologies, \nthree design features that are relevant for the cloud \nenvironments are the use of confidential clouds, \nfederated learning, and cryptography-based measures \nsuch as homomorphic encryption and secure multiparty \ncomputation. A growing number of vendors offer \n“confidential cloud” options as an emerging technical \nsolution to fully cyber secure cloud computation that is \nsecure throughout execution.35 Confidential clouds offer \nhigh-security, end-to-end, isolated operation by executing \nworkloads within trusted execution environments. For \nexample, virtualization enables an operating system \nto run another operating system within it as a virtual \nenvironment with additional firewall or other network \nA Blueprint for the National Research Cloud 75\nCHAPTER 8\nbarriers, effectively simulating another device within the \nhost computer. \nDISTRIBUTED COMPUTING AND FEDERATED LEARNING\nAnother computing paradigm, known as distributed \ncomputing or federated learning, considers situations \nwhere multiple parties have individual shards of data they \nare interested in leveraging in aggregate, without sharing \noutright. Federated learning addresses this situation, \nfor example, demonstrating how users’ mobile phones \ncan send information—possibly differentially private—to \ncentral servers without exposing the precise details of \nany one individual’s information. A second scenario more \nrelevant to the large-scale decentralized nature of the \nNRC is distributed computing—in which many institutions \ncollectively share compute, akin in some respects to \ncrowd-sourced computing. These approaches enable \nmultiple parties to leverage existing computational \ninfrastructure, while retaining some guarantees on \nprivacy. \nCRYPTOGRAPHY-BASED MEASURES\nFinally, there are two types of cryptography-based \nmeasures worth noting. \nCryptography researchers have developed ways of \ncomputing mathematical operations over encrypted data, \nknown as homomorphic encryption. This impressive \nfeat has valuable implications because it obviates the \nneed for decryption, which can potentially expose the \nintermediate values of computation, and grant access to \npublic and secret encryption keys during computation. \nInitially, only partially homomorphic encryption schemes \nthat supported limited arithmetic operations like addition \nand multiplication were possible. But fully homomorphic \nencryption schemes have recently been developed that \nenable what is known as “arbitrary” computation for \npromising use cases in predictive medicine and machine \nlearning. That said, standardization is still underway to \nbroader adoption, and homomorphic encryption (by \ndesign) is malleable—a property in cryptography that \nis usually undesirable as it allows attackers to modify \nencrypted ciphertexts without needing to know their \ndecrypted value. These and other limitations of any \ntechnical approach are worth taking into account when \nconsidering which technologies to adopt and for what \npurpose. \nComplementing the distributed, decentralized \ncomputing model discussed throughout this White Paper \nis the subfield known as secure multiparty computation \n(also known as privacy-preserving computation), \nwhich presents methods for multiple parties to jointly \ncompute a function over all their respective inputs, while \nkeeping those inputs private from other parties. These \nmethods have matured in their origins from a theoretical \ncuriosity to techniques with practical application in \nstudies on tax and education records, cryptographic key \nmanagement for the cloud, and more.36 This makes secure \nmultiparty computation methods a potential candidate \nfor applications pertaining to secure, distributed \ncomputation. \nUltimately, it will be central for the NRC to \ncontinuously learn about the most effective security \nstandards (including such other creative strategies as red \nteaming or bug bounties37 to identify vulnerabilities) in this \nrapidly evolving space.\nA Blueprint for the National Research Cloud 76\nCHAPTER 9\nChapter 9: \nIntellectual Property\nWho should own the IP rights to outputs developed using NRC resources?1 When \nprivate research is funded, subsidized, or influenced by the federal government, the \nlaws and rules have evolved, so that both the researcher and the government have \ncertain rights in the intellectual property developed under the research. While IP \nprotection is theoretically designed to incentivize research and innovation, some signs \nindicate that AI researchers in particular are already amenable to sharing the fruits \nof their research. Indeed, over 2,000 researchers signed a 2018 petition to boycott \na new machine intelligence journal started by Nature, because it promised to place \nits articles behind a paywall.2 The Open Science and Open Research movements \nhave also encouraged AI researchers to make their machine-learning software and \nalgorithms publicly available.3 Furthermore, as we discuss below, the advancement of \ntechniques like transfer learning depend on researchers being able to distribute the \nfruits of their research freely. \nThis chapter surveys the existing IP-sharing agreements between researchers and \nthe government, and explores whether and to what extent the government should \nretain IP rights over researchers’’ outputs, as a condition of using the NRC.4 While the \nevidence on optimal IP rights varies, we recommend that: (1) Academic researchers \nand universities should retain the same IP rights as the Bayh-Dole Act provides for \npatents developed under federally funded research; (2) The government should retain \nits copyrights and data rights under the Uniform Guidance, but contract around \nthose rights where applicable to incentivize NRC usage and AI innovation; and (3) \nThe government should consider conditions for requiring researchers to share their \nresearch outputs under an open-access license.\nKEY \nTAKEAWAYS\n To harmonize with the \nfederal grant process, the \nNRC should adopt the same \napproach to allocating \npatent rights, copyrights, \nand data rights to NRC \nusers as applies to federal \nfunding agreements.\n The NRC should contract \naround government \nintellectual property \nrights where applicable to \nincentivize NRC usage and \nAI innovation.\n The NRC should consider \nconditions for requiring \nresearchers to share \noutputs under an open\u0002source license.\n[A]cademic researchers and universities should retain \nthe same IP rights as the Bayh-Dole Act provides for \npatents developed under federally funded research.\nPATENTS RIGHTS \nA core question is whether NRC users should retain patent rights in inventions \nsupported by the NRC. The Bayh-Dole Act regulates patent rights for inventions \ndeveloped under federal funding agreements and its applicability depends on the \nA Blueprint for the National Research Cloud 77\nCHAPTER 9\nnature of NRC access; for instance, if cloud credits are \napportioned using federal grants, as described in \nChapter 2, they may be considered federal funding \nagreements.5 In such cases, Bayh-Dole Act permits \nresearchers to hold the title to the patent and to license \nthe patent rights.6 However, these patent rights come with \ncertain restrictions: For example, the funding agency has \na free, nonexclusive license to use the invention “for or on \nbehalf of the United States,” and the agency may use “[m]\narch-in rights” to grant additional licenses.7\nThe broader policy question about the government’s \nexercise of its patent rights is whether and how patents \nstimulate innovation in AI. Some commentators have \nargued that the U.S. suffers from over-patenting in \nsoftware,8\n and AI is no exception.9 The total number of AI \npatent applications received annually by the U.S. Patent \nand Trademark Office more than doubled from 30,000 in \n2002 to over 60,000 in 2018,10 and some argue that this \nproliferation of broad AI patents, especially those filed by \ncommercial companies, is hindering future innovation.11\nIn the Bayh-Dole context, researchers have also found that \nthe benefits of university patenting may justify the costs \nonly where industry licensees need exclusivity to justify \nundertaking the costs of commercialization, as, for instance, \nin the pharmaceutical context.12 For the substantial portion \nof university patenting, including AI, this rationale may not \ncarry much weight.13\nSome research shows that patents actually may not \nactually have any net effect on the amount or quality of \nAI research conducted in the university context. In an \nempirical study of faculty at the top computer science and \nelectrical engineering universities in the United States, \nresearch has found that the prospect of obtaining patent \nrights to the fruits of their research does not motivate \nresearchers to conduct more or higher-quality research.14\nEighty-five percent of professors reported that patent \nrights were not among the top four factors motivating their \nresearch activities, and 57 percent of professors reported \nthat they did not know whether or how their university \nshares licensing revenue with inventors.15 The patent \nscheme adopted by the NRC, therefore, may not have a \nstrong influence on researcher adoption. \nThat said, as a practical matter, there is a virtue \nto treating innovations stemming from NRC usage in a \nfashion that is consistent with Bayh-Dole. Particularly \nif cloud credits are awarded through the expansion of \nprograms like NSF CloudBank, it would be confusing to \nhave distinct patent rights out of the research and cloud \ngrant. In addition, many university tech transfer offices \nappear to have strong preferences for patent rights.16 To \nthe extent that universities view retaining patent rights \nas a condition for using the NRC, aligning NRC patent \nrights with Bayh-Dole may be preferred, but the evidence \nunderpinning this recommendation is not strong. \nCOPYRIGHT, DATA RIGHTS, AND \nTHE UNIFORM GUIDANCE\nThe Uniform Guidance (2 C.F.R. § 200) streamlines and \nconsolidates government requirements for receiving and \nusing federal awards to reduce administrative burden.17\nGrants.gov describes it as a “government-wide framework \nfor grants management,” a groundwork of rules for federal \nagencies in administering federal funding.18 The Uniform \nGuidance includes provisions on, for instance, cost \nprinciples, audit requirements, and requirements for the \ncontents of federal awards.19\nThe Uniform Guidance is applicable to “federal \nawards,”20 but IP provisions do not require the government \nto assert their rights over researcher outputs.21 Whether \nand how the government allocates its IP rights under the \nUniform Guidance is therefore an important question.\nThis section first covers government copyright \nand data rights to IP under the Uniform Guidance and \nThe government should . . . \nconsider conditions for requiring \nNRC researchers to disclose or \nshare their research outputs under \nan open-access license.\nA Blueprint for the National Research Cloud 78\nCHAPTER 9\ndiscusses how sharing copyright and data rights might \nimpact the AI innovation landscape. We then examine \nthe extent to which the government should retain its \nrights to research generated using the NRC. While the \nevidence is mixed, we ultimately recommend that the \ngovernment retain its copyrights and data rights under \nthe Uniform Guidance, but contract around those rights \nwhere applicable, to incentivize NRC usage and further AI \ninnovation. \nCOPYRIGHT\nUnder U.S. copyright law, NRC researchers can obtain \ncopyrights over various aspects of their work. For instance, \nNRC researchers may wish to copyright the software they \nused to build the model, since software is considered a \nliterary work under the Copyright Act.22 Researchers may \neven obtain copyrights over various aspects of the model, \nincluding the choices of training parameters, model \narchitectures, and training labels, if they can show that \nthose choices required creativity.23 Many scholars have \neven opined, without reaching consensus, on whether \noutputs such as text and art that are artificially generated \ncan be copyrighted.24\nUnder the Uniform Guidance,25 the recipient of \nfederal funds may copyright any work that was developed \nor acquired under a federal award. However, even \nif researchers are permitted to maintain copyrights, \nthe federal awarding agency reserves a “royalty-free, \nnonexclusive and irrevocable right to reproduce, publish, \nor otherwise use the work for federal purposes, and to \nauthorize others to do so.”26 Notably, this right is limited to \n“federal purposes,” meaning that third parties who acquire \nlicenses to the researchers’ copyrighted works cannot use \nthem for exclusively commercial purposes.27\nIt is unclear to what extent copyrights over NRC \noutputs should be fully vested in the researcher to \nstimulate basic AI research. One class of AI research \nand development output that has received significant \nacademic attention has been whether AI-generated \ncreative works, like music from OpenAI’s Jukebox,28 can \nor should receive copyright protection.29 However, the \ntechnology and copyright community has hardly reached \na consensus on whether the public interest in AI research \nrequires granting copyright in these scenarios. On one \nhand, in a survey of AI scientists, tech policy experts, and \ncopyright scholars, roughly 54 percent of respondents \nagreed that copyright protection is an important incentive \nfor authors to make their work commercially available, \nand 63 percent agreed that an increase in the number \nof commercially available AI-produced works would \nstimulate further AI growth and research.30 On the other \nhand, in the same survey approximately 56 percent of \nrespondents agreed that the U.S. Copyright Office should \ndeny copyright protection to creative works produced \nindependently by AI without creative intervention from a \nhuman author.31\nNotwithstanding the prominent debate about \ncopyright over creative works generated by AI models, such \nworks are only a subset of possible copyright protection \nin the AI context. As discussed above, researchers could \ntheoretically seek additional copyright protection over, \namong other things, their code, architecture, or model. \nHere, AI innovation may depend on sharing these \ncopyrightable elements. For instance, transfer learning \nuses existing ML models and “fine-tunes” those models for \na related target task,32 and various fine-tuning approaches \nhave emerged to perform transfer learning on different \nclasses of tasks.33\nDATA RIGHTS\nUnder the Uniform Guidance, when “data” is \n“produced” under a federal award, the government \nreserves the right to: (1) obtain, reproduce, publish or \notherwise use such data; and (2) authorize others to \nreceive, reproduce, publish or otherwise use such data.34\nNotably, this does not limit the use of such data for \nfederal government purposes. In other words, such data \ncan be promulgated for any use. The outstanding question, \ntherefore, is whether this “data,” which is not explicitly \ndefined in the Uniform Guidance, covers data generated \nfor AI and machine-learning purposes. Below, we examine \ntwo classes of data generated for AI purposes—synthetic \ndata and data labels—and how sharing this data could \nimpact AI innovation. \nA Blueprint for the National Research Cloud 79\nCHAPTER 9\nOne class of data generated for AI purposes is \nsynthetic data. Researchers have turned to deep \ngenerative models such as Variational Autoencoders35 and \nGenerative Adversarial Networks36 to generate synthetic \ndata to train their machine learning models. As noted by \nthe World Intellectual Property Organization, synthetic \ndata is an entirely new class of data that does not fit \nneatly under existing IP law.37 While a researcher may seek \ncopyright protection over the subset of synthetic data that \nis “creative,” therefore implicating the copyright provisions \nof the Uniform Guidance (described above), the broad \nclass of synthetic data, whether “creative” or not, may \nalso implicate the data rights provision. On the one hand, \ntraining data is often carefully guarded,38 so requirements \nto share synthetic data, which is often used to train AI \nmodels, may be a non-starter for NRC users. On the other \nhand, many scholars have written about the promise of \nsynthetic data to actually enable further data sharing by \npreserving privacy and researchers’ trade secrets.39 In fact, \nsharing synthetic datasets would spur additional research \nand innovation in fields such as healthcare, where data \nsharing has been limited.40\n Another class of data generated for AI is labeled data, \nnamely data that has been tagged and classified to provide \nground truth for supervised machine learning models.41\nWhile techniques have been developed to decrease the \ncosts associated with data labeling,42 it nevertheless \nremains a resource and time-intensive task. For example, \nCognilytics Research reports that 25 percent of the total \ntime spent building machine learning models is devoted to \ndata labeling.43 Researchers using the NRC may therefore \nseek to protect their investment in data labeling by opting \nnot to share their labels with others, especially if the \nunderlying data is proprietary.44 However, recognizing the \ndifficulty of data labeling, some researchers have built \nonline platforms for sharing data labels.45 In the case \nof ImageTagger, a data labeling and sharing platform \nfor RoboCup Soccer, the developers wanted to solve \nthe problem that no single team, acting alone, could \neasily build its own high-quality training sets.46 Similarly, \nin the NRC’s case, the sharing of labeled government \ndata—where labeling may have been augmented by NRC \nresources47—could act as a rising tide that lifts all boats, \nimproving the quality of not only the government data \nas a training dataset, but also all subsequent research \nusing that data. Furthermore, sharing data labels could \nbe instrumental in conducting bias and fairness of NRC \nresearch outputs where necessary, as discussed in \nChapter 7.\n48\nRETAINING IP RIGHTS IN THE UNIFORM GUIDANCE\nAs the preceding discussion suggests, sharing AI \nresearch output covered by copyrights and data rights \ncould be beneficial to AI innovation. We therefore \nrecommend that the NRC at least retain the same rights to \ncopyrights and data rights as under the Uniform Guidance, \nyielding several additional benefits. First, similar to our \nrecommendation in Chapter 3 that federal agencies \nshould be allowed to use the NRC’s compute resources, \nretaining the same Uniform Guidance IP allocation scheme \ncould produce welfare benefits by improving government \ndecision-making using AI. For instance, federal agencies \ncan reduce the cost of core governance functions and \nincrease agency efficiency and effectiveness by using \ndata labels shared by NRC researchers or by fine-tuning \nmodels generated by NRC researchers. Second, retaining \nthe Uniform Guidance IP allocation scheme would result \nin more consistency across the federal award landscape. \nIndeed, as mentioned above in the patent context, it \ncould be confusing to diverge from the Uniform Guidance, \nespecially if the cloud credit grant is apportioned through \nprograms like CloudBank but the research grant is \nadministered as a federal award.\nIn sum, we recommend that the government at least \nretain its copyrights and data rights under the Uniform \nGuidance. However, we also reiterate that the Uniform \nGuidance serves merely as a helpful framework, not as an \nimmutable rule. Where the Uniform Guidance IP allocation \nwould dissuade researchers from using the NRC or hinder \nAI innovation in specific scenarios, the government \ncan and should explicitly modify its rights and contract \nseparately with researchers on what rights the government \nretains, if any.\nA Blueprint for the National Research Cloud 80\nCHAPTER 9\nCONSIDERATIONS FOR OPEN\u0002SOURCING\nShould the government go beyond its rights and \nmandate that researchers share their NRC research \noutputs with others under an open-source license? As \nan initial matter, we note that agencies can modify the \nIP allocation schemes under the Uniform Guidance, but \nnot under the Bayh-Dole Act. Some federal agencies \nsupplement and/or replace the IP rights set out in the \nUniform Guidance with restrictions that are more specific \nto the IP being developed for that particular agency or \nunder a specific award.49 For instance, the Department of \nLabor requires that intellectual property developed under \na federal award must not only comply with the terms \nspecified in the Uniform Guidance, but also be available \nfor open licensing to the public.50 NSF grantees are also \nexpected to share their data with others.51 However, \nthe government cannot change the allocation of patent \nownership under the Bayh-Dole Act, unless the Act itself is \nmodified or unless the NRC isn’t administered as a federal \naward, rendering the Act inapplicable. \nRequiring researchers to open-source their research \noutputs may be possible, but the considerations \naround it are complex. On the one hand, an open\u0002source requirement could negatively affect downstream \ncommercialization, given the wide range of potential AI \nresearch.52 While the NRC might protect commercialization \nto some degree by adopting a restrictive open-source \nlicense,53 the mere divergence from the Uniform Guidance \nor the Bayh-Dole Act could be confusing for researchers \nin navigating federal awards and understanding open\u0002source licensing interactions across multiple situations.54\nFurthermore, requiring researchers to share research \noutputs comes with its own host of privacy and \ncybersecurity issues.55 If researchers are permitted to use \nthe NRC to conduct classified research,56 for instance, then \nkeeping research outputs proprietary would serve the \nnational interest.57 In this case, however, the NRC should \nconsider limiting any open-source requirement to research \nthat has fewer privacy and security implications.\nOn the other hand, as discussed, sharing research \noutputs with other NRC researchers could be beneficial, \nand many scholars argue that AI researchers should \nopen-source their software to stimulate innovation.58 A \nrequirement to open-source software code, which can \nbe the subject of both copyrights and patent rights,59\nmay contravene Bayh-Dole and face challenges from \nuniversities that seek to retain their patent rights, but \nsoftware patent disclosures alone are often limited and \nover-broad, and fail to enhance social welfare.60 Requiring \nfuller disclosure of code generated on the NRC can \ntherefore decrease the risk of over-patenting and increase \nAI innovation. The growth of the robust open-source and \nopen science movements also suggests that an open\u0002sourcing requirement for the NRC would not be a complete \nbarrier to NRC usage.61\nA strong argument for mandating open-sourcing \nalso comes from the increasing private- sector reliance \non trade secrets for IP protection in AI.62 Some argue that \nthis heightened emphasis on trade secret protection \nconstitutes “artificial stupidity,”63 as it has stifled \ninnovation in AI by preventing disclosure, providing \nprotection for a potentially unlimited duration, and \nattaching immediately and broadly to any output with \nperceivable economic value.64 The reliance on secrecy, \ntherefore, contravenes many of the principles described \nabove—which argue that sharing code and data is crucial \nin AI—and results in significant AI industry consolidation \nand suboptimal levels of AI innovation.65 This harkens back \nto the goal of the NRC discussed in Chapter 1: To address \nproblems with AI research being concentrated in the hands \nof a few private-sector players. Because the NRC should \nexplicitly avoid replicating these private-sector challenges, \nthis lends additional support to a recommendation that \nthe NRC should contemplate requiring researchers to share \ntheir research outputs.\nIn sum, while AI raises a host of novel IP issues (e.g., \nwhether AI output is itself eligible for IP protection), \nwe think the government can steer clear of many of \nthese complications by tracking Bayh-Dole and the \nUniform Guidance. The government should also consider \nconditions for requiring NRC researchers to disclose or \nshare their research outputs under an open-access license.\nA Blueprint for the National Research Cloud 81\nConclusion \nAs we have articulated in this White Paper, the ambitious call for an NRC has transformative potential for the AI \nresearch landscape.\nIts biggest promise is to ensure more equitable access to core ingredients for AI research: compute and data. \nLeveling this playing field could shift the current ecosystem from one that focuses on narrow commercial problems to \none that fosters basic, noncommercial AI research to ensure long-term national competitiveness, to solve some of the \nmost pressing problems, and to rigorously interrogate AI models.\nAs we have spelled out in this White Paper, the NRC does raise a host of policy, legal, and normative questions. How \ncan such compute resources be provided in a way that is expeditious and user-friendly, but does not preclude the \npotential cost savings from a publicly owned resource? How can the NRC be designed to adhere to the Privacy Act of \n1974, which was animated by concerns about a national system of records that surveils its citizens? How can we ensure \nthat NRC mitigates, rather than heightens, concerns about the unethical use of AI? And how can one prevent the NRC \nfrom becoming the biggest target for cyberattacks?\nThese are tough questions, and we hope to have sketched out our initial attempt at answers above. We are hopeful, \nif designed well, the NRC could help to realign the AI innovation space from one that is fixated with short-term private \nprofit to one that is infused with long-term public values.\nA Blueprint for the National Research Cloud 82\nADP Alberta Data Partnerships\nADR UK Administrative Data Research UK\nADRF Administrative Data Research Facility\nAI artificial intelligence\nAPI application programming interface\nARPA Advanced Research Projects Agency\nARPANET Advanced Research Projects Agency \nNetwork\nATO authority-to-operate\nATO Authorization to Operate\nAWS Amazon Web Services\nCaaS Compute as a Service\nCIPSEA Confidential Information Protection\nand Statistical Efficiency Act\nCMS Centers for Medicare & Medicaid Services\nCPL California Policy Lab\nCPU central processing unit\nDFARS Defense Federal Acquisition Regulation \nSupplement\nDHS U.S. Department of Homeland Security\nDOD U.S. Department of Defense\nDOE U.S. Department of Energy\nDOT U.S. Department of Transportation\nDUA Data Use Agreement\nEBPA Foundations for Evidence Based \nPolicymaking Act or Evidence Act\nEO Executive Order\nESRC Economic and Social Research Council\nEULA End-User Licensing Agreement\nFAA Federal Aviation Administration\nFAR Federal Acquisition Regulation\nFDS Federal Data Strategy\nFedRAMP Federal Risk and Authorization Management \nProgram\nFFRDC Federally-Funded Research and Development \nCenter\nFIPS Federal Information Processing Standards\nFISMA Federal Information Security Modernization \nAct\nFSRDC Federal Statistical Research Data Center\nGAO U.S. Government Accountability Office\nGCP Google Cloud Platform\nGDPR General Data Protection Regulation\nGPS Global Positioning System\nGPU graphics processing unit\nGSA U.S. General Services Administration\nHAI Stanford Institute for Human-Centered \nArtificial Intelligence\nHECToR High-End Computing Terascale Resource\nHHS U.S. Department of Health and Human \nServices\nHIPAA Health Insurance Portability and \nAccountability Act\nHPC high-performance computing\nHTTP Hypertext Transfer Protocol\nHTTPS Hypertext Transfer Protocol Secure\nIC U.S. Intelligence Community\nIDA Institute for Defense Analyses\nIPTO Information Processing Techniques Office\nIRB Institutional Review Board\nJV joint venture\nLIDAR Light Detection and Ranging\nML machine learning\nMOU memorandum of understanding\nGlossary of Acronyms\nA Blueprint for the National Research Cloud 83\nNAIRR National Artificial Intelligence Research \nResource Task Force Act\nNASA National Aeronautics and Space \nAdministration\nNDAA National Defense Authorization Act\nNIH National Institutes of Health\nNISE NSF’s Directorate for Computer and \nInformation Science and Engineering\nNIST National Institute of Standards and \nTechnology\nNIST SP NIST Special Publications\nNOAA National Oceanic and Atmospheric \nAdministration\nNORC National Opinion Research Center\nNRC National Research Cloud\nNSCAI National Security Commission on Artificial \nIntelligence\nNSDS National Secure Data Service\nNSF National Science Foundation\nODNI Office of the Director of National Intelligence\nOECD Organization for Economic Co-operation and \nDevelopment\nOMB U.S. Office of Management and Budget\nONS Office for National Statistics\nORNL Oak Ridge National Laboratory\nOLCF Oak Ridge Leadership Computing Facility\nOSI Open Systems Interconnection\nOT Other Transaction\nPCLOB Privacy and Civil Liberties Oversight Board\nPHI protected health information\nPHS Stanford Center for Population \nHealth Sciences\nPI principal investigator\nPII personally identifiable information\nPPP public-private partnership\nR&D research and development\nRFI Request for Information\nRFP Request for Proposal\nRIST Research Organization for Information \nScience and Technology\nSDSC San Diego Supercomputer Center\nSRCC Stanford Research Computing Center\nSSL Secure Sockets Layer\nSTPI Science & Technology Policy Institute\nTLS Transport Layer Security\nUC Berkeley University of California, Berkeley\nUC San Diego University of California, San Diego\nUCLA University of California, Los Angeles\nUSDA U.S. Department of Agriculture\nVRDC CMS’ Virtual Research Data Center\nWIPO World Intellectual Property Organization\nA Blueprint for the National Research Cloud 84\nAppendix\nA. COMPUTING INFRASTRUCTURE \nCOST COMPARISONS\nThis Appendix provides a sample cost-estimate \ncomparison between a commercial cloud service, AWS, and \na dedicated government HPC system, Summit. In sum, our \nestimations show that AWS P3 instances with comparable \nhardware to Summit would be 7.5 times as expensive \nas estimated costs under constant usage, and 2.8 times \nSummit’s estimated costs under fluctuating demand.\nTable 3 lists the three infrastructure models used \nin this comparison. Summit was used as the reference \ngovernment HPC system because it is one of the DOE’s \nnewest systems and has hardware well-suited for AI \nresearch.1 The other infrastructure model used is AWS EC2 \nP3.2 Both are commonly used in AI research and general \nHPC applications. Other commercial cloud platforms, \nsuch as GCP or Azure, could also feasibly provide the \ninfrastructure for the NRC. AWS EC2 P3 was used here \nbecause AWS has a robust cost calculator that allows for \nvariable workloads. \nThe number of AWS instances were set such that those \nmodels would have the exact same number of GPUs as \nSummit. GPUs were the fixed variable because GPUs are \nthe most important hardware for AI research applications, \nspecifically deep learning. Both Summit and AWS P3 \ninstances use NVIDIA V100 GPUs.\nWe conduct our cost comparison for the two \ninfrastructure models over five years, as Summit’s initial \nRFP documents include a five-year maintenance contract. \nAWS, however, only provides one-year or three-year pricing \nplans, so we extrapolated the five-year cost based on its \nthree-year plan.\nFor the cost estimate of Summit, we based our \ncalculation on the budget details in the original \nDepartment of Energy (DOE) Request for Proposal \n(RFP) in January 2014.3\n The RFP includes a $155 million \nmaximum budget for building Summit, an expected $15 \nmillion maximum for the non-recurring engineering cost,4\nand around $15 million for five-year maintenance,5 plus \ninterest based on the U.S. Treasury securities at five-year \nconstant maturity as specified in the price schedule.6 Upon \ncalculation, we estimated Summit costs around $192 \nmillion in total, which is consistent with public reporting of \nthe cost of Summit.7\nFor the cost estimate of AWS, we used the AWS pricing \ncalculator, choosing U.S. East (N. Virginia) as the data \ncenter and publicly available rates under the cheapest \npossible pricing plan (EC2 Instance Savings Plans). To \napproximate a negotiated discount, we applied a 10 \npercent discount based on the negotiated rate of one \nmajor university.\nSince commercial cloud platform costs scale with \nhow many instances are actually in use, two costs were \ncalculated for each AWS model representing usage \nextremes: (1) with the infrastructure under constant usage; \n(2) with the infrastructure under dramatically fluctuating \nusage each day. For the daily spike traffic calculation, \nwe set the model to run five days a week with 8.4 hours \neach day at peak performance. The maximum number of \ninstances used is the same as one would use for constant \nuse while the minimum number is zero. This workload \nsetting is based on the assumption that GPUs used for \ntraining AI models sit idle 30 percent of the time.8\n These \nestimates should provide hard upper and lower bounds on \ncosts for using each instance type. \nFigure 1 plots costs on the y-axis over a five-year \nperiod on the x-axis. The turquoise line indicates the \ncost to a Summit-like system and the purple and blue \nlines indicate the cost of the same AWS instances under \nvariable and constant usage. Overall, this simple analysis \ncorroborates the analysis conducted by Compute Canada, \nwhich found that commercial cloud “ranged from 4x to \n10x more than the cost of owning and operating our own \nclusters.”9\n Over five years and under constant usage, \nAWS P3 instances with comparable hardware to Summit \nwould be 7.5 times as expensive as estimated costs. Under \nfluctuating demand, AWS P3 instances would cost 2.8 \ntimes Summit’s estimated costs.\nA Blueprint for the National Research Cloud 85\nWe note that this simple analysis omits many potential factors (see discussion in Chapter 2), but provides a starting \npoint to understanding the considerable cost implications for the make-or-buy decision. \nFIGURE 1: ESTIMATED COST OF AWS INSTANCES COMPARED TO SUMMIT OVER 3 YEARS\nTABLE 3: SUMMIT & AWS COMPARISON\nSummit IBM AC922\nAWS p3dn.24xlarge\n(3456 nodes)\nGPUs RAM Network Bandwidth\n27,648\n(NVIDIA Volta V100)\n27,648\n(NVIDIA Volta V100)\n2.8 PB\n2.6 PB\n200 Gb/s\n100 Gb/s\n$1,600\n$1,400\n$1,200\n$1,000\n$800\n$600\n$400\n$200\n$0\n0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60\nMillions\nMonths\nAWS P3 (Constant Usage) AWS P3 (Daily Spike Traffic) Summit\nA Blueprint for the National Research Cloud 86\nB. FACILITATING PRIVATE DATASET \nSHARING\nUnique IP challenges arise if researchers are permitted \nto share their own private datasets with the NRC. Indeed, \nresearchers who “upload” proprietary data may be \nconcerned about how other NRC users utilize that data.10\nThrough interviews conducted for this White Paper, \ncorporate stakeholders representing the entertainment \nindustry, as well as other creative industries, have further \nexpressed fear that researchers may upload and share data \nto which they do not hold rights. However, if the NRC does \ndecide to facilitate private data-sharing, it should consider \nadopting two requirements to address these concerns: (1) \nThe NRC should require all users to affirm they either have \nthe original IP rights to the data or the data is already in \nthe public domain; and (2) The NRC should have a scheme \nfor its users to license their data.\n(a) NRC users must own IP rights to the data they are \nuploading\nResearchers uploading data need to agree that they \nown the intellectual property rights to the data prior to \nupload, or that the data is already in the public domain. \nThis should be the case whether researchers share the data \nbroadly with other researchers or simply use their data for \ntheir own private use. \nOf course, despite mandating that uploaders \nguarantee legitimate ownership or public domain status \nof their uploaded IP, uploaders may nevertheless upload \ndata they don’t own the IP rights to. This may happen \nbecause computer engineers and researchers are not \ninformed about IP law, anticipate that fair use will excuse \ntheir behavior, or simply hope not to get caught.11 Industry \nstakeholders were also concerned that AI researchers \nwould pull out “facts” from a copyrighted work (e.g., \ncertain melodies in the chorus of a song) or apply certain \nalgorithms to the work and “wrongly” claim a copyright \nover the transformed work. Whatever the case may be, this \nassembly of protected input data represents the “clearest \ncopyright liability in the machine learning process” \nbecause assembling protected data violates the right to \nreproduction, and any preprocessing on the data could \nviolate the right to derivative works.12\nIn interviews, corporate stakeholders expressed a \ndesire to stymie the upload of copyrighted works by having \nthe NRC itself assess whether uploaded data is already \nprotected by copyright. Diligencing data can be completed \nmanually, or by using such automated systems as Content \nID, which is also used by corporations such as YouTube.\n13\nThe former option would be very labor intensive,14 whereas \nthe latter may be prohibitively expensive,15 so the value of \naddressing these concerns must be weighed against these \nburdensome costs.\nFinally, it is unclear the extent to which uploading and \nsharing copyrighted data for machine learning amounts \nto fair use.16 The most analogous case is Author’s Guild \nv. Google Books.17 In that case, Google scanned over 20 \nmillion books, many of which were copyright-protected, \nand assembled a corpus of machine-readable texts to \npower its Google Books service.18 The 2nd Circuit held that \nGoogle Books’ unauthorized reproductions of copyrighted \nworks was transformative fair use, largely because Google \nBooks provided information about books through small \nsnippets, without threatening the rights-holders’ core \nprotectable expression in the books.19 While some have \nopined that the Author’s Guild holding categorically \nprotects using copyrighted material in datasets for \nmachine learning purposes,20 many legal scholars are not \nso sure about such a broad holding, especially because fair \nuse is so fact-intensive.21 Indeed, while Google Books used \ncopyrighted works for a nonexpressive purpose, Sobel \nnotes that machine learning models may increasingly be \nable to glean value from a work’s expressive aspects.22\nTherefore, until courts and legislators provide more clarity \non the applicability of fair use in the machine learning \ncontext, the NRC should still require data uploaders to \nattest that they own the rights to the data.\n(b) Users must be able to license their data to other \nusers. \nIf the NRC enabled private data sharing, users would \nneed to make clear what rights other NRC users have over \nthe uploaders’ shared data. The NRC would have two basic \noptions for creating IP licensing schemes: (1) The NRC \nA Blueprint for the National Research Cloud 87\ncould permit researchers to use whatever IP license they \nwish when sharing their private data; or (2) The NRC could \nmandate a uniform license across the board for all data \nthat is uploaded.\n(1) Researcher’s Choice of License \nAllowing researchers to craft their own IP licensing \nagreements when sharing private data with other \nresearchers would be the most frictionless solution from \nthe perspective of the uploader; it would allow them to \nshare exactly what they want and restrict use to only \ncertain contexts. This choice of license seems to be \nimportant to data sharers.23 Indeed, many data scientists \nand engineers have written guides advising members \nof the open-source community on how they should go \nabout choosing specific licenses for their work.24 GitHub, \nan open-source code-sharing platform, permits its users \nto choose from dozens of licenses,25 and FigShare, a data\u0002sharing platform for researchers, likewise supports a host \nof different Creative Commons licenses.26 Some datasets \neven have their own custom IP licensing agreements. \nThe Twitter academic dataset, for instance, is licensed \naccording to Twitter’s own developer agreement and \nnoncommercial use policies, not to an existing open\u0002source license.27\nHowever, there are disadvantages to such flexibility. \nJust because different licenses might be allowed doesn’t \nmean these licenses will be fully understood by all users. \nAdopting multiple licenses may result in increased \naccidental infringement. Indeed, a study conducted by \nthe Institute of Electrical and Electronics Engineers found \nthat “although [software] developers clearly understood \ncases involving one license, they struggled when multiple \nlicenses were involved,”28 and in particular, were found \nto “lack the knowledge and understanding to tease apart \nlicense interactions across multiple situations.”29\nIn particular, researchers unfamiliar with the \nallowances provided by different data licenses, in \ncontexts where more than one license is implemented, \nmay lead to certain licenses being violated. For example, \nwhen researchers were surveyed regarding their \nunderstanding of copyright transfer agreements in the IP \ncommercialization process, they only demonstrated an \naverage 33 percent score on a knowledge-testing survey.30\n(2) Uniform Licensing Agreement \nThe second option available to the NRC would be to \nmandate that all private data be licensed under a single \nuniform license. For the NRC administration itself, this \nmay be the more straightforward option, since users \ncould be notified upon login about the appropriate use \nof data. The disadvantage of this strategy is that it may \ndeter would-be researchers who would share data under \na narrower license.31 Given the desire to allow researchers \nto innovate freely, there may be concerns about adopting \na restrictive licensing agreement. Nonetheless, several \noptions of licensing agreements would still be available \nfor adoption, and this pathway would require choosing a \nuniform agreement from these options, with the possibility \nof allowing an opt-out of this default license. \nIf the NRC were to implement a uniform license, \nit could look to the licensing agreements leveraged \nby institutional research clouds, such as the Harvard \nDataverse as an analogy in determining best practices \nfor its own licensing agreements. The model adopted by \nthe Dataverse is a default use of the CC0 Public Domain \nDedication “because of its name recognition in the \nscientific community” and its “use by repositories as well \nas scientific journals that require the deposit of open \ndata.”32 Like an unrestricted Creative Commons or Open \nData license, a public domain license would allow the data \nit governs to be used in any context, even commercial \nones, and would also allow reproduction and creation of \nderivatives from the data. \nAlternatively, the NRC could have a default open \nlicense while also permitting researchers to choose from \na handful of more restrictive licenses if they wish. For \nexample, the Harvard Dataverse notably allows uploaders \nto optout of the CC0 if needed and specify custom terms \nof use. The Australian Research Data Commons and \ndata-sharing platform FigShare33 also use a default CC0 \nlicense but nevertheless permit researchers to use a \nconditioned Creative Commons license. These conditioned \nlicenses can, for instance, require attribution to the \nA Blueprint for the National Research Cloud 88\noriginal owner, prevent exact reproduction, or only allow \nuse for noncommercial contexts. This may also help \naccommodate researchers who seek to upload datasets \nincorporating third-party data that holds a more restrictive \nlicense, since a “combined dataset will adopt the most \nrestrictive condition(s) of its component parts.”34\nIf the NRC goes down this route of giving users the \nchoice of a narrower license, it would also shift some \nliability to users—or to the NRC itself—by relying on \nusers to abide by the license. Approaches to enforcement \nwould vary, depending on the amount of responsibility \nin enforcement, and by extension liability the NRC seeks \nto take on. For example, in the Harvard Dataverse, if an \nuploader decides to opt out of a default open license \nand pursue their own custom licensing agreement \nover uploaded data, the Dataverse’s General Terms of \nUse absolve this particular cloud from resource-heavy \nenforcement responsibilities by stating that it “has no \nobligation to aid or support either party of the Agreement \nin the execution or enforcement of the Data Use \nAgreement’s terms.”35\nC. CURRENT STATE OF AI ETHICS \nFRAMEWORKS\nAI ethics frameworks (or principles, guidelines) \nattempt to address the ethical concerns related to \nthe development, deployment, and use of AI within \nprospective organizations. We briefly discuss the current \nlandscape of AI ethics frameworks, while noting that this is \nstill an emergent topic without broad consensus.\nBetween 2015 and 2020, governments, technology \ncompanies, international organizations, professional \norganizations, and researchers around the world have \npublished some 117 documents related to AI ethics.36\nThese frameworks aim to tackle the disruptive potential \nof AI technologies by producing normative principles \nand “best practice” recommendations.37 Due to the \nprominence of essentially contested concepts in AI ethics—\ni.e., words such as fairness, equity, privacy that have \ndifferent meanings for different audiences38—as well as \nthe lack of binding professional history and accountability \nmechanisms, those frameworks are often high level and \nself-regulatory, posing little threat to potential breaches to \nethical conduct.39\nFederal Frameworks\nIn the United States, there is no central guiding \nframework on the responsible development and \napplication of AI across the federal government. Some \ngovernment agencies have adopted or are in the process \nof adopting their own AI framework, while others have not \npublished such guidelines. The following are published \nfederal AI ethical frameworks as of August 2021:\n• After 15 months of deliberation with leading \nAI experts, the Department of Defense (DOD) \nadopted a series of ethical principles for the use \nof AI in February 2020 that align with the existing \nDOD mission and stakeholders.40\n• The General Services Administration (GSA), \ntasked by the Office of Management and Budget \n(OMB) in the Federal Data Strategy 2020 Action \nPlan, developed a Data Ethics Framework in \nFebruary 2020 to help federal personnel make \nethical decisions as they acquire, manage, and \nuse data.41\n• The Government Accountability Office (GAO) \ndeveloped an AI accountability framework \nin June 2020 for federal agencies and other \nentities involved in the design, development, \ndeployment, and continuous monitoring of \nAI systems to help ensure accountability and \nresponsible use of AI.42\n• The Office of the Director of National Intelligence \n(ODNI) released the Principles of AI Ethics for \nthe Intelligence Community in July 2020 to \nguide the intelligence community’s (IC) ethical \ndevelopment and use of AI to solve intelligence \nproblems.43\n• The National Security Commission on Artificial \nIntelligence (NSCAI) published a set of best \npractices in July 2020 (later revised and \nA Blueprint for the National Research Cloud 89\nintegrated into the Commission’s 2021 Final \nReport) for agencies critical to national security \nto implement as a paradigm for the responsible \ndevelopment and fielding of AI systems.44\nWhile these frameworks can help guide the NRC’s \napproach to ethics, we refrain from recommending a \nspecific framework for several reasons. First, despite \ngrowing calls for applied ethics in the AI community, \ndeveloping an AI ethics framework is still an emerging \narea. The lack of a unified government standard poses \nchallenges to the establishment of the NRC’s ethics review \nprocess.\nSecond, there are, in fact, significant differences \namong ethics frameworks published by various federal \nagencies. For example, NSCAI laid out differences between \nits recommended practices and those by DOD and IC.45\nMoreover, among the five frameworks above, the GSA \nFramework focused only on the ethical conduct of federal \nemployees when dealing with data while others focused \non the ethical development and application of AI systems \nspecifically. \nThird, the ethics framework for adopting AI technology \nmay be different from a framework for assessing research. \nMost federal agencies develop frameworks to guide the \nuse of AI-driven solutions for agency-specific tasks. For \nexample, DOD’s ethical principles only apply to defense\u0002specific combat or noncombat AI systems.46 In the absence \nof a central federal guideline, the NRC should not adopt \na framework by a particular agency because these \nframeworks are not necessarily designed for the wide \nrange of research contemplated for the NRC. The work on \nframeworks may nonetheless provide a useful starting \npoint for NRC’s ethics process.\nD. STAFFING AND EXPERTISE\nAs noted throughout this White Paper, the success of \nthe NRC will depend on human resources—both within \nthe NRC as well as across government— to resolve the \nmany challenges the NRC promises to tackle. While we \nrefrain from providing an organizational chart, we list the \ndimensions where staffing and expertise will be critical \nto the success of the NRC. This list is not meant to be \nexhaustive, but to highlight the vital importance of human \nresources.\nHuman Resource Areas\n• Computing\n°\n System administrators\n°\n Data center engineers\n°\n Research software engineers\n°\n Research application developers\n• Data\n°\n Data officers\n°\n Agency liaisons\n°\n Data architects\n°\n Data scientists\n• Grant administrators\n• Contracting officers\n• Support and training staff \n• Privacy staff (technical and legal)\n• Ethics staff\n• Cybersecurity staff\nA Blueprint for the National Research Cloud 90\nExecutive Summary\n1 Klaus Schwab, The Fourth Industrial Revolution (2016).\n2 Tae Yano & Moonyoung Kang, Taking Advantage of Wikipedia in Natural Language Processing, Carnegie Mellon U. (2008), https://www.cs.cmu.\nedu/~taey/pub/wiki.pdf.\n3 See, e.g., Anthony Alford, Google Trains Two Billion Parameter AI Vision Model, InfoQ (June 22, 2021), https://www.infoq.com/news/2021/06/google\u0002vision-transformer/; Anthony Alford, OpenAI Announces GPT-3 AI Language Model with 175 Billion Parameters, InfoQ (June 2, 2020), https://www.infoq.\ncom/news/2020/06/openai-gpt3-language-model/.\n4 AlphaGo, DeepMind (2021), https://deepmind.com/research/case-studies/alphago-the-story-so-far/. \n5 Benjamin F. Jones & Lawrence H. Summers, A Calculation of the Social Returns to Innovation (Nat’l Bureau of Econ. Research, Working Paper No. \n27863, 2020); J.G. Tewksbury, M.S. Crandall & W.E. Crane, Measuring the Societal Benefits of Innovation, 209 Sci. Mag. 658-62 (1980); see also National \nAcademies of Sciences, Engineering, and Medicine, Returns to Federal Investments in the Innovation System (2017)\n6 Stuart Zweben & Betsy Bizot, 2019 Taulbee Survey: Total Undergrad CS Enrollment Rises Again, but with Fewer New Majors; Doctoral Degree \nProduction Recovers From Last Year’s Dip (2019). \n7 Jathan Sadowski, When Data is Capital: Datafication, Accumulation, and Extraction, 2019 Big Data & Soc’y 1 (2019). \n8 Amy O’Hara & Carla Medalia, Data Sharing in the Federal Statistical System: Impediments and Possibilities, 675 Annals Am. Acad. Pol. & Soc. Sci. 138, \n140-41 (2018).\n9 Nat’l Security Comm’n on Artificial Intelligence, Final Report 186 (2021).\n10 Stan. U. Inst. for Human-Centered Artificial Intelligence, 2021 Artificial Intelligence Index Report 118 (2021).\n11 Id.\n12 Id.\n13 Neil C. Thompson, Shuning Ge & Yash M. Sherry, Building the Algorithm Commons: Who Discovered the Algorithms that Underpin Computing in the \nModern Enterprise?, 11 Global Strategy J. 17-33 (2020).\n14 See, e.g., U.S. Gov’t Accountability Office, Federal Agencies Need to Address Aging Legacy Systems (2016); U.S. Gov’t Accountability Office, \nCloud Computing: Agencies Have Increased Usage and Realized Benefits, but Cost and Savings Data Need To Be Better Tracked (2019).\n15 David Freeman Engstrom, Daniel E. Ho, Catherine M. Sharkey & Mariano-Florentino Cuéllar, Government by Algorithm: Artificial \nIntelligence in Federal Administrative Agencies 6, 71-72 (2020).\n16 William M. (Mac) Thornberry National Defense Authorization Act for Fiscal Year 2021, Pub. L. No. 116-283, § 5106.\n17 The Biden Administration Launches the National Artificial Intelligence Research Resource Task Force, The White House (June 10, 2021), https://www.\nwhitehouse.gov/ostp/news-updates/2021/06/10/the-biden-administration-launches-the-national-artificial-intelligence-research-resource-task-force/. \n18 William M. (Mac) Thornberry National Defense Authorization Act for Fiscal Year 2021, Pub. L. No. 116-283, § 5107 (g).\n19 Nat’l Security Comm’n on Artificial Intelligence, supra note 9, at 191.\n20 See, e.g., Cloudbank, https://www.cloudbank.org; Fact Sheet: National Secure Data Service Act Advances Responsible Data Sharing in Government, \nData Coalition (May 13, 2021), https://www.datacoalition.org/fact-sheet-national-secure-data-service-act-advances-responsible-data-sharing-in\u0002government/. \n21 Steve Lohr, Universities and Tech Giants Back National Cloud Computing Project, N.Y. Times (June 30, 2020), https://www.nytimes.com/2020/06/30/\ntechnology/national-cloud-computing-project.html; John Etchemendy & Fei-Fei Li, National Research Cloud: Ensuring the Continuation of American \nInnovation, Stan. U. Inst. for Human-Centered Artificial Intelligence, (Mar. 28, 2020), https://hai.stanford.edu/news/national-research-cloud\u0002ensuring-continuation-american-innovation.\n22 Jennifer Villa & Dave Troiano, Choosing Your Deep Learning Infrastructure; The Cloud vs. On-Prem Debate, Determined AI (July 30, 2020), https://\ndetermined.ai/blog/cloud-v-onprem/; Is HPC Going to Cost Me a Fortune?, InsideHPC (last visited July 23, 2021), https://insidehpc.com/hpc-basic\u0002training/is-hpc-going-to-cost-me-a-fortune/.\n23 See, e.g., US Plans $1.8 Billion Spend on DOE Exascale Supercomputing, HPCwire (Apr. 11, 2018), https://www.hpcwire.com/2018/04/11/us-plans-1-\n8-billion-spend-on-doe-exascale-supercomputing/; Federal Government, Advanced HPC (last visited July 23, 2021), https://www.advancedhpc.com/\npages/federal-government; United States Continues to Lead World In Supercomputing, U.S. Dep’t. Energy (Nov. 18, 2019), https://www.energy.gov/\narticles/united-states-continues-lead-world-supercomputing.\n24 See NSF Funds Five New XSEDE-Allocated Systems, Nat’l Sci. Found. (Aug. 10, 2020), https://www.xsede.org/-/nsf-funds-five-new-xsede-allocated\u0002systems.\n25 Cloudbank, supra note 20.\n26 See, e.g., National Data Service, http://www.nationaldataservice.org; The Open Science Data Cloud, https://www.opensciencedatacloud.org; Harvard \nDataverse, https://dataverse.harvard.edu; FigShare, https://figshare.com. \n27 FedRAMP, https://www.fedramp.gov.\n28 See Fact Sheet: National Secure Data Service Act Advances Responsible Data Sharing in Government, Data Coalition (May 13, 2021), https://www.\ndatacoalition.org/fact-sheet-national-secure-data-service-act-advances-responsible-data-sharing-in-government/.\n29 See Administrative Data Research Facility, Coleridge Initiative, https://coleridgeinitiative.org/adrf/ (last visited July 26, 2021). \n30 See Landsat Data Access, U.S. Geological Survey, https://www.usgs.gov/core-science-systems/nli/landsat/landsat-data-access (last visited July 23, \n2021); Fed. Geographic Data Comm., The Value Proposition for Landsat Applications (2014); Crista L. Straub, Stephen R. Koontz & John B. Loomis, \nEconomic Valuation of Landsat Imagery (2019).\nEndnotes\nA Blueprint for the National Research Cloud 91\n31 See Bipartisan Pol’y Ctr., Barriers to Using Government Data: Extended Analysis of the U.S. Commission on Evidence-Based Policymaking’s \nSurvey of Federal Agencies and Offices 18-20 (2018); see also U.S. Dep’t of Health & Human Services, The State of Data Sharing at the U.S. \nDepartment of Health and Human Services 4 (2018) (describing how data at the agency is “largely kept in silos with a lack of organizational awareness \nof what data are collected across the Department and how to request access.”).\n32 Privacy Act, 5 U.S.C. § 552a (1974).\n33 Michael S. Bernstein et al., ESR: Ethics and Society Review of Artificial Intelligence Research, Cornell. U. (July 9, 2021), https://arxiv.org/\npdf/2106.11521.pdf.\n34 Courtenay R. Bruce et al., An Embedded Model for Ethics Consultation: Characteristics, Outcomes, and Challenges, 5 AJOB Empirical Bioethics 8 \n(2014).\nIntroduction\n1 National Research Cloud Call to Action, Stan. U. Inst. For Human-Centered Artificial Intelligence, https://hai.stanford.edu/national-re\u0002search-cloud-joint-letter.\n2 See id.; John Etchemendy & Fei-Fei Li, National Research Cloud: Ensuring the Continuation of American Innovation, Stan. U. Inst. For Human-Centered \nArtificial Intelligence (Mar. 28, 2020), https://hai.stanford.edu/news/national-research-cloud-ensuring-continuation-american-innovation.\n3 William M. (Mac) Thornberry National Defense Authorization Act for Fiscal Year 2021, Pub. L. No. 116-283, § 5106.\n4 The Biden Administration Launches the National Artificial Intelligence Research Resource Task Force, The White House (June 10, 2021), https://www.\nwhitehouse.gov/ostp/news-updates/2021/06/10/the-biden-administration-launches-the-national-artificial-intelligence-research-resource-task-force/. \n5 Privacy Act of 1974, 5 U.S.C. § 552a (2012).\n6 Foundations for Evidence-Based Policymaking Act of 2017, Pub. L. No. 115-435, 132 Stat. 5529 (2019).\n7 See Fact Sheet: National Secure Data Service Act Advances Responsible Data Sharing in Government, Data Coalition (May 13, 2021), https://www.data\u0002coalition.org/fact-sheet-national-secure-data-service-act-advances-responsible-data-sharing-in-government/.\n8 See, e.g., Facial Recognition and Biometric Technology Moratorium Act, S. 4084, 116th Cong. (2020); Bhaskar Chakravorti, Biden’s ‘Antitrust Revolution’ \nOverlooks AI—at Americans’ Peril, Wired (July 27, 2021), https://www.wired.com/story/opinion-bidens-antitrust-revolution-overlooks-ai-at-ameri\u0002cans-peril/.\nChapter 1\n1 See Stephen Breyer, Regulation and Its Reform (1982); Clifford Winston, Government Failure Versus Market Failure (2006).\n2 Largest Companies by Market Cap, Companies Market Cap (2021), https://companiesmarketcap.com.\n3 Stan. U. Inst. for Human-Centered Artificial Intelligence, 2021 Artificial Intelligence Index Report 93 (2021).\n4 See, e.g., Mary L. Gray & Siddarth Suri, Ghost Work: How to Stop Silicon Valley from Building a New Global Underclass (2019); Craig Webster & \nStanislav Ivanov, Robotics, Artificial Intelligence, and the Evolving Nature of Work 132-35 (2019); Weiyu Wang & Keng Siau, Artificial Intelligence, \nMachine Learning, Automation, Robotics, Future of Work and Future of Humanity: \nA Review and Research Agenda, 30 J. Database Mgmt. 61 (2019).\n5 AlphaFold: A Solution to a 50-Year-Old Grand Challenge in Biology, DeepMind (Nov. 30, 2020), https://deepmind.com/blog/article/alphafold-a-solution\u0002to-a-50-year-old-grand-challenge-in-biology.\n6 Tanha Talaviya et al., Implementation of Artificial Intelligence in Agriculture for Optimisation of Irrigation and Application of Pesticides and Herbicides, 4 \nArtificial Intelligence in Agriculture 58 (2020). \n7 Greg Allen & Taniel Chan, Artificial Intelligence and National Security, Harv. Kennedy Sch. Belfer Ctr. (July 2017), https://www.belfercenter.org/\npublication/artificial-intelligence-and-national-security. \n8 Stan. U. Inst. for Human-Centered Artificial Intelligence, supra note 3.\n9 Jeffrey Ding, Deciphering China’s AI Dream (2018).\n10 Fugaku is being used extensively for AI research initiatives. See Atsushi Nukariya et al., HPC and AI Initiatives for Supercomputer Fugaku and Future \nProspects, Fujitsu (Nov. 11, 2020), https://www.fujitsu.com/global/about/resources/publications/technicalreview/2020-03/article09.html.\n11 Eng’g & Physical Sciences Research Council, The Impact of HECToR (2014).\n12 Joshua New, Why the United States Needs a National Artificial Intelligence Strategy and What it Should Look Like (2018).\n13 Maggie Miller, White House Establishes National Artificial Intelligence Office, The Hill (Jan. 12, 2021), https://thehill.com/policy/cybersecurity/533922-\nwhite-house-establishes-national-artificial-intelligence-office.\n14 See Fast Track Action Comm. on Strategic Computing, National Strategic Computing Initiative Update: Pioneering the Future of Computing\n(2019), https://www.nitrd.gov/pubs/National-Strategic-Computing-Initiative-Update-2019.pdf.\n15 Nat’l Security Comm’n on Artificial Intelligence, Final Report (2021).\n16 The COVID-19 High Performance Computing Consortium, COVID-19 HPC Consortium, https://covid19-hpc-consortium.org.\n17 See Aaron L. Friedberg, Science, the Cold War, and the American State, 20 Diplomatic Hist. 107, 112 (1996); Sean Pool & Jennifer Erickson, The High \nReturn on Investment for Publicly Funded Research, Ctr. for Am. Progress (Dec. 10, 2012), https://www.americanprogress.org/issues/economy/\nreports/2012/12/10/47481/the-high-return-on-investment-for-publicly-funded-research/.\n18 Peter L. Singer, Federally Supported Innovations: 22 Examples of Major Technology Advances That Stem from Federal Research Support \n14-15 (2014).\n19 Nat’l Research Council, Government Support for Computing Research 136-55 (1999). \n20 Nat’l Security Comm’n on Artificial Intelligence, supra note 15, at 185.\n21 Philippe Aghion, Benjamin F. Jones & Charles I. Jones, Artificial Intelligence and Economic Growth, in The Economics of Artificial Intelligence: An \nAgenda 237 (2019).\nA Blueprint for the National Research Cloud 92\n22 Ian Moll, The Myth of the Fourth Industrial Revolution, 68 Theoria 1 (2021); see also Tim Unwin, 5 Problems with 4th Industrial Revolution, ICT Works \n(Mar. 23, 2019), https://www.ictworks.org/problems-fourth-industrial-revolution/.\n23 See, e.g., Geoffrey A. Manne & Joshua D. Wright, Google and the Limits of Antitrust: The Case Against the Antitrust Case Against Google, 34 Harv. J.L. & \nPub. Pol’y 1 (2011); Lina M. Khan, Amazon’s Antitrust Paradox, 126 Yale L.J. 710 (2016).\n24 David Patterson et al., Carbon Emissions and Large Neural Network Training, Cornell U. (Apr. 23, 2021), https://arxiv.org/pdf/2104.10350.pdf. To be \nclear, however, the study found that training other sophisticated but smaller NLP models such as Meena and T5 required approximately 96 and 48 tons \nof carbon dioxide, respectively. Id. Another study found that the training state-of-the-art NLP models produced approximately 626,000 pounds (313 \ntons) of carbon dioxide, five times the lifetime emissions of the average car in the United States. Emma Strubell, Ananya Ganesh & Andrew McCallum, \nEnergy and Policy Considerations for Deep Learning in NLP, Cornell U. (2019), https://arxiv.org/pdf/1906.02243.pdf.\n25 Calculate Your Carbon Footprint, The Nature Conservancy, https://www.nature.org/en-us/get-involved/how-to-help/carbon-footprint-calculator/.\n26 Economic studies in other fields also show that increasing access, supply, or quality of certain goods without appropriate pricing mechanisms or \nregulatory interventions can lead to over-use and waste. See, e.g., Chengri Ding & Shunfeng Song, Traffic Paradoxes and Economic Solutions, 1 J. Urban \nMgmt. 63 (2012) (roads and traffic congestion); Ari Mwachofi & Assaf F. Al-Assaf, Health Care Market Deviations from the Ideal Market, 11 Sultan Qaboos \nUniv. Med. J. 328 (2011) (doctors and quality of care).\n27 See Emily M. Bender et al., On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?, 2021 Proceedings ACM Conf. on Fairness, \nAccountability & Transparency 610 (2021).\n28 See Joy Buolamwini & Timnit Gebru, Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification, 81 Proceeding Machine \nLearning Res. 1 (2018); Inioluwa Deborah Raji & Joy Buolamwini, Actionable Auditing: Investigating the Impact of Publicly Naming Biased Performance \nResults of Commercial AI Products, 2019 Proceedings AAAI/ACM Conf. on AI, Ethics & Soc’y 429 (2019).\n29 See Virginia Eubanks, Automating Inequality (2018); Cathy O’Neil, Weapons of Math Destruction (2016).\n30 See Christopher Whyte, Deepfake News: AI-Enabled Disinformation as a Multi-Level Public Policy Challenge, 5 J. Cyber Pol’y 199 (2020); Jeffrey Dastin, \nAmazon Scrapes Secret AI Recruiting Tool that Showed Bias Against Women, Reuters (Oct. 10, 2018), https://www.reuters.com/article/us-amazon-com\u0002jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G; James Vincent, Google ‘Fixed’ its \nRacist Algorithm by Removing Gorillas from its Image-Labeling Tech, The Verge (Jan. 12, 2018), https://www.theverge.com/2018/1/12/16882408/google\u0002racist-gorillas-photo-recognition-algorithm-ai;.\n31 Kate Crawford, Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence 211 (2021) (“AI systems are built to see and \nintervene in the world in ways that primarily benefit the states, institutions, and corporations that they serve. In this sense, AI systems are expressions \nof power that emerge from wider economic and political forces, created to increase profits and centralize control for those who wield them.”).\n32 Elizabeth Gibney, Self-Taught AI is Best Yet at Strategy Game Go, Nature (Oct. 18, 2017), https://www.nature.com/news/self-taught-ai-is-best-yet-at\u0002strategy-game-go-1.22858.\n33 Bill Schackner, Carnegie Mellon’s Prestigious Computer Science School has a New Leader, Pittsburgh Post-Gazette (Aug. 8, 2019), https://www.post\u0002gazette.com/news/education/2019/08/08/Carnegie-Mellon-University-computer-science-Martial-Hebert-dean-artificial-intgelligence-google-robotics/\nstories/201908080096.\n34 Bipartisan Pol’y Ctr, Cementing American Artificial Intelligence Leadership: AI Research & Development (2020).\n35 Nur Ahmed & Muntasir Wahed, The De-Democratization of AI: Deep Learning and the Compute Divide in Artificial Intelligence Research, Cornell U.\n(Oct. 22, 2020), https://arxiv.org/pdf/2010.15581.pdf.\n36 Id.\n37 Fei-Fei Li, America’s Global Leadership in Human-Centered AI Can’t Come From Industry Alone, The Hill (July 6, 2021), https://thehill.com/opinion/\ntechnology/561638-americas-global-leadership-in-human-centered-ai-cant-come-from-industry?rl=1.\n38 Cade Metz, A.I. Researchers Are Making More Than $1 Million, Even at a Nonprofit, N.Y. Times (Apr. 19, 2018), https://www.nytimes.com/2018/04/19/\ntechnology/artificial-intelligence-salaries-openai.html. \n39 Stan. U. Inst. for Human-Centered Artificial Intelligence, supra note 3, at 118.\n40 Michael Gofman & Zhao Jin, Artificial Intelligence, Education, and Entrepreneurship, SSRN (Sept. 17, 2019), https://papers.ssrn.com/sol3/papers.\ncfm?abstract_id=3449440.\n41 Jathan Sadowski, When Data is Capital: Datafication, Accumulation, and Extraction, 2019 Big Data & Soc’y 1 (2019).\n42 For example, researchers have clamored for Facebook to share some of its proprietary data so they can better understand the effect of social media \non politics and societal discourse. Simon Hegelich, Facebook Needs to Share More with Researchers, Nature (Mar. 24, 2020), https://www.nature.com/\narticles/d41586-020-00828-5.\n43 Ashlee Vance, This Tech Bubble Is Different, Bloomberg (Apr. 14, 2011), https://www.bloomberg.com/news/articles/2011-04-14/this-tech-bubble-is\u0002different. \n44 Amy O’Hara & Carla Medalia, Data Sharing in the Federal Statistical System: Impediments and Possibilities, 675 Annals Am. Acad. Pol. & Soc. Sci. 138, \n140-41 (2018).\n45 Dario Amodei & Danny Hernandez, AI and Compute, Open AI (May 16, 2018), https://openai.com/blog/ai-and-compute/.\n46 See, e.g., Ahmed & Wahed, supra note 35; Ian Sample, ‘We Can’t Compete’: Why Universities Are Losing Their Best AI Scientists, The Guardian (Nov. 1, \n2017), https://www.theguardian.com/science/2017/nov/01/cant-compete-universities-losing-best-ai-scientists.\n47 Neil C. Thompson, Shuning Ge & Yash M. Sherry, Building the Algorithm Commons: Who Discovered the Algorithms that Underpin Computing in the \nModern Enterprise?, 11 Global Strategy J. 17-33 (2020).\n48 Minkyung Baek, RoseTTAFold: Accurate Protein Structure Prediction Accessible to All, U. Wash. Inst. for Protein Design (July 15, 2021), https://www.\nipd.uw.edu/2021/07/rosettafold-accurate-protein-structure-prediction-accessible-to-all/; Minkyung Baek et al., Accurate Prediction of Protein Structures \nand Interactions Using a Three-Track Neural Network, Sci. Mag. (July 15, 2021), https://science.sciencemag.org/content/sci/early/2021/07/19/science.\nabj8754.full.pdf.\n49 How Diplomacy Helped to End the Race to Sequence the Human Genome, Nature (June 24, 2020), https://www.nature.com/articles/d41586-020-\n01849-w.\n50 Joel Klinger et al., A Narrowing of AI Research?, Cornell U. (Nov. 17, 2020), https://arxiv.org/pdf/2009.10385.pdf.\n51 Id. \n52 Alex Tamkin et al., Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models, Cornell U. (Feb. 4, 2021), https://arxiv.\norg/pdf/2102.02503.pdf.\nA Blueprint for the National Research Cloud 93\n53 Those 5 were Boston, San Francisco, San Jose, Seattle and San Diego. See Robert D. Atkinson, Mark Muro & Jacob Whiton, The Case for Growth \nCenters: How to Spread Tech Innovation Across America, Brookings (Dec. 9, 2019), https://www.brookings.edu/research/growth-centers-how-to-spread\u0002tech-innovation-across-america/.\n54 Interview with Professor Erik Brynjolfsson, Director, Stanford Digital Economy Lab (2021). \n55 Solon Barocas & Andrew D. Selbst, Big Data’s Disparate Impact, 104 Cal. L. Rev. 671 (2016).\nChapter 2\n1 “Principal Investigator” status may differ from university to university, but typically represents the core faculty that are eligible to oversee research \nprojects at their home institutions.\n2 See Beth Jensen, AI Index Diversity Report: An Unmoving Needle, Stan. U. Inst. for Human-Centered Artificial Intelligence (May 3, 2021), https://hai.\nstanford.edu/news/ai-index-diversity-report-unmoving-needle.\n3 For a perspective, for instance, on the importance of modeling and simulation in physics, see Karen E. Wilcox, Omar Ghattas & Patrick Heimbach, The \nImperative of Physics-Based Modeling and Inverse Theory in Computational Science, 1 Nature Comp. Sci. 166 (2021).\n4 15 U.S.C. § 9415 (emphasis added). \n5 Id. (emphasis added). \n6 Contemporaneous accounts corroborate this core focus. The National Security Commission on AI, for instance, describes the proposal as “provid[ing] \nverified researchers and students subsidized access to scalable compute resources” with a specific reference to the “compute divide” that has left “middle\u0002and lower-tier universities [lacking] the resources necessary for cutting-edge AI research.” Nat’l Security Comm’n on A.I., Final Report 191, 197 (2021) \n(emphasis added). Upon the announcement of the NRC legislation, Jeff Dean, SVP of Google Research and Google Health, noted, “A National AI Research \nResource will help accelerate US progress in artificial intelligence and advanced technologies by providing academic researchers access to the cloud \ncomputing resources necessary for experiments at scale.” Brandi Vincent, Congress Inches Closer to Creating a National Cloud for AI Research, NextGov (July \n2, 2020), https://www.nextgov.com/emerging-tech/2020/07/congress-inches-closer-creating-national-cloud-ai-research/166624/ (emphasis added). Others \nhave suggested that “researchers” under NRC could include individuals at small businesses, start-up companies, non-profits, and certain technology firms. \nOne co-sponsor of the legislation, for instance, suggested that NRC resources should be provided to “developers” and “entrepreneurs.” Portman, Heinrich \nIntroduce Bipartisan Legislation to Develop National Cloud Computer for AI Research, Rob Portman, U.S. Senator for Ohio (June 4, 2020), https://www.\nportman.senate.gov/newsroom/press-releases/portman-heinrich-introduce-bipartisan-legislation-develop-national-cloud.\n7 Frequently Asked Questions About Small Businesses, U.S. Small Bus. Admin. Office of Advoc. (Oct. 2020), https://cdn.advocacy.sba.gov/wp-content/\nuploads/2020/11/05122043/Small-Business-FAQ-2020.pdf. \n8 Louise Balle, Information on Small Business Startups, Houston Chron., https://smallbusiness.chron.com/information-small-business-startups-2491.\nhtml. \n9 Such entities could potentially collaborate with academic partners, and the NRC would of course also need to set rules about collaborator eligibility. \n10 PI status provides a level of standardization across faculty compared to other metrics, such as tenure-track or designation as research faculty. For \nexample, the University of Michigan appoints individuals focused on full-time research as “research faculty,” which is not a tenure track position. In \ncontrast, research faculty at Purdue are eligible for tenure-track. Distinct from the categorization used by both universities, MIT designates full-time \nresearchers as “academic staff” rather than faculty. All three types of researchers, however, qualify for principal investigator status at their respective \nuniversities. Some universities go further by providing temporary PI status to non-PI status individuals affiliated with the university for a single project \n(including all three universities mentioned previously).\n11 Community & Education Resource Requests, CloudBank, https://www.cloudbank.org/training/cloudbank-community#toc-eligibilit-36nfpcrS. \n12 Apply for an Account, Compute Canada, https://www.computecanada.ca/research-portal/account-management/apply-for-an-account/. \n13 Nat’l Sci. Bd., Science & Engineering Indicators 2016, Academic Research and Development 72 (2016). \n14 Id.\n15 College Enrollment in the United States from 1965 to 2019 and Projections up to 2029 for Public and Private Colleges, Statista (Jan. 2021), https://www.\nstatista.com/statistics/183995/us-college-enrollment-and-projections-in-public-and-private-institutions/.\n16 Colaboratory – Frequently Asked Questions, Google, https://research.google.com/colaboratory/faq.html.\n17 Weekly Maximum GPU Usage, Kaggle (2019), https://www.kaggle.com/general/108481.\n18 Community & Education Resource Requests, supra note 11.\n19 Merit Review: Why You Should Volunteer to Serve as an NSF Reviewer, Nat’l Sci. Found., https://www.nsf.gov/bfa/dias/policy/merit_review/reviewer.\njsp#1.\n20 See XSEDE Campus Champions, XSEDE, https://www.xsede.org/community-engagement/campus-champions.\n21 Compute Canada, for instance, provides access to 15% of PIs to increased compute capacity based on a merit competition. In 2021, Compute \nCanada completed its review of 650 research submissions in about five months with only 80 volunteer reviewers from Canadian academic institutions \nto assess the scientific merit of the proposal. Resource Allocation Competitions, Compute Canada, https://www.computecanada.ca/research-portal/\naccessing-resources/resource-allocation-competitions/; 2021 Resource Allocations Competition Results, Compute Canada, https://www.computecanada.\nca/research-portal/accessing-resources/resource-allocation-competitions/rac-2021-results/. Compare this with CloudBank, which allocates compute \nresources by leveraging NSF’s grant administration process: In 2019, NSF needed 30,000 volunteer reviewers to handle over 40,000 proposals, with \neach proposal requiring about 10 months to process from start to finish. Nat’l. Sci. Found., Merit Review Process: Fiscal Year 2019 Digest (2020); NSF \nProposal and Award Process, Nat’l Sci. Found., https://www.nsf.gov/attachments/116169/public/nsf_proposal_and_award_process.pdf.\n22 Another boundary question will be the resource allocation to PIs that are affiliated both with universities and with private companies. As a default, \nNRC resources should go toward academic projects, and not subsidize work that is conducted in private researcher capacity. \n23 Resource Allocation Competitions, supra note 21.\n24 Simplifying Cloud Services, Sci. Node (Dec. 2, 2019), https://sciencenode.org/feature/An%20easier%20cloud.php. \n25 Frequently Asked Questions (FAQ), CloudBank, https://www.cloudbank.org/faq.\n26 Simplifying Cloud Services, supra note 25.\n27 Id.\nA Blueprint for the National Research Cloud 94\n28 Id.\n29 Frequently Asked Questions (FAQ), supra note 26.\n30 Frequently Asked Questions (FAQs) for Budgeting for Cloud Computing Resources via CloudBank in NSF Proposals, Nat’l Sci. Found., https://www.nsf.\ngov/pubs/2020/nsf20108/nsf20108.jsp.\n31 Simplifying Access to Cloud Resources for Researchers: CloudBank, Amazon Web Serv. (Nov. 16, 2020), https://aws.amazon.com/blogs/publicsector/\nsimplifying-access-cloud-resources-researchers-cloudbank/.\n32 Community & Education Resource Requests, supra note 11.\n33 Larry Dignan, AWS Cloud Computing Ops, Data Centers, 1.3 Million Servers Creating Efficiency Flywheel, ZDNet (June 17, 2016), https://www.zdnet.\ncom/article/aws-cloud-computing-ops-data-centers-1-3-million-servers-creating-efficiency-flywheel/; Rich Miller, Ballmer: Microsoft Has 1 Million \nServers, Data Ctr. Knowledge (July 15, 2013), https://www.datacenterknowledge.com/archives/2013/07/15/ballmer-microsoft-has-1-million-servers; \nDaniel Oberhaus, Amazon, Google, Microsoft: Here’s Who Has the Greenest Cloud, Wired (Dec. 18, 2019), https://www.wired.com/story/amazon-google\u0002microsoft-green-clouds-and-hyperscale-data-centers/; Russell Brandom, Mapping out Amazon’s Invisible Server Empire, The Verge (May 10, 2019), \nhttps://www.theverge.com/2019/5/10/18563485/amazon-web-services-internet-location-map-data-center.\n34 See, e.g., AWS Pricing, Amazon Web Services, https://aws.amazon.com/pricing/; Overview of Cloud Billing Concepts, Google Cloud, https://cloud.\ngoogle.com/billing/docs/concepts; Azure Pricing, Azure, https://azure.microsoft.com/en-us/pricing/#product-pricing. \n35 Large research universities already negotiate enterprise agreements with cloud providers.\n36 What We Do, XSEDE, https://www.xsede.org/about/what-we-do (last visited Sept. 19, 2021).\n37 XSEDE Overall Organization, XSEDE Wiki, https://confluence.xsede.org/display/XT/XSEDE+Overall+Organization (last visited Sept. 19, 2021).\n38 XSEDE Allocations Info & Policies, XSEDE, https://portal.xsede.org/allocations/policies (last visited Sept. 19, 2021).\n39 Id.\n40 Id.\n41 Startup Allocations, XSEDE, https://portal.xsede.org/allocations/startup (last visited Sept. 19, 2021).\n42 Id.\n43 Id.\n44 Id.\n45 Research Allocations, XSEDE, https://portal.xsede.org/allocations/research (last visited Sept. 19, 2021).\n46 Id.\n47 Id.\n48 Id.\n49 XSEDE Allocations Info & Policies, supra note 36.\n50 XSEDE Campus Champions, supra note 20.\n51 Id.\n52 Id.\n53 XSEDE as a Collaborator on Proposals, XSEDE, https://www.xsede.org/about/collaborating-with-xsede (last visited Sept. 19, 2021).\n54 COVID-19 HPC Consortium, XSEDE, https://www.xsede.org/covid19-hpc-consortium (last visited Sept. 19, 2021).\n55 Amazon, for example, introduced its P4, P3, and P2 instances in 2020, 1997, and 1996, respectively. Frederic Lardinois, AWS Launches Its Next-Gen \nGPU Instances with 8 Nvidia A100 Tensor Core GPUs, TechCrunch (Nov. 2, 2020), https://social.techcrunch.com/2020/11/02/aws-launches-its-next-gen\u0002gpu-instances/; Ian C. Schafer, Amazon Elastic Compute Cloud P3 Launched alongside NVIDIA GPU Cloud, SD Times (Oct. 26, 2017), https://sdtimes.com/\nai/amazon-elastic-compute-cloud-p3-launched-alongside-nvidia-gpu-cloud/; Jeff Barr, New P2 Instance Type for Amazon EC2 – Up to 16 GPUs, Amazon \nWeb Services (Sept. 29, 2016), https://aws.amazon.com/blogs/aws/new-p2-instance-type-for-amazon-ec2-up-to-16-gpus/. The introduction years of \nthe P4 and P3 instances line up with the release of NVIDIA’s newest general purpose data center GPUs. \n56 See, e.g., Sarah Wang & Martin Casado, The Cost of Cloud, a Trillion Dollar Paradox, Andreessen Horowitz (May 27, 2021), https://a16z.\ncom/2021/05/27/cost-of-cloud-paradox-market-cap-cloud-lifecycle-scale-growth-repatriation-optimization/.\n57 Preston Smith et al., Community Clusters or the Cloud: Continuing Cost Assessment of On-Premises and Cloud HPC in Higher Education, 2019 \nProceedings Practice & Experience Advanced Res. Computing on Rise of the Machines 1 (2019). The amortized cost includes the annual compute \ncost, subsidized hardware cost, and power costs, but does not include personnel costs, as such costs are fixed and would be recurred regardless of \nwhether a cluster existed physically on-prem or on the cloud. Id.\n58 Craig A. Stewart et al., Return on Investment for Three Cyberinfrastructure Facilities: A Local Campus Supercomputer; the NSF-Funded Jetstream Cloud \nSystem; and XSEDE, 11 Int’l Conf. on Utility & Cloud Computing 223 (2018).\n59 Srijith Rajamohan & Robert E. Settlage, Informing the On/Off-prem Cloud Discussion in Higher Education, 2020 Practice & Experience Adv. Res. \nComputing 64 (2020). The cost sources include hardware, software services, software administration, electricity, and facilities but do not include \ncomputational scientists support, scientific software licenses, and data transfer costs. The study is also limited to Virginia Tech’s particular cloud \nworkload.\n60 Jennifer Villa & Dave Troiano, Choosing Your Deep Learning Infrastructure: The Cloud vs. On-Prem Debate, Determined AI (July 30, 2020), https://\ndetermined.ai/blog/cloud-v-onprem/; Is HPC Going to Cost Me a Fortune?, insideHPC, https://insidehpc.com/hpc-basic-training/is-hpc-going-to-cost\u0002me-a-fortune/.\n61 Interview with Suzanne Talon, Regional Director, Compute Canada (Jan. 14, 2021).\n62 Compute Canada, Cloud Computing for Researchers 1 (2016), https://www.computecanada.ca/wp-content/uploads/2015/02/CloudStrategy2016-\n2019-forresearchersEXTERNAL-1.pdf.\n63 US Plans $1.8 Billion Spend on DOE Exascale Supercomputing, HPCwire (Apr. 11, 2018), https://www.hpcwire.com/2018/04/11/us-plans-1-8-billion\u0002spend-on-doe-exascale-supercomputing/; Federal Government, Advanced HPC, https://www.advancedhpc.com/pages/federal-government; United \nStates Continues To Lead World In Supercomputing, Energy.gov, https://www.energy.gov/articles/united-states-continues-lead-world-supercomputing; \nHigh Performance Computing, Energy.gov, https://www.energy.gov/science/initiatives/high-performance-computing.\n64 See, e.g., DOE Announces Five New Energy Projects at LLNL, LLNL (Nov. 13, 2020), https://www.llnl.gov/news/doe-announces-five-new-energy\u0002projects-llnl; New HPCMP System at the AFRL DSRC DoD Supercomputing Resource Center to Provide over Nine PetaFLOPS of Computing Power to \nA Blueprint for the National Research Cloud 95\nAddress Physics, AI, and ML Applications for DoD Users, DOD HPC, https://www.hpc.mil/images/hpcdocs/newsroom/21-19_TI-21_web_announcement_\nAFRL_DSRC.pdf; Public Announcement, DOD HPC, https://www.hpc.mil/images/hpcdocs/newsroom/awards_and_press/HC101321D0002_PUBLIC_\nANNOUNCEMENT_20210505.pdf.\n65 Devin Coldewey, $600M Cray Supercomputer Will Tower Above the Rest — to Build Better Nukes, TechCrunch (Aug. 13, 2019), https://social.\ntechcrunch.com/2019/08/13/600m-cray-supercomputer-will-tower-above-the-rest-to-build-better-nukes/; CORAL-2 RFP, Oak Ridge Nat’l Laboratory\n(Apr. 9, 2018), https://procurement.ornl.gov/rfp/CORAL2/.\n66 See, e.g., NSF Funds Five New XSEDE-Allocated Systems, Nat’l Sci. Found. (Aug. 10, 2020), https://www.xsede.org/-/nsf-funds-five-new-xsede\u0002allocated-systems. \n67 Timothy Prickett Morgan, Bending The Supercomputing Cost Curve Down, The Next Platform (Dec. 2, 2019), http://www.nextplatform.\ncom/2019/12/02/bending-the-supercomputing-cost-curve-down/; Ben Dickson, The GPT-3 Economy, TechTalks (Sept. 21, 2020), https://bdtechtalks.\ncom/2020/09/21/gpt-3-economy-business-model/.\n68 Elijah Wolfson, The US Just Retook the Title of World’s Fastest Supercomputer from China, Quartz (June 9, 2018), https://qz.com/1301510/the-us-has\u0002the-worlds-fastest-supercomputer-again-the-200-petaflop-summit/.\n69 November 2020, TOP500 (Nov. 2020), https://www.top500.org/lists/top500/2020/11/.\n70 U.S. Department of Energy and Cray to Deliver Record-Setting Frontier Supercomputer at ORNL, Oak Ridge Nat’l Laboratory (May 7, 2019), https://\nwww.ornl.gov/news/us-department-energy-and-cray-deliver-record-setting-frontier-supercomputer-ornl.\n71 Coury Turczyn, Building an Exascale-Class Data Center, Oak Ridge Leadership Computing Facility (Dec. 11, 2020), https://www.olcf.ornl.\ngov/2020/12/11/building-an-exascale-class-data-center/.\n72 Don Clark, Intel Slips, and a High-Profile Supercomputer Is Delayed, N.Y. Times (Aug. 27, 2020), https://www.nytimes.com/2020/08/27/technology/\nintel-aurora-supercomputer.html; Mila Jasper, 10 of 15 of DOD’s Major IT Projects Are Behind Schedule, GAO Found, Nextgov (Jan. 4, 2021), https://www.\nnextgov.com/it-modernization/2021/01/10-15-dods-major-it-projects-are-behind-schedule-gao-found/171155/.\n73 See Nattakarn Phaphoom et al., A Survey Study on Major Technical Barriers Affecting the Decision to Adopt Cloud Services, 103 J. Systems & Software 167, \n171-72 (2015) (describing data portability, integration with existing systems, migration complexity, and availability as major barriers to cloud adoption); \nAbdulrahman Alharthi et al., An Overview of Cloud Services Adoption Challenges in Higher Education Institutions, 2 Proceedings of the Int’l Workshop \non Emerging Software as a Service & Analytics 102, 107-08 (2015) (acknowledging the low rate of cloud computing adoption in higher education and \nemphasizing that bolstering both the perceived ease of use and the actual usefulness of cloud computing can increase the adoption rate).\n74 See Dep’t of Energy, FY 2021 Budget Justification Volume 4: Science (2020).\n75 Joe Weinman, Cloudonomics: The Business Value of Cloud Computing (2012).\n76 OLCF supports and manages ORNL’s supercomputing resources, including Summit and eventually Frontier. This figure accounts for “operations and \nuser support at the LCF facilities–including power, space, leases, and staff. Id. at 37-38.\n77 ACLF supports and manages Argonne National Laboratory’s computing resources, including the Theta system and, later this year, the new Aurora \ncomputer, another DOE exascale HPC system. Id. \n78 OLCF operated its Titan HPC system for 7 years. See Coury Turczyn, supra note 72. ACLF also operated its Mira HPC system for 7 years. Argonne’s Mira \nSupercomputer to Retire After Years of Enabling Groundbreaking Science, HPCwire (Dec. 20, 2019), https://www.hpcwire.com/2019/12/20/argonnes\u0002mira-supercomputer-to-retire-after-years-of-enabling-groundbreaking-science/. If still operational, these systems would rank about the 19th and 29th \nfastest in the world, respectively. Compare November 2020, supra note 70, with TOP500 List - June 2019, TOP500 (June 2019), https://www.top500.org/\nlists/top500/list/2019/06/.\n79 See, e.g., Kim Zetter, Top Federal Lab Hacked in Spear-Phishing Attack, Wired (Apr. 20, 2011), https://www.wired.com/2011/04/oak-ridge-lab-hack/; \nNatasha Bertrand & Eric Wolff, Nuclear Weapons Agency Breached amid Massive Cyber Onslaught, Politico (Dec. 17, 2020), https://www.politico.com/\nnews/2020/12/17/nuclear-agency-hacked-officials-inform-congress-447855 (last visited Mar. 2, 2021); Ryan Lucas, List Of Federal Agencies Affected By \nA Major Cyberattack Continues To Grow, NPR (Dec. 18, 2020), https://www.npr.org/2020/12/18/948133260/list-of-federal-agencies-affected-by-a-major\u0002cyberattack-continues-to-grow (last visited Mar. 2, 2021).\n80 We discuss data access models in Chapter Three.\n81 See Ongoing Projects, RIKEN Ctr. for Computational Sci., https://www.r-ccs.riken.jp/en/fugaku/research/covid-19/projects/.\n82 Fugaku Retains Title as World’s Fastest Supercomputer, HPCWire (Nov. 17, 2020), https://www.hpcwire.com/off-the-wire/fugaku-retains-title-as\u0002worlds-fastest-supercomputer/.\n83 November 2020, supra note 70.\n84 Id.\n85 Behind the Scenes of Fugaku as the World’s Fastest Supercomputer, Fujitsu (Feb. 2, 2021), https://blog.global.fujitsu.com/fgb/2021-02-02/behind\u0002the-scenes-of-fugaku-as-the-worlds-fastest-supercomputer-1manufacturing/.\n86 Id.\n87 Don Clark, Japanese Supercomputer Is Crowned World’s Speediest, N.Y. Times (June 22, 2020), https://www.nytimes.com/2020/06/22/technology/\njapanese-supercomputer-fugaku-tops-american-chinese-machines.html.\n88 Justin McCurry, Non-Woven Masks Better to Stop Covid-19, Says Japanese Supercomputer, The Guardian (Aug. 26, 2020), http://www.theguardian.\ncom/world/2020/aug/26/non-woven-masks-better-to-stop-covid-19-says-japanese-supercomputer.\n89 Fujitsu and RIKEN Complete Joint Development of Japan’s Fugaku, the World’s Fastest Supercomputer, Fujitsu (Mar. 9, 2021), https://www.fujitsu.com/\nglobal/about/resources/news/press-releases/2021/0309-02.html.\n90 Id.\n91 See, e.g., Rolf Harms & Michael Yamartino, The Economics of the Cloud (2010); Srijith Rajamohan & Robert E. Settlage, Informing the On/Off-Prem \nCloud Discussion in Higher Education, 2020 Practice & Experience in Advanced Res. Computing 64 (2020); Byung Chul Tak et al., To Move or Not To Move: \nThe Economics of Cloud Computing, 3 USENIX Conf. on Hot Topics in Cloud Computing 1 (2011); Edward Walker, Walter Brisken & Jonathan Romney, \nTo Lease or Not To Lease from Storage Clouds, 43 Computer 44 (2010).\n92 See, e.g., Di Zhang et al., RLScheduler: An Automated HPC Batch Job Scheduler Using Reinforcement Learning, Cornell U. (Sept. 2, 2020), https://arxiv.\norg/pdf/1910.08925.pdf.\n93 For instance, we have not been able to identify good estimates of electricity and cooling costs for DOE supercomputers. \n94 Hugh Couchman et al., Compute Canada — Calcul Canada: A Proposal to the Canada Foundation for Innovation – National Platforms Fund\nA Blueprint for the National Research Cloud 96\n58 (2006).\n95 About, Compute Canada, https://www.computecanada.ca/about/.\n96 National Systems, Compute Canada, https://www.computecanada.ca/techrenewal/national-systems/.\n97 Compute Canada Technology Briefing, Compute Canada (Nov. 2017), https://www.computecanada.ca/wp-content/uploads/2015/02/Technology\u0002Briefing-November-2017.pdf. \n98 Cloud Computing for Researchers, Compute Canada (Dec. 2016), https://www.computecanada.ca/wp-content/uploads/2015/02/CloudStrategy2016-\n2019-forresearchersEXTERNAL-1.pdf.\n99 Id.\n100 Budget Submission 2018, Compute Canada (2018), https://www.computecanada.ca/wp-content/uploads/2015/02/UTF-8Compute20Canada20Budg\net20Submission202018.pdf at 5.\n101 Compute Canada projected it had only met about 55% of total demand for CPU compute hours in 2018. Id.\n102 Id.\n103 Compute Canada, Annual Report 2019-2020 4 (2020).\n104 Rapid Access Service, Compute Canada, https://www.computecanada.ca/research-portal/accessing-resources/rapid-access-service/.\n105 Id.\n106 Resource Allocation Competitions, supra note 21.\n107 Id.\n108 Id.\n109 Id.\n110 Id.\n111 2021 Resource Allocations Competition Results, supra note 21.\nChapter 3\n1 National Research Cloud Call to Action, Stan. U. Inst. for Human-Centered Artificial Intelligence (2020), https://hai.stanford.edu/national\u0002research-cloud-joint-letter. \n2 We discuss the Privacy Act and privacy considerations in more detail in Chapter Five.\n3 Amy O’Hara & Carla Medalia, Data Sharing in the Federal Statistical System: Impediments and Possibilities, 675 Annals Am. Acad. Pol. & Soc. Sci. 138, \n140-41 (2018); see also President’s Mgmt. Agenda, Federal Data Strategy 2020 Action Plan (2020).\n4 Improved data access would, as we describe below, also promote evidence-based policymaking and improve trust in science (as data access makes \nreplication efforts much easier). \n5 See, e.g., Nick Hart & Nancy Potok, Modernizing U.S. Data Infrastructure: Design Considerations for Implementing a National Secure Data \nService to Improve Statistics and Evidence Building (2020).\n6 These initiatives are successful in that they are sustainable and have been used by researchers to access multi-agency government data. The only \nexception is the National Secure Data Service (NSDS), which has not yet been implemented. We discuss the NSDS alongside the Census Bureau and \nthe Evidence-Based Policy-Making Act of 2018 below. Importantly, our focus in these case studies is not to evaluate their efforts or measure their exact \nlevels of success but to identify and understand some of the differences and similarities in the range of data-sharing efforts.\n7 For instance, private sector data may facilitate research regarding social media use, internet behavior, or fill in gaps for federal statistics research \nthrough big data analysis. See Robert M. Groves & Brian A. Harris-Kojetin, Using Private-Sector Data for Federal Statistics, Nat’l Ctr. for Biotechnology \nInfo. (Jan. 12, 2017), https://www.ncbi.nlm.nih.gov/books/NBK425876/.\n8 See, e.g., National Data Service, Nat’l Data Serv., http://www.nationaldataservice.org; The Open Science Data Cloud, Open Sci. Data Cloud, https://\nwww.opensciencedatacloud.org, Harvard Dataverse, Harv. Dataverse, https://dataverse.harvard.edu, FigShare, https://figshare.com. \n9 Facebook Data for Research provides access to a variety of libraries, via in-house platforms. See, e.g., Facebook Data For Good, Facebook (2020), https://\ndataforgood.fb.com/; What is the Facebook Ad Library and How do I Search it?, Facebook (2021), https://www.facebook.com/help/259468828226154; \nFacebook Disaster Maps Methodology, Facebook (May 15, 2019), https://research.fb.com/facebook-disaster-maps-methodology/.\n10 For example, Twitter has a Developer Portal that provides access to their API to allow researchers to use user data for noncommercial purposes. \nSee Twitter Developers, Twitter (2021), https://developer.twitter.com/en/portal/petition/academic/is-it-right-for-you; Take Your Research Further with \nTwitter Data, Twitter (2021), https://developer.twitter.com/en/solutions/academic-research. Thus, uploading Twitter data to a separate Cloud may \nprovide few incentives to researchers who can use the API route.\n11 See Nat’l Acad. of Sci., Innovations in Federal Statistics 31-42 (2017).\n12 See Jennifer M. Urban, Joe Karaganis & Brianna M. Schofield, Notice & Takedown in Everyday Practice 39 (2017) (illustrating the difficulty \nthat online service providers face in manually evaluating a large volume of data for potential infringement; for example, one online service provider \nexplained that “out of fear of failing to remove infringing material, and motivated by the threat of statutory damages, its staff will take “six passes to try \nto find the [identified content].”); see also Letter from Thom Tillis, Marsha Blackburn, Christopher A. Coons, Dianne Feinstein et. al, to Sundar Pichai, \nChief Executive Officer, Google Inc. (Sept. 3, 2019), https://www.ipwatchdog.com/wp-content/uploads/2019/09/9.3-Content-ID-Ltr.pdf (“We have \nheard from copyright holders who have been denied access to Content ID tools, and as a result, are at a significant disadvantage to prevent repeated \nuploading of content that they have previously identified as infringing. They are left with the choice of spending hours each week seeking out and \nsending notices about the same copyrighted works, or allowing their intellectual property to be misappropriated.”).\n13 To illustrate the costs of implementing Content ID on a large-scale platform, Google announced in a report in 2016 that YouTube had invested more \nthan $60 million in Content ID. See Google, How Google Fights Piracy 6 (2016).\n14 See, e.g., AWS Customer Agreement, Amazon (Nov. 30, 2020), https://aws.amazon.com/agreement/. \n15 For instance, across the 29 distinct agencies in the Department of Health and Human Services (HHS), data “are largely kept in silos with a lack of \norganizational awareness of what data are collected across the Department and how to request access. Each agency operates within its own statutory \nauthority and each dataset can be governed by a particular set of regulations.” U.S. Dep’t of Health & Human Services, The State of Data Sharing at \nthe U.S. Department of Health and Human Services 4 (2018). \nA Blueprint for the National Research Cloud 97\n16 See, e.g., id. at 8 (“HHS lacks consistent and standardized processes for one agency to request data from another agency.”).\n17 O’Hara & Medalia, supra note 3, at 140-41.\n18 See id. at 142 (“Most [data-sharing] agreements rely heavily on interpersonal relationships and informal quid pro quo arrangements, handling data \nrequests in a less centralized fashion.”).\n19 Jeffrey Mervis, How Two Economists Got Direct Access to IRS Tax Records, Sci. Mag. (May 22, 2014), https://www.sciencemag.org/news/2014/05/how\u0002two-economists-got-direct-access-irs-tax-records. \n20 See Robert M. Groves & Adam Neufeld, Accelerating the Sharing of Data Across Sectors to Advance the Common Good 17 (2017).\n21 See, e.g., Data Use Agreement, Dep’t Health & Human Services, https://www.hhs.gov/sites/default/files/ocio/eplc/EPLC%20Archive%20\nDocuments/55-Data%20Use%20Agreement%20%28DUA%29/eplc_dua_practices_guide.pdf. \n22 O’Hara & Medalia, supra note 3, at 138, 141.\n23 Bipartisan Pol’y Ctr., Barriers to Using Government Data: Extended Analysis of the U.S. Commission on Evidence-Based Policymaking’s \nSurvey of Federal Agencies and Offices 18-20 (2018).\n24 See Research Data Assistance Center (ResDAC), Ctr. for Medicare & Medicaid Services (Aug. 30, 2018), https://www.cms.gov/Research-Statistics\u0002Data-and-Systems/Research/ResearchGenInfo/ResearchDataAssistanceCenter. \n25 O’Hara & Medalia, supra note 3, at 141.\n26 Michelle Mello et al., Waiting for Data: Barriers to Executing Data Use Agreements, 367 Sci. Mag. 150 (Jan. 10, 2020), https://www.\nsciencemagazinedigital.org/sciencemagazine/10_january_2020/MobilePagedArticle.action?articleId=1552284#articleId1552284. \n27 Interview with Amy O’Hara, Executive Director, Georgetown Federal Statistical Research Data Center (Apr. 22, 2021); see also Special Sworn Research \nProgram, Bureau of Econ. Analysis, https://www.bea.gov/research/special-sworn-researcher-program; Nat’l Ctr. for Educ. Stat., Restricted-Use \nData Procedures Manual (2011). \n28 See U.S. Gov’t Accountability Office, Federal Agencies Need to Address Aging Legacy Systems (2016).\n29 O’Hara & Medalia, supra note 3, at 140-41.\n30 See, e.g., id.; U.S. Gov’t Accountability Office, supra note 28. \n31 President’s Mgmt. Agenda, supra note 3, at 11.\n32 Groves & Neufeld, supra note 20, at 12-13. For a precise definition of sensitive data, see Glossary: Sensitive Information, Nat’l Inst. Standards & \nTech., https://csrc.nist.gov/glossary/term/sensitive_information. \n33 Shanna Nasiri, FedRAMP Low, Moderate, High: Understanding Security Baseline Levels, Reciprocity (Sept. 24, 2019), https://reciprocity.com/fedramp\u0002low-moderate-high-understanding-security-baseline-levels/.\n34 Michael McLaughlin, Reforming FedRAMP: A Guide to Improving the Federal Procurement and Risk Management of Cloud Services, Info. Tech. & \nInnovation Found. (June 15, 2020), https://itif.org/publications/2020/06/15/reforming-fedramp-guide-improving-federal-procurement-and-risk\u0002management.\n35 Frequently Asked Questions, FedRAMP, https://www.fedramp.gov/faqs. \n36 Do Once, Use Many - How Agencies Can Reuse a FedRAMP Authorization, FedRAMP (May 7, 2020), https://www.fedramp.gov/how-agencies-can-reuse\u0002a-fedramp-authorization/. \n37 FedRAMP, FedRAMP Low, Moderate, and High Security Control Baselines (2021).\n38 Security and Privacy Controls for Information Systems and Organizations, Nat’l Inst. Standards & Tech. (Sept. 23, 2020), https://csrc.nist.gov/\npublications/detail/sp/800-53/rev-5/final.\n39 See, e.g., id.; NIST Risk Management Framework AC-2: Account Management, Nat’l Inst. Standards & Tech., https://csrc.nist.gov/Projects/risk\u0002management/sp800-53-controls/release-search#!/control?version=4.0&number=AC-2; NIST Risk Management Framework AC-3: Access Enforcement, \nNat’l Inst. Standards & Tech., https://csrc.nist.gov/Projects/risk-management/sp800-53-controls/release-search#!/control?version=5.1&number=AC-3.\n40 See Mark Bergen, Google Engineers Refused to Build Security Tool to Win Military Contracts, Bloomberg (June 21, 2018), https://www.bloomberg.com/\nnews/articles/2018-06-21/google-engineers-refused-to-build-security-tool-to-win-military-contracts.\n41 See Nat’l Inst. Standards & Tech., Standards for Security Categorization of Federal Information and Information Systems (2004).\n42 Partnering with FedRAMP, FedRAMP, https://www.fedramp.gov/cloud-service-providers/. While it may cost cloud service providers between $365,000 \nand $865,000 and take 6-12 months to receive FedRAMP compliance, Adam Isles, Securing Your Cloud Solutions: Research and Analysis on Meeting \nFedRAMP/Government Standards 21 (2017), such costs are borne by the cloud service providers themselves, not the providers’ customers. Indeed, \nFedRAMP uses a “do once, use many” model: Once a cloud service provider obtains an authorization to operate (ATO), that ATO can be leveraged and \nreciprocated across multiple customers, eliminating duplicative efforts and inconsistencies that would come from requiring multiple re-authorizations. \nId. at 11.\n43 Even within FedRAMP there are substantial amounts of variation in how different organizations ensure compliance with the relevant controls \nand standards, with many of the controls written broadly enough to give room for substantial interpretation. However, it does lay out a variety of \nconsiderations and requirements that are consistent across domains and allows a degree of predictability and reliance that is not present in other \naspects of federal data governance. \n44 O’Hara & Medalia, supra note 3, at 141 (“Data sharing is taking place on a mandatory or voluntary basis, and data requests are managed through a \ndesignated staff/process or diffusely through an organization.”).\n45 Bipartisan Pol’y Ctr., supra note 23, at 17 (“The lack of standard procedures or guidelines for sharing data across federal agencies that fund \nresearch makes efforts to link and share data difficult or inefficient.”).\n46 See, e.g., Amy O’Hara, US Federal Data Policy: An Update on The Federal Data Strategy and The Evidence Act, 5 Int’l J. Population Data Sci. 5 (2020).\n47 While existing federal efforts and initiatives are already aimed at harmonizing data sharing best practices, see, e.g., 2020 Action Plan, Federal Data \nStrategy (May 14, 2020), https://strategy.data.gov/action-plan/, the NRC can accelerate these efforts. Indeed, the development of clear, consistent \nstandards is crucial in facilitating data-sharing. David Crotty, Ida Sim & Michael Stebbins, Open Access to Federally Funded Research Data 7 (2020). \n48 These requirements are inconsistent and out-of-date due to difficulties in defining risk as well as risk aversion on the parts of agencies. See O’Hara \n& Medalia, supra note 3, at 140-41; see also David S. Johnson et al., The Opportunities and Challenges of Using Administrative Data Linkages to Evaluate \nMobility, 657 Annals Am. Acad. Pol. & Soc. Sci. 252-53 (2015).\n49 For a discussion of inference threats, see Nat’l Acad. of Sci., Eng’g & Med., Federal Statistics, Multiple Data Sources, and Privacy Protection: \nNext Steps 68 (2017).\n50 Congzheng Song & Ananth Raghunathan, Information Leakage in Embedding Models, Cornell U. (Mar. 31, 2020), https://arxiv.org/abs/2004.00053. \n51 See, e.g., Statistical Safeguards, Census Bureau (July 1, 2021), https://www.census.gov/about/policies/privacy/statistical_safeguards.html. \nA Blueprint for the National Research Cloud 98\n52 Alexandra Wood et al., Differential Privacy: A Primer for a Non-Technical Audience, 21 Vand. J. Ent. & Tech. L. 209 (2018).\n53 Regulating Access to Data, UK Data Serv., https://www.ukdataservice.ac.uk/manage-data/legal-ethical/access-control/five-safes.\n54 Administrative Data Research Facility, Coleridge Initiative, https://coleridgeinitiative.org/adrf/. \n55 See O’Hara, supra note 46. \n56 For additional discussion of the privacy implications of the NRC, see Chapter Five. \n57 See U.S. Office of Mgmt. & Budget, Barriers to Using Administrative Data for Evidence-Building 7 (2016).\n58 Administrative Data Research Facility, supra note 54.\n59 Id.\n60 Training, Coleridge Initiative, https://coleridgeinitiative.org/training/.\n61 ADRF User Guide: Data Explorer, Coleridge Initiative, https://coleridgeinitiative.org/adrf/documentation/using-the-adrf/data-explorer/. \n62 ADRF User Guide: Exporting Results, Coleridge Initiative, https://coleridgeinitiative.org/adrf/documentation/using-the-adrf/exporting-results/.\n63 Id.\n64 ADRF User Guide: Data Hashing Application, Coleridge Initiative, https://coleridgeinitiative.org/adrf/documentation/adrf-overview/data-hashing\u0002application/. \n65 ADRF User Guide: Security Model and Compliance, Coleridge Initiative, https://coleridgeinitiative.org/adrf/documentation/adrf-overview/security\u0002model-and-compliance/. \n66 Overview for Collaborators, Coleridge Initiative, https://coleridgeinitiative.org/collaborators/. \n67 Data, Stan. Med. Ctr. for Population Health Sci., https://med.stanford.edu/phs/data.html. \n68 Stanford Ctr. for Philanthropy & Civ. Soc’y, Trusted Data Intermediaries 2-3 (2018).\n69 Others have also recognized the benefit of universal DUA templates. See Mello et al., supra note 26, at 150; Guidance for Providing and Using \nAdministrative Data for Statistical Purposes, Office of Mgmt. & Budget (Feb. 14, 2014), https://obamawhitehouse.archives.gov/sites/default/files/omb/\nmemoranda/2014/m-14-06.pdf. \n70 Data | Center for Population Health Sciences | Stanford Medicine, Stan. Med. Ctr. for Population Health Sci., https://med.stanford.edu/phs/data.\nhtml.\n71 See Stanford PHS – Datasets, Redivis, https://redivis.com/StanfordPHS/datasets?orgDatasets-tags=109.medicare.\n72 Access Levels, Redivis (July 2020), https://docs.redivis.com/reference/data-access/access-levels.\n73 Step 1: Getting Access, Stan. Med. PHS Documentation, https://phsdocs.developerhub.io/start-here/getting-data-access.\n74 Id.\n75 Id.\n76 PHS Data-Use Workflow, Stan. Med. PHS Documentation, https://phsdocs.stanford.edu/start-here/phs-data-use-workflow.\n77 Id.\n78 PHS Computing Environment, Stan. Med. PHS Documentation, https://phsdocs.stanford.edu/computing-environment.\n79 Id.\n80 See U.S. Gov’t Accountability Office, Federal Agencies Need to Address Aging Legacy Systems 15 (2016) (noting that from 2010-2015, many \nfederal agencies increased their spending on operations and maintenance due to legacy systems).\n81 David Freeman Engstrom, Daniel E. Ho, Catherine M. Sharkey & Mariano-Florentino Cuéllar, Government by Algorithm: Artificial \nIntelligence in Federal Administrative Agencies 6, 71-72 (2020).\n82 Id. at 6-7.\n83 Id. at 71-72.\n84 Id. at 73.\n85 Id. at 6.\n86 See Results for America, The Promise of the Foundations for Evidence-Based Policymaking Act and Proposed Next Steps (2019) \n87 For example, the Uniform Federal Crime Reporting Act of 1988 requires federal law enforcement agencies to share crime data with the FBI. See 34 \nU.S.C. §§41303(c)(2), (3), (4). Unfortunately, though, no federal agencies apparently currently share their data with the FBI under this law. Nat’l Acad. of \nSci., supra note 11, at 41 (2017).\n88 Katharine G. Abraham & Ron Haskins, The Promise of Evidence-Based Policymaking; Comm’n on Evidence-Based Policymaking (2018).\n89 These privacy-preserving mechanisms are especially important in light of ongoing legal and political challenges in differential privacy application to \nFederal data. See, e.g., Dan Bouk & danah boyd, Democracy’s Data Infrastructure (2021).\n90 Foundations for Evidence-Based Policymaking Act of 2018, Pub. L. No. 115-435.\n91 Confidential Information Protection and Statistical Efficiency Act of 2002, Pub. L. No. 107-347.\n92 Overview, Federal Data Strategy (2020), https://strategy.data.gov/overview/.\n93 UK Data Service, UK Data Serv., https://www.ukdataservice.ac.uk/ (last visited Jun. 21, 2021).\n94 For example, the Social Security Administration alone has over 14 petabytes of data, stored in roughly 200 databases. Engstrom, Ho, Sharkey & \nCuéllar, supra note 81, at 72.\n95 Google Earth Engine, Google Earth Engine, https://earthengine.google.com (last visited Aug. 15, 2021). \n96 World of Work, ADR UK, https://www.adruk.org/our-work/world-of-work/.\n97 Annual Respondents Database, 1973-2008: Secure Access, UK Data Serv. (2020), https://beta.ukdataservice.ac.uk/datacatalogue/studies/\nstudy?id=6644. \n98 UK Innovation Survey, UK Data Serv. (2021), https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=6699. \n99 Quarterly Labour Force Survey, 1992-2021: Secure Access, UK Data Serv. (2021), https://beta.ukdataservice.ac.uk/datacatalogue/studies/\nstudy?id=6727.\n100 Understanding Society: Waves 1-10, 2009-2019 and Harmonised BHPS: Waves 1-18, 1991-2009: Secure Access, UK Data Serv. (2021), https://beta.\nukdataservice.ac.uk/datacatalogue/studies/study?id=6676.\n101 These datasets have helped researchers tackle some specific, public good questions. See, e.g., Francisco Perales, Why Does the Work Women Do Pay \nLess Than the Work Men Do?, UK Data Serv. (Dec. 8, 2011), https://beta.ukdataservice.ac.uk/impact/case-studies/case-study?id=62; Eva-Maria Bonin, Do \nParenting Programmes Reduce Conduct Disorder?, UK Data Serv. (Apr. 4, 2012), https://beta.ukdataservice.ac.uk/impact/case-studies/case-study?id=93. \n102 Identifying Priority Access or Quality Improvements for Federal Data and Models for Artificial Intelligence Research and Development (R&D), and \nTesting; Request for Information, 84 Fed. Reg. 32962 (July 10, 2019). \nA Blueprint for the National Research Cloud 99\n103 Nick Hart, Data Coalition Comments on AI Data and Model R&D RFI, Data Coalition (Aug. 9, 2019), http://www.datacoalition.org/wp-content/\nuploads/2019/09/Comment.RFI_.OMB_.2019-14618.DataCoalition.pdf. \n104 Id.\n105 See Adam R. Pah et al., How to Build a More Open Justice System, 369 Sci. 134 (2020); see also Seamus Hughes, The Federal Courts Are Running an \nOnline Scam, Politico (Mar. 20, 2019), https://www.politico.com/magazine/story/2019/03/20/pacer-court-records-225821/.\n106 Legal Authority and Policies for Data Linkage at Census, Census Bureau (Apr. 4, 2018), https://www.census.gov/about/adrm/linkage/about/\nauthority.html.\n107 BLS Restricted Data Access, U.S. Bureau of Lab. Stat., https://www.bls.gov/rda/restricted-data.htm (last updated May 20, 2021).\n108 Welcome to the PDS, NASA, https://pds.nasa.gov.\nChapter 4\n1 While we believe that these are the primary axes for consideration, some secondary considerations include organizational clout, talent retention, and \nbureaucratic overhead.\n2 Congressional Research Serv., Federally Funded Research and Development Centers (FFRDCs): Background and Issues for Congress 1 (2020). \n3 Id. See also About IDA, Inst. Defense Analyses, https://www.ida.org/about-ida (emphasizing that IDA, the private sector subcontractor that oper\u0002ates the Science & Technology Policy Institute and several other FFRDCs, “enjoys unusual access to classified government information and sensitive \ncorporate proprietary information.”); U.S. Gov’t Accountability Office, Federally Funded Research and Development Centers: Improved Oversight \nand Evaluation Needed for DOD’s Data Access Pilot Program 6 (2020) (discussing how the Department of Defense was able to establish a three-year \npilot program that allowed its FFRDC researchers to forgo having to obtain nondisclosure agreements with each data owner in order to streamline the \ndata-access process).\n4 Nick Hart & Nancy Potok, Modernizing U.S. Data Infrastructure: Design Considerations for Implementing a National Secure Data Service to \nImprove Statistics and Evidence Building (2020).\n5 Id. at 26.\n6 Id. at 26-27, 29-30.\n7 U.S. Gov’t Accountability Office, supra note 3, at 6. Note that while the FFRDC must operate to serve its sponsors, in establishing an FFRDC, the \nsponsor must ensure that it operates with substantial independence; the FFRDC must be “operated, managed, or administered by an autonomous or\u0002ganization or as an identifiably separate operating unit of a parent organization.” See Federal Acquisition Regulations [hereinafter “FAR”] § 35.017(a)(2).\n8 One example of this is the Science & Technology Policy Institute, which we discuss in a case study below.\n9 U.S. Dep’t of Energy, The State of the DOE National Laboratories 11-13 (2020).\n10 See, e.g., More Federal Agencies Head to the Cloud With Azure Government, Applied Info. Sci. (Feb. 23, 2018), https://www.ais.com/more-federal\u0002agencies-head-to-the-cloud-with-azure-government/; see also AWS GovCloud, Amazon, https://aws.amazon.com/govcloud-us/. Microsoft was also \npreviously awarded a $10 billion contract from the Pentagon. See Kate Conger, Microsoft Wins Pentagon’s $10 Billion JEDI Contract, Thwarting Amazon, \nN.Y. Times (Sept. 4, 2020), https://www.nytimes.com/2019/10/25/technology/dod-jedi-contract.html. However, this contract was recently canceled “due \nto evolving requirements, increased cloud conservancy and industry advances.” Ellie Kaufman & Zachary Cohen, Pentagon Cancels $10 Billion Cloud \nContract Given to Microsoft Over Amazon, CNN (July 6, 2021), https://www.cnn.com/2021/07/06/tech/defense-department-cancels-jedi-contract-am\u0002azon-microsoft/index.html. The Pentagon will now instead seek new bids for an updated Joint Warfighting Cloud Capability (JWCC) contract from \nAmazon and Microsoft. Id.\n11 See, e.g., Bram Bout, Helping Universities Build What’s Next with Google Cloud Platform, Google (Oct. 25, 2016), https://blog.google/outreach-ini\u0002tiatives/education/helping-universities-build-whats-next-google-cloud-platform; Cloud Computing for Education, Amazon, https://aws.amazon.com/\neducation/. \n12 Congressional Research Serv., supra note 2, at 11-12 (2020).\n13 U.S. Dep’t of Energy, Annual Report on the State of the DOE National Laboratories 87 (2017).\n14 Congressional Research Serv., supra note 2, at 19.\n15 Congressional Research Serv., Office of Science and Technology Policy (OSTP): History and Overview 9 (2020). STPI’s duties are also specified \nin 42 U.S.C. § 6686.\n16 What are FFRDCs?, Inst. Defense Analyses, https://www.ida.org/ida-ffrdcs. \n17 Id.\n18 Sponsors, Inst. Defense Analyses, https://www.ida.org/en/about-ida/sponsors. \n19 Id.\n20 Congressional Research Serv., supra note 15, at 9-10.\n21 For instance, from 2008-2012, these other federal agencies contributed a total of $9.8 million of funding to STPI while NSF contributed about $24 \nmillion. U.S. Gov’t Accountability Office, Federally Funded Research Centers: Agency Reviews of Employee Compensation and Center Perfor\u0002mance 43-44 (2014).\n22 Congressional Research Serv., supra note 15, at 9-10.\n23 Id.\n24 42 U.S.C. § 6686(d).\n25 42 U.S.C. § 6686(e).\n26 Sci. & Tech. Pol’y Inst., Report to the President Fiscal Year 2020 (2020).\n27 See, e.g., Open Government, Millennium Challenge Corp., https://www.mcc.gov/initiatives/initiative/open; Nat’l Geospatial Advisory Comm., \nAdvancing the National Spatial Data Infrastructure Through Public-Private Partnerships and Other Innovative Partnerships (2020); Nat’l \nAeronautics & Space Admin., Public-Private Partnerships for Space Capability Development 33-36 (2014).\n28 Big Data Value Public-Private Partnership, European Comm’n (Mar. 9, 2021), https://digital-strategy.ec.europa.eu/en/library/big-data-value-pub\u0002lic-private-partnership.\n29 RAND, Public-Private Partnerships for Data-Sharing: A Dynamic Environment 33, 99 (2000).\n30 See Homepage - Alberta Data Partnerships, Alberta Data Partnerships, http://abdatapartnerships.ca (last visited Aug. 15, 2021). \nA Blueprint for the National Research Cloud 100\n31 Alberta Data Partnerships, A P3 Success Story 1 (2017).\n32 Id.\n33 Id. at 19, 35.\n34 Id. at 15.\n35 Id.\n36 Id. at 1.\n37 Nat’l Geospatial Advisory Comm., Public-Private Partnership Use Case: Alberta Data Partnerships 1 (2020).\n38 Alberta Data Partnerships, supra note 31, at 15.\n39 Id. at 16.\n40 The COVID-19 High Performance Computing Consortium, COVID-19 HPC Consortium, https://covid19-hpc-consortium.org. \n41 Id.\n42 See, e.g., David Hall, Why Public-Private Partnerships Don’t Work (2015); Disadvantages and Pitfalls of the PPP Option, APMG Int’l, https://\nppp-certification.com/ppp-certification-guide/54-disadvantages-and-pitfalls-ppp-option. \n43 Graeme A. Hodge, Carsten Greve & Anthony E. Boardman, International Handbook on Public–Private Partnerships, 187-90 (2012).\n44 For example, on one end of a spectrum, the California Teale Data Center creates, owns, maintains, and archives its own datasets for private sector \nuse. In contrast, the Pennsylvania Spatial Data Access houses metadata, requiring users to ask the actual data sources for access. RAND, supra note 29, \nat 102-03. We encourage the Task Force to examine this comprehensive report to assess the various organizational options for a PPP data clearinghouse \nmodel. \n45 Angela Ballantyne & Cameron Stewart, Big Data and Public-Private Partnerships in Healthcare and Research, 11 Asian Bioethics R. 315, 315 (2019).\n46 See Gov’t Accountability Office, Human Capital: Improving Federal Recruiting and Hiring Efforts; see also Catch and Retain: Improving Recruit\u0002ing and Retention at Government Agencies, Salesforce, https://www.salesforce.com/solutions/industries/government/resources/government-recruit\u0002ment-software/.\n47 Partnership for Public Service, Survey on the Future of Government Service 2 (2020).\n48 Id.\nChapter 5\n1 National Research Cloud Call to Action, Stan. U. Inst. for Human-Centered Artificial Intelligence (2020), https://hai.stanford.edu/national-re\u0002search-cloud-joint-letter.\n2 Sensitive information, as defined by the National Institute of Standards and Technology, is information where the loss, misuse, or unauthorized \naccess or modification could adversely affect the national interest or the conduct of federal programs, or the privacy to which individuals are entitled \nunder 5 U.S.C. § 552a (the Privacy Act); that has not been specifically authorized under criteria established by an Executive Order or an Act of Congress \nto be kept classified in the interest of national defense or foreign policy. See Glossary: Sensitive Information, Nat’l Inst. Standards & Tech., \nhttps://csrc.nist.gov/glossary/term/sensitive_information.\n3 We thank Mark Krass for these insights. \n4 Agencies covered by the Act include “any Executive department, military department, Government corporation, Government controlled corporation, \nor other establishment in the executive branch of the [federal] Government (including the Executive Office of the President), or any independent regula\u0002tory agency.” 5 U.S.C. § 552(f)(1).\n5 U.S. General Accounting Office, Record Linkage and Privacy: Issues in Creating New Federal Research and Statistical Information 10 (2001). \n6 Interview with Marc Groman, Former Senior Advisor for Privacy, White House Office of Management and Budget (Feb. 18, 2021); see also Bipartisan \nPol’y Ctr., Barriers to Using Government Data: Extended Analysis of the U.S. Commission on Evidence-Based Policymaking’s Survey of Federal \nAgencies and Offices 10 (2018).\n7 See Joseph Near & David Darais, Differentially Private Synthetic Data, Nat’l Inst. Standards & Tech. (May 3, 2021), https://www.nist.gov/blogs/cyber\u0002security-insights/differentially-private-synthetic-data; see also Steven M. Bellovin et al., Privacy and Synthetic Datasets, 22 Stan. L. Rev. 1 (2019).\n8 E-Government Act of 2002, Pub. L. No. 107-347. \n9 Confidential Information Protection and Statistical Efficiency Act of 2002, 44 U.S.C. § 3501 (2012).\n10 Foundations for Evidence-Based Policymaking Act of 2017, Pub. L. No. 115-435, 132 Stat. 5529 (2019).\n11 President’s Mgmt. Agenda, Federal Data Strategy 2020 Action Plan (2020).\n12 Privacy Act of 1974, 5 U.S.C. § 552a (2012).\n13 There are many versions of the Fair Information Practice Principles, and the U.S. government has not institutionalized a specific version, though the \nversion used by the Department of Homeland Security is commonly referenced (available at: https://www.dhs.gov/publication/privacy-policy-guid\u0002ance-memorandum-2008-01-fair-information-practice-principles). The Organisation for Economic Cooperation and Development produced an influen\u0002tial version of them in 1980 (revised in 2013), which remains an authoritative source. OECD Guidelines on the Protection of Privacy and Transborder Flows \nof Personal Data, OECD (2013), https://www.oecd.org/digital/ieconomy/oecdguidelinesontheprotectionofprivacyandtransborderflowsofpersonaldata.\nhtm.\n14 See David Freeman Engstrom, Daniel E. Ho, Catherine M. Sharkey & Mariano-Florentino Cuéllar, Government by Algorithm: Artificial Intelli\u0002gence in Federal Administrative Agencies (2020) (documenting present use of AI by government agencies).\n15 5 U.S.C. § 552a (a)(5).\n16 5 U.S.C. §§ 552a(a)(8)(A)(i)(I), (II).\n17 The Privacy Act of 1974, Elec. Privacy Info. Ctr., https://epic.org/privacy/1974act/ (last visited Aug. 15, 2021).\n18 See Fact Sheet: National Secure Data Service Act Advances Responsible Data Sharing in Government, Data Coalition (May 13, 2021), https://www.\ndatacoalition.org/fact-sheet-national-secure-data-service-act-advances-responsible-data-sharing-in-government/; U.S Gov’t Accountability Office, \nRecord Linkage and Privacy: Issues in Creating New Federal Research and Statistical Information (2001).\n19 It is no small irony that private companies in the U.S. have fulfilled that mission today. In fact, the U.S. government now approaches private industry, \neither through legal process or through procurement, when it requires data about individuals that the government itself does not collect. Senator Ron \nA Blueprint for the National Research Cloud 101\nWyden has proposed legislation to prevent the government from making these purchases. Wyden, Paul and Bipartisan Members of Congress Introduce \nThe Fourth Amendment Is Not For Sale Act, Ron Wyden U.S. Senator for Or. (Apr. 21, 2021), https://www.wyden.senate.gov/news/press-releases/\nwyden-paul-and-bipartisan-members-of-congress-introduce-the-fourth-amendment-is-not-for-sale-act-.\n20 See, e.g., World Econ. Forum, The Next Generation of Data-Sharing in Financial Services (2019).\n21 See, e.g., Stacie Dusetzina et al., Linking Data for Health Services Research: A Framework and Instructional Guide, Agency for Healthcare Research & \nQuality (Sept. 1, 2014), https://www.ncbi.nlm.nih.gov/books/NBK253315/.\n22 See, e.g., European Comm’n, A European Strategy for Data (2020) (arguing for cross-border data aggregation and linkage of both private and pub\u0002lic sector data); M Sanni Ali et al., Administrative Data Linkage in Brazil: Potentials for Health Technology Assessment, 10 Frontiers in Pharmacology 984 \n(2019); Data Linkage, Australian Inst. of Health & Welfare (Jan. 4, 2020), https://www.aihw.gov.au/our-services/data-linkage. \n23 See, e.g., Elsa Augustine, Vikash Reddy & Jesse Rothstein, Linking Administrative Data: Strategies and Methods (2018) (describing tips for \nconducting data linkages in California); see also U.S. Dep’t of Health & Human Services, Status of State Efforts to Integrate Health and Human \nServices Systems and Data (2016).\n24 Ben Moscovitch, How President Biden Can Improve Health Data Sharing For COVID-19 And Beyond, Health Affairs (Mar. 1, 2021), https://www.\nhealthaffairs.org/do/10.1377/hblog20210223.611803/full/.\n25 Home, Johns Hopkins Coronavirus Resource Ctr., https://coronavirus.jhu.edu/.\n26 The COVID Tracking Project, https://covidtracking.com/. \n27 Fred Bazzoli, COVID-19 Emergency Shows Limitations of Nationwide Data Sharing Infrastructure, Healthcare IT News (June 2, 2020), https://www.\nhealthcareitnews.com/news/covid-19-emergency-shows-limitations-nationwide-data-sharing-infrastructure.\n28 See, e.g., C. Jason Wang et al., Response to COVID-19 in Taiwan: Big Data Analytics, New Technology, and Proactive Testing, JAMA (Mar. 3, 2020), https://\njamanetwork.com/journals/jama/fullarticle/2762689/; Fang-Ming Chen et al., Big Data Integration and Analytics to Prevent a Potential Hospital Outbreak \nof COVID-19 in Taiwan, 54 J. Microbiology, Immunology & Infection 129-30 (2020).\n29 See, e.g., Q&A on the Pentagon’s “Total Information Awareness” Program, Am. C.L. Union, https://www.aclu.org/other/qa-pentagons-total-informa\u0002tion-awareness-program; The Five Problems with CAPPS II: Why the Airline Passenger Profiling Proposal Should Be Abandoned, Am. C.L. Union, https://\nwww.aclu.org/other/five-problems-capps-ii. \n30 See, e.g., Barton Gellman, Dark Mirror: Edward Snowden and the American Surveillance State (2020); Edward Snowden, Permanent Record \n(2019).\n31 5 U.S.C. § 552(b).\n32 5 U.S.C. § 552(b)(3).\n33 The Privacy Act also contains specific carve-outs for disclosures to the Census Bureau and to the National Archives and Records Administration. \nHowever, the carve-outs for these two agencies require that the disclosures be made for the purposes of a census survey and of recording historical \nvalue, respectively. Because the NRC’s explicit purpose is to democratize AI innovation, it is unlikely that the NRC can take advantage of this existing \nexception to dataset disclosures under the Privacy Act.\n34 For example, the Federal Emergency Management Agency’s list of routine uses includes broad disclosure “[t]o an agency or organization for the \npurpose of performing audit or oversight operations as authorized by law, but only such information as is necessary and relevant to such audit or over\u0002sight function.” Privacy Act of 1974; Department of Homeland Security Federal Emergency Management Agency-008 Disaster Recovery Assistance Files \nSystem of Records, 78 Fed. Reg. 25282 (May 30, 2013). \n35 See, e.g., Britt v. Naval Investigative Service, 886 F.2d 544 (3d Cir. 1989).\n36 The Privacy Act of 1974, supra note 17. \n37 5 U.S.C. § 552(b)(5). \n38 5 U.S.C. §§ 552a(a)(8)(B)(i), (ii) (emphasis added).\n39 44 U.S.C. § 3561(8), (12).\n40 U.S. Dep’t of Health & Human Services, The State of Data Sharing at the U.S. Department of Health and Human Services 16 (2018).\n41 44 U.S.C. § 3575(4).\n42 See Engstrom, Ho, Sharkey & Cuéllar, supra note 14, at 16 (finding that the Bureau of Labor Statistics is one of the top ten agencies that use \nartificial intelligence); Machine Learning, Census Bureau (Apr. 17, 2019), https://www.census.gov/topics/research/data-science/about-machine-learn\u0002ing.html (asserting that the Census Bureau “needs” machine learning capabilities); Bureau of Econ. Analysis, 2020 Strategic Action Plan 7 (2020) \n(highlighting the importance of artificial intelligence and machine learning to BEA’s strategy).\n43 Group level data analyses also have inherent privacy risks and harms. See, e.g., Linnet Taylor, Safety in Numbers? Group Privacy and Big Data Analyt\u0002ics in the Developing World, in Group Privacy: New Challenges of Data Technologies 13 (2017).\n44 See 34 U.S.C. §§ 41303(c)(2), (3), (4).\n45 Nat’l Acad. of Sci., Innovations in Federal Statistics 41 (2017).\n46 See 13 U.S.C. § 6. \n47 Nat’l Acad. of Sci., supra note 45, at 40.\n48 According to the study, “an agency’s legal counsel may advise against sharing data as a precautionary measure rather than because of an explicit \nprohibition.” U.S. Gov’t Accountability Office, Sustained and Coordinated Efforts Could Facilitate Data Sharing While Protecting Privacy 1 \n(2013).\n49 See Amy O’Hara & Carla Medalia, Data Sharing in the Federal Statistical System: Impediments and Possibilities, 675 Annals Am. Acad. Pol. & Soc. Sci.\n138, 141 (2018).\n50 Robert M. Groves & Adam Neufeld, Accelerating the Sharing of Data Across Sectors to Advance the Common Good 12 (2017). \n51 Bipartisan Pol’y Ctr., supra note 6, at 18-20.\n52 See O’Hara & Medalia, supra note 49, at 141.\n53 Administrative Data Research UK, ADR UK, https://www.adruk.org.\n54 About ADR UK, ADR UK, https://www.adruk.org/about-us/about-adr-uk/.\n55 Id.\n56 See World of Work, ADR UK, https://www.adruk.org/our-work/world-of-work/.\n57 Funding Opportunities, ADR UK, https://www.adruk.org/news-publications/funding-opportunities/.\n58 Id.\nA Blueprint for the National Research Cloud 102\n59 Id.\n60 Funding Opportunity: A Unique Chance to Shape Data Science at the Heart of UK Government, ADR UK (Apr. 8, 2021), https://www.adruk.org/\nnews-publications/news-blogs/funding-opportunity-a-unique-chance-to-shape-data-science-at-the-heart-of-uk-government-384/. \n61 Funding Opportunities, supra note 57.\n62 Digital Economy Act 2017 (Gr. Br.).\n63 ADR UK, Trust, Security and Public Interest: Striking the Balance 28 (2020).\n64 Id.\n65 Id.\n66 How Do We Work with Researchers?, ADR UK, https://www.adruk.org/our-mission/working-with-researchers/.\n67 Accessing Secure Research Data as an Accredited Researcher, Off. for Nat’l Stat., https://www.ons.gov.uk/aboutus/whatwedo/statistics/requesting\u0002statistics/approvedresearcherscheme. \n68 See Nick Hart & Nancy Potok, Modernizing U.S. Data Infrastructure: Design Considerations for Implementing a National Secure Data Service \nto Improve Statistics and Evidence Building 17, 21 (2020).\n69 Id.\n70 Id. at 15.\n71 President’s Mgmt. Agenda, supra note 11, at 9.\n72 Id. at 31.\n73 See What is Open Data?, Open Data Handbook, https://opendatahandbook.org/guide/en/what-is-open-data/. \nChapter 6\n1 See Keeping Secrets: Anonymous Data Isn’t Always Anonymous, Berkeley Sch. of Info. (Mar. 15, 2014), https://ischoolonline.berkeley.edu/blog/anony\u0002mous-data/; Arvind Narayanan & Vitaly Shmatikov, How to Break Anonymity of the Netflix Prize Dataset, Cornell U. (Nov. 22, 2007), https://arxiv.org/pdf/\ncs/0610105.pdf.\n2 Matt Fredrikson, Somesh Jha & Thomas Ristenpart, Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures, 22 Pro\u0002ceedings of the ACM Special Interest Group on Security, Audit & Control 1322 (2015); Nicholas Carlini et al., Extracting Training Data from Large \nLanguage Models, Cornell U. (June 15, 2021), https://arxiv.org/pdf/2012.07805.pdf. \n3 See, e.g., HIPAA Training, Certification, and Compliance, HIPAA Training, https://www.hipaatraining.com/; Research Data Management, UK Data Serv., \nhttps://ukdataservice.ac.uk/learning-hub/research-data-management/.\n4 Ashwin Machanavajjhala et al., L-Diversity: Privacy Beyond K-Anonymity, 22 Int’l Conf. Data Eng’g 24 (2006). \n5 Cynthia Dwork & Aaron Roth, The Algorithmic Foundations of Differential Privacy (2014). \n6 See, e.g., Tara Bahrampour & Marissa J. Lang, New System to Protect Census Data May Compromise Accuracy, Some Experts Way, Wash. \nPost (June 1, 2021), https://www.washingtonpost.com/local/social-issues/2020-census-differential-privacy-ipums/2021/06/01/6c\u000294b46e-c30d-11eb-93f5-ee9558eecf4b_story.html; Kelly Percival, Court Rejects Alabama Challenge to Census Plans for Redistricting and Privacy, \nBrennan Ctr. (June 30, 2021), https://www.brennancenter.org/our-work/analysis-opinion/court-rejects-alabama-challenge-census-plans-redistrict\u0002ing-and-privacy.\n7 See, e.g., Leonard E. Burman et al., Safely Expanding Research Access to Administrative Tax Data: Creating a Synthetic Public Use File and a \nValidation Server (2018); see also The Synthetic Data Vault, https://sdv.dev. \n8 Valerie Chen, Valerio Pastro & Mariana Raykova, Secure Computation for Machine Learning with SPDZ, Cornell U. (Jan. 2, 2019), https://arxiv.org/\npdf/1901.00329.pdf. \n9 Louis J. M. Aslett et al., A Review of Homomorphic Encryption and Software Tools for Encrypted Statistical Machine Learning, Cornell U. (Aug. 26, 2015), \nhttps://arxiv.org/pdf/1508.06574.pdf. \n10 See Hongyan Chang & Reza Shokri, On the Privacy Risks of Algorithmic Fairness, Cornell U. (Apr. 7, 2021), https://arxiv.org/pdf/2011.03731.pdf.\n11 Ruggles et al., Differential Privacy and Census Data: Implications for Social and Economic Research, 109 Am. Econ. Ass’n Papers & Proceedings 403, \n406 (2019).\n12 In Computer Science literature, such algorithmic settings are often referred to as hyperparameters. For instance, k is a hyperparameter for k-ano\u0002nymity. By setting k to different values (e.g., 5, 10, 100), practitioners can modulate the amount of anonymity afforded to records in the data. As we note \nhowever, the choice of hyperparameters controls both the privacy effected on a dataset as well as the fidelity of that data. \n13 See Differential Privacy for Census Data Explained, Nat’l Conf. of State Legislatures (July 1, 2021), https://www.ncsl.org/research/redistricting/dif\u0002ferential-privacy-for-census-data-explained.aspx; Hongyan Chang & Reza Shokri, On the Privacy Risks of Algorithmic Fairness, Cornell U. (Apr. 7, 2021), \nhttps://arxiv.org/pdf/2011.03731.pdf. Some scholars even find that the incorporation of differential privacy into machine learning algorithms can have \ndisparate impact on underrepresented groups. See Eugene Bagdasaryan & Vitaly Shmatikov, Differential Privacy Has Disparate Impact on Model Accura\u0002cy, Cornell U. (Oct. 27, 2019), https://arxiv.org/pdf/1905.12101.pdf.\n14 Steven Ruggles, Differential Privacy and Census Data: Implications for Social and Economic Research 17.\n15 Id. at 18-19.\n16 Nat’l Acad. of Sci., Innovations in Federal Statistics 86 (2017). The fragmented FSRDC review process is similar to the fragmented data access \nregime we discussed in Chapter Three.\n17 Special Sworn Researcher Program, Bureau of Econ. Analysis, https://www.bea.gov/research/special-sworn-researcher-program (last updated July \n23, 2021).\n18 13 U.S.C. § 9.\n19 The institutional form of the NRC is discussed in depth in Chapter Four.\n20 NORC Data Enclave, NORC, https://www.norc.org/PDFs/BD-Brochures/2016/Data%20Enclave%20One%20Sheet.pdf. \n21 CMS Virtual Research Data Center (VRDC), Research Data Assistance Ctr., https://resdac.org/cms-virtual-research-data-center-vrdc. \n22 Request for Information (RFI) Seeking Stakeholder Input on the Need for an NIH Administrative Data Enclave, Nat’l Inst. of Health (Mar. 1, 2019), \nhttps://grants.nih.gov/grants/guide/notice-files/NOT-OD-19-085.html. \nA Blueprint for the National Research Cloud 103\n23 See FASEB Response to NIH Request for Information (RFI): Seeking Stakeholder Input on the Need for an NIH Administrative Data Enclave, Fed’n of Am. \nSocieties for Experimental Biology (2019), https://www.faseb.org/Portals/2/PDFs/opa/2019/FASEB_Response_Data_Enclave_RFI_NOT-OD-19-085.\npdf; Am. Soc’y of Biochemistry & Molecular Biology (May 30, 2019), https://www.asbmb.org/getmedia/e3401ed5-3210-4ed2-a82a-7363cb86071d/\nASBMB-Response-to-NIH-RFI-NOT-09-19-085.pdf. \n24 What We Do, Cal. Pol’y Lab, https://www.capolicylab.org/what-we-do/.\n25 Id.\n26 CPL Roadmap to Government Administrative Data in California, Cal. Pol’y Lab, https://www.capolicylab.org/data-resources/california-data-road\u0002map/.\n27 Interview with Evan White, Executive Director, California Policy Lab (Apr. 29, 2021).\n28 Id.\n29 Id.; see, e.g., Policy Evaluation and Research Linkage Initiative (PERLI), Cal. Pol’y Lab, https://www.capolicylab.org/data-resources/perli/; University \nof California Consumer Credit Panel, Cal. Pol’y Lab, https://www.capolicylab.org/data-resources/university-of-california-consumer-credit-panel/.\n30 Interview with Evan White, supra note 27.\n31 Id.\n32 See, e.g., Life Course Dataset, Cal. Pol’y Lab, https://www.capolicylab.org/life-course-dataset/.\n33 See CPL Roadmap to Government Administrative Data in California, supra note 26.\n34 We note that it is possible that the organizational form could affect the authority of NRC staff to speak to the legality of data transfers. \nChapter 7\n1 Christopher Whyte, Deepfake News: AI-Enabled Disinformation as a Multi-Level Public Policy Challenge, 5 J. Cyber Pol’y 199 (2020); Don Fallis, What Is \nDisinformation?, 63 Libr. Trends 601 (2015). \n2 Mary L. Gray & Siddharth Suri, Ghost Work: How to Stop Silicon Valley From Building a New Global Underclass (2019); Science Must Examine \nthe Future of Work, Nature (Oct. 19, 2017), \nhttps://www.nature.com/articles/550301b. \n3 David Danks & Alex John London, Algorithmic Bias in Autonomous Systems, 26 Int’l Joint Conf. on Artificial Intelligence 4691 (2017); Joy Buolam\u0002wini & Timnit Gebru, Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification, 81 Proceeding of Machine Learning Res.\n1 (2018); Ben Hutchinson et al., Unintended Machine Learning Biases as Social Barriers for Persons with Disabilities, 125 ACM SIGACCESS Accessibility & \nComputing 1 (2020).\n4 Oscar H. Gandy Jr., The Panoptic Sort: A Political Economy of Personal Information (1993); Virginia Eubanks, Automating Inequality (2018); \nRashida Richardson, Racial Segregation and the Data-Driven Society: How Our Failure to Reckon with Root Causes Perpetuates Separate and Unequal \nRealities, 36 Berkeley Tech. L. J. 101 (2021). \n5 For approaches to improve machine learning practices, see Timnit Gebru et al., Datasheets for Datasets, Cornell U. (Mar. 19, 2020), https://arxiv.org/\npdf/1803.09010.pdf; Margaret Mitchell et al., Model Cards for Model Reporting, 2019 Proceedings ACM Conf. on Fairness, Accountability & Transpar\u0002ency 220 (2019); Kenneth Holstein et al., Improving Fairness in Machine Learning Systems: What do Industry Practitioners Need?, 2019 CHI Conf. on Hum. \nFactors in Computing Sys. 1 (2019); Michael A Madaio et al., Co-Designing Checklists to Understand Organizational Challenges and Opportunities Around \nFairness in AI, 2020 CHI Conf. on Hum. Factors in Computing Sys. 318 (2019). The literature on AI’s societal impacts and fairness, accountability, and \ntransparency of AI is vast, but see Michael Kearns & Aaron Roth, The Ethical Algorithm: The Science of Socially Aware Algorithm Design (2019); \nEubanks, supra note 4; Solon Barocas, Moritz Hardt & Arvind Narayanan, Fairness and Machine Learning (2019); Cathy O’Neil, Weapons of Math \nDestruction (2016).\n6 45 C.F.R §§ 46.101-124.\n7 J. Britt Holbrook & Robert Frodeman, Peer Review and the Ex Ante Assessment of Societal Impacts, 20 Res. Evaluation 239 (2011). \n8 Id.\n9 Institutional Review Boards (IRBs) and Protection of Human Subjects in Clinical Trials, U.S. Food & Drug Admin., https://www.fda.gov/about-fda/cen\u0002ter-drug-evaluation-and-research-cder/institutional-review-boards-irbs-and-protection-human-subjects-clinical-trials (last updated Sept. 11, 2019).\n10 There are crucial questions with regards to consent even with data considered “publicly” available. See generally Casey Fiesler & Nicholas Proferes, \n“Participant” Perceptions of Twitter Research Ethics, 4 Social Media + Society 1 (2018); Sarah Gilbert, Jessica Vitak & Katie Shilton, Measuring Americans’ \nComfort with Research Uses of Their Social Media Data, 7 Social Media + Society 1 (2021). \n11 Sara R. Jordan, Future of Privacy Forum, Designing an Artificial Intelligence Research Review Committee (2019), https://fpf.org/wp-content/\nuploads/2019/10/DesigningAIResearchReviewCommittee.pdf.\n12 Agatta Feretti et al., Ethics Review of Big Data Research: What Should Stay and What Should Be Reformed?, 22 BMC Medical Ethics 1, 6 (2021); Kathryn \nM. Porter et al., The Emergence of Clinical Research Ethics Consultation: Insights from a National Collaborative, 2018 Am. J. Bioethics 39 (2018). \n13 Feretti et al., supra note 12.\n14 See, e.g., Mark Diaz et al., Addressing Age-Related Bias in Sentiment Analysis, 2018 Proceedings CHI Conf. on Hum. Factors in Computing Sys. 1 \n(2018); Buolamwini & Gebru, supra note 3.\n15 See, e.g., Timnit Gebru et al., Datasheets for Datasets, Cornell U. (Mar. 19, 2020), https://arxiv.org/pdf/1803.09010.pdf; Margaret Mitchell et al., Model \nCards for Model Reporting, 2019 Proceedings ACM Conf. on Fairness, Accountability & Transparency 220 (2019); Emily M. Bender et al., On the Dan\u0002gers of Stochastic Parrots: Can Language Models Be Too Big?, 2021 Proceedings ACM Conf. on Fairness, Accountability & Transparency 610 (2021); \nChristo Wilson et al., Building and Auditing Fair Algorithms: A Case Study in Candidate Screening, 2021 Proceedings ACM Conf. on Fairness, Account\u0002ability & Transparency 666 (2021); Pauline T. Kim, Auditing Algorithms for Discrimination, 166 U. Pa. L. Rev. Online 189 (2017).\n16 Phase II: Proposal Review and Processing, Nat’l Sci. Found., https://www.nsf.gov/bfa/dias/policy/merit_review/phase2.jsp#select.\n17 Harvey A. Averch, Criteria for Evaluating Research Projects and Portfolios, in Evaluating R&D Impacts: Methods and Practice 263 (1993).\n18 Nat’l Security Comm’n on Artificial Intelligence, Final Report 141-54 (2021). \n19 History and Mission, U.S. Privacy & Civil Liberties Oversight Bd., https://www.pclob.gov/About/HistoryMission.\n20 AI in Counterterrorism Oversight Enhancement Act of 2021, H.R. 4469, 117th Cong. (2021). \n21 In instances where a researcher is using data obtained from one of the agencies that falls under ORI’s oversight, it may make sense to have ORI adju-\nA Blueprint for the National Research Cloud 104\ndicate those cases directly. For more information about the ORI, see ORI, Office Of Research Integrity, https://ori.hhs.gov/.\n22 Michael S. Bernstein et al., ESR: Ethics and Society Review of Artificial Intelligence Research, Cornell U. (July 9, 2021), https://arxiv.org/\npdf/2106.11521.pdf.\n23 Nat’l Sci. Found., Broader Impacts, https://www.nsf.gov/od/oia/special/broaderimpacts/.\n24 See, e.g., Notice of Special Interest: Administrative Supplement for Research and Capacity Building Efforts Related to Bioethical Issues, Nat’l Inst. of \nHealth (Nov. 17, 2020), https://grants.nih.gov/grants/guide/notice-files/NOT-OD-21-020.html; Notice of Special Interest: Administrative Supplement \nfor Research on Bioethical Issues, Nat’l Inst. of Health (Dec. 30, 2019), https://grants.nih.gov/grants/guide/notice-files/NOT-OD-20-038.html; see also \nCourtenay R. Bruce et al., An Embedded Model for Ethics Consultation: Characteristics, Outcomes, and Challenges, 5 AJOB Empirical Bioethics 8 (2014); \nSharon Begley, In a Lab Pushing the Boundaries of Biology, an Embedded Ethicist Keeps Scientists in Check, Stat (Feb. 23, 2017), https://www.statnews.\ncom/2017/02/23/bioethics-harvard-george-church/. Private foundations also promote the use of embedded bioethicists. See, e.g., Making a Differ\u0002ence Request for Proposals – Fall 2021, The Greenwall Found. (2021), https://greenwall.org/making-a-difference-grants/request-for-proposals-MAD\u0002fall-2021.\nChapter 8\n1 Paul Cichonski et al., Computer Security Incident Handling Guide (2012). \n2 Putative attacks could include the deployment of ransomware, phishing schemes, gaining root access (the highest level of privilege available which \ngives users access to all commands and files by default), exposure of secret credentials, data poisoning, data exfiltration, as well as other types of \nunauthorized network intrusions.\n3 Karen Hao, AI Consumes a Lot of Energy. Hackers Could Make it Consume More, MIT Tech. R. (May 6, 2021), https://www.technologyreview.\ncom/2021/05/06/1024654/ai-energy-hack-adversarial-attack/.\n4 Catalin Cimpanu, Vast Majority of Cyber-Attacks on Cloud Servers Aim to Mine Cryptocurrency, ZDNet (Sept. 14, 2020), https://www.zdnet.com/article/\nvast-majority-of-cyber-attacks-on-cloud-servers-aim-to-mine-cryptocurrency/.\n5 We note that the NRC will likely need to comply with data specific security regulations as well. For instance, medical data security will need to comply \nwith HIPAA, and financial data will need to comply with The Gramm-Leach-Bliley Act.\n6 Ray Dunham, FISMA Compliance: Security Standards & Guidelines Overview, Linford & Co. (Nov. 29, 2017), https://linfordco.com/blog/fisma-compliance/.\n7 Amy J. Frontz, Review of the Department of Health and Human Services Compliance with the Federal Information Security Modernization Act \nof 2014 for Fiscal Year 2020 (2021). \n8 U.S. Senate Comm. on Homeland Security & Governmental Affairs, Federal Cybersecurity: America’s Data at Risk 18 (2019). \n9 Federal Information Security Modernization Act (FISMA) Background, Nat’l Inst. Standards & Tech., https://csrc.nist.gov/projects/risk-management/\nfisma-background (last updated Aug. 4, 2021).\n10 Dunham, supra note 6.\n11 U.S. Senate Comm. on Homeland Security & Governmental Affairs, supra note 8, at 19.\n12 Id. at 18.\n13 Id. at 19.\n14 Id. at 20.\n15 Kevin Stine, et al., Guide for Mapping Types of Information and Information Systems to Security Categories (2008). Specifically, FISMA defines \ncompliance in terms of three levels: low impact, moderate impact, and high impact. Low impact indicates that the loss of confidentiality, integrity, or \navailability of the system will have a limited adverse effect, while high impact indicates that such losses will have severe or catastrophic effects. See Sar\u0002ah Harvey, 3 FISMA Compliance Levels: Low, Moderate, High, KirkpatrickPrice (Apr. 24, 2020), https://kirkpatrickprice.com/blog/fisma-compliance-lev\u0002els-low-moderate-high/.\n16 Nat’l Inst. Standards & Tech., Security and Privacy Controls for Information Systems and Organizations (2020). \n17 Marianne Swanson et al., Guide for Developing Security Plans for Federal Information Systems (2006). \n18 Michael McLaughlin, Reforming FedRAMP: A Guide to Improving the Federal Procurement and Risk Management of Cloud Services, Info. Tech. & Innova\u0002tion Found. (June 15, 2020), https://itif.org/publications/2020/06/15/reforming-fedramp-guide-improving-federal-procurement-and-risk-management.\n19 Program Basics, FedRAMP, https://www.fedramp.gov/program-basics/; see also Steven VanRoekel, Security Authorization of Information Sys\u0002tems in Cloud Computing Environments (2011). \n20 FISMA vs. FedRAMP and NIST: Making Sense of Government Compliance Standards, Foresite, https://foresite.com/fisma-vs-fedramp-and-nist-making\u0002sense-of-government-compliance-standards/. However, we note that FedRAMP approval is exempted for certain types of cloud models: (i) where the \ncloud is private to the agency, (ii) where the cloud is physically located within a Federal facility, (iii) where the agency is not providing cloud services \nfrom the cloud-based information system to any external entities. See VanRoekel, supra note 19.\n21 FedRAMP, FedRAMP Security Assessment Framework 5 (2017).\n22 Doina Chiacu, White House Warns Companies to Step Up Cybersecurity: ‘We Can’t Do it Alone’, Reuters (June 3, 2021), https://www.reuters.com/tech\u0002nology/white-house-warns-companies-step-up-cybersecurity-2021-06-03/; see also Significant Cyber Incidents, Ctr. Strategic & Int’l Studies, https://\nwww.csis.org/programs/strategic-technologies-program/significant-cyber-incidents (last visited Aug. 19, 2021).\n23 U.S. Senate Comm. Homeland Security & Governmental Affairs, supra note 8, at 5.\n24 Id. at 6.\n25 Frontz, supra note 7.\n26 Jonathan Reiber & Matt Glenn, The U.S. Government Needs to Overhaul Cybersecurity. Here’s How., Lawfare (Apr. 9, 2021), https://www.lawfareblog.\ncom/us-government-needs-overhaul-cybersecurity-heres-how.\n27 Nat’l Security Agency, Embracing a Zero Trust Security Model (2021).\n28 McLaughlin, supra note 18.\n29 Id.\n30 Id.\n31 Exec. Order No. 14,028, 86 Fed. Reg. 26633 (May 17, 2021).\n32 U.S. Office of Mgmt. & Budget, Moving the U.S. Government Towards Zero Trust Cybersecurity Principles (2021).\nA Blueprint for the National Research Cloud 105\n33 See, e.g., David Kushner, The Real Story of Stuxnet, IEEE Spectrum (Feb. 26, 2013), https://spectrum.ieee.org/the-real-story-of-stuxnet.\n34 HTTP is the protocol at the highest level of abstraction targeting the application layer, and its secure variant HTTPS additionally encrypts the data \nusing an encryption protocol. Without encryption, HTTP is insecure and should not be used. The encryption protocol in original use was SSL but this \nhas since been deprecated in the realm of network security in favor of its newer version, TLS. Both SSL and TLS rely on public key certificates signed by \na trusted certificate authority. When these certificates have expired, the websites providing them can no longer necessarily be trusted. Although these \nmeasures have their own limitations, not adopting them can only be less secure.\n35 See, e.g., Azure Confidential Computing, Microsoft, https://azure.microsoft.com/en-ca/solutions/confidential-compute/; Nataraj Nagaratnam, \nConfidential Computing, IBM (Oct. 16, 2020), \nhttps://www.ibm.com/cloud/learn/confidential-computing; Confidential Computing, Google Cloud, https://cloud.google.com/confidential-computing.\n36 David Archer et al., From Keys to Databases—Real-World Applications of Secure Multi-Party Computation, 61 Computer J. 1749 (2018). \n37 Amit Elazari Bar On, We Need Bug Bounties for Bad Algorithms, Motherboard (May 3, 2018) https://www.vice.com/en/article/8xkyj3/we-need-bug\u0002bounties-for-bad-algorithms.\nChapter 9\n1 Importantly, this chapter discusses the extent to which researchers should be required to share their research outputs, not the extent to which re\u0002searchers should be required to share their private data. The latter was discussed in Chapter Three.\n2 Dan Robitzski, AI Researchers Are Boycotting A New Journal Because It’s Not Open Access, Futurism (May 3, 2018), https://futurism.com/artificial-intel\u0002ligence-journal-boycot-open-access.\n3 Mikio L. Braun & Cheng Soon Ong, Open Science in Machine Learning (2014).\n4 Since researchers using the NRC are not “contractors” under FAR/DFARS, and since evidence is lacking on the value of Other Transactions to AI re\u0002searchers, we do not cover FAR/DFARS and Other Transactions in this section. \n5 Under the Bayh-Dole Act, Aa “federal funding agreement” is defined as “any contract, grant, or cooperative agreement entered into between any \nFederal agency, other than the Tennessee Valley Authority, and any contractor for the performance of experimental, developmental, or research work \nfunded in whole or in part by the Federal Government.” 35 U.S.C. § 201.\n6 35 U.S.C. § 202.\n7 35 U.S.C. § 203.\n8 See, e.g., Mark A. Lemley & Julie E. Cohen, Patent Scope and Innovation in the Software Industry, 89 Cal. L. Rev. 1 (2001); Mark A. Lemley, Software \nPatents and the Return of Functional Claiming, 2013 Wis. L. Rev. 905 (2013).\n9 Jeremy Gillula & Daniel Nazer, Stupid Patent of the Month: Will Patents Slow Artificial Intelligence?, Elec. Frontier Found. (Sept. 29, 2017), https://\nwww.eff.org/deeplinks/2017/09/stupid-patent-month-will-patents-slow-artificial-intelligence.\n10 U.S. Patent & Trademark Off., Inventing AI: Tracing the Diffusion of Artificial Intelligence with Patents 2 (2020). \n11 See, e.g., Mike James, Google Files AI Patents, I Programmer (July 8, 2015), https://www.i-programmer.info/news/105-artificial-intelli\u0002gence/8765-google-files-ai-patents.html. This is especially problematic because companies represent 26 out of the top 30 AI patent applicants world\u0002wide, while only four are universities or public research organizations. World Intell. Prop. Org., Artificial Intelligence 7 (2019).\n12 Lisa Ouellette & Rebecca Weires, University Patenting: Is Private Law Serving Public Values?, 2019 Mich. St. L. Rev. 1329 (2019).\n13 Id. at 1331; see also Arti Kaur Rai, Regulating Scientific Research: Intellectual Property Rights and the Norms of Science, 94 Nw. U. L. Rev. 77, 136 (1999).\n14 See Brian J. Love, Do University Patents Pay Off? Evidence From a Survey of University Inventors in Computer Science and Electrical Engineering, 16 Yale \nJ. L & Tech. 285 (2014).\n15 See id. at 286.\n16 See, e.g., Tech Transfer FAQ, U. Mich., https://techtransfer.umich.edu/for-inventors/resources/inventor-faq/ (“We carefully review the commercial \npotential for an invention before investing in the patent process. However, because the need for commencing a patent filing usually precedes finding a \nlicensee, we look for creative and cost-effective ways to seek early protections for as many promising inventions as possible”); What is Technology Trans\u0002fer, Princeton U., https://patents.princeton.edu/about-us/what-technology-transfer (“[T]echnologies and everyday products are possible because of \ntechnology transfer . . . Because the discoveries emerging from university research tend to be early-stage, high-risk inventions, successful university \ntechnology transfer transactions require a patent system that protects such innovations.”).\n17 The Uniform Guidance for intellectual property is laid out in 2 C.F.R. § 200.315.\n18 Uniform Administrative Requirements, Cost Principles, and Audit Requirements for Federal Awards, Grants.gov, https://www.grants.gov/learn-grants/\ngrant-policies/omb-uniform-guidance-2014.html (last visited Aug. 27, 2021).\n19 See Key Sections of the Uniform Guidance, AICPA.org, https://www.aicpa.org/interestareas/governmentalauditquality/resources/singleaudit/uni\u0002formguidanceforfederalrewards/key-sections-uniform-guidance.html.\n20 2 C.F.R. § 200.315. A “federal award” under the Uniform Guidance includes, among other things, “the federal financial assistance that a recipient \nreceives directly from a Federal awarding agency or indirectly from a pass-through entity;” or “the cost-reimbursement contract under the Federal \nAcquisition Regulations;” or a “grant agreement, cooperative agreement, [or] other agreement [for federal financial assistance].” 2 C.F.R. § 200.1.\n21 2 C.F.R. §§ 200.315(b), (c). These provisions specify that the government merely “reserves” its “right” to copyright and data rights over research \nproduced under the federal award. \n22 U.S. Copyright Office, Compendium of U.S. Copyright Office Practices 35 (2021, 3d ed.). \n23 Wil Michiels, How Do You Protect Your Machine Learning Investment?, EETimes (Mar. 31, 2020), https://www.eetimes.com/how-do-you-protect-your\u0002machine-learning-investment-part-ii/. \n24 See, e.g., Tabrez Y. Ebrahim, Data-Centric Technologies: Patent and Copyright Doctrinal Disruptions, 43 Nova L. Rev. 287, 304; Daryl Lim, AI & IP: Innova\u0002tion & Creativity in an Age of Accelerated Change, 52 Akron L. Rev. 813, 835 (2018)\n25 2 C.F.R. § 200.315(b).\n26 Id.\n27 For a comprehensive report on how artificial intelligence is used in various government agencies, see David Freeman Engstrom, Daniel E. Ho, Cath\u0002erine M. Sharkey & Mariano-Florentino Cuéllar, Government by Algorithm: Artificial Intelligence in Federal Administrative Agencies (2020).\nA Blueprint for the National Research Cloud 106\n28 Jukebox, OpenAI (Apr. 30, 2020), https://openai.com/blog/jukebox/.\n29 See, e.g., Shlomit Yanisky-Ravid, Generating Rembrandt: Artificial Intelligence, Copyright, and Accountability in the 3A Era--the Human-Like Authors \nare Already Here--a New Model, 27 Mich. St. L. Rev. 659 (2017); Kalin Hristov, Artificial Intelligence and the Copyright Dilemma, 57 J. Franklin Pierce Ctr. \nIntell. Prop. 431 (2017). \n30 Kalin Hristov, Artificial Intelligence and the Copyright Survey, 16 J. Sci. Pol’y & Governance 1, 14-15 (2020).\n31 Id. at 16.\n32 See What is Transfer Learning?, TensorFlow (Mar. 31, 2020), https://www.tensorflow.org/js/tutorials/transfer/what_is_transfer_learning. \n33 See, e.g., Yunhui Guo et al., SpotTune: Transfer Learning Through Adaptive Fine-Tuning, Cornell U. (Nov. 2018), https://arxiv.org/pdf/1811.08737.pdf. \n34 2 C.F.R. § 200.315(d).\n35 See Zhiqiang Wan, Yazhou Zhang & Haibo He, Variational Autoencoder Based Synthetic Data Generation for Imbalanced Learning, IEEE (2017).\n36 See Noseong Park, Mahmoud Mohammadi & Kshitij Gorde, Data Synthesis Based on Generative Adversarial Networks, 11 Proc. VLDB Endowment\n1071 (2018).\n37 See Ron Bakker, Impact of Artificial Intelligence on IP Policy 12. \n38 See Marta Duque Lizarralde, A Guideline to Artificial Intelligence, Machine Learning and Intellectual Property 4-7 (2020). \n39 Steven M. Bellovin et al., Privacy and Synthetic Datasets, 22 Stan. Tech. L. Rev. 1, 2-3 (2019); see also Fida K. Dankar & Mahmoud Ibrahim, Fake It Till \nYou Make It: Guidelines for Effective Synthetic Data Generation, 5 Applied Sci. 11 (2021); but see Theresa Stadler et al., Synthetic Data – Anonymisation \nGroundhog Day, Cornell U. (July 8, 2021), https://arxiv.org/pdf/2011.07018.pdf.\n40 See, e.g., Daniel S. Quintana, A Synthetic Dataset Primer for the Biobehavioural Sciences to Promote Reproducibility and Hypothesis Generation, 9 eLife\n1 (2020).\n41 Yuji Roh et al., A Survey on Data Collection for Machine Learning, Cornell U. (Aug. 12, 2019), https://arxiv.org/pdf/1811.03402.pdf.\n42 See, e.g., Hang Qiu et al., Minimum Cost Active Labeling, Cornell U. (June 24, 2020), https://arxiv.org/pdf/2006.13999.pdf; Eric Horvitz, Machine \nLearning, Reasoning, and Intelligence in Daily Life: Directions and Challenges, 18 Proceeding of the Conf. on Uncertainty in Artificial Intelligence 3 \n(2007).\n43 Cognilytics Research, Data Engineering, Preparation, and Labeling for AI 2019 3 (2019).\n44 See Wil Michiels, How Do You Protect Your Machine Learning Investment?, EETimes (Mar. 26, 2020), https://www.eetimes.com/how-do-you-protect\u0002your-machine-learning-investment/. In fact, in the European Union, labeled datasets are awarded with database rights protections. Mauritz Kop, \nMachine Learning & EU Data Sharing Practices, Stan.-Vienna Transatlantic Tech. L. F.(Mar. 24, 2020), https://ttlfnews.wordpress.com/2020/03/24/\nmachine-learning-eu-data-sharing-practices/.\n45 See, e.g., Niklas Fiedler et al., ImageTagger: An Open Source Online Platform for Collaborative Image Labeling, 11374 Lecture Notes in Computer Sci. \n162 (2019).\n46 Id. at 162.\n47 Researchers may, for instance, use NRC data and compute resources to implement active learning strategies, procedures to manually label a subset \nof available data and infer the remaining labels automatically using a machine learning model. See, e.g., Oscar Reyes et al., Effective Active Learning \nStrategy for Multi-Label Learning, 273 Neurocomputing 494 (2018). Similarly, researchers may augment existing public sector data with valuable labels. \n48 See, e.g., Pedro Saleiro et al., Aequitas: A Bias and Fairness Audit Toolkit, Cornell U. (Apr. 29, 2019), https://arxiv.org/pdf/1811.05577.pdf; Florian \nTramèr et al., FairTest: Discovering Unwarranted Associations in Data-Driven Applications, Cornell U. (Aug. 16, 2019), https://arxiv.org/pdf/1510.02377.\npdf.\n49 While we do not discuss the idiosyncratic modifications to the Uniform Guidance that vary from agency-to-agency, we encourage the task force to \nassess these modifications if it decides to implement the NRC through a particular agency. If the NRC is administered through multiple agencies, the \ncomplex amalgam of agency-specific IP rules may increase the friction in using the NRC if researchers must context-switch from one set of regulations \nto the next depending on the funding agency.\n50 2 C.F.R. § 2900.13. Previously, the Department of Labor explicitly required IP generated under a federal award to be licensed under a Creative \nCommons Attribution license, but this rule was changed in April 2021 to replace the proprietary term “Creative Commons Attribution license” with the \nindustry-recognized standard “open license.” 86 Fed. Reg. 22107 (Apr. 27, 2021).\n51 Dissemination and Sharing of Research Results - NSF Data Management Plan Requirements, Nat’l Sci. Found., https://www.nsf.gov/bfa/dias/policy/\ndmp.jsp. \n52 See, e.g., Aidan Courtney et al., Balancing Open Source Stem Cell Science with Commercialization, Nature Biotechnology (Feb. 7, 2011), https://\nwww.nature.com/articles/nbt.1773.\n53 See Klint Finley, When Open Source Software Comes with a Few Catches, Wired (July 31, 2019), https://www.wired.com/story/when-open-source-soft\u0002ware-comes-with-catches/; Guide to Open Source Licenses, Synopsys (Oct. 7, 2016), https://www.synopsys.com/blogs/software-security/open-source-li\u0002censes/.\n54 See Daniel A. Almeida et. al, Do Software Developers Understand Open Source Licenses?, 25 IEEE Int’l Conf. on Program Comprehension 1 (2017) \n(finding that software developers “struggle[] when multiple [open-source] licenses [are] involved” and “lack the knowledge and understanding to tease \napart license interactions across multiple situations.”).\n55 See, e.g., Alexandra Theben et al., Challenges and Limits of an Open Source Approach to Artificial Intelligence 14 (2021); Stadler et al., supra \nnote 39; Milad Nasr et al., Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Feder\u0002ated Learning, Cornell U. (June 6, 2020), https://arxiv.org/abs/1812.00910.pdf.\n56 Some universities have decided to eliminate classified research. See, e.g., At the Hands of Radicals, Stan. Mag. (Jan. 2009), https://stanfordmag.org/\ncontents/at-the-hands-of-the-radicals.\n57 See Donald Kennedy, Science and Secrecy, 289 Sci. 724 (2000); Peter J. Westwick, Secret Science: A Classified Community in the National Laboratories, \n38 Minerva 363 (2000).\n58 See Braun & Ong, supra note 3; Sören Sonnenburg et al., The Need for Open Source Software in Machine Learning, 8 J. Machine Learning Res. 2443 \n(2007); see also Katie Malone & Richard Wolski, Doing Data Science on the Shoulders of Giants: The Value of Open Source Software for the Data Science \nCommunity, HDSR (May 31, 2020), https://hdsr.mitpress.mit.edu/pub/xsrt4zs2/release/4. \n59 See Laura A. Heymann, Overlapping Intellectual Property Doctrines: Election of Rights Versus Selection of Remedies, 17 Stan. Tech. L. Rev. 239, 240 \n(2013); Oracle Am. Inc. v. Google Inc., 750 F.3d 1339 (Fed. Cir. 2014) (accepting that software is both patentable and copyrightable).\nA Blueprint for the National Research Cloud 107\n60 Robert E. Thomas, Debugging Software Patents: Increasing Innovation and Reducing Uncertainty in the Judicial Reform of Software Patent Law, 25 \nSanta Clara Computer & High Tech. L.J. 191, 222-23 (2008).\n61 See, e.g., Joaquin Vanschorin et al., OpenML: Networked Science in Machine Learning, Cornell U. (Aug. 1, 2014), https://arxiv.org/pdf/1407.7722.pdf \n(developing a collaboration platform through which scientists can automatically share, organize and discuss machine learning experiments, data, and \nalgorithms); see also Sarah O’Meara, AI Researchers in China Want to Keep the Global-Sharing Culture Alive, Nature (May 29, 2019), https://www.nature.\ncom/articles/d41586-019-01681-x; Shuai Zhao et al., Packaging and Sharing Machine Learning Models via the Acumos AI Open Platform, 17 ICMLA (2018).\n62 Jeanne C. Fromer, Machines as the New Oompa-Loompas: Trade Secrecy, the Cloud, Machine Learning, and Automation, 94 N.Y.U. L. Rev. 706, 712 \n(2019); Jordan R. Raffe et al., The Rising Importance of Trade Secret Protection for AI- Related Intellectual Property 1, 5-6 (2020); Jessica \nM. Meyers, Artificial Intelligence and Trade Secrets, Am. Bar Ass’n (Feb. 2019), https://www.americanbar.org/groups/intellectual_property_law/pub\u0002lications/landslide/2018-19/january-february/artificial-intelligence-trade-secrets-webinar/; AIPLA Comments Regarding “Request for Comments on \nIntellectual Property Protection for Artificial Intelligence Innovation”, Am. Intell. Prop. L. Ass’n (Jan. 10, 2020), https://www.uspto.gov/sites/default/files/\ndocuments/AIPLA_RFC-84-FR-58141.pdf.\n63 Clark D. Asay, Artificial Stupidity, 61 Wm. & Mary L. Rev. 1187, 1197, 1241-42 (2020).\n64 See id.; Am. Intell. Prop. L. Ass’n, supra note 62, at 16.\n65 See Asay, supra note 63, at 1242.\nAppendix\n1 Department of Energy Awards $425 Million for Next Generation Supercomputing Technologies, Energy.gov (Nov. 14, 2014), https://www.energy.gov/\narticles/department-energy-awards-425-million-next-generation-supercomputing-technologies.\n2 Amazon EC2 P3 Instances, Amazon, https://aws.amazon.com/ec2/instance-types/p3/ (last visited Sept. 9, 2021).\n3 CORAL Request for Proposal B604142, Lawrence Livermore Nat’l Laboratory (2014), https://web.archive.org/web/20140816181824/ https://asc.llnl.\ngov/CORAL/. We note that we were not able to locate the final award documents, nor is Summit budgeted in sufficient detail to back out cost from the \nDOE budget statements. Our cost estimates here, however, are comparable to publicly reported estimates for the total cost of the Summit system. \n4 This is based on a $30M maximum in the DOE Office of Science contract for non-recurring engineering (NRE) costs for the systems at Argonne National \nLaboratory and Oak Ridge National Laboratory. \n5 This is based on the difference in the RFP terms between the inclusion of maintenance under the Lawrence Livermore National Laboratory system \n(with a maximum budget of $170M) and the exclusion of maintenance under the systems for the Oak Ridge National Laboratory and the Argonne \nNational Laboratory (with a maximum budget for the build contract of $155M). This is likely an upper bound on maintenance, given that the difference \nreflects the combination of NRE and 5-year maintenance. \n6 See CORAL Price Schedule, Lawrence Livermore Nat’l Laboratory (2014), https://web.archive.org/web/20140816181824/ https://asc.llnl.gov/CORAL/\nRFP_components/04_CORAL_Price_Schedule_ANL_ORNL_tabs.xlsx. We used 1.62% as the interest rate to calculate the cost over 60 months. It is the \n5-year Treasury constant maturity rate on November 14, 2014, see Selected Interest Rates (Daily) – H.15, Fed. Res., https://www.federalreserve.gov/\nreleases/H15/default.htm, when DOE announced the award of the HPC system, see Department of Energy Awards $425 Million for Next Generation Super\u0002computing Technologies, supra 1.\n7 For instance, this estimate is in line with the cost of $200M reported by the New York Times. Steve Lohr, Move Over, China: U.S. is Again Home to World’s \nSpeediest Supercomputer, N.Y. Times (June 8, 2018), https://www.nytimes.com/2018/06/08/technology/supercomputer-china-us.html. Some reporting \nconflates the procurement of multiple systems that occurred contemporaneously. \n8 Research shows that for training compute-intensive deep learning models, such as ResNet-101, the GPU utilization is around 70%. Jingoo Han et \nal., A Quantitative Study of Deep Learning Training on Heterogeneous Supercomputers, 2019 IEEE Conf. on Cluster Computing 1, 5 (2019). However, \nResNet-50 has a GPU utilization of approximately 40%, see id., and other accounts report that GPUs are utilized only 15-30% of the time, see, e.g., Lukas \nBiewald, Monitor and Improve GPU Usage for Training Deep Learning Models, Towards Data Sci. (Mar. 27, 2019), https://towardsdatascience.com/mea\u0002suring-actual-gpu-usage-for-deep-learning-training-e2bf3654bcfd; Janet Morss, Giving Your Data Scientists a Boost with GPUaaS, CIO (June 2, 2020), \nhttps://www.cio.com/article/3561090/giving-your-data-scientists-a-boost-with-gpuaas.html.\n9 Compute Canada, Cloud Computing for Researchers 1 (2016), https://www.computecanada.ca/wp-content/uploads/2015/02/CloudStrate\u0002gy2016-2019-forresearchersEXTERNAL-1.pdf.\n10 Jennifer Shkabatur, The Global Commons of Data, 22 Stan. Tech. L.R. 407, 407-09 (2019).\n11 Benjamin Sobel, Artificial Intelligence’s Fair Use Crisis, 41 Colum. J.L. & Arts 61 (2017).\n12 Id.\n13 See Protecting What We Love About the Internet: Our Efforts to Stop Online Piracy, Google Pub. Pol’y Blog (Nov. 7, 2019), https://www.blog.google/\noutreach-initiatives/public-policy/protecting-what-we-love-about-internet-our-efforts-stop-online-piracy/. \n14 See Jennifer M. Urban, Joe Karaganis & Brianna M. Schofield, Notice & Takedown in Everyday Practice 39 (2017) (illustrating the difficulty that \nonline service providers face in manually evaluating a large volume of data for potential infringement; for example, one online service provider ex\u0002plained that “out of fear of failing to remove infringing material, and motivated by the threat of statutory damages, its staff will take “six passes to try to \nfind the [identified content].”); see also Letter from Thom Tillis, Marsha Blackburn, Christopher A. Coons, Dianne Feinstein et. al, to Sundar Pichai, Chief \nExecutive Officer, Google Inc. (Sept. 3, 2019), https://www.ipwatchdog.com/wp-content/uploads/2019/09/9.3-Content-ID-Ltr.pdf (“We have heard from \ncopyright holders who have been denied access to Content ID tools, and as a result, are at a significant disadvantage to prevent repeated uploading of \ncontent that they have previously identified as infringing. They are left with the choice of spending hours each week seeking out and sending notices \nabout the same copyrighted works, or allowing their intellectual property to be misappropriated.”).\n15 See Google, How Google Fights Piracy 6 (2016). To illustrate the costs of implementing Content ID on a large-scale platform, Google announced in \na report in 2016 that YouTube had invested more than $60 million in Content ID. \n16 See Sobel, supra note 11, at 66-79.\n17 See Authors Guild v. Google Inc., 804 F.3d 202 (2d Cir. 2015).\n18 Id.\n19 Id. at 216-17.\n20 Matthew Stewart, The Most Important Court Decision For Data Science and Machine Learning, Towards Data Sci. (Oct. 31, 2019), https://towardsdata-\nA Blueprint for the National Research Cloud 108\nscience.com/the-most-important-supreme-court-decision-for-data-science-and-machine-learning-44cfc1c1bcaf. \n21 See, e.g., James Grimmelmann, Copyright for Literate Robots, 101 Iowa L. Rev. 657, 661; Sobel, supra note 11, at 51-57.\n22 See Sobel, supra note 11, at 57.\n23 See Anna I. Krylov et. al. What is the Price of Open Source Software? 6 J. Physical Chemistry Letters 2751, 2753 (2015) (explaining that budding \nresearchers considering commercialization may be particularly concerned about what licenses are available, since a “strictly open-source environment \nmay furthermore disincentivize young researchers to make new code available right away, lest their ability to publish papers be short-circuited by a \nmore senior researcher with an army of postdocs poised to take advantage of any new code.”).\n24 See, e.g., A Data Scientist’s Guide to Open-Source Licensing, Towards Data Sci. (Nov. 4, 2018), https://towardsdatascience.com/a-data-scientists\u0002guide-to-open-source-licensing-c70d5fe42079; Choose an Open-Source License, https://choosealicense.com. \n25 Licensing a Repository, GitHub, https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/licensing-a-repository. \n26 What is the Most Appropriate Licence for My Data?, FigShare, https://help.figshare.com/article/what-is-the-most-appropriate-licence-for-my-data. \n27 See Developer Agreement, Twitter (Mar. 10, 2020), https://developer.twitter.com/en/developer-terms/agreement; Non-commercial Use of the Twitter \nAPI, Twitter, https://developer.twitter.com/en/developer-terms/commercial-terms.\n28 See Daniel A. Almeida et. al, Do Software Developers Understand Open Source Licenses?, 25 IEEE Int’l Conf. on Program Comprehension 1 (2017). \n29 Id. at 9.\n30 Alexandra Kohn & Jessica Lange, Confused About Copyright? Assessing Researchers’ Comprehension of Copyright Transfer Agreements, 6 J. Librarian\u0002ship & Scholarly Commc’n. 1, 9 (2018). \n31 See Will Frass, Jo Cross & Victoria Gardner, Taylor & Francis Open Access Survey June 2014 15 (2014). Note that lack of IP literacy could act as \nan additional deterrent to uploaders. The Taylor and Francis Open Access Survey of 2014 found that “63% of respondents indicated a lack of under\u0002standing of publisher policy as an important or very important factor in failing to deposit an article in an IR [Institutional Repository].” Id.\n32 Dataverse Community Norms, Harv. Dataverse, https://dataverse.org/best-practices/dataverse-community-norms.\n33 Copyright and License Policy, FigShare, https://help.figshare.com/article/copyright-and-license-policy. \n34 Australian Data Research Commons, Research Data Rights Managing Guide 6 (2019).\n35 See Harvard Dataverse General Terms of Use, Harv. Dataverse (2021), https://dataverse.org/best-practices/harvard-dataverse-general-terms-use.\n36 Stan. U. Inst. of Human-Centered Artificial Intelligence, Artificial Intelligence Index Report 2021 125-34 (2021).\n37 Thilo Hagendorff, The Ethics of AI Ethics: An Evaluation of Guidelines, 30 Minds & Machines 99 (2020). \n38 Andrew D. Selbst, An Institutional View of Algorithmic Impact Assessments, 35 Harv. J.L. & Tech. 1, 66 (forthcoming 2021).\n39 Brent Mittlestadt, Principles Alone Cannot Guarantee Ethical AI, 1 Nature Mach. Intelligence 501 (2019). \n40 DOD Adopts Ethical Principles for Artificial Intelligence, U.S. Dep’t Defense (Feb. 24, 2020), https://www.defense.gov/Newsroom/Releases/Release/\nArticle/2091996/dod-adopts-ethical-principles-for-artificial-intelligence/. \n41 President’s Mgmt. Agenda, Federal Data Strategy: Data Ethics Framework (2020).\n42 Artificial Intelligence: An Accountability Framework for Federal Agencies and Other Entities, U.S. Gov’t Accountability Office (June 30, 2021), https://\nwww.gao.gov/products/gao-21-519sp.\n43 Principles of Artificial Intelligence Ethics for the Intelligence Community, Office of the Director of Nat’l Intelligence, https://www.odni.gov/index.\nphp/features/2763-principles-of-artificial-intelligence-ethics-for-the-intelligence-community. \n44 Key Considerations for Responsible Development and Fielding of Artificial Intelligence, Nat’l Security Comm’n Artificial Intelligence (2021), https://\nwww.nscai.gov/key-considerations/. \n45 Recommended Practices, Nat’l Security Comm’n Artificial Intelligence, https://www.nscai.gov/wp-content/uploads/2021/01/Key-Consider\u0002ations-Supporting-Visuals.pdf. \n46 Defense Innovation Bd., AI Principles: Recommendations on the Ethical Use of Artificial Intelligence by the Department of Defense (2019). \n",
    "length": 431144,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Generative AI: Perspectives from Stanford HAI",
    "url": "https://hai.stanford.edu/generative-ai-perspectives-stanford-hai?sf175848990=1",
    "text": "[Skip to main content] [Skip to secondary navigation] \n\n[Stanford University(link is external)] \n\nPage Content\n\n# Generative AI: Perspectives from Stanford HAI\n\n**March 2023**\n\n[![Generative AI Document]] The current wave of generative AI is a subset of artificial intelligence that, based on a textual prompt, generates novel content. ChatGPT might write an essay, Midjourney could create beautiful illustrations, or MusicLM could compose a jingle. Most modern generative AI is powered by foundation models, or AI models trained on broad data using self-supervision at scale, then adapted to a wide range of downstream tasks.\n\nThe opportunities these models present for our lives, our communities, and our society are vast, as are the risks they pose. While on the one hand, they may seamlessly complement human labor, making us more productive and creative, on the other, they could amplify the bias we already experience or undermine our trust of information.\n\nWe believe that interdisciplinary collaboration is essential in ensuring these technologies benefit us all. The following are perspectives from Stanford leaders in medicine, science, engineering, humanities, and the social sciences on how generative AI might affect their fields and our world. Some study the impact of technology on society, others study how to best apply these technologies to advance their field, and others have developed the technical principles of the algorithms that underlie foundation models.\n\n[Read the full perspective on Generative AI] \n\n## Authors\n\n![Russ headshot photo] \n\n### [Russ Altman] \n\n![Erik Brynjolfsson] \n\n### [Erik Brynjolfsson] \n\n![photo] \n\n### [Michele Elam] \n\n![Surya Ganguli] \n\n### [Surya Ganguli] \n\n![Daniel E. Ho] \n\n### [Daniel E. Ho] \n\n![James Landay] \n\n### [James Landay] \n\n![Curt Langlotz] \n\n### [Curt Langlotz] \n\n![Fei-Fei headshot photo] \n\n### [Fei-Fei Li] \n\n![Percy Liang] \n\n### [Percy Liang] \n\n![photo] \n\n### [Christopher Manning] \n\n![Peter Norvig] \n\n### [Peter Norvig] \n\n![photo] \n\n### [Rob Reich] \n\n![Vanessa Parli] \n\n### [Vanessa Parli] \n\n* * *\n\n## Contact\n\nStay up to date with Stanford HAI by [subscribing to our newsletter].",
    "length": 2148,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "NTIA RFC [Stanford, Princeton]",
    "url": "https://hai.stanford.edu/sites/default/files/2023-06/Response-to-Request.pdf",
    "text": "Department of Commerce\nNational Telecommunications and Information Administration\nDocket No. 230407-0093\nRIN 0660-XC057\nAI Accountability Policy Request for Comment\nJune 12, 2023\nTo Whom It May Concern:\nWe, researchers from the Stanford Center for Research on Foundation Models (CRFM), part of\nthe Stanford Institute for Human-Centered Artificial Intelligence (HAI), and Princeton\nUniversity’s Center for Information Technology Policy (CITP), offer the following submission in\nresponse to the Request for Comment (RFC) by the National Telecommunications and\nInformation Administration on AI accountability policy.\n1 We center our response on foundation\nmodels (FMs), which constitute a broad paradigm shift in AI. Foundation models require\nsubstantial data and compute to provide striking capabilities that power countless downstream\nproducts and services.2 While many prominent uses and abuses of FMs have been highlighted,\nwe focus on consequential aspects that, if addressed effectively, will significantly improve the\nstate of the FM ecosystem.\nGiven the significance of foundation models, we argue that pervasive opacity compromises\naccountability for foundation models. Foundation models and the surrounding ecosystem are\ninsufficiently transparent, with recent evidence showing this transparency is deteriorating\nfurther.\n3 Without sufficient transparency, the federal government and industry cannot implement\nmeaningful accountability mechanisms as we cannot govern what we cannot see. The history of\nregulating online platforms and social media foretells the story for foundation models: If we fail\nto act now to ensure foundation models are sufficiently transparent, we are destined to repeat the\navoidable errors of the past.\nOur submission recommends the following in response to questions posed in the RFC:\n● Invest in digital supply chain monitoring for foundation models (Section 2; Questions\n5, 11, 15, 20).\n● Invest in public evaluations of foundation models (Section 3; Questions 3, 21, 23, 29,\n30b).\n● Incentivize research on guardrails for open-source models (Section 4; Question 7,\n32).\n3 Rishi Bommasani et al. Ecosystem Graphs: The Social Footprint of Foundation Models. 2023.\nhttps://crfm.stanford.edu/2023/03/29/ecosystem-graphs.html.\n2 Rishi Bommasani, …, and Percy Liang. On the Opportunities and Risks of Foundation Models. 2021.\nhttps://crfm.stanford.edu/report.html.\n1 This response reflects the independent views of the undersigned scholars.\n1\nFoundation models are an immature technology advancing at an unprecedented clip. To\nsuccessfully implement our recommendations, collaboration between the federal government,\nacademia, and industry will be necessary. As academic researchers, we highlight academia’s\nunique strengths: interdisciplinary scholarship and neutral science divorced from commercial\ninterests. We highlight the importance of additional investment in resources and infrastructure to\nadvance these efforts on AI accountability: academic compute infrastructure4 and federal funding\nfor AI research in the public interest.5\n1. Background on foundation models\nFoundation models are general-purpose technologies that function as platforms for a wave of AI\napplications, including generative AI: AI systems that can generate compelling text, images,\nvideos, speech, music, and more. Well-known examples include OpenAI’s ChatGPT, which is a\nlanguage model that can converse with users and perform complex tasks as instructed through its\nlanguage interface, and Stability AI’s Stable Diffusion, which is a text-to-image model that can\ngenerate photorealistic images from text-based prompts.\nFoundation models constitute a paradigm shift in AI development and deployment. Rather than\ndeveloping a single bespoke model for each application, foundation models require tremendous\nupfront resource investment (e.g., tens or hundreds of millions of dollars and trillions of bytes of\ndata for the most capable systems like OpenAI’s GPT-4). These high upfront costs are justified\nby the significant new capabilities of these models, which can be reused across many\ndownstream use cases.\nWhy are foundation models a critical priority? Foundation models underpin many of the recent\nadvances in AI: We highlight five properties that indicate why they merit significant priority.\n1. Nascent. Given their recent development, there is no well-developed understanding of\nhow their risks will be addressed by any combination of self-regulation and regulation.\n2. Prominent. Foundation models are the center of the public awareness on AI, mediated by\ndaily global media attention from many of the world’s largest news outlets.\n3. Burgeoning. Foundation models are the fastest-growing consumer technology in U.S.\nhistory6 with tremendous commercial investment in startups7 and established companies.8\n8\nAccording to an Accenture market survey, “98% of global executives agree AI foundation models will play an\nimportant role in their organizations’ strategies in the next 3 to 5 years.” See Accenture. Technology Vision 2023:\nWhen Atoms meet Bits. 2023. https://www.accenture.com/content/dam/accenture/final/accenture-com/\na-com-custom-component/iconic/document/Accenture-Technology-Vision-2023-Full-Report.pdf.\n7 More than $11 billion in the first fiscal quarter of 2023 was directed toward foundation model startups. See: Ian\nHogarth. We Must Slow Down the Race to God-like AI. Financial Times. 2023. https://www.ft.com/content/\n03895dc4-a3b7-481e-95cc-336a524f2ac2\n6 Krystal Hu. ChatGPT Sets Record for Fastest-Growing User Base. Reuters. 2023. https://www.reuters.com/\ntechnology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/\n5 White House. FACT SHEET: Biden-Harris Administration Announces New Actions to Promote Responsible AI\nInnovation that Protects Americans’ Rights and Safety. 2023. https://www.whitehouse.gov/briefing-room/\nstatements-releases/2023/05/04/fact-sheet-biden-harris-administration-announces-new-actions-to-promote-responsib\nle-ai-innovation-that-protects-americans-rights-and-safety/.\n4 White House. National Artificial Intelligence Research Resource Task Force Releases Final Report. 2023.\nhttps://www.whitehouse.gov/ostp/news-updates/2023/01/24/national-artificial-intelligence-research-resource-task-fo\nrce-releases-final-report/.\n2\nThere has also been significant investment in building the technology by a number of\ncountries around the world.9\n4. Pervasive. Foundation models already power products spanning dozens of market\nsectors10 and will only continue to spread, akin to other defining technologies like the\ninternet, computers, mobile phones, and semiconductors.\n5. Consequential. Foundation models are expected to influence multiple dimensions of the\nlives of almost every American. Their importance has already been recognized by\npolicymakers around the world: They are the centerpiece of most recent revisions to the\nEU AI Act, the subject of a new task force reporting to the U.K. prime minister, and the\ntopic of interest across multiple U.S. federal entities beyond the NTIA, including the\nWhite House Office of Science and Technology Policy (OSTP), the National AI Advisory\nCommittee (NAIAC), the Federal Trade Commission (FTC), and the National Institute of\nStandards and Technology (NIST).11\n2. Invest in digital supply chain monitoring for foundation models (Questions 5, 11, 15, 20)\nFoundation models function as the understructure for a diverse range of products. Consequently,\nthey play a central role in the broader AI ecosystem and digital supply chain.12 The foundation\nmodel ecosystem can be summarized by three categories of assets:\n1. Resources, meaning the data (e.g., the billions of words and images on the Internet) and\ncomputation (e.g., through cloud providers like Amazon, Google, and Microsoft)\nnecessary for training foundation models;\n2. Foundation models, such as ChatGPT and Stable Diffusion; and\n3. Products and services built atop these models (e.g., Bing Search, Khan Academy’s\nKhanmigo AI-powered tutor).\nThese assets determine much of the digital supply chain, thereby intermediating dependences\nbetween organization (e.g., Khan Academy depends on OpenAI because GPT-4 powers\nKhanmigo13) and, in turn, the sectors affected by foundation models. The ecosystem view makes\nclear where existing sector-level regulatory authority can be used to hold foundation models, the\n13 OpenAI. Khan Academy. 2023. https://openai.com/customer-stories/khan-academy.\n12 Aleksander Mądry. Written Statement for the Hearing, Advances in AI: Are We Ready for a Tech Revolution? In\nFront of the House Cybersecurity, Information Technology, and Government Innovation Subcommittee. 2023.\nhttps://oversight.house.gov/wp-content/uploads/2023/03/madry_written_statement100.pdf; Hopkins et al. The\nDiverse Landscape of AI Supply Chains: The AIaaS Supply Chain Dataset. Thoughts on AI Policy. 2023.\nhttps://aipolicy.substack.com/p/supply-chains-3-5.\n11 See White House. FACT SHEET; National AI Advisory Committee. Year 1 Report. 2023. https://www.ai.gov/\nwp-content/uploads/2023/05/NAIAC-Report-Year1.pdf; Federal Trade Commission. Chatbots, Deepfakes, and Voice\nClones: AI Deception for Sale. 2023. https://www.ftc.gov/business-guidance/blog/2023/03/chatbots-deepfakes\u0002voice-clones-ai-deception-sale; National Institute of Standards and Technology. Artificial Intelligence Risk\nManagement Framework. 2023. https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf.\n10 Rishi Bommasani et al. Ecosystem Graphs.\n9 U.K. Department for Science, Innovation and Technology. A Pro-Innovation Approach to AI Regulation. 2023.\nhttps://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper; Jeffrey Ding and\nJenny Xiao. Recent Trends in China’s Large Language Model Landscape. 2023. https://www.governance.ai/\nresearch-paper/recent-trends-chinas-llm-landscape; Pat Brans. Sweden is Developing its Own Big Language Model.\n2023. https://www.computerweekly.com/news/366538232/Sweden-is-developing-its-own-big-language-model.\n3\ncompanies that provide them, and their downstream products and services (e.g., in medicine or\nlaw) to account.\nThis digital supply chain is critical to many dimensions of commerce in the United States,\nincluding well-functioning, competitive markets. To date, the supply chain has been recognized\nas a key focus area for regulatory efforts per the EU AI Act,14 the foundation model market\nreview of the U.K.’s Competition and Markets Authority,\n15 and the recent testimony of MIT\nProfessor Aleksander Madry before the House Oversight Subcommittee on Cybersecurity,\nInformation Technology, and Government Innovation.16 To parallel the market surveillance\ninitiatives in both the EU and the U.K., we recommend federal investment into digital supply\nchain monitoring for foundation models. We review precedent for such monitoring in other\nsettings for digital technology as well as motivations for why such an initiative would improve\nAI accountability.\nThe practice of digital supply chain monitoring. While the foundation model ecosystem is\ncomplex and evolving, it is hardly unique. Almost every consumer product is the composite of\nmany materials or ingredients. In the setting of digital technologies, we can look to the Software\nBill of Materials (SBOM) as an example of effective dependency tracking mediated by\ngovernment intervention.17 As described by the Cybersecurity and Infrastructure Security\nAgency (CISA), an SBOM is “a list of ingredients that make up software components [that] has\nemerged as a key building block in software security and software supply chain risk\nmanagement.”18 In particular, “SBOM work has advanced since 2018 as a collaborative\ncommunity effort, driven by National Telecommunications and Information Administration’s\n(NTIA) multistakeholder process.”19 As a direct analogy, the federal government should track the\nassets and supply chain in the foundation model ecosystem to understand market structure,\naddress supply chain risk, and promote resiliency. As an example implementation, Stanford’s\nEcosystem Graphs currently documents the foundation model ecosystem, supporting a variety of\ndownstream policy use cases and scientific analyses.20\nHow digital supply chain monitoring improves accountability. Supply chain monitoring is highly\nmultifunctional—we describe three clear benefits. First, supply chain monitoring enables\nrecourse to stop further harm. For example, the National Highway Traffic Safety Administration\n(NHTSA) monitors the automobile supply chain, conducting recalls when a part (e.g., a batch of\n20 Rishi Bommasani et al. Ecosystem Graphs.\n19 National Telecommunications and Information Administration. Software Bill of Materials. https://ntia.gov/page/\nsoftware-bill-materials.\n18 Cybersecurity & Infrastructure Security Agency. Software Bill of Materials (SBOM). https://www.cisa.gov/sbom.\n17 Executive Order 14028. Improving the Nation’s Cybersecurity. 2021. https://www.whitehouse.gov/briefing-room/\npresidential-actions/2021/05/12/executive-order-on-improving-the-nations-cybersecurity/.\n16 Aleksander Mądry. Written Statement for the Hearing.\n15 U.K. Competition and Markets Authority. AI Foundation Models: Initial Review. 2023. https://www.gov.uk/\ncma-cases/ai-foundation-models-initial-review.\n14 For example, see Article 28 entitled “Responsibilities Along the AI Value Chain of Providers, Distributors,\nImporters, Deployers or Other Third Party” in the current European Parliament version: European Parliament.\nDRAFT Compromise Amendments on the Draft Report, Proposal for a regulation of the European Parliament and of\nthe Council on harmonised rules on Artificial Intelligence (Artificial Intelligence Act) and amending certain Union\nLegislative Acts. 2023. https://www.europarl.europa.eu/resources/library/media/20230516RES90302/\n20230516RES90302.pdf.\n4\ncar brakes) has been identified to be faulty.\n21 Second, supply chain monitoring identifies\nalgorithmic monoculture22: the pervasive dependence on a single asset (e.g., foundation model).\nAs SEC Chair Gary Gensler has noted, such dependence can yield “concentrated risk.”23 Finally,\nsupply chain monitoring for foundation models, which cuts across sectoral boundaries, exposes\nregulatory opportunities and weaknesses. Fundamentally, supply chain monitoring identifies\nwhich sectors are impacted by foundation models (e.g., products in the sector depend on FMs;\ndata from the sector powers FMs). This provides a principled means for identifying regions\nwhere sector-specific regulatory authority will not suffice to hold AI to account and chokepoints\nfor precision regulation in the future.\n3. Invest in public evaluations of foundation models (Questions 3, 21, 23, 29, 30b)\nEvaluation is the standard methodology for quantifying the capabilities, limitations, and risks of\nAI systems. Standardized evaluations simultaneously clarify the current status and orient future\nprogress. At present, some researcher evaluations exist for foundation models (especially\nlanguage models), but none have risen to the status of bona fide standards. Effective evaluations\nare complex to design: How should the endless use cases for a general-purpose technology be\nevaluated?24 The most recent Parliament draft version of the EU AI Act, for example, requires\nthat foundation model providers evaluate their models on public or industry standard\nbenchmarks.25 We highlight three specific considerations for effective evaluation of foundation\nmodels that the federal government should implement. The requirement for the National Institute\nof Standards and Technology (NIST) to develop AI testbeds as directed in the CHIPS and\nScience Act provides a direct opportunity to implement these recommendations.26\nPublic. To ensure foundation models are transparent, evaluations should be public and follow\nclear, openly disclosed practices.27 Such evaluations will improve the scientific discourse\nsurrounding these models, combatting various forms of hype and advancing our collective\nunderstanding of this emerging technology. Public evaluations set the baseline for how we\nshould reason about this technology. To evaluate models publicly and to engender trust, we\nreference efforts like Stanford’s Holistic Evaluation of Language Models (HELM).28 These\n28 Rishi Bommasani, Percy Liang, Tony Lee. Language Models Are Changing AI: The Need for Holistic Evaluation.\n2023. https://crfm.stanford.edu/2022/11/17/helm.html.\n27 Rishi Bommasani et al. Improving Transparency in AI Language Models: A Holistic Evaluation. Stanford Institute\nfor Human-Centered Artificial Intelligence. 2023. https://hai.stanford.edu/foundation-model-issue-brief-series.\n26 U.S. Congress. H.R.4346: CHIPS and Science Act (Section 10232). 2022. https://www.congress.gov/bill/\n117th-congress/house-bill/4346/text.\n25 Annex VIII, Section C on the registration of foundation models in a free and public EU database\nhttps://www.europarl.europa.eu/resources/library/media/20230516RES90302/20230516RES90302.pdf\n24 Inioluwa Deborah Raji et al. AI and the Everything in the Whole Wide World Benchmark. Proceedings of the\nNeural Information Processing Systems Track on Datasets and Benchmarks. 2021. https://arxiv.org/pdf/2111.\n15366.pdf\n23 Betsy Vereckey. SEC’s Gary Gensler on How Artificial Intelligence Is Changing Finance. 2022.\nhttps://mitsloan.mit.edu/ideas-made-to-matter/secs-gary-gensler-how-artificial-intelligence-changing-finance.\n22 Jon Kleinberg and Manish Raghavan. Algorithmic Monoculture and Social Welfare. Proceedings of the National\nAcademy of Sciences. 2021. https://www.pnas.org/doi/10.1073/pnas.2018340118; Rishi Bommansani, et al. Picking\non the Same Person: Does Algorithmic Monoculture Lead to Outcome Homogenization? Advances in Neural\nInformation Processing Systems. 2022. https://arxiv.org/abs/2211.13972.\n21 National Highway Traffic Safety Administration. Motor Vehicle Safety Defects and Recalls.\nhttps://www.nhtsa.gov/sites/nhtsa.gov/files/documents/14218-mvsdefectsandrecalls_041619-v2-tag.pdf.\n5\nefforts demonstrate the components required for useful public evaluations: (i) clear evaluation\nmethodology and definition of the relevant metrics, (ii) easily inspected results for individual\nmodels, and (iii) the underlying predictions or specific model behaviors that get aggregated to\nyield the results.\nHolistic. To ensure public evaluations surface the relevant dimensions of foundation models,\nthese evaluations should be holistic. Since foundation models are broad-reaching,\ngeneral-purpose technologies, they can be used across a range of use cases and should satisfy a\nrange of objectives (e.g., be accurate, robust, trustworthy, fair, efficient). Evaluation should\naddress these many dimensions: The NIST AI Risk Management Framework recognizes the\nimportance of such multidimensional evaluations.29 Of specific importance are the inevitable\ntrade-offs: For example, one foundation model may be more accurate but also more\ndiscriminatory than another in a given context. Or, in other cases, different metrics may be highly\ncorrelated: Multiple works establish that more accurate models are more robust or reliable.30As\nan example implementation, we point to HELM once again: language models are evaluated\nacross a range of use cases (e.g., question answering, document summarization), desiderata (e.g.,\nfairness, uncertainty), and capabilities/risks (e.g., world knowledge, disinformation generation).\nAll-encompassing. To ensure public and holistic evaluations hold all foundation models to\naccount, evaluations should encompass models that are restricted or closed, meaning models that\nare not openly accessible to researchers and the public. At present, foundation model providers\nadopt a variety of policies to release models.31 32 Some models like EleutherAI’s GPT-NeoX are\nopen-sourced, whereas others, like Google’s Flamingo, are entirely closed to the public. The\ninability of the public, including researchers and civil society, to investigate and interrogate these\nmodels inhibits external scrutiny. Given these models provide the core capabilities that power\nproducts, including technologies like Google Search that affect hundreds of millions of users, the\nmodels themselves should be evaluated.33 To improve the status quo, the government should\nrequire all model developers to create programs for external researcher access.34 Given\ncompanies may be disincentivized to provide meaningful access (e.g., due to concerns of\nintellectual property or competitive pressure), we encourage investigation into innovative\n34 Microsoft. Microsoft Turing Academic Program (MS-TAP). https://www.microsoft.com/en-us/research/\ncollaboration/microsoft-turing-academic-program/.\n33 Rishi Bommasani et al. Improving Transparency in AI Language Models.\n32 Irene Solaiman. The Gradient of Generative AI Release: Methods and Considerations. 2023. https://arxiv.org/\npdf/2302.04844.pdf.\n31 Percy Liang et al. The Time Is Now to Develop Community Norms for the Release of Foundation Models. 2022.\nhttps://crfm.stanford.edu/2022/05/17/community-norms.html.\n30 John Miller et al. Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and\nIn-Distribution Generalization. Proceedings of the International Conference on Machine\nLearning. 2021. https://arxiv.org/pdf/2107.04649.pdf; Percy Liang et al. Holistic Evaluation of Language Models.\n2023. https://arxiv.org/pdf/2211.09110.pdf.\n29 National Institute of Standards and Technology. AI Risk Management Framework. 2023. https://www.nist.gov/itl/\nai-risk-management-framework.\n6\nstrategies such as sandboxes35 to allow for testing in contained environments or\ndeveloper-mediated access36 to allow evaluations to be gated by provider consent.\n4. Incentivize research on guardrails for open-source models (Questions 7, 32)\nFor foundation models to advance the public interest, their development and deployment should\nensure transparency, support innovation, distribute power, and minimize harm. The release\npolicies for foundation models,37 in particular, directly influence these four goals. Currently,\nfoundation model providers adopt different release policies, with the most capable foundation\nmodels often restricted in terms of the access available to the public.38 In other words, while\nsome of these models (e.g., OpenAI's ChatGPT, Anthropic’s Claude) are available for the public\nto interact with, the internals (i.e., model weights) and training data are not publicly available.\nOpen-source efforts for digital technologies like operating systems (e.g., Linux) and browsers\n(e.g., Mozilla Firefox) establish the precedent that open-source can simultaneously achieve these\nfour goals. Therefore, we consider open-source foundation models: foundation models where the\ninternals (i.e., model weights) are available (without major use restrictions) and the training is\nreproducible by researchers.39 We argue open-source foundation models can achieve all four of\nthese objectives, in part due to inherent merits of open-source (pro-transparency, pro-innovation,\nanti-concentration), if the federal government incentivizes a responsible open-source ecosystem.\nTransparency. Transparency is a hallmark virtue of open-source: We should expect open-source\napproaches to perform especially well in terms of their transparency. Open-source foundation\nmodels guarantee a certain degree of transparency: If the model is open-sourced, other entities\ncan access and scrutinize the model, improving public understanding and trust. In practice,\nopen-source models are often released alongside the entire training data and codebase to\nreproduce them.40 This improves their auditability, since independent researchers can examine\nhow well they work. In particular, the transparency interventions recommended in earlier\nsections could be easier to implement for open-source models.\nInnovation. With the release of open-source models, many researchers and technologists can\nexperiment with new directions for developing FMs. As one example of how open-source\nmodels enable innovation, consider LLaMA. The release of the LLaMA language model by\nMeta spawned a large number of research projects, including advances in miniaturization41,\n41 Georgi Gerganov’s ggml library allows users to run capable open-source models like Meta’s LLaMA and\nOpenAI’s Whisper on their local computers. See GGML - AI at the edge. https://ggml.ai/.\n40 The MosaicML NLP Team. Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs.\n2023. https://www.mosaicml.com/blog/mpt-7b.\n39 For a deeper discussion into the various release strategies for foundation models, see: Solaiman. The Gradient of\nGenerative AI Release.\n38 Solaiman. The Gradient of Generative AI Release.\n37 Percy Liang et al. The Time Is Now to Develop Community Norms for the Release of Foundation Models.\n36 Toby Shevlane. Structured Access: An Emerging Paradigm for Safe AI Deployment. 2022. https://arxiv.org/ftp/\narxiv/papers/2201/2201.05159.pdf.\n35 Title V, Article 53, entitled “AI Regulatory Sandboxes.” https://www.europarl.europa.eu/resources/library/\nmedia/20230516RES90302/20230516RES90302.pdf.\n7\ninstruction-following42\n, and fine-tuning models efficiently43. These innovations would not be\npossible without access to a model’s weights, so in the absence of open-source models, these\ninnovations would either be restricted to AI companies, or they wouldn’t take place at all. There\nis precedent for such innovation—most notably under the umbrella of Free and Open Source\nSoftware (FOSS), like Linux and Mozilla Firefox.\nDistribution of power and expertise. Concentration of AI resources in corporations would make\nit harder for people in the public sector to develop expertise in FMs and to develop public\ninterest technology atop FMs. A robust open-source ecosystem would allow developers and\nresearchers from a diversity of backgrounds to build expertise in and contribute to the\ndevelopment of FMs. We have seen such efforts before; notably, the BLOOM language model\nwas built by an open-source collaboration of over a thousand researchers.44\nSecurity and risk mitigation. Open-source generally involves contributions from many\nindividuals, amounting to a more chaotic ecosystem that might be less secure. Namely, several\norganizations have argued AI systems are more secure if restrictions are enforced on who can\ncreate state-of-the-art FMs, similar to nuclear weapons.45 However, such efforts could have\nundesirable effects and hamper our ability to deal with AI risks.46\nIf closed-source models cannot be examined by researchers and technologists, security\nvulnerabilities might not be identified before they cause harm. (One example of such a\nvulnerability is memorization: language models’ tendency to memorize data, including sensitive\ninformation like credit card numbers, which can later be extracted by users.47 Another example is\nprompt injection, where a malicious instruction can trick a language model into performing\nunintended tasks, such as leaking private information when using personal assistants.48) On the\nother hand, experts across domains can examine and analyze open-source models, which makes\nsecurity vulnerabilities easier to find and address.\nIn addition, restricting who can create FMs would reduce the diversity of capable FMs and may\nresult in single points of failure in complex systems. If the same FM powers many different\nproducts and services, a security vulnerability in the FM would affect all of them.49 50 A diverse\n50 Rishi Bommasani, …, and Percy Liang. On the Opportunities and Risks of Foundation Models.\n49 Monoculture has also affected security issues in the past. For examples of security risks due to lack of diversity in\ncomputational infrastructure, see Peter Eder-Neuhauser, Tanja Zseby, and Joachim Fabini. Malware Propagation in\nSmart Grid Monocultures. 2018. https://link.springer.com/article/10.1007/s00502-018-0616-5.\n48 Simon Willison. Prompt Injection: What’s the Worst That Can Happen? 2023. https://simonwillison.net/2023/\nApr/14/worst-that-can-happen/.\n47 Nicholas Carlini et al. The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks.\nhttps://www.usenix.org/conference/usenixsecurity19/presentation/carlini.\n46 For an overview of why such restrictions are unlikely to be effective, see: Sayash Kapoor and Arvind Narayanan.\nLicensing Is Neither Feasible Nor Effective for Addressing AI Risks. 2023. https://aisnakeoil.substack.com/p/\nlicensing-is-neither-feasible-nor.\n45 Sam Altman, Greg Brockman, and Ilya Sutskever. Governance of Superintelligence. OpenAI, 2023. https://openai.\ncom/blog/governance-of-superintelligence.\n44 Teven Le Scao et al. BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. 2023. https://arxiv.\norg/abs/2211.05100.\n43 Tim Dettmers et al. QLoRA: Efficient Finetuning of Quantized LLMs. 2023. https://arxiv.org/abs/2305.14314.\n42 Rohan Taori et al. Alpaca: A Strong, Replicable Instruction-Following Model. 2023. https://crfm.stanford.edu/\n2023/03/13/alpaca.html.\n8\nselection of capable open-source models could avoid single points of failure that arise from\nrestrictions on developing state-of-the-art models.\nGuardrails for open-source FMs: a four-pronged approach. Open-source FMs do bear risks.\nUsers could harm themselves or others using these models.51 For the federal government to\nincentivize a flourishing and responsible open-source ecosystem as we have seen for other\ntechnologies, the regulatory approach for open-source must differ from proprietary FMs. For\nexample, open-source developers are often ill-equipped to meet requirements on downstream\nuses of these models, whereas providers who bring models to market are better targets for such\nrequirements. We suggest a more nuanced approach that addresses different steps in the\nfoundation model life cycle such as development, deployment, and use.\n1. Transparency of model developers. The federal government should require developers\nto perform transparent evaluations of open-source foundation models prior to release, so\nthat stakeholders can understand the capabilities and risks. Previous sections illustrate\nhow these requirements could be shaped.\n2. Compliance of downstream providers. A foundation model is not a product by itself.\nConsumers won’t use FMs directly, but rather products and services that incorporate\nthem. These products and services are subject to sectoral consumer protections and\nproduct safety restrictions. Regulatory agencies should enforce these requirements on\nproviders of the consumer-facing products or services built using open-source FMs.\n3. Resilience of attack surfaces. Bad actors may use FMs to generate disinformation, find\nsecurity vulnerabilities in software, or cause other forms of harm.52 53 Each of these\nmalicious uses involves an attack surface. For example, the attack surface for\ndisinformation is typically a social media platform—that is, where influence operators\nseek to disseminate disinformation and persuade people. For security vulnerabilities, the\nattack surface may be software codebases. While efforts may be taken to prevent the\nadaptation of foundation models for malicious purposes,54 policy should focus on attack\nsurfaces that come under greater pressure due to the proliferation of FMs. Such policies\ncould include incentivizing social media platforms to strengthen their information\nintegrity efforts, and increasing funding for cybersecurity and infrastructure defense\nefforts such as via CISA.\n4. Research on open-source FMs. Open-source foundation models are a nascent\nmethodology where our understanding of the risks of FMs and ways to address them is\nrapidly evolving. To realize a thriving and responsible open-source foundation model\n54 Eric Mitchell et al. Self-Destructing Models: Increasing the Costs of Harmful Dual Uses in Foundation Models.\n2022. https://arxiv.org/pdf/2211.14946.pdf.\n53 Josh A. Goldstein et al. Generative Language Models and Automated Influence Operations: Emerging Threats\nand Potential Mitigations. 2023. https://arxiv.org/abs/2301.04246.\n52 For instance, the June 6, 2023, letter from Sen. Richard Blumenthal and Sen. Josh Hawley to Meta outlines\nvarious risks of releasing open-source models. See: https://www.blumenthal.senate.gov/imo/media/doc/06062023\nmetallamamodelleakletter.pdf.\n51 Laura Weidinger et al. Taxonomy of Risks Posed by Language Models. ACM FAccT, 2022.\nhttps://dl.acm.org/doi/10.1145/3531146.3533088.\n9\necosystem, we must resolve fundamental research problems in transparency, compliance,\nand malicious use. Federal funding for research on the risks and mitigations of\nopen-source FMs would ensure that our understanding of the policy options can keep\npace with technology.\n55\nSincerely,\nRishi Bommasani\nResearcher & Society Lead, Stanford Center for Research on Foundation Models\nPh.D. Candidate, Stanford University\nSayash Kapoor\nResearcher, Princeton Center for Information Technology Policy\nPh.D. Candidate, Princeton University\nDaniel Zhang\nSenior Manager for Policy Initiatives, Stanford Institute for Human-Centered Artificial\nIntelligence\nDr. Arvind Narayanan\nIncoming Director, Princeton Center for Information Technology Policy\nProfessor of Computer Science, Princeton University\nDr. Percy Liang\nDirector, Stanford Center for Research on Foundation Models\nAssociate Professor of Computer Science and (By Courtesy) of Statistics, Stanford University\n55 As an example of research on the data governance practices of open-source models, see Jernite et al. on the release\nof the BLOOM model: Yacine Jernite et al. Data Governance in the Age of Large-Scale Data-Driven Language\nTechnology. ACM FAccT, 2022. https://dl.acm.org/doi/abs/10.1145/3531146.3534637.\n10",
    "length": 33756,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "GenAI | University IT",
    "url": "https://uit.stanford.edu/ai/overview",
    "text": "GenAI | University IT[Skip to main content] \n[![Stanford] University IT] Main Menu\nTopic Menu\n# GenAI\nGet to know the growing world of generative artificial intelligence\n* [About GenAI] \n* [Tips and tools] \n* [Learn more] \n* [Questions and ideas] \n![] \n## Now Available: Google Gemini, NotebookLM, and Microsoft Copilot Chat\nDecember 16, 2025\n[Learn more about Now Available: Google Gemini, NotebookLM, and Microsoft Copilot Chat] \n![] \n## In the AI Playground, Now You Can…\nDecember 1, 2025\n[Learn more about In the AI Playground, Now You Can…] \n## About GenAI\nWhile artificial intelligence (AI) has existed in various forms for many years, the latest advancementsin generative AI (GenAI) capabilities present significant potentialformeaningfulreal-world impact. With access tounprecedentedcomputing power, these technologies standpoisedto reshape ourapproach to teaching and learning, enhanceresearch endeavors, and optimizebusiness operations.\nWe've developedthisresource to offer insightsonGenAI technologies and their applications. The content presented herewill evolve over timeas we strive to supportan inclusive, innovative, informed, and responsible AI community at Stanford.\nIt's important to note this site isn't intended to be comprehensive. Additional information about AI and GenAI has been publishedby various schools, units, and initiatives at Stanford.For more specific guidance on GenAI use, we encourage you to consult your departmental resources.\n## Tips and tools\nTo make the most ofAI, we need tounderstandhow to engage with it. That's where the importance of tools,prompts, and responsible experimentation comes in. Dive in and explore—no technical expertise is required for getting started with prompts.\n### Get started with GenAI prompts\nThe data you enter can significantly impactthe quality of the results you receive. Discover effective methods forguiding GenAI systems throughtext instructions and visual cues.\n[Discover how to generate better prompts »] \n### Experiment with GenAI\nA greatway to learn about AI is through hands-on practice. Experiment with AI, responsibly. Take a look at guidance and dive in with Stanford's AI Playground.\n[Learn about experimenting with GenAI »] [Try out the AI Playground »] \n### Get familiar with AI best practices\nExplore Stanford resources for guidance on how to use AI tools effectively while keeping the university's data safe with the Responsible AI page. You'll also find[GenAI tools being explored by University IT].\n[Visit the Responsible AI webpage »] \n## Learn more\nWant to know more about AI? There are many learning opportunities to help you understand what AI is and how it can be used. Begin with the foundations as explained in the FAQs.\n### Start with GenAI FAQs\nNew to GenAI? You're in the right place.\nBrowse these frequently asked questions as a warm-up exercise to establish afoundational understanding.\n[Visit the GenAI FAQs »] \n### Grow your expertise with training\nBuild your skills and AI confidence with these courses.\n* [UIT Tech Training: Upcoming Artificial Intelligence (AI) Sessions] \n* [LinkedIn Learning: Artificial Intelligence] \n* Coursera([log in with your Stanford IDto audit])\n* [AI for Everyone] \n* [Prompt Engineering for ChatGPT] \n### AI at Stanford\nHere's a partial list to learn more about AI at Stanford.\n* [Stanford University Human-Centered Artificial Intelligence (HAI)] \n* [AI Meets Education at Stanford (AImES)] by the Center for Teaching and Learning\n* [Insights by Stanford Business: Technology and AI] \n* Discover how campus units are exploring[AI in the Campus IT Plan] \n[More AI efforts at Stanford »] \n## Questions and ideas\nDo you have questions, suggestions, orthoughts to share aboutAI for University IT (UIT)? Reach out and let us know what's on your mind.\n[Share your feedback] \n[![Stanford University]] \n* [Stanford Home] \n* [Maps & Directions] \n* [Search Stanford] \n* [Emergency Info] \n* [Terms of Use] \n* [Privacy] \n* [Copyright] \n* [Trademarks] \n* [Non-Discrimination] \n* [Accessibility] \n©CopyrightStanford University.Stanford,California94305.",
    "length": 4075,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Generative AI: Perspectives from Stanford HAI",
    "url": "https://hai.stanford.edu/generative-ai-perspectives-stanford-hai?sf175588029=1",
    "text": "[Skip to main content] [Skip to secondary navigation] \n\n[Stanford University(link is external)] \n\nPage Content\n\n# Generative AI: Perspectives from Stanford HAI\n\n**March 2023**\n\n[![Generative AI Document]] The current wave of generative AI is a subset of artificial intelligence that, based on a textual prompt, generates novel content. ChatGPT might write an essay, Midjourney could create beautiful illustrations, or MusicLM could compose a jingle. Most modern generative AI is powered by foundation models, or AI models trained on broad data using self-supervision at scale, then adapted to a wide range of downstream tasks.\n\nThe opportunities these models present for our lives, our communities, and our society are vast, as are the risks they pose. While on the one hand, they may seamlessly complement human labor, making us more productive and creative, on the other, they could amplify the bias we already experience or undermine our trust of information.\n\nWe believe that interdisciplinary collaboration is essential in ensuring these technologies benefit us all. The following are perspectives from Stanford leaders in medicine, science, engineering, humanities, and the social sciences on how generative AI might affect their fields and our world. Some study the impact of technology on society, others study how to best apply these technologies to advance their field, and others have developed the technical principles of the algorithms that underlie foundation models.\n\n[Read the full perspective on Generative AI] \n\n## Authors\n\n![Russ headshot photo] \n\n### [Russ Altman] \n\n![Erik Brynjolfsson] \n\n### [Erik Brynjolfsson] \n\n![photo] \n\n### [Michele Elam] \n\n![Surya Ganguli] \n\n### [Surya Ganguli] \n\n![Daniel E. Ho] \n\n### [Daniel E. Ho] \n\n![James Landay] \n\n### [James Landay] \n\n![Curt Langlotz] \n\n### [Curt Langlotz] \n\n![Fei-Fei headshot photo] \n\n### [Fei-Fei Li] \n\n![Percy Liang] \n\n### [Percy Liang] \n\n![photo] \n\n### [Christopher Manning] \n\n![Peter Norvig] \n\n### [Peter Norvig] \n\n![photo] \n\n### [Rob Reich] \n\n![Vanessa Parli] \n\n### [Vanessa Parli] \n\n* * *\n\n## Contact\n\nStay up to date with Stanford HAI by [subscribing to our newsletter].",
    "length": 2148,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "University IT AI",
    "url": "https://uit.stanford.edu/ai",
    "text": "University IT AI | University IT[Skip to main content] \n[![Stanford] University IT] Main Menu\nTopic Menu\n# University IT AI\nUniversity IT services and training to support AI innovation at Stanford\n* [What is Generative AI?] \n* [Using AI responsibly] \n* [Learning to leverage AI in your work] \n* [Enhancing services with AI] \n* [Empowering Stanford with Impactful Solutions] \n## What is Generative AI?\nWhile artificial intelligence (AI) has existed in various forms for many years, the latest advancements in generative AI (GenAI) capabilities present significant potential for meaningful real-world impact. With access to unprecedented computing power, these technologies stand poised to reshape our approach to teaching and learning, enhance research endeavors, and optimize business operations.\n[Learn moreabout GenAI] \n### **\nAI Index\nFind the pioneering generative AI initiatives at our university and medical school.\n[View GenAI Efforts at Stanford »] [View GenAI Topics and Services List »] \n### **\nExperiment with GenAI\nA great way to learn about AI is through hands-on practice. Experiment with AI, responsibly.\n[Learn about experimenting with GenAI »] [Discover how to generate better prompts »] \n### **\nAI Playground\nThe Stanford AI Playground is a user-friendly platform, built on open-source technologies, that allows you to safely try various AI models from vendors like OpenAI, Google, and Anthropic in one spot.\n[Try out the AI Playground »] \n## Using AI responsibly\nGenerative artificial intelligence (AI) is built using algorithms that can generate text, images, videos, audio, and 3D models in response to prompts. Popular examples of generative AI include ChatGPT and Google Gemini.\nThis type of AI can introduce both positive and negative impacts.\n[Learn moreabout Responsible AI at Stanford] \n### **\nAI Catalyst\nOur AI Catalyst service offers campus units an opportunity to collaborate with implementation experts to meet custom AI needs for our campus community.\n[Learn moreabout AI Catalyst»] \n### **\nTools being explored\nView a list of generative AI tools being evaluated by University IT (UIT) for potential implementation in various contexts, according to the needs of the Stanford community.\n[GenAI Tool Evaluation Matrix »] \n## Learning to leverage AI in your work\nChoose from various technology sessions to support your personal and professional development. All sessions are eligible for STAP funds and Healthcare Education Assistance.\n### **\nUpcoming AI Sessions\nTechnology training or information sessions for personal and professional development, open to the Stanford community and the general public.\n[Learn more »] \n### **\nAI Demystified\nLearn about AI, prompt engineering, large language models (LLMs), and more.\n[Learn more »] \n## Enhancing services with AI\n### **\nAI API Gateway\nGet API access to the models within the AI Playground.\n[Learn moreabout the AI API Gateway»] \n### **\nSlack AI\n[Stanford Slack] is now smarter and more efficient. Two new AI-driven features help you organize your conversations, capture meeting highlights, and find the information you need, when you need it.\n[Learn more »] \n### **\nZoom AI Companion\nZoom's AI Companion introduces a range of features designed to boost productivity and accessibility in meetings and communications.\n[Learn more »] \n### **\nGoogle AI\nTry one of these options from Google, available at no cost for Stanford faculty, students, and staff with SUNet IDs.\n[Google Gemini (free) »] [NotebookLM (free) »] \n### **\nMicrosoft AI\nChoose which option fits your needs with these Microsoft services.\n[Microsoft Copilot Chat (free) »] [Github Copilot (purchase) »] [Microsoft 365 Copilot (purchase) »] \n### **\nMore AI Services\nCheck the “Stanford Availability” column in our GenAI Tool list.\n[GenAI Tools »] \n## Empowering Stanford with impactful solutions\nUIT supports and empowers the Stanford community with impactful solutions, in keeping with our[UIT Priority Themes] and[Strategic Values].\n### Russ Altman - Conversations with Extraordinary Leaders\n[![Embedded YouTube video]] \nA recent UIT Town Hall featured Professor Russ Altman, Kenneth Fong Professor of Bioengineering, Genetics, Medicine, Biomedical Data Science and (by courtesy) of Computer Science, and Senior Fellow at the Stanford Institute for HAI. He shared how AI is making an impact across Stanford, and how UIT is playing a critical role in its success.\n[![Stanford University]] \n* [Stanford Home] \n* [Maps & Directions] \n* [Search Stanford] \n* [Emergency Info] \n* [Terms of Use] \n* [Privacy] \n* [Copyright] \n* [Trademarks] \n* [Non-Discrimination] \n* [Accessibility] \n©CopyrightStanford University.Stanford,California94305.",
    "length": 4678,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Generative AI: Perspectives from Stanford HAI",
    "url": "https://hai.stanford.edu/generative-ai-perspectives-stanford-hai?sf175669088=1",
    "text": "[Skip to main content] [Skip to secondary navigation] \n\n[Stanford University(link is external)] \n\nPage Content\n\n# Generative AI: Perspectives from Stanford HAI\n\n**March 2023**\n\n[![Generative AI Document]] The current wave of generative AI is a subset of artificial intelligence that, based on a textual prompt, generates novel content. ChatGPT might write an essay, Midjourney could create beautiful illustrations, or MusicLM could compose a jingle. Most modern generative AI is powered by foundation models, or AI models trained on broad data using self-supervision at scale, then adapted to a wide range of downstream tasks.\n\nThe opportunities these models present for our lives, our communities, and our society are vast, as are the risks they pose. While on the one hand, they may seamlessly complement human labor, making us more productive and creative, on the other, they could amplify the bias we already experience or undermine our trust of information.\n\nWe believe that interdisciplinary collaboration is essential in ensuring these technologies benefit us all. The following are perspectives from Stanford leaders in medicine, science, engineering, humanities, and the social sciences on how generative AI might affect their fields and our world. Some study the impact of technology on society, others study how to best apply these technologies to advance their field, and others have developed the technical principles of the algorithms that underlie foundation models.\n\n[Read the full perspective on Generative AI] \n\n## Authors\n\n![Russ headshot photo] \n\n### [Russ Altman] \n\n![Erik Brynjolfsson] \n\n### [Erik Brynjolfsson] \n\n![photo] \n\n### [Michele Elam] \n\n![Surya Ganguli] \n\n### [Surya Ganguli] \n\n![Daniel E. Ho] \n\n### [Daniel E. Ho] \n\n![James Landay] \n\n### [James Landay] \n\n![Curt Langlotz] \n\n### [Curt Langlotz] \n\n![Fei-Fei headshot photo] \n\n### [Fei-Fei Li] \n\n![Percy Liang] \n\n### [Percy Liang] \n\n![photo] \n\n### [Christopher Manning] \n\n![Peter Norvig] \n\n### [Peter Norvig] \n\n![photo] \n\n### [Rob Reich] \n\n![Vanessa Parli] \n\n### [Vanessa Parli] \n\n* * *\n\n## Contact\n\nStay up to date with Stanford HAI by [subscribing to our newsletter].",
    "length": 2148,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "AI Meets Education at Stanford",
    "url": "https://ctl.stanford.edu/aimes",
    "text": "AI Meets Education at Stanford | Center for Teaching and Learning\n[Skip to main content] [Skip to secondary navigation] \n[Stanford University(link is external)] \n[\nStanford\nCenter forTeaching and Learning\n] \nSearch this siteSubmit Search\n![AIMES, AI Meets Education at Stanford] \n# AI Meets Education at Stanford\n## Main navigation\n[Skip Secondary Navigation] \nMain content start\nAI Meets Education at Stanford (AIMES) is a VPUE effort to catalyze and support critical engagement with generative AI in Stanford teaching and learning contexts, coordinated by the Center for Teaching and Learning.\nAIMES fosters discussions among faculty, instructors, and students, and lower barriers for Stanford educators to engage productively and teach effectively in a world where generative AI appears to be here to stay. This effort is focused specifically on teaching and learning at Stanford University, complementing projects that advance AI research and policy in the broader world by bringing insights home to Stanford teaching and learning.\nAs a collection of activities and resources, AIMES sparks engagement fueled by the kinds of deep critical thinking and discourse that are hallmarks of a Stanford education. As the homophone “aims” suggests, AIMES takes academic goals as the starting point for considering, rejecting, integrating, or leveraging generative AI affordances in Stanford teaching and learning. The purpose of AIMES is to amplify a range of approaches to AI in undergraduate education and how they function ethically, pedagogically, practically, ecologically, and disciplinarily within the university context.\n![] \n## AIMES Library of Examples\nExamples of how Stanford instructors have responded to generative AI in their classes, including a range of teaching artifacts. These examples are meant to spark ideas and showcase a variety of approaches to teaching in a university setting where generative AI exists.\n[Explore the AIMES Library] \n![] \n## AI Teaching Strategies\nAn overview of strategies that may be helpful when assigning, limiting, and prohibiting AI use.\n[Discover AI Teaching Strategies] \n![four connected spheres under the heading AI literacy framework containing text headings that read &quot;functional&quot;, &quot;ethical&quot;, &quot;rhetorical&quot; and &quot;pedagogical&quot;; underneath is a line of text reading &quot;Human-centered values: why do we engage with AI?&quot;] \n## Teaching Commons Artificial Intelligence Teaching Guide\nLearn how to positively influence the dialogue around AI in education, in your own courses and at Stanford. Modules cover AI literacy, pedagogical uses, implications for your course, creating your course policy, and integrating AI into assignments, as well as a DIY workshop kit.\n[Explore the AI Teaching Guide] \n![] \n## Critical AI Literacy for Instructors Canvas Resource\nThis self-paced resource is for instructors who are new to generative AI (genAI) technology. It provides the foundational knowledge and skills to begin critically and effectively navigating genAI in teaching and learning contexts.\n[Learn more and enroll in Critical AI Literacy for Instructors] \n![] \n## AI and Your Learning: A Guide for Students\nThis guide from CTL provides information and guidelines to help students make informed decisions about navigating AI tools.\n[AI and Your Learning] \n![] \n## Academic Technology Tools List\nThe new Academic Technology Tools List is an interactive guide to Stanford supported academic technologies. You can browse or filter the list by integration level, tool type, and**AI capabilities**.\n[Browse the Academic Technology Tools List] \n![] \n## Share Input: Thoughtful Approaches to AI in Education\nShare examples of how Stanford instructors are engaging with generative AI in their course. Whether it's a creative use of AI tools, strategies to help students navigate AI, or methods that encourage students to think critically about when and why not to use AI, we want to hear it. Your input will help CTL facilitate community discussions and share diverse practices and perspectives.\n[Share Thoughtful Approaches to AI] \n![a group of pf three at a table looking thoughtfully at and discussing papers and notes] \n## Request a Consultation or Workshop\nFor one-to-one advice on creating your course AI policy or revising assignments in light of generative AI, to try out generative AI approaches at the CTL Academic Technology Solutions Lab, or to request a customized workshop on generative AI in your department or academic community at Stanford, contact us.\n[Request a CTL Consultation or Workshop] \n![View of the front of Stanford Campus] \n## Share Feedback on AIMES\nWe welcome your feedback on all aspects of AI Meets Education at Stanford. Please share your input via this anonymous[feedback form], or email AIMES co-leads,[Michele Elam], Senior Associate Vice Provost for UndergraduateEducation, WilliamRobertson Coe Professor of Humanities and Senior Fellow at the Institute for Human-Centered AI, and[Cassandra Volpe Horii], Associate Vice Provost for Education and Director, CTL\n[Share Feedback on AIMES] \n## **AIMES Approaches**\n* Enact proposed principles from the[January 9, 2025 Report of the AI at Stanford Advisory Committee] (AISAC):\n* human oversight\n* human alignment\n* human professionalism\n* ethical and safe use\n* privacy, security, and confidentiality\n* data quality and control\n* AI golden rule.\n* Build coherent and reusable resources when investing in the creation of new resources. Make them as evergreen and reusable as possible, while planning ahead for sustainable refresh cycles in this rapidly developing domain.\n* Emphasize a campus culture of reflection, curiosity, and experimentation to promote conversation and experimentation with AI, within boundaries that are openly discussed.## AIMES Goals\n* Address education-specific needs and recommendations from the[AISAC 2025 report], such as developing and sharing “frameworks and worked-out examples to help instructors think through… aspects of pedagogy impacted by AI” and facilitating “setting[s] where community members can experiment with AI tools.”\n* Support Stanford academic communities to more readily discover, create, and share discipline-specific AI approaches.\n* Integrate AI approaches with known evidence and with the university’s core mission related to teaching and learning: for example, metacognition, scaffolding, transparency, purposes of a liberal education, preparation for citizenship and discovery.\n* Connect across initiatives and groups working on AI and AI-adjacent issues in teaching and learning at Stanford.## Coming Soon\nAIMES team members at CTL, along with partners in VPUE, schools, and departments, are preparing the following new resources and opportunities:\n* Discipline-based and departmental community conversations sharing and discussing specific examples and practices within relevant shared educational contexts\n* Forums for discussion about generative AI across instructor, TA, and student roles## Additional Stanford Resources\nThe AIMES team in CTL and VPUE is committed to coordinating and collaborating with colleagues across the university, each working on distinct and connected aspects of generative AI. This network of experts and endeavors provides a resource-rich environment for Stanford instructors, researchers, and learners, including the following:\n* University IT provides resources on[Responsible AI at Stanford] and the[Stanford AI Playground]. The AI Playground is a user-friendly platform, built on open-source technologies, that allows Stanford faculty, staff, students, postdocs, and visiting scholars to safely try various AI models.\n* The Office of Community Standards provides[guidance on generative AI] with respect to the Honor Code.\n* The AI at Stanford Advisory Committee published a[report on January 9, 2025].\n* The[Stanford Accelerator for Learning] addresses research and policy on various facets of digital learning, including generative AI, and hosts the[AI Tinkery], “a collaborative space for educators to learn and make with generative AI.”\n* The[Stanford Institute for Human-Centered Artificial Intelligence (HAI)] is an interdisciplinary institute that advances AI research, education, policy, and practice.## Questions?\nFor questions and suggestions related to AIMES, please contact co-leads:\n* Cassandra Volpe Horii, associate vice provost for education and director of the Center for Teaching and Learning, at[cvhorii@stanford.edu] \n* Michele Elam, senior associate vice provost for undergraduate education, William Robertson Coe Professor of Humanities, and senior fellow at the Institute for Human-Centered AI\nBack to Top",
    "length": 8679,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Generative AI: Perspectives from Stanford HAI",
    "url": "https://hai.stanford.edu/generative-ai-perspectives-stanford-hai?sf176265430=1",
    "text": "[Skip to main content] [Skip to secondary navigation] \n\n[Stanford University(link is external)] \n\nPage Content\n\n# Generative AI: Perspectives from Stanford HAI\n\n**March 2023**\n\n[![Generative AI Document]] The current wave of generative AI is a subset of artificial intelligence that, based on a textual prompt, generates novel content. ChatGPT might write an essay, Midjourney could create beautiful illustrations, or MusicLM could compose a jingle. Most modern generative AI is powered by foundation models, or AI models trained on broad data using self-supervision at scale, then adapted to a wide range of downstream tasks.\n\nThe opportunities these models present for our lives, our communities, and our society are vast, as are the risks they pose. While on the one hand, they may seamlessly complement human labor, making us more productive and creative, on the other, they could amplify the bias we already experience or undermine our trust of information.\n\nWe believe that interdisciplinary collaboration is essential in ensuring these technologies benefit us all. The following are perspectives from Stanford leaders in medicine, science, engineering, humanities, and the social sciences on how generative AI might affect their fields and our world. Some study the impact of technology on society, others study how to best apply these technologies to advance their field, and others have developed the technical principles of the algorithms that underlie foundation models.\n\n[Read the full perspective on Generative AI] \n\n## Authors\n\n![Russ headshot photo] \n\n### [Russ Altman] \n\n![Erik Brynjolfsson] \n\n### [Erik Brynjolfsson] \n\n![photo] \n\n### [Michele Elam] \n\n![Surya Ganguli] \n\n### [Surya Ganguli] \n\n![Daniel E. Ho] \n\n### [Daniel E. Ho] \n\n![James Landay] \n\n### [James Landay] \n\n![Curt Langlotz] \n\n### [Curt Langlotz] \n\n![Fei-Fei headshot photo] \n\n### [Fei-Fei Li] \n\n![Percy Liang] \n\n### [Percy Liang] \n\n![photo] \n\n### [Christopher Manning] \n\n![Peter Norvig] \n\n### [Peter Norvig] \n\n![photo] \n\n### [Rob Reich] \n\n![Vanessa Parli] \n\n### [Vanessa Parli] \n\n* * *\n\n## Contact\n\nStay up to date with Stanford HAI by [subscribing to our newsletter].",
    "length": 2148,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "The Starling Lab for Data Integrity",
    "url": "https://democracy.stanford.edu/starling-lab-data-integrity",
    "text": "The Starling Lab for Data Integrity\n[\n![logo] \n] \n![] \n![] \n![] \nThe Starling Lab for Data Integrity prototypes tools and principles to bring historians, legal experts and journalists into the new era of Web3.\n## [Watch: Our Framework] \n## Exhibition\nTo Trust or Not to Trust:\nDesigning for Authenticity\n&#8211;\nCecil H. Green Library, Stanford University\n[Visit In VR] \n![] \n## A new methodology to restore trust in digital media\nThe Starling Lab is an academic research lab innovating with the latest cryptographic methods and decentralized web protocols to**meet the technical and ethical challenges of establishing trust in our most sensitive digital records**.\nStarling uses open-source tools, best practices, and case studies to securely capture, store and verify digital content.**With applications across news media, historical preservation and legal accountability**, the potential use cases for the Starling Framework are numerous.\n## OUR AREAS OF PRACTICE\n![] \n## Journalism\n## The Starling Lab Journalism Fellowship is harnessing the power of cutting-edge authentication technology and open source tools to start a new conversation between journalists and technologists.\n[Learn More] \n## History\n## Preserving digital records with cryptography to ensure important stories are never lost to time or manipulation.\n## Law\n## Exploring how authenticated metadata can create secure and robust chains of evidence.\n## Featured Case Studies\n[![]] The Link Rot Rescue Project\n[![]] Documenting Stockton's Homeless\n[![]] New Evidence Techniques Document Bombed Ukrainian Schools For ICC\n[![]] Setting the Record Straight in Brazil’s Burning Wetlands\n## Our Framework: Capture, Store &amp; Verify\n## We established The Starling Framework to address new challenges to data integrity as we enter into a new era of generative AI.\nMore data is generated today than at any time in recorded history. More people are able to access the internet. Humanity is empowered as information democratized, but at the same time society must confront rising authenticity issues.\nIn its simplest form, this framework is:\n![] \n## CAPTURE\n## Starling prototypes mobile apps and camera firmware to authenticate digital content and metadata at the point of capture.\n![] \n## STORE\n## Starling researches how advanced cryptography and decentralized networks can securely distribute and govern content over time.\n![] \n## VERIFY\n## Starling experiments with immutable ledgers to register digital content, enabling experts to audit, or verify, the provenance and authenticity of that content.\nOur methodology provides a set oftechniques and guidelines that tracksalong the lifecycle of any piece of digitalinformation. Front and center is strongly-authenticated data –data which containsverifiable markers of its provenance –andcarefully designed measures forpreservation.\n## Co-Founders of The Starling Lab\n![] \nThe Department of Electrical Engineering (EE) at Stanford innovates by conducting fundamental and applied research to develop physical technologies, hardware and software systems, and information technologies. Throughout its 125-year history, EE at Stanford has supported innovation and entrepreneurship that helped build Silicon Valley, from the invention of microprocessors to public-key cryptography to wireless technologies. EE’s faculty and students continue to advance the state-of-the-art, define new directions for electrical engineering, and address critical societal challenges.\n![] \nThe Institute for Visual History and Education develops empathy, understanding and respect through testimony, using its Visual History Archive of more than 55,000 video testimonies, academic programs and partnerships across USC and 170 universities. USC Shoah Foundation’s award-winning, interactive IWitness education program, research and materials are accessed in museums and universities, cited by government leaders and NGOs, and taught in classrooms around the world. Now in its third decade, USC Shoah Foundation reaches millions of people on six continents from its home at the University of Southern California.\n## Journalism Projects\n## [The DJ and the War Crimes] \n## [ Rolling Stone] \n## [Documenting Stockton’s Homeless] \n## [ Bay City News] \n## [In Brazil, the World’s Largest Tropical Wetland Has Been Overwhelmed With Unprecedented Fires and Clouds of Propaganda] \n## [Inside Climate News] \n## [Police Seize on COVID-19 Tech to Expand Global Surveillance] \n## [ Associated Press] \n## Law Projects\n## [Empowering Human Rights Defenders in Remote Areas] \n## [\nStarling Lab Dispatch] \n## [Authenticity In VR: Photogrammetry From Project Dokaz] \n## [\nAWE XR Panel] \n## [A Crypto-Based Dossier Could Help Prove Russia Committed War Crimes] \n## [\nCNN] \n## [Trustless Evidence: Web 3 Is Helping Document War Crimes in Ukraine] \n## [ Jonathan Dotan/CoinDesk] \n## History Projects\n## Mom, I See War\n## Groundstorm Media\n## [Inside Starling Lab, a Moonshot Project to Preserve the World's Most Important Information] \n## [ FastCompany] \n## [USC and Stanford Launch Starling Lab to Protect Human Rights with Decentralization] \n## [ VentureBeat] \n## Starling Lab In The News\n## [Photos Are Disappearing One Archive At A Time] \n## [\nKira Pollack/Washington Post] \n## [As websites disappear, link rot threatens journalism. One Stanford fellow is working on a fix] \n## [\nPoynter] \n## [Journalists fight digital decay] \n## [Basile Simon/Nieman Lab] \n## [Green Library exhibit emphasizes the importance of transparency in the digital age] \n## [\nStanford Daily] \n## [Documenting War in Ukraine: Trustless Evidence] \n## [ Jonathan Dotan/Davos Promenade] \n## In Ukraine, War Crimes Go on-Chain\n## Politico\n## Innovative Uses of Data\n## CBC's Spark with Nora Young (begins 27:17)\n## The Authentication of a Photograph with Ron Haviv\n## [ Rolling Stone] \n## Preserving Trust in Photojournalism Through Authentication Technology\n## Rueters\n## New ZK Use Cases with Dan Boneh\n## Zero Knowledge Podcast\n## The Dangerous Rise of Alternative Facts\n## Jonathan Dotan &amp; Cheryl Phillips\n## Social Media's Selective Historical Memory is a Human Rights Issue\n## Jonathan Dotan/The Independent\n## How Cryptography and Web3 Can Help Restore Trust in Digital Media\n## Stanford Engineering\n## Information, Theory, Power and Competition\n## Stanford Research Talks\n## Restoring Trust in our Digital Age with Compression\n## Stanford Research Talks\n## The Crucial Role of Data Compression\n## Stanford Engineering\n## Videos\n## The Authentication of a Photograph with Ron Haviv\n## Rolling Stone\n## Starling Case Study: Linking Protocols\n## Starling Lab\n## The Crucial Role of Data Compression\n## Stanford Engineering\n## [ Scripps Howard Awards] \n## [Excellence in Innovation Nomination: \"The DJ and the war Crimes\"] \n## New Models of Trust on the Frontiers of Authenticity\n## Jonathan Dotan, Web3 Summit\n## Special Thanks\n*Starling Lab’s work is made possible with generous support from:*\n* Filecoin Foundation for the Decentralized Web\n* Protocol Labs\n* Hedera\n* Ampathy / Ampathy Foundation\n* SDG Impact Fund\nWe use cookies from third party services to offer you a better experience. Read about how we use cookies and how you can control them by clicking \"Privacy Preferences.\"\nPrivacy PreferencesI Agree\n### Privacy Preference Center\n#### Privacy Preferences\nWhen you visit any website, it may store or retrieve information through your browser, usually in the form of cookies. Since we respect your right to privacy, you can choose not to permit data collection from certain types of services. However, not allowing these services may impact your experience and what we are able to offer you.\nPrivacy Policy\nRequired\nYou agree to our[Privacy Policy] \nYouTube\nWe use YouTube Videos to enable video content on our site.\nGoogle Fonts\nWe use Google Fonts to enhance the design of our site.",
    "length": 7847,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Strategies for Prohibiting AI Use",
    "url": "https://ctl.stanford.edu/strategies-prohibiting-ai-use",
    "text": "Strategies for Prohibiting AI Use | Center for Teaching and Learning\n[Skip to main content] [Skip to secondary navigation] \n[Stanford University(link is external)] \n[\nStanford\nCenter forTeaching and Learning\n] \nSearch this siteSubmit Search\n![AI-Prohibited] \n# Strategies for Prohibiting AI Use\nMain content start\nWhile the[AIMES Library of Examples] allows you to filter and explore real course policies, assignments, and other teaching artifacts that prohibit AI use, this page provides a brief overview of strategies. AI use in higher education is a rapidly evolving domain with new research emerging frequently. We do not yet have an extensive body of evidence to draw from, but the strategies included here are consistent with broader educational research findings, as well as having been found useful by instructors at Stanford. Please consider them “emerging practices” rather than “best practices.”\n## General Approaches to Prohibiting AI Use\n### Increase transparency and motivation:\nShare with students why they should not use AI in this course or assignment. What skills do you want them to develop on their own? How will those skills serve them in their studies, life, and work? Why does it matter that students develop and share their own ideas, reasoning, and points of view in this class, subject, and discipline?\nRevisit these reasons throughout the course–for example, in the context of specific course learning goals, class activities, assignments, and assessments.\nNormalize cognitive effort and struggle, which are crucial for learning, but can be shortchanged by use of AI. Discuss what it is like to be unsure, confused, and to grapple with concepts in this course, and why those challenging experiences are important for learning.\nEmphasize students’ progress on non-AI assisted thinking and skills through individual feedback and discussion with the whole class.\n### Emphasize alignment and process:\nEnsure that course policies are clear and aligned with the goals and reasons you have for students to avoid AI use.\nProvide a “safety valve” for students to admit to making mistakes in following the AI policy without it being catastrophic to their grade. For example, some instructors ask students to disclose AI use on every assignment, even when it is prohibited, and then discuss what went wrong or led to the inappropriate use of AI, in order to support students in planning a revised approach next time. You could limit the number of times students may use such a safety valve.\nRequire that students show their processes of doing the work of the course (e.g., problem solving process, writing process, analysis process, coding process, reasoning process) as components of assignments.\nFor major assignments such as projects, long papers, and capstones, include several stages that provide scaffolding and feedback on the way to the completed product. Feedback and discussion along the way can increase students’ engagement and accountability for the development of their own ideas, as well as lead to strong final products without AI assistance. Logs or journals that accompany major projects and are reviewed with the instructor or TA periodically can also help.\n### Model ways of working without AI:\nBring some of the important intellectual and academic work into class, where you can provide guidance and modeling. For example, have students generate and refine ideas through short, individual writing and discussion with peers and instructors in class. Doing so may help students feel more ownership and be less likely to turn to AI for when they are working on their own outside of class.\nBecome familiar with where and how students may inadvertently encounter AI assistance built into software, searches, and other platforms they use for coursework. Demonstrate and discuss the drawbacks of relying on AI assistance that is prohibited in your class and advise students about how to disable AI in the applications they use. Seek support from university technology offices to learn more about AI in campus applications.\nRecognize that incoming students may be accustomed to relying on AI assistance. If students have offloaded crucial skills and habits of mind to AI in prior learning environments, modeling and guidance may be necessary.\n### Design assignments and assessments that demonstrate learning, not AI outputs:\nFollow up on assignments completed outside of class, e.g., by asking students to explain their reasoning, discuss an example from their work, or explain a technique they used. If students do not seem to understand their work, they may need to redo the assignment with your guidance on how to develop a deeper understanding, whether or not they used AI assistance.\nEspecially on lower-stakes assignments or those leading up to longer projects and papers, provide feedback that focuses on progress toward learning goals and how to improve, rather than only on getting the right answer.\nConsider conducting assessments in class (e.g., blue books or other in-class exam formats). As of fall 2025, proctored exams are only allowed as part of the[Academic Integrity Working Group Proctoring Pilot], but additional courses may request to join the pilot each quarter.\nHelp students prepare for in-class and/or proctored assessments. Remind them of the conditions they will encounter and encourage them to practice under similar conditions before the exam. Scaffold learning experiences leading up to timed, in-class writing so that a higher-stakes assessment is not the first time students experience the format.\n## Further Reading\nThe following publications highlight recent discussion about prohibiting AI use in higher education courses. This list is not a comprehensive bibliography.\n* Bertram Gallant, T., &amp; Rettinger, D. A. (2025).*The Opposite of Cheating: Teaching for Integrity in the Age of AI.*University of Oklahoma Press.[Stanford University Libraries Catalog] (login required).\n* Fritts, M. (2025, May 23). What I Learned Serving on My University’s AI Committee: We need to embrace a more radical response.*Chronicle of Higher Education*.[www.chronicle.com/article/what-i-learned-serving-on-my-universitys-ai-committee] \n* Hsu, H. (2025). The End of the Essay.*New Yorker, 101(19)*, 21-27.[Stanford University Libraries full text] (login required).\n* O’Rourke, M. (2025, July 18). I Teach Creative Writing. This is What A.I. is Doing to Students.*New York Times.*Stanford access:[guides.library.stanford.edu/newspapers].\n* Petrocelli, J. V. (2025). Return of the Blue Books: Grading in the Time of Artificial Intelligence.*Change: The Magazine of Higher Learning, 57(4)*, 25–28.[doi.org/10.1080/00091383.2025.2511577] ## **Last updated**\n**Date**\nBack to Top",
    "length": 6722,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "",
    "url": "https://hai.stanford.edu/assets/files/2022-01/HAI_NRCR_v17.pdf",
    "text": "Building a \nNational \nAI Research \nResource: \nA Blueprint for \nthe National \nResearch Cloud\nDaniel E. Ho\nJennifer King\nRussell C. Wald\nChristopher Wan\nWHITE PAPER\nOCTOBER 2021\nPrincipal Authors \nDaniel E. Ho, J.D., Ph.D., is the William Benjamin Scott and Luna M. Scott Professor of Law, \nProfessor of Political Science, and Senior Fellow at the Stanford Institute for Economic Policy \nResearch at Stanford University. He directs the Regulation, Evaluation, and Governance Lab \n(RegLab) at Stanford, and is a Faculty Fellow at the Center for Advanced Study in the Behavioral \nSciences and Associate Director of the Stanford Institute for Human-Centered Artificial \nIntelligence (HAI). He received his J.D. from Yale Law School and Ph.D. from Harvard University \nand clerked for Judge Stephen F. Williams on the U.S. Court of Appeals for the District of \nColumbia Circuit. \nJennifer King, Ph.D., is the Privacy and Data Policy Fellow at the Stanford HAI. She completed \nher doctorate in information management and systems (information science) at the University of \nCalifornia, Berkeley School of Information. Prior to joining HAI, she was the Director of Consumer \nPrivacy at the Center for Internet and Society at Stanford Law School from 2018 to 2020.\nRussell C. Wald is the Director of Policy for the Stanford HAI, leading the team that advances \nHAI’s engagement with governments and civil society organizations. Since 2013, he has held \nvarious government affair roles representing Stanford University. He is a Term Member with the \nCouncil on Foreign Relations, Visiting Fellow with the National Security Institute at George Mason \nUniversity, and a Partner with the Truman National Security Project. He is a graduate of UCLA.\nChristopher Wan is a JD/MBA candidate at Stanford University and was teaching assistant for \nthe Stanford Policy Practicum: Creating a National Research Cloud. He also serves as a research \nassistant for the Stanford HAI and as an investor at Bessemer Venture Partners. He received his \nB.S. in computer science from Yale University and worked as a software engineer at Facebook and \nas a venture investor at In-Q-Tel and Tusk Ventures.\nThe Stanford Institute for Human-Centered Artificial Intelligence\nCordura Hall, 210 Panama Street, Stanford, CA 94305-4101\nOctober 2021, V1.0\n2\nContributors \nMany dedicated individuals contributed to this White Paper. To acknowledge these contributions, \nwe list here the contributors for each chapter and section. \nExecutive Summary and Introduction \nDaniel E. Ho, Tina Huang, Jennifer King, Marisa Lowe, Diego Núñez, Russell Wald, Christopher Wan, \nDaniel Zhang \n \nThe Theory for a National Research Cloud \nNathan Calvin, Shushman Choudhury, Tina Huang, Daniel E. Ho, Kanishka Narayan, Diego Núñez, \nFrieda Rong, Russell Wald, Christopher Wan \nEligibility, Allocation, and Infrastructure for Computing\nDaniel E. Ho, Krithika Iyer, Tyler Robbins, Jasmine Shao, Russell Wald, Daniel Zhang \nSecuring Data Access\nNathan Calvin, Shushman Choudhury, Daniel E. Ho, Ananya Karthik, Jennifer King, Christopher Wan \nOrganizational Design \nSabina Beleuz, Drew Edwards, Daniel E. Ho, Jennifer King, Christopher Wan \nData Privacy Compliance \nSimran Arora, Neel Guha, Jennifer King, Sahaana Suri, Christopher Wan, Sadiki Wiltshire \nTechnical Privacy and Virtual Data Safe Rooms\nNeel Guha, Jennifer King, Christopher Wan \nSafeguards for Ethical Research \nDaniel E. Ho, Jennifer King, Diego Núñez, Russell Wald, Daniel Zhang \nManaging Cybersecurity Risks \nNeel Guha, Diego Núñez, Frieda Rong, Russell Wald \nIntellectual Property \nSabina Beleuz, Daniel E. Ho, Ananya Karthik, Diego Núñez, Christopher Wan \nCase Studies \nDaniel E. Ho, Krithika Iyer, Jennifer King, Marisa Lowe, Kanishka Narayan, Tyler Robbins \nWe also would like to thank Jeanina Casusi, Celia Clark, Shana Lynch, Kaci Peel, Stacy Peña, \nMike Sellitto, Eun Sze, and Michi Turner for their help in preparing this White Paper. \n3\nExternal Participants \nErik Brynjolfsson \nStanford University \n \nIsabella Chu \nPopulation Health \nSciences \nStanford University \n \nJack Clark \nAnthropic \nJohn Etchemendy \nStanford University \n \nFei-Fei Li \nStanford University \nMarc Groman \nGroman Consulting \nGroup LLC \n \nEric Horvitz \nMicrosoft \nSara Jordan \nThe Future of \nPrivacy Forum \nVince Kellen \nUC San Diego, \nCloudBank \n \nEd Lazowska \nUniversity of \nWashington \nNaomi Lefkovitz \nNational Institute of \nStandards and \nTechnology (NIST) \nBrenda Leong \nThe Future of \nPrivacy Forum \nAmy O’Hara \nGeorgetown Federal \nStatistical Research \nData Center \nGeorgetown University \n \nWade Shen \nActuate Innovation \n \nSuzanne Talon \nCompute Canada \n \nLee Tiedrich \nCovington & Burling LLP \n \nEvan White \nCalifornia Policy Lab \nUC Berkeley \nIn our process, we also engaged many civil society leaders and advocates who have expressed many \nperspectives about building a National Research Cloud. We have incorporated their feedback where \npossible and are grateful for their shared thoughts and for helping us shape a better White Paper.\nGuest Lecturers and Interviewees \nWe relied on extraordinary outside expert reviewers for feedback and guidance. We are grateful to \nLeisel Bogan, Jack Clark, John Etchemendy, Mark Krass, Marietje Schaake, and Christine Tsang for \ntheir thoughtful review of the full White Paper, and thank Isabella Chu, Kathleen Creel, Luciana Herman, \nSara Jordan, Vince Kellan, Brenda Leong, Ruth Marinshaw, Amy O’Hara, and Lisa Ouellette for their \nsubject expertise on specific chapters. \nReviewers \n4\nTaka Ariga \nGovernment Accountability \nOffice \nKathy Baxter \nSalesforce \nLeisel Bogan \nBelfer Center \nHarvard University \nJeffrey Brown \nIBM \nMiles Brundage \nOpen AI \nL. Jean Camp \nUniversity of Indiana \nat Bloomington \nDakota Cary \nCenter for Security and \nEmerging Technology\nGeorgetown University \nShikai Chern \nVeritas Technologies \nIsabella Chu \nPopulation Health Sciences \nStanford University \nJack Clark \nAnthropic \nMeaghan English \nPatrick J. McGovern \nFoundation \nCyrus Hodes \nThe Future Society \nSara Jordan \nThe Future of Privacy Forum \nVince Kellen \nUC San Diego, CloudBank \nMichael Kratsios \nScale AI \nSamantha Lai \nBrookings Institution \nBrenda Leong \nThe Future of Privacy Forum \nRuth Marinshaw \nStanford Research \nComputing Center \nJoshua Meltzer \nBrookings Institution \nSam Mulopulous \nU.S. Senate \nDewey Murdick \nCenter for Security and \nEmerging Technology \nGeorgetown University \nHodan Omaar \nCenter for Data Innovation \nCalton Pu \nGeorgia Tech \nAsad Ramzanali \nU.S. House of \nRepresentatives \nDavid Robinson \nUpturn \nSaiph Savage \nNortheastern University \nMichael Sellitto \nStanford HAI \nIshan Sharma \nFederation of \nAmerican Scientists \nBrittany Smith \nData and Society \nJohn Smith \nIBM \nBrittany Smith \nData and Society \nVictor Storchan \nJP Morgan Chase \nKeith Strier \nAI Compute Task \nForce Organisation \nfor Economic \nCo-operation and \nDevelopment (OECD) \nLee Tiedrich \nCovington & Burling LLP \n \nEvan White \nCalifornia Policy Lab \nUC Berkeley \nOn August 5, 2021, the co-authors hosted a feedback session to hear from a variety of stakeholders \nin academia, civil society, government, and industry. We are thankful for the time and helpful \nadvice participants offered. Workshop attendee affiliations are listed for identification purposes \nonly. Individuals from Microsoft and AI Now also attended the workshop but did not want to be \npersonally identified. \nWorkshop Participants \n5\nDisclosures\nStanford University actively engaged and lobbied Congress to pass the National Artificial Intelligence \nResearch Resource Task Force Act, working with a coalition of academic, civil society, and industry \nstakeholders. Co-author Russell Wald provided support to the advocacy efforts. \nHAI Co-Director Fei-Fei Li, who served as a guest lecturer in the class, was an early supporter of a \ntask force to study the National Research Cloud. Dr. Li has been appointed to serve as a member of \nthe National Artificial Intelligence Research Resource (NAIRR) Task Force. \nCo-author Daniel Ho directs the Stanford RegLab, which has received compute support from HAI’s \ncloud credit program (AWS and GCP), Microsoft’s AI for Earth Azure compute credit grant program, \nand Google’s Cloud credit grant for COVID-19 research. \nCo-author Jennifer King received unrestricted gift funding for research from Mozilla, Facebook, \nand Accenture in her previous role at the Center for Internet and Society. \nThe Stanford Institute for Human-Centered Artificial Intelligence (HAI) receives financial and cloud \ncomputing support from A121 Labs, Amazon Web Services, Google, IBM, Microsoft, and OpenAI. \nAbout HAI\nAbout the SLS Policy Lab\nStanford University’s Institute for Human-Centered Artificial Intelligence (HAI) applies rigorous \nanalysis and research to pressing policy questions on artificial intelligence. A pillar of HAI is to inform \npolicymakers, industry leaders, and civil society by disseminating scholarship to a wide audience. \nHAI is a nonpartisan research institute, representing a range of voices. The views expressed in this \nWhite Paper reflect the views of the authors. \nThe Policy Lab at Stanford Law School offers students an immersive experience in finding solutions \nto some of the world’s most pressing issues under the direction of Stanford faculty and researchers. \nDirected by former SLS Dean Paul Brest, the Policy Lab reflects the school’s belief that systematic \nexamination of societal problems, informed by rigorous research, can generate solutions to society’s \nmost challenging public problems.\nAcademic Independence\nThis White Paper was developed independently by the research team. While we solicited feedback from \na wide range of stakeholders, no HAI donors, corporations, or other stakeholders had any involvement \nwith the research and production of this White Paper. Per HAI policy, “Donors cannot dictate research \ntopics pursued by HAI researchers” nor “control permission to publish research results.” For more \ninformation, please see HAI’s policy: https://hai.stanford.edu/about/fundraising-policy. \n6\nBuilding a National AI Research Resource: \nA Blueprint for the National Research Cloud\n7\nTable of Contents\nEXECUTIVE SUMMARY: Creating a National Research Cloud 9\nINTRODUCTION 15\nCHAPTER 1: A Theory for a National Research Cloud 17\nCHAPTER 2: Eligibility, Allocation, and Infrastructure for Computing 22\nCHAPTER 3: Securing Data Access 35\nCHAPTER 4: Organizational Design 48\nCHAPTER 5: Data Privacy Compliance 53\nCHAPTER 6: Technical Privacy and Virtual Data Safe Rooms 61\nCHAPTER 7: Safeguards for Ethical Research 66\nCHAPTER 8: Managing Cybersecurity Risks 70\nCHAPTER 9: Intellectual Property 76\nGLOSSARY OF ACRONYMS 82\nAPPENDIX 84\nENDNOTES 90\nBuilding a National AI Research Resource: \nA Blueprint for the National Research Cloud\n8\nCOMPUTE MODELS\nNSF CloudBank 27\nNSF XSEDE 29\nFugaku 32\nCompute Canada 34\nDATA MODELS\nColeridge Initiative 42\nStanford Population Health Sciences 43\nThe Evidence Act 46\nORGANIZATIONAL MODELS\nScience and Technology Policy Institute 50\nAlberta Data Partnerships 51\nOTHER MODELS\nAdministrative Data Research UK 58\nCalifornia Policy Lab 64\nCase Studies\nBuilding a National AI Research Resource: \nA Blueprint for the National Research Cloud\n9\nArtificial intelligence (AI) appears poised to transform the economy across sectors ranging from healthcare and finance \nto retail and education. What some have coined the “Fourth Industrial Revolution”1 is driven by three key trends: greater \navailability of data, increases in computing power, and improvements to algorithm design. First, increasingly large amounts \nof data have fueled the ability for computers to learn, such as by training an algorithmic language model on all of Wikipedia.2\nSecond, better computational capacity (often termed “compute”) and compute capability have enabled researchers to build \nmodels that were unimaginable merely 10 years ago, sometimes spanning billions of parameters (an exponential increase \nin scope from previous models).3 Third, basic innovations in algorithms are helping scientists to drive forward AI, such as the \nreinforcement learning techniques that enabled a computer to defeat the world champion in the board game Go.4\nHistorically, partnerships between government(s), universities, and industries have anchored the U.S. innovation \necosystem. The federal government played a critical role in subsidizing basic research, enabling universities to undertake \nhigh-risk research that can take decades to commercialize. This approach catalyzed radar technology, the internet, and \nGPS devices. As the economists Ben Jones and Larry Summers put it, “[e]ven under very conservative assumptions, it is \ndifficult to find an average return below $4 per $1 spent” on innovation, and the social returns might be closer to $20 for \nevery dollar spent.5 Industry in turn, scales and commercializes applications. \nCHALLENGES TO THE AI INNOVATION ECOSYSTEM\nYet this innovation ecosystem faces serious potential challenges. Computing power has become critical for the \nadvancement of AI, but the high cost of compute has placed cutting-edge AI research in a position accessible only to key \nindustry players and a handful of elite universities.6 Access to data—the raw ingredients used to train most AI models—is \nincreasingly limited to the private sector and large platforms7, since government data sources remain largely inaccessible \nto the AI research community.8 As the National Security Commission on AI (NSCAI) has determined, “[t]he consolidation \nof the AI industry threatens U.S. technological competitiveness.”9\nFour interrelated challenges illustrate this finding: First, we are seeing a significant brain drain of researchers \ndeparting universities.10 In 2011, AI Ph.D.s were roughly as likely to go into industry as academia.11 Ten years later, two\u0002thirds of AI Ph.D.s go into industry, and less than one quarter go into academia.12 Second, these trends indicate that \nmany university researchers struggle to engage in cutting-edge science, draining the field of the diverse set of research \nvoices that it needs. Third, the fundamental research that would guarantee the United States stays at the helm of AI \ninnovation is being crowded out. By one estimate, 82 percent of algorithms used today originated from federally funded \nnonprofits and universities, but “U.S. leadership has faded in recent decades.”13 Fourth, government agencies have faced \nchallenges in building compute infrastructure,14 and there are societal benefits to reducing the cost of core governance \nfunctions and improving government’s internal capacity to develop, test, and hold AI systems accountable.15 In short, \na growing imbalance in AI innovation tilts toward industry, leaving academic and noncommercial research behind. \nGiven the long-standing role of academic and non-commercial research in innovation, this shift has substantial negative \nconsequences for the American research ecosystem. \nExecutive Summary: \nCreating a National Research Cloud\nBuilding a National AI Research Resource: \nA Blueprint for the National Research Cloud\n10\nTHE NATIONAL AI RESEARCH \nRESOURCE TASK FORCE ACT\nResponding to these challenges, Congress enacted \nthe National AI Research Resource Task Force Act as \npart of the National Defense Authorization Act (NDAA) \nin January 2021.16 The Act forms part of the National \nArtificial Intelligence Initiative, which identifies further \nsteps to increase research investments, set technical \nstandards, and build a stronger AI workforce. The Act \ncreated a Task Force—the composition of which was \nannounced on June 10, 202117—to study and plan for \nthe implementation of a “National Artificial Intelligence \nResearch Resource” (NAIRR), namely “a system that \nprovides researchers and students across scientific \nfields and disciplines with access to compute resources, co-located with publicly available, artificial intelligence-ready \ngovernment and non-government data sets.”18 This research resource has also been referred to as the National Research \nCloud (NRC) and was strongly endorsed by the NSCAI, which wrote that the NRC “will strengthen the foundation of \nAmerican AI innovation by supporting more equitable growth of the field, expanding AI expertise across the country, and \napplying AI to a broader range of fields.”19\nWhile other initiatives have sought to improve access to compute or data in isolation,20 the NRC will generate distinct \npositive externalities by integrating compute and data, the two bottlenecks for high-quality AI research. Specifically, the \nNRC will provide affordable access to high-end computational resources, large-scale government datasets in a secure \ncloud environment, and the necessary expertise to benefit from this resource through a close partnership between \nacademia, government, and industry. By expanding access to these critical resources in AI research, the NRC will support \nbasic scientific AI research, the democratization of AI innovation, and the promotion of U.S. leadership in AI. \nTHEMES\nStanford Law School’s Policy Lab program convened a multidisciplinary research team of graduate students, staff, \nand faculty drawn from Stanford’s business, law, and engineering schools to study the feasibility of and considerations for \ndesigning the NRC. Over the past six months, this group studied existing models for compute resources and government \ndata, interviewed a wide range of government, computer science, and policy experts, and examined the technical, business, \nlegal, and policy requirements. This White Paper was commissioned by Stanford’s Institute for Human-Centered Artificial \nIntelligence (HAI), which originated the proposal for the NRC in partnership with 21 other research universities.21\nThroughout our research, we observed three primary themes that cut across all areas of our investigation. We have \nintegrated these themes into each section of our White Paper and drawn on them to explain our findings. \n• Complementarity between compute and data. As we evaluated the existing computing and data-sharing ecosystems, \none of the systemic challenges we observed was a decoupling of compute resources from data infrastructures. \nThe NRC directs more resources \ntoward AI development in the public \ninterest and helps ensure long-term \nleadership by the United States in the \nfield by supporting the kind of pure, \nbasic research that the private sector \ncannot undertake alone.\nBuilding a National AI Research Resource: \nA Blueprint for the National Research Cloud\n11\nHigh-performance computing can be useless \nwithout data, and a major impediment to data \nsharing—particularly for high-value government \ndata—lies in requirements for a secure, privacy\u0002protecting computing environment. \n• Rebalancing AI research toward long-term, \nacademic, and noncommercial research. Presently, \nAI innovation is disproportionately dependent on \nthe private sector. Public investment in basic AI \ninfrastructure can both support innovation in the \npublic interest and complement private innovation \nefforts. The NRC directs more resources toward \nAI development in the public interest and helps \nensure long-term leadership by the United States in the field by supporting the kind of pure, basic research that the \nprivate sector cannot undertake alone.\n• Coordinating short-term and long-term approaches to creating the NRC. Our research considers many near-term \npathways for standing up a working version of the NRC by spelling out how to work within existing constraints. We \nalso identify the structural, legal, and policy challenges to be addressed in the long term for executing the full vision \nof the NRC. \nWe summarize our main recommendations here.\nCOMPUTE MODEL\n• The “Make or Buy” Decision. The main policy choice will be whether to build public computing infrastructure or \npurchase services from existing commercial cloud providers. \n° It is well-established that, based solely on hardware costs, it is more cost-effective to own infrastructure when \ncomputing demand is close to continuous.22 The government also has experience building high-performance \ncomputing clusters, typically built by contractors and operated by national laboratories.23 The National Science \nFoundation (NSF) has also supported many supercomputing initiatives at academic institutions.24\n° The main countervailing concerns are that existing commercial cloud providers have software stacks and usability \nthat AI researchers have widely adopted and may consider to be a more user-friendly platform. Commercial cloud \nproviders offer a way to expand capacity expeditiously, although scale and availability will still be constrained by \nthe availability of current graphics processing unit (GPU) computing resources. \n° We recommend a dual investment strategy: \n■ First, the compute model of the NRC can be quickly launched by subsidizing and negotiating cloud \ncomputing for AI researchers with existing vendors, expanding on existing initiatives like the NSF’s \nCloudBank project.25\n■ Second, the NRC should invest in a pilot for public infrastructure to assess the ability to provide similar \nresources in the long run. Such publicly owned infrastructure would still be built under contract or \nOne of the systemic challenges \n[to basic AI research is] a decoupling \nof compute resources from data \ninfrastructures. . . . [A] secure, \nprivacy-protecting computing \nenvironment [will be critical].\nBuilding a National AI Research Resource: \nA Blueprint for the National Research Cloud\n12\ngrant, but could be operated much like national laboratories (e.g., Sandia National Laboratories, \nOak Ridge National Laboratory) that own sophisticated supercomputing facilities or academic \nsupercomputing facilities. \n• Researcher Eligibility. While some have argued the NRC should be open for commercial access, for the purposes of \nthis White Paper, we adhered to the spirit of the legislation forming the NAIRR Task Force and only reviewed the use of \nan NRC for academic and nonprofit AI research. We recommend that the NRC eligibility start with academics who hold \n“Principal Investigator” (PI) status (i.e., most faculty) at U.S. colleges and universities, as well as “Affiliated Government \nAgencies” willing to contribute previously unreleased, high-value datasets to the NRC in return for subsidized compute \nresources. PI status should be interpreted expansively to encompass all fields of AI application. Students working with \nPIs should presumptively gain access to the NRC. Scaling the NRC to meet the demand of all students in the United \nStates may be challenging, but we also recommend the creation of educational programs as part of the new resource \nto help train the next generation of AI researchers. \n• Mechanism. In order to keep the award processing costs down, we recommend a base level of compute access to \nmeet the majority of researcher computing needs. Base-level access avoids high overhead for grant administration \nand may meet the compute demands for the supermajority of researchers. For researchers with exceptional needs, \nwe recommend a streamlined grant process for additional compute access. \nDATA ACCESS MODEL\n• Focus on Government Data. We focus our recommendations for data provision/access to government data \nbecause: (1) there are already a wide range of platforms for sharing private data,26 and (2) distribution by the NRC \nof private datasets would raise a tangle of thorny IP issues. We recommend that researchers be allowed to compute \non any datasets they themselves contribute, provided they certify they have the rights to that data, and the use of \nsuch data is for academic research purposes.\n• Tiered Access. We recommend a tiered access model: By default, researchers will gain access to government data \nthat is already public; researchers can then apply through a streamlined process to gain access at higher security \nlevels on a project-specific basis. It will be critical for the NRC to ultimately displace the current fragmented, agency\u0002by-agency relational approach. By providing secure virtual environments and harmonizing security standards (e.g., \nFederal Risk and Authorization Management Program (FedRAMP)27), the NRC can collaborate with proposals for a \nNational Secure Data Service28 to provide a model for accelerating AI research, while protecting data privacy and \nprioritizing data security.\n• Agency Incentives. To incentivize federal agencies to share data with the NRC and improve the state of public \nsector technology, we recommend the NRC permit federal agency staff to use the NRC’s compute resources. In \nkeeping with the practices of existing data-sharing programs, such as the Coleridge Initiative,29 we also recommend \nthat the NRC provide training and support to work with agencies to modernize and harmonize their data standards.\n• Strategic Investment for Data Sources. In the short term, we recommend that the NRC focus its efforts on making \navailable non-sensitive, low- to moderate-risk government datasets, rather than sensitive government data (e.g., \ndata about individuals) or data from the private sector, due to data privacy and intellectual property concerns. \nResearchers can still use NRC compute resources on private data but should rely on existing mechanisms to acquire \ndata for their own private buckets on the NRC. For example, images taken from Earth observation satellites, such as \nBuilding a National AI Research Resource: \nA Blueprint for the National Research Cloud\n13\nLandsat imagery, provide a promising low-risk, high-reward government dataset, as making such satellite imagery \nfreely available to researchers has generated an estimated $3-4 billion in annual economic benefits, particularly \nwhen combined with high-performance computing.30 Agencies such as the National Oceanic and Atmospheric \nAdministration, the U.S. Geological Survey, the Census Bureau, the Administrative Office of the U.S. Courts, and \nthe Bureau of Labor Statistics, for instance, also have rich datasets that can more readily be deployed. In the long \nrun, access to high-risk datasets, such as those owned by the Internal Revenue Service (IRS) and the Department of \nVeterans Affairs (VA), will depend on the tiered access model. \nORGANIZATIONAL FORM\nWhere to institutionally locate the NRC poses a tradeoff between ease of coordination to obtain compute and ease \nof data access. For instance, locating the NRC within a single agency would make coordination with compute providers \neasier, but would make data access across agencies more difficult, absent further statutory authority. Many efforts to \nmake data access to government data easier, most notably the Foundations for Evidence-Based Policymaking Act of \n2018, have proven to be among the most daunting challenges of government modernization.31 Building on those insights, \nwe ultimately recommend that the NRC be instituted as a Federally Funded Research and Development Center (FFRDC) \nin the short run, and a public-private partnership (PPP) in the long run.\n• FFRDC. FFRDCs at Affiliated Government Agencies would reduce the significant costs of securing data from those \nhost agencies. This approach will also cohere with the greater reliance on commercial cloud credits in the short run, \nmaking compute and data coordination less central. In the long run, however, streamlined coordination between \ndata and compute may be more difficult with FFRDCs hosted at specific agencies when (1) the NRC moves away \nfrom commercial cloud credits and toward its own high-performance computing cluster, and (2) a greater number \nof interagency datasets become available. \n• PPP. In the long run, we recommend the creation of a PPP model, governed by officers from Affiliated Government \nAgencies, academic researchers, and representatives from the technology sector, which can house both compute \nand data resources. \nADDITIONAL CONSIDERATIONS\n• Data Privacy. As an initial matter, an NRC where sensitive or individually identifiable administrative data from \nmultiple agencies are used to build and train AI models will face challenges from the Privacy Act of 1974.32 The Act is \nintended to put a check on interagency data-sharing and disclosure of sensitive data without consent. \n° In order to avoid conflicts with nonconsensual interagency data-sharing, we recommend that the NRC should \nnot be instituted as its own federal agency, nor should federal agency staff be allowed access to interagency \ndata. \n° To avoid conflicts with the Act’s “no disclosure without consent” requirement, any data released to the NRC \nmust not be individually identifiable. Despite these constraints, the majority of AI research will likely fall under \nthe Act’s statistical research exception, contingent on proposals aligning with an agency’s core purpose. \n° Given concerns about the potential privacy risks, federal agencies may desire to share data, contingent on \nthe use of technical privacy measures (e.g., differential privacy). While useful in many instances, technical \nBuilding a National AI Research Resource: \nA Blueprint for the National Research Cloud\n14\napproaches are no panacea and should not \nsubstitute for data access policies. \n° The NRC should explore the design of virtual \n“data safe rooms” that enable researchers to \naccess data in a secure, monitored, and cloud\u0002based environment.\n° Additional legislative interventions could \nalso facilitate data-sharing with the NRC (e.g., \nrequiring IT modernization to include data\u0002sharing plans with the NRC).\n• Ethics. Rapid innovation in AI research raises a \nhost of potential ethical challenges. Given the \nscope of the NRC, it will not be feasible to review \nevery single research proposal for potential ethical \nviolations, particularly since ethical standards are still in flux. The NRC should adopt a twofold approach. \n° First, for default PI access to base-level data and compute, the NRC should establish an ex-post review process \nfor allegations of ethical research violations. Access may be revoked when research is shown to manifestly and \nseriously violate ethical standards. We emphasize that the high standard for a violation should be informed \nby the academic speech implications and potential political consequences of government involvement in \nadministering the NRC and determining academic research directions. \n° Second, for applications requesting access to restricted datasets or resources beyond default compute, which \nwill necessarily undergo some review, researchers should be required to provide an ethics impact statement. \nOne of the advantages of beginning with PIs is that university faculty are accountable under existing IRBs for \nhuman subjects research, as well as to the tenets of peer review. \n° We urge non-NRC parties (e.g., universities) to explore a range of measures to address ethical concerns in AI \ncompute (e.g., an ethics review process33 or embedding ethicists in projects34).\n• Security. We recommend that the NRC take the lead in setting security classifications and protocols, in part to \ncounteract a balkanized security system across federal agencies that would stymie the ability to host datasets. The \nNRC should use dedicated security staff to work with Affiliated Government Agencies and university representatives \nto harmonize and modernize agency security standards. \n• Intellectual Property (IP). While the evidence on optimal IP incentives for innovation is mixed, we recommend \nthat the NRC adopt the same approach to allocating patent rights, copyrights, and data rights to NRC users \nthat apply to federal funding agreements. The NRC should additionally consider conditions for requiring NRC \nresearchers to disclose or share their research outputs under an open-access license.\n• Human Resources. Given its ambition, significant human resources—from systems engineers to data officers, and \nfrom grants administrators to privacy, ethics, and cybersecurity staff—will be necessary to make the NRC a success.\nGiven its ambition, significant \nhuman resources—from systems \nengineers to data officers, and \nfrom grants administrators to \nprivacy, ethics, and cybersecurity \nstaff—will be necessary to make \nthe NRC a success.\nBuilding a National AI Research Resource: \nA Blueprint for the National Research Cloud\n15\nIntroduction\nIn March 2020, Stanford’s Institute for Human-Centered Artificial Intelligence (HAI) published an open letter, co\u0002signed by the presidents and provosts of 22 top universities in the country, to the president of the United States and \nU.S. Congress urging adoption of a National Research Cloud (NRC).1 The NRC proposal aims to close a significant gap in \naccess to computing and data that, proponents argue, has distorted the long-term trajectory of artificial intelligence (AI) \nresearch.2 Without access to such critical resources, AI research may be dominated by short-term commercial interests \nand undermine the historical innovation ecosystem where basic, fundamental, and noncommercial research have laid \nthe foundations for applications that may be decades away, not yet marketable, or promote the public interest.\nIn January 2021, Congress enacted the National Artificial Intelligence Research Resource Task Force Act (NAIRR), \nconstituting a task force to consider the design of the NRC.3\n The task force was announced in June of this year and \nincludes one of the original proponents of the NRC and co-director of HAI (Fei-Fei Li).4\nThis White Paper is the culmination of a two-quarter, independent policy practicum at Stanford Law School’s \nPolicy Lab program, which was co-taught by three of us (Ho, King, Wald) and a teaching assistant (Wan) and brought \ntogether law, business, and engineering students to contemplate key design dimensions of the NRC. We interviewed \nand convened a wide range of stakeholders, including privacy attorneys, cloud computing technologists, government \ndata experts, cybersecurity professionals, potential users, and public interest groups. Students researched governing \nlegal provisions, policy options, and avenues for the institutional design of the NRC. The practicum team worked \nindependently to shape its recommendations.\nThe proposal for an NRC is an ambitious one, and this White Paper covers a lot of ground. We begin with the \nfundamental question—why build the NRC (Chapter 1)?—and spell out what we view as a cogent theory of impact. \nWe then cover who should have access to the NRC (Chapter 2), what comprises the NRC (Chapter 2), how access to \nrestricted data may (or may not) be granted (Chapter 3), and where the NRC should be located (Chapter 4). We spend \nextensive time on the data access portion (Chapters 3, 5, and 6), due to the complexities of government data-sharing \nunder the Privacy Act of 1974.5\n As we note in those chapters, the data portion of the NRC is complementary to long\u0002standing efforts to enable greater research access to administrative data under, for instance, the Foundations for \nEvidence-Based Policymaking Act of 20186 and the National Secure Data Service Act proposal.7 Such sharing must be \ncarried out securely and in a privacy-protecting fashion. We also consider questions of ethical standards (Chapter 7), \ncybersecurity (Chapter 8), and intellectual property (Chapter 9) that inform the design of the NRC. \nWe recognize the complexity of the enterprise and that there are many questions not answered herein. The \ncontemplated scale of the NRC may be to AI what the Human Genome Project was to genomics (or what particle \naccelerators were to physics): public investment for ambitious, noncommercial fundamental scientific research to ensure \nthe long-term flourishing of a critical area of innovation for the United States. There are many areas where we wish we \nhad had the opportunity to engage in more extensive research. We hope this White Paper nonetheless, will provide a \nuseful contribution for the NAIRR Task Force, Congress, the White House, and all those interested in the AI innovation \necosystem. \nBuilding a National AI Research Resource: \nA Blueprint for the National Research Cloud\n16\nWe owe gratitude to the many people who contributed time, feedback, and insights. Most importantly, we thank \nthe extraordinary students who shaped this White Paper: Simran Arora, Sabina Beleuz, Nathan Calvin, Shushman \nChoudhury, Drew Edwards, Neel Guha, Krithika Iyer, Ananya Karthik, Kanishka Narayan, Tyler Robbins, Frieda Rong, \nJasmine Shao, and Sadiki Wiltshire. We benefited from too many individuals to name, but special thanks go to Taka \nAriga, Kathy Baxter, Miles Brundage, Jean Camp, Shikai Chern, Bella Chu, Jack Clark, Kathleen Creel, John Etchemendy, \nDeep Ganguli, Eric Horvitz, Sara Jordan, Vince Kellen, Mark Krass, Sebastien Krier, Ed Lazowska, Brenda Leong, Fei-Fei Li, \nRuth Marinshaw, Michelle Mello, Amy O’Hara, Hodan Omaar, Saiph Savage, Marietje Schaake, Mike Sellitto, Wade Shen, \nKeith Strier, Suzanne Talon, Lee Tiedrich, Christine Tsang, and Evan White for helpful insights and feedback. HAI staff \nand research assistants who were essential in helping us during the final stages of editing and compiling the White Paper \ninclude Tina Huang, Marisa Lowe, Diego Núñez, and Daniel Zhang. \nAs we spell out in this White Paper, the NRC is an idea worth taking seriously. It is worth being clear, however, what \nit would and would not solve. The NRC would enable much greater access to—and in that sense, democratize—forms \nof AI and AI research that have increased in computational demands, but it would not categorically prevent or shift \nthe centralization of power within the tech industry. The NRC would shift the attention of current AI efforts into more \npublic and socially driven dimensions by providing access to previously restricted government datasets, addressing \nlongstanding efforts to improve access to high-value public sector data, but it would not create a system to prevent all \nunethical uses of AI. The NRC would facilitate audits of large-scale models, datasets, and AI systems for privacy violations \nand bias, but it would not be tantamount to a regulatory requirement for fairness assessments and accountability. It \nis neither a tool of antitrust nor a certification body for ethical algorithms, which are areas worth taking seriously in \nindependent policy proposals.8\n These broader considerations, however, do play into key areas of design and have very \nmuch informed our recommendations below on the design of the NRC. \nWhile it alone cannot solve all that ails AI, the NRC promises to take a major affirmative step forward.\nA Blueprint for the National Research Cloud 17\nCHAPTER 1\nThis chapter articulates a theory of impact for the NRC. In conventional policy \nanalytic terms,1 what problem (or market failure) does the NRC address? From \none perspective, AI innovation is vibrant in the United States, with major advances \noccurring in language, vision, and structured data and applications developing across \nall sectors. Yet from another perspective, current commercialization of past innovation \nmasks systematic underinvestment in basic, noncommercial AI research that could \nensure the long-term health of technological innovation in this country. \nChapter 1: \nThe Theory for a \nNational Research Cloud\nKEY \nTAKEAWAYS\n The federal government \nwill play a central role in \nshaping, coordinating, and \nenabling the development \nof AI.\n AI research and \ndevelopment is \nincreasingly dependent \non access to large-scale \ncompute and data, causing \nmigration of AI talent from \nthe academic to private \nsector and limiting the \nrange of voices able to \ncontribute to AI research.\n Noncommercial and basic \nAI research is critical to the \nlong-term health of the \ninnovation ecosystem. \n An NRC that provides data \nand compute access will \nhelp to promote the long\u0002term national health of the \nAI ecosystem and mitigate \nthe risks of widening \ninequalities in the nation’s \nAI landscape.\nCurrent commercialization of past innovation masks \nsystematic underinvestment in basic, noncommercial \nAI research that could ensure the long-term health of \ntechnological innovation in this country. \nOur case for the NRC is grounded in both efficiency and distributive rationales. \nFirst, the NRC may yield positive externalities, particularly over time, by supporting \ninvestments in basic research that may be commercialized decades later. Second, it \nmay help to level the playing field by broadening researcher access to both compute \nand data, ensuring that AI research is feasible for not just the most elite academic \ninstitutions or large technology firms. Given the scale of economic transformation AI \nis posited to initiate over the next few decades, the stakes are potentially significant. \nWhile the largest private interests like platform technology companies and certain elite \nacademic institutions continue to design, develop, and deploy AI systems that can be \nreadily commercialized, a different story is playing out for the public sector and the vast \nmajority of academic institutions, which lack access to core inputs of AI research. The \nrising costs associated with carrying out research and development are exacerbating the \ndisconnect between current winners and losers in the AI space.\nThis chapter proceeds in three parts. First, we survey the current landscape of \nAI research. Second, we articulate shifting trends in AI research and the academic\u0002industry balance. Third, we spell out the risks of federal inaction and the benefits to an \ninvestment strategy that couples data and compute resources. \nA Blueprint for the National Research Cloud 18\nCHAPTER 1\nTHE AI RESEARCH LANDSCAPE\nThe field of AI research, as we consider it in this White \nPaper, is broadly construed. It includes not only academics \nwho identify themselves as researchers in artificial intelligence \nor machine learning, but also the broader community of \nresearchers who use applied AI in their work, as well as those \nwho examine its impacts on society and the environment. \nMany believe, consistent with the legislation calling for \nthe NAIRR Task Force, that AI will have a dramatic impact \non society. Nine of the world’s 10 current largest companies \nby market capitalization are technology companies that \nplace AI at the core of their business models.2 Recent figures \nfrom the AI Index demonstrate the growing amount of \ninvestment AI companies have drawn. The most recent 2021 \niteration of the Index details how global private investment \nin AI has grown by 40 percent since 2019 to a total of $67.9 \nbillion, with the United States alone accounting for over \n$23.6 billion.3 While multiple private sector predictions \nof the economic impact of AI emphasize the potential for \nAI to drive significant economic growth through a strong \nincrease in labor productivity, others worry about the pace \nof structural change in the labor market and economic \ndislocation for workers automated out of their jobs or \nimpacted by the gig economy.4\nSuch impacts are expected across domains. AI holds \nsubstantial promise to transform healthcare and scientific \nresearch: AI-related progress in the field of protein folding \nis poised to dramatically expedite vaccine development \nand pharmaceutical drug development.5 The integration \nof AI-related systems into agriculture may improve \ncrop yields through targeted use of pesticides and soil \nmonitoring.6\n And national security experts have identified \nAI as a key driver of novel defense capabilities,7 including \ncyberwarfare and intelligence collection. \nMany countries have recognized the significance \nof AI as a driver of progress in economic, scientific, and \nnational security, releasing national plans coordinating \ninvestment for continued progress in AI.8 China’s national \nplan announced billions of dollars in funding aimed at \nmaking the country the global leader in AI by 2030.9 The \nJapanese government partnered with Fujitsu to build \nthe world’s fastest supercomputer (Fugaku).10 Compute \nCanada has similarly provided research computing access \nto academics across the country. The U.K.’s national high\u0002end computing resource, HECToR, was launched in 2007 at \na cost of $118 million and used by nearly 2,500 researchers \nfrom more than 250 separate organizations who produced \nover 800 academic publications.11\nThe U.S. government initially presented a more \ndecentralized approach, providing support for AI \ndevelopment through National Science Foundation \ngrants and defense spending, but refrained from releasing \na unified national plan to coordinate resources across \ngovernment, private industry, and universities.12 The \ncreation of a National AI Initiative Office,13 the updating \nof the National Strategic Computing Initiative,14 and the \nrelease of the National Security Commission on Artificial \nIntelligence’s (NSCAI) final report15 introduced a more \ncomprehensive and coordinated approach. Within the \nUnited States, the closest model to the NRC may be the \nCOVID-19 HPC consortium, which quickly provisioned \ncompute of 50K GPUs and 6.8 million cores for close to \n100 projects across 43 academic, industry, and federal \ngovernment consortium members united by the common \ngoal of combating the COVID-19 pandemic.16\nHistorically, partnerships between government, \nuniversities, and industry have anchored the U.S. \ninnovation ecosystem. The federal government played \ncritical roles in subsidizing basic research, enabling \nuniversities to undertake high-risk research that can take \ndecades to commercialize. This approach catalyzed radar \ntechnology,17 the internet, 18 and GPS devices.19 This history \ninformed the NSCAI’s recommendation for substantial \nnew investments in AI R&D by establishing a national AI \nresearch infrastructure that democratizes access to the \nresources that fuel AI. Many policymakers believe that \nsubstantial investment will be needed over the next \nseveral years to support these efforts, while returns on \nsuch investments could potentially transform America’s \neconomy, society, and national security.20 \nTo be sure, some may challenge the theory of impact. \nFirst, some studies dispute the premise that AI will be \neconomically transformative. Some economists argue that \nA Blueprint for the National Research Cloud 19\nCHAPTER 1\nmany of the optimistic assessments fail to consider how \nconstrained the uptake of AI innovation may be due to AI’s \ninability to change essential yet hard-to-improve tasks.21\nOthers similarly critique the evidence for a fourth industrial \nrevolution.22 Second, some suggest that the provisioning \nof the NRC may strengthen the position of large platform \ntechnology companies (which of course provokes debates \nover antitrust in the technology sector23), as the NRC may \nbe hard to launch without some involvement of hardware \nor cloud providers in the procurement process. Third, some \nwould argue that the NRC would generate large negative \nexternalities in the form of energy footprints. For instance, \none study found that the amount of energy needed to train \nGPT-3, a leading natural language processing (NLP) model, \nrequired the greenhouse emissions equivalent of 552.1 \ntons of carbon dioxide,24 approximately 35 times the yearly \nemissions of an average American.25 Expanding access to \ncompute without appropriate controls may contribute \nto wasteful computing.26 Finally, some critics argue that \nany advances in AI are inherently too risky for further \ninvestment,27 given widely documented risks of bias,28\nunintended consequences,29 and harm.30\nWe are cognizant of these critiques and take them \nseriously. This White Paper proceeds on the operative \npremise animating the NRC legislation: that it will be \nimportant for the country to maintain leadership in AI— \nincluding rigorous interrogation of its uses, limits, and \npromises—and that this requires supporting access to \ncompute and data. Public investment in AI research for \nnoncommercial purposes may help to address some of \nthe issues of social harm we see presently in commercial \ncontexts31, as well as contribute to shifting the broader \nfocus of the field toward technology developed in the \npublic interest by the public sector and civil society, \nincluding academia. The preceding considerations, \nhowever, have shaped our views in key respects, such as \nthe sequential investment strategy, given the uncertainty \nof AI’s potential; the serious consideration of publicly \nowned infrastructure; the provisions for ethical review \nof compute and data access; and, most importantly, \nthe enablement of independent academic inquiry into \nthe potential harms of AI systems. The NRC is not an \nendorsement of blind and naïve AI adoption across the \nboard; it is a mechanism to ensure that a greater range of \nvoices will have access to the basic elements of AI research. \nSHIF TING SOURCES OF \nAI RESEARCH\nWe now articulate how and why AI research has \nmigrated away from basic, long-term research into \ncommercial, short-term applications. \nFirst, many current advances fueled by large-scale \nmodels are costly to train, relative to the size of typical \nacademic budgets. For example, the estimated cost \nof training Alphabet subsidiary DeepMind’s AlphaGo \nZero algorithm, capable of beating the human world \nchampion of the game Go, was more than $25 million.32\nFor reference, the total annual 2019 budget for Carnegie \nMellon University’s Robotics Institute, one of the premier \nacademic research institutions in the nation, was $90 \nmillion.33 A white paper from the Bipartisan Policy Center34\nand the Center for a New American Security noted that the \nFY2020 budget for non-defense AI R&D announced by the \nWhite House was $973 million. In contrast, the combined \nspending on R&D in 2018 by five of the major technology \nplatform companies was $80 billion. In sum, research \nuniversities cannot keep pace with private sector resources \nfor compute. This is not to say that large-scale compute is \nnecessary for all academic AI research, or that academic \nresearch is in competition with industry research, but it \ndoes illustrate why certain sectors of AI research are no \nlonger accessible to the academic researcher. \nThe NRC is not an endorsement \nof blind and naïve AI adoption \nacross the board; it is a mechanism \nto ensure that a greater range of \nvoices will have access to the basic \nelements of AI research. \nA Blueprint for the National Research Cloud 20\nCHAPTER 1\nSecond, the academic-industry divide masks \nsignificant disparities between academic institutions. \nUsing the QS World University Rankings since 2012, \nFortune 500 technology companies and the top 50 \nuniversities have published five times more papers \nannually per AI conference than universities ranked \nbetween 200 and 500.35 Private firms also collaborate \nsix times more with top 50 universities than with those \nranked between 301 and 500.36 This internal compute \ndivide across universities poses significant challenges \nfor who is at the table. \nThird, basic AI research has lost human capital.37\nWhen this is combined with decreased access to \ncompute and data in the academy, the prospect of \nconducting basic research at universities becomes \nless attractive. Top talent in AI now commands private \nsector salaries far in excess of academic salaries.38 The \ndeparture of AI faculty from American universities has \nled to what some analysts have dubbed the AI Brain \nDrain: While AI Ph.D.s in 2011 were roughly as likely to \ngo into industry as academia, two- thirds of AI Ph.D.s \nnow go into industry and less than a quarter go into \nacademia.39 One study suggests that the departure of AI \nfaculty also has a negative effect on startup formation \nby students.40\nFourth, as large-scale AI research migrates to industry, \nthe focus of research inevitably shifts. While academic \nresearchers in AI may lack access to the volume of data \nneeded to train AI models,41 large-platform companies \nhave access to vast datasets, including those about or \ncreated by their customers. This data divide in turn distorts \nAI research toward applications that are focused on private \nprofit, rather than public benefit.42 Put more colorfully by \nJeff Hammerbacher, “The best minds of my generation are \nthinking about how to make people click ads.”43 The NRC \ncan play a key role in unlocking access to public sector \ndata, which may help to reorient the focus of AI research \naway from private sector datasets.44\nThe hollowing out of academic AI capacity can be \nseen in OpenAI’s analysis of the relationship between \ncompute and 15 relatively well-known “breakthroughs” \nin AI between 2012 and 2018.45 Although the analysis was \nmeant to emphasize the role of computing power, it also \nillustrates an emerging gap between private sector and \nacademic contributions over time. Of the 15 developments \nexamined, 11 were achieved by private companies while \nonly four came from academic institutions. Furthermore, \nthis imbalance increases over time: Though private sector \nresearch has continued accelerating since 2012, academic \noutput has stagnated. The last of the major compute\u0002intensive breakthroughs in OpenAI’s analysis stemming \nfrom academia was Oxford’s 2014 release of its VGG image\u0002recognition program; NYU’s work on Convolutional Neural \nNetworks dates back to 2013. From 2015 to 2018, all eight \nbreakthroughs included in OpenAI’s analysis came out of \nprivate companies. Taken together, this leads observers \nto argue that academic researchers are increasingly \nunable to compete at the frontier of AI research.46 While \nacademic researchers have continued to make important \ncontributions in AI, these are increasingly restricted to \nless compute-intensive problems. With fewer compute\u0002intensive academic breakthroughs, AI innovations have \nfocused on private interests (e.g., online advertising) as \nopposed to long-term, noncommercial benefits. To be \nsure, the private sector has, of course, been central to AI \nresearch, but the concern is about the long-term balance \nof the AI innovation ecosystem. \nWhile AI Ph.D.s in 2011 were \nroughly as likely to go into \nindustry as academia, two-thirds \nof AI Ph.D.s now go into \nindustry and less than a \nquarter go into academia.\nA Blueprint for the National Research Cloud 21\nCHAPTER 1\nSCOPING FEDERAL INTERVENTION \nIN DATA AND COMPUTE\nHow can we achieve a more balanced approach \ntoward research and development? We first consider the \nrisks of federal inaction and discuss some of the unique \nadvantages of addressing data and compute together. \nRisks of Federal Inaction\nThe risks of federal inaction are twofold. First, basic \nAI research that has to date paved the way for advances \nin AI and machine learning will slow. According to a recent \nstudy, approximately 82 percent of the algorithms used \ntoday originated from nonprofit groups and universities \nsupported by government spending.47 Even when industry \nresearch is successful, it is typically product-focused \nor incremental, harder to reproduce, and may not be \npublished or open-sourced. An interesting case lies in \nrecent breakthroughs in protein folding. In late 2020, \nthe Alphabet subsidiary DeepMind announced that it \nhad developed a program called AlphaFold, an AI-driven \nsystem capable of accurately predicting the structure of \na vast number of proteins, using only the sequence of \nnucleotides contained in its DNA. Whether out of concern \nfor the privatization or to accelerate adoption of related \nsystems, a consortium of academics, led by scientists at \nthe University of Washington, developed an open source \ncompetitor called RoseTTaFold.48 DeepMind did make \nAlphaFold available to a broad audience, but the concerns \nillustrate the risks of science posed by exclusively private \nAI research, reminiscent of the race to sequence the human \ngenome, where public investment in the Human Genome \nProject preempted concerns about a private firm patenting \nthe human genome.49\nSecond, federal inaction could widen significant \ninequalities in the AI landscape. Without increased \naccess to computing, education, and training, large parts \nof the economy may be unable to adapt—whether in \nfinancial services, healthcare, education, or government. \nDiversifying the range of AI research may also promote \nprogress and productivity. One study suggests that the \ndiversity of AI research trajectories—that is, the specific \nquestions, topics, and problems researchers choose to \ninvestigate—has become more constrained in recent years \nand that private sector AI research is less diverse than \nacademic research.50 Smaller academic groups with lower \nprivate sector collaboration appear to bolster the diversity \nof AI research.51 From the standpoint of underdeveloped \navenues of research, such as ethics and accountability in \nAI, increasing the range of research topics and methods \nin the field raises the likelihood of finding breakthroughs \nthat make additional progress in the long term possible.52\nRecent evidence suggests that between 2005 and 2017, \njust five metro areas in the U.S. accounted for 90 percent \nof the growth in innovation sector jobs.53 According to \nStanford economist Erik Brynjolfsson, the likely impact \nof geographic concentration is “there are a whole lot \nof people—hundreds of millions in the U.S. and billions \nworldwide—who could be innovating and who are not \nbecause they do not have access to basic computer \nscience skills, or infrastructure, or capital, or even culture \nand incentives to do so.”54 AI technologies can be hard to \ndiagnose and interpret and be prone to substantial bias.55\nBroadening the set of voices that can interrogate such \nsystems will be critical to an inclusive and equitable future. \nIn sum, federal investment in public AI infrastructure \nmay promote a more equitable distribution of \nparticipation in and gains to AI innovation broadly, bolster \nU.S. competitiveness, and support fundamental research \ninto noncommercial and public sector applications.\nA Blueprint for the National Research Cloud 22\nCHAPTER 2\nThis chapter discusses eligibility, resource allocation, and computing \ninfrastructure for the NRC: Who should get access to what and how? \nFirst, when determining who should get access, it is critical to bear in mind \nthe broad goals of the NRC. As discussed in Chapter 1, there is a large resource gap \nin academia as compared to private industry. In the interest of supporting basic \nresearch and democratizing the field, this section will focus on identifying a target \ngroup for eligibility. As we articulate below, we refrain from considering expansion \nto a broader set of commercial, nonacademic parties because of the NRC’s focus on \nlong-term, fundamental scientific research. One of the narrowest approaches would \nbe a specialty faculty model that would target researchers engaged in core AI work. \nBut, the difficulties with defining AI and the rapidly expanding domains in which AI is \nbeing applied make this model too constrained to realize the full impact of the NRC. \nInstead we recommend tracking the most common criterion for federal research \nfunding and advocate that eligibility hinge on “Principal Investigator” (PI) status at \nU.S. universities.1\n One of the tradeoffs is that PIs may be less diverse than a broader \nsegment of researchers,2 so a longer-term expansion could consider moving beyond \nthis group. While the NRC aims to train the next generation of AI researchers, we \ncaution that an immediate expansion to all graduate and undergraduate students \nwould pose considerable challenges in scaling. Therefore, we recommend that \nstudents primarily gain access by participation in faculty-sponsored AI research, \ninstead of blanket student access, and that they gain training through the creation of \neducational programs. \nSecond, we discuss three models for allocating computing credit: development \nof a new grant process, delegating block compute grants to universities for internal \nallocation among faculty, or universal access. Each of these models trades off the \nease of administration against tailoring for specific NRC goals. We recommend an \napproach used by other national research clouds—namely a hybrid approach of \nuniversal default access for the majority of researchers, with a grant process for \nexcess computing beyond the default allocation. Such an approach would keep \nadministrative costs low for the vast majority of researchers, while enabling tailoring \nthrough a competitive grant process for the highest-need users. \nChapter 2: \nEligibility, Allocation, \nand Infrastructure \nfor Computing\nKEY \nTAKEAWAYS\n Researcher eligibility \nfor NRC access should \nbegin with “Principal \nInvestigator” status at U.S. \nuniversities.\n The NRC should adopt \na hybrid approach of \nuniversal default access for \nthe majority of researchers \nand a grant process when \nrequests for compute or \ndata exceed base levels.\n The NRC should adopt a \ndual investment strategy \nby developing programs \nfor expanding access to \nexisting cloud services \nand piloting the ability to \nprovide publicly owned \nresources. \nA Blueprint for the National Research Cloud 23\nCHAPTER 2\nThird, we consider the “make-or-buy” decision for \nthe NRC. One option would be for the NRC to provide \nresearch grants for the use of commercial cloud services \nthat many researchers already rely on (the “buy” decision). \nAlternatively, the NRC could create and provision access \nto a publicly high-performance computing cluster (the \n“make” decision). It is well-established that, based \nsolely on hardware costs, it is more cost-effective to \nown infrastructure when computing demand is close to \ncontinuous. On the other hand, existing commercial cloud \nproviders have developed highly usable software stacks \nthat AI researchers have widely adopted. Commercial \ncloud providers offer a way to quickly expand capacity. \nWe hence recommend a dual investment strategy to (a) \nquickly launch the NRC by subsidizing and negotiating \ncloud computing for AI researchers with existing vendors, \nexpanding on existing initiatives like the National Science \nFoundation’s CloudBank project; and (b) invest in a pilot \nfor public infrastructure to assess the ability to provide \nsimilar resources in the long run. Such publicly owned \ninfrastructure would likely be built under contract or grant, \nbut could be operated much like national laboratories \nthat own sophisticated supercomputing facilities, as is the \ncase with other national research resources (e.g., Compute \nCanada, Japan’s Fugaku).\nOur recommendations are informed by a series of \ncase studies that are presented throughout this chapter, \nas well as through the remainder of the White Paper. \nTable 1 summarizes how existing models compare on the \nthree key design decisions. At the outset, we note that few \nexisting initiatives have attempted to provide compute \npower at the scale of the NRC. At the same time, we view \nthe NRC as complementary to more traditional areas of \nscientific computing.3\nELIGIBILITY ALLOCATION OWNERSHIP\nExisting \nProgram\nPI \nOnly\nAny \nFaculty Students\nExisting \nGrant \nProcess\nUniversity \nAllocation\nNew \nProcess\nDefault \nAccess \nw/Tiers\nPrivate Public\nCloudBank X X X X\nStanford \nHAI-AWS \nCloud Program\nX X X\nStanford \nSherlock Cluster X X X\nGoogle Colab X X X X\nCompute \nCanada X X X\nFugaku X X X\nXSEDE X X X X\nDOE INCITE X X X\nTable 1: Key design differences between computing case studies. “Other faculty” indicates an eligibility set for faculty other than PI status \n(e.g., requiring Stanford affiliation for the Sherlock cluster) and “new process” is used to indicate the creation of any process other than those \ncurrently listed (e.g., Fugaku is currently soliciting proposals with research facilities). \nA Blueprint for the National Research Cloud 24\nCHAPTER 2\nEligibility\nThe first task is identifying which researchers should \nbe eligible for the NRC. Chapter 1 discussed the need \nto support AI innovation in universities. Therefore, this \nsection will scope eligibility within academia by analyzing \nthe access-resource trade-offs in alignment with the NRC \ngoals.\nAt the outset, we note that we do not analyze eligibility \nin depth beyond academic researchers. The legislation \nconstituting the NRC task force specifically contemplates \n“access to computing resources for researchers across the \ncountry.”4 The NRC is defined as “a system that provides \nresearchers and students across scientific fields and \ndisciplines with access to compute resources.”5 The most \nnatural interpretation of this language suggests a core \nfocus on scientific and academic research.6\nIntroducing commercial access to the NRC, particularly \nfor under-resourced firms such as small businesses \nand startups, may very well benefit the U.S. innovation \necosystem. But the challenges of incorporating commercial \naccess to the NRC are enormously complex. First, including \nsoftware developers at startup companies as “researchers” \nwithin the meaning of the NDAA would raise a wide \nrange of boundary questions that the NRC may be poorly \nequipped to adjudicate. According to the Small Business \nAdministration (SBA), there are over 31 million small \nbusinesses in the United States.7\n Over 627,000 businesses \nopen each year.8 Should all such businesses be eligible to \ncompute on the NRC? How would one avoid gaming (e.g., \nstrategic subsidiaries/spinoffs) eligibility? And, how would \nthis advance the scientific mission of the NRC? Second, \nwhile potentially valuable, it is less clear how the inclusion \nof startups and small businesses meets the theory of impact \nof the NRC. As currently construed, the concern animating \nthe NRC lies in the importance of long-term, noncommercial \nfundamental research that can ensure AI leadership for \ndecades to come. Commercialization is not the element \nof the AI innovation ecosystem that faces the structural \nchallenges articulated in Chapter 1. Finally, scaling the NRC \nto allow meaningful commercial access would pose serious \npractical challenges. Because the Task Force must also \nconsider the feasibility of the NRC, we have not considered \nin depth a conception that would extend the term \n“researcher” to encompass large portions of the commercial \nprivate sector. Expansion to non-academic, nonprofit \norganizations may be a more reasonable consideration, \nas the objective of some entities (e.g., not-for-profit \ninvestigative journalism, civil society organizations) may be \ncloser to the core of the NRC’s mission of empowering long\u0002term beneficial research that cannot currently occur.9\n In the \nlong term, the NRC should consider the trade-offs to such an \nexpansion. \nEven if the NRC adopts a broader computing model \ndown the road, we believe that focusing on academic \nresearchers is an important starting point as it illuminates \nsome of the main operational considerations for NRC \naccess. \nSPECIALT Y FACULT Y MODEL \nOne of the narrowest approaches to NRC eligibility \nwould be to restrict it to faculty engaged in AI research. \nUnder this approach, policymakers would direct \ncomputing resources exclusively toward faculty working \non identifiable AI projects, which often need large amounts \nof compute power. A benefit of this approach is that \nresearchers’ familiarity with the infrastructure would likely \nmean that fewer funds would be devoted to cloud service \ntraining for novice users. \nYet the set of self-identified core AI faculty are few \nand concentrated in a small number of universities, \nwhich are already more likely to gain access to large-scale \ncomputing. Limiting access to core AI faculty would hence \nundermine the mission of democratizing AI research. In \naddition, the application of AI is expanding rapidly across \ndomains. Interdisciplinary research deploying AI in new \ndomains will be vital for maintaining American leadership \nin AI, as well as for animating basic research questions. \nRestricting eligibility to core AI faculty (however defined) \ncould jeopardize the ability of researchers from all \nacademic disciplines (e.g., in the physical sciences, social \nsciences, and humanities) to contribute to realizing AI’s full \npotential.\nA Blueprint for the National Research Cloud 25\nCHAPTER 2\nGENERAL FACULTY MODEL\nA more natural starting point for NRC eligibility is with \nPrincipal Investigators (PIs) at U.S. colleges and universities, \nthe most commonly deployed criterion for federal grants. \nRequirements for PI status are set by individual universities \nand include a broad range of researchers certified by their \nuniversity as qualified to lead large research projects.10\nWhile PI certification may vary from institution to institution, \nan important baseline criterion of PI status is that the \nresearcher is subject to their institution’s training and \ncertification processes, which in turn clarify a researcher’s \nresponsibilities regarding the management and execution \nof their research proposals. Existing programs for allocating \ncomputing power typically set eligibility based on PI \nstatus as it ensures the researcher has the infrastructure \nto carry out a large-scale research project. CloudBank, an \nNSF program that distributes funds for commercial cloud \ncomputing resources, awards grants to PIs, who may \ndistribute funds to other researchers and students on the \nproject.11 Compute Canada allows all faculty granted PI \nstatus by their university to automatically receive a preset \namount of computing credits and apply for further credit \nas needed. The PI may then sponsor others to access the \ncredit.12\nWe recognize that PI status does not include all \nuniversity-affiliated researchers. In 2013, of the over \n200,000 self-identified academic researchers, just under \n60,000 were employed in a role other than full-time \nfaculty, a position that may not be eligible for PI status.13\nFrom 1973 to 2013, the percentage of full-time faculty \namong engineering doctorate holders decreased by 2 \npercent, while the percentage of “other” academic jobs \n(including research associates) increased by 12 percent.14\nBut, the reliance on PI status would not prevent PIs from \nallocating access to non-PI status researchers on a project, \nand administrative ability weighs strongly in favor of \nconsistency with current grant eligibility criteria. \nSTUDENTS \nShould graduate and undergraduate students be \nable to access the NRC? One of the principal challenges \nhere lies in scale and administrability. One estimate \nis that there are nearly 20 million college students \nin the U.S.15 Second, PI-oriented eligibility does not \npreclude university students from accessing resources \nto undertake AI research under the direction of PIs. The \nCompute Canada model, for example, restricts eligibility \nto faculty, but allows faculty to sponsor collaborators, \nincluding any student researcher. An access model for the \nNRC that allows PIs to sponsor students provides further \nresearch and training opportunities for students. Third, a \nnumber of existing cloud services already provide limited \naccess to computing credits for educational purposes. \nGoogle Colaboratory, for instance, provides free, but not \nreliably guaranteed, access to cloud services.16 Amazon \nWeb Services provides up to $35 of AWS credits for free \nto all university faculty and students. Despite existing \nresources, students may need more resources. The \nGoogle subsidiary and online community Kaggle, for \nexample, provides 30 hours of GPU access per week for \nfree and found that 15 percent of users exceeded the \nlimit.17\nWhile the exact scope of student computing power \nneeds is unclear, we recommend funding an educational \nresource once researcher needs and resource limitations \nhave been gauged. Currently, the NSF’s CloudBank is \npiloting a Community & Education Resource to earmark \na small set of credits for educational purposes.18 This \nresource allows a university professor to request a small \nnumber of credits for student coursework or small-scale \nresearch. \nRegardless of which eligibility model the NRC adopts, \nthere will also be a significant need for support staff, \ntraining documentation, and educational materials so \nresearchers can effectively make use of the compute \nand data resources (see Appendix D). The reason some \nstudents and researchers may not take advantage of all \navailable cloud credits could, for instance, stem from \nthe difficulty in using cloud platforms. If the NRC serves \nacademics from a range of disciplines, this question of \nhuman capital will be especially relevant to serve different \nmodels of research. A robust training program for users of \nthe NRC will ensure ease of use and encourage appropriate \nutilization of the cloud.\nA Blueprint for the National Research Cloud 26\nCHAPTER 2\nResource Allocation \nModels\nWe now consider three resource allocation models: (1) a \nnew grant process; (2) block grant allocation to universities; \nand (3) universal—but potentially tiered—access. \nNRC GRANT PROCESS\nEstablishing a new grant process for compute access \nwould have one main advantage. The program could \nbe built specifically for the purpose of AI research, with \nreviewers who are familiar with AI concepts, practices, and \ntrends. Such a process might therefore enable improved \nallocation decisions and provide the NRC with greater \ncontrol over its investments. \nThat said, establishing a peer-review process for \nall applications would be resource intensive, requiring \nthe establishment of a grant administration program \nakin to those at the National Science Foundation (NSF) \nor the National Institutes of Health (NIH). For instance, \nto implement peer review required for the merit review \nprocess, the NSF annually needs a community large \nenough to conduct nearly 240,000 reviews per year.19 Since \nthe contemplated reach is broad, we are mindful of adding \na significant service burden for faculty conversant in AI \nfor every application for compute access. Peer review for \ncompute access would require significant overhead and \ndelays in compute allocation. \nUNIVERSITY ACCESS\nTo reduce administrative costs, an alternative scheme \nwould be to allocate credits to universities based on the \nnumber of eligible researchers. The NRC could allocate \nresources to universities as block grants, and in turn, rely \non the university to distribute computing access. (For \nexample, the NRC could purchase significant amounts of \ncompute from cloud providers, create virtual credits that \nare convertible into appropriate cloud resources, and \ndelegate allocation to universities.) This approach would \nhave the advantage of tapping into the universities’ local \nexpertise for reviewing and distributing resources. It would, \nhowever, lead to a highly decentralized process, providing \nlittle oversight to understand the distribution of usage, and \ngive the NRC little control over resource allocation. While \nwe do not recommend this route as the principal allocation \nscheme, we do believe that some allocation to university\u0002based IT support teams may be warranted to support \nresearchers in using the NRC. XSEDE’s “Campus Champions” \nprogram, for instance, provides university employees access \nto the system to support the computational transition.20\nUNIVERSAL ACCESS\nThe last potential model would provision universal \naccess to base-level compute to all eligible PIs. The closest \nmodel is Compute Canada’s national research cloud, \nwhich provides base-level compute access to all faculty \nin Canada. This would significantly reduce administrative \noverhead, both for an institution running the review \nprocess, and academics seeking NRC access. The primary \ndownside is that base-level compute may be insufficient \nfor specialized needs.\nWe recommend combining a universal baseline model \nwith a grant process for compute needs beyond base-level \naccess. The reduced complexity in administering a universal \nbaseline access compute model makes it an attractive \noption for the NRC in allocating compute resources, \nespecially with respect to the NRC’s goal of opening access \nto compute resources.21 XSEDE, for instance, uses a similar \nmodel of streamlined “Startup Allocations” (issued for \none-year terms, typically within two weeks of application) \nand “Research Allocations” for more significant compute \nrequests. Compute Canada provides access to 15 percent \nof PIs to increased compute capacity based on a merit \ncompetition. A critical question will, of course, be the level \nof baseline computing that will determine overall costs, \nphysical space requirements, and the like. To benchmark \nthis, we recommend an in-depth study of the anticipated \ncomputing needs, based on existing academic computing \ncenters.22\nThe grant process for additional compute could \ntake multiple forms; for example, while one could allow \nindividual PIs to apply directly to the NRC for excess \nA Blueprint for the National Research Cloud 27\nCHAPTER 2\ncompute, the NRC could also allocate “blocks” of resources \nat the university level and allow universities to oversee \ntheir administration. In any case, due to the size of such \nrequests, grant reviews should be conducted on a merit \nbasis and administered by a combination of NRC staff and \nan external advisory board of university faculty. In 2021, \nCompute Canada, for instance, completed its review of 650 \nresearch submissions in about five months, with only 80 \nvolunteer reviewers from Canadian academic institutions \nto assess the scientific merit of the proposal.23 In order \nto avoid conflicts of interest, we strongly recommend \nagainst the participation of any faculty or private sector \nadvisers who have conflicts of interest with any vendors \nthat provide services to the NRC. Ideally, proposal review \nshould be independent, blinded, and based on scientific \nmerit to the extent possible.\nKEY TAKEAWAYS\n Built into existing \ngrant process: \nResearchers eligible \nfor certain existing \nNSF grants can \nsimply request \naccess to CloudBank \nthrough the same \ngrant application.\n Single point of entry \nfor compute access:\nThe CloudBank \nportal provides a \nsingle point of entry \nfor researchers to \naccess funds to \nuse on whichever \ncommercial cloud \nprovider they prefer.\n Cost reduction: No \noverhead costs are \nassociated with using \nCloudBank.\n Student access:\nLimited funds are set \naside for grants to \nstudents and classes.\nCASE STUDY: CLOUDBANK\nIn 2018, the National Science Foundation’s (NSF) Directorate for Computer and \nInformation Science and Engineering (CISE) created the Cloud Access Solicitation to \nprovide funding for AI-related research endeavors.Initially created to meet the needs \nof the NSF funding recipients to access public clouds, CloudBank is an interesting \ncase study for exploring resource allocation models. Accessible through a portal, \nCloudBank aids researchers in using cloud resources fully by facilitating the process \nof “managing costs, translating and upgrading computing environments to the cloud, \nand learning about cloud-based technologies.”24\nCloudBank is a collaboration project established via an NSF Cooperative \nAgreement with the San Diego Supercomputer Center (SDSC) and the Information \nTechnology Services Division at UC San Diego, the University of Washington \neScience Institute, and UC Berkeley’s Division of Data Science and Information.25\nEach of these institutions handles an area, according to its comparative advantage.26\nFor example, SDSC is responsible for building the online portal, and UC San Diego is \nin charge of managing the accounts of the users.27\nCloudBank also aims to reduce the cost of cloud computing: It uses both \nthe ongoing discounts with cloud providers from the University of California and \nthe discounts that come with bulk cloud purchase from the cloud procurement \nconsulting firm Strategic Blue, which regularly partners with the likes of AWS, \nMicrosoft, and Google.28 Furthermore, there is no overhead cost associated with \nthe cloud allocations through CloudBank, since the terms of the NSF cooperative \nagreement prohibit indirect costs.29 With these cost-saving mechanisms, researchers \ncan afford more computing capacities from a variety of major cloud vendors. \nBy requesting the use of CloudBank during their application to the selected \nNSF projects,30 researchers can gain access not only to various advanced hardware \nresources, but also to a variety of services to make the process more supported and \nmonitored.31 CloudBank also gives research community members access to its \neducation and training information.32 \nA Blueprint for the National Research Cloud 28\nCHAPTER 2\nComputing \nInfrastructure \nCloud computing environments connect local \ncomputing devices such as desktop computers to large, \ntypically geographically distributed servers containing \nphysical hardware. This hardware, in turn, is responsible for \nstoring data and performing computation over computer \nnetworks—all of which is mediated through a collection \nof software services. This model centralizes the usual \noperational management for those using the network and \nprovides adjustable units of computation and data storage \nto allow for fluctuations in demand. Users interact with the \ncloud by launching virtual connections to the server—cloud \ninstances—and running containerized processes remotely. \nThese operations are managed by the cloud and available \nfor monitoring through dashboards. Cloud computing \nmay be serviced through on-premises clusters, via external \nvendors, or some combination thereof, and accessed over \nnetworks with varying security and connectivity, from \ninternet-accessible to air-gapped regions.\nThe infrastructure to the NRC could be developed \nwith two general approaches: (1) the NRC could use \ncommercial cloud platforms as its infrastructure backbone; \nor (2) the federal government could engage a contractor \nto build a high-performance computing (HPC) public \nfacility specifically for the NRC. This section addresses \nsome advantages and disadvantages of both. (We provide \nan estimated cost comparison of these two approaches \nin Appendix A.) The two approaches discussed here are \nnot mutually exclusive, and we ultimately recommend \na hybrid investment strategy. In the short run, the NRC \nshould scale up cloud credit programs (similar to NSF’s \nCloudBank program) to provide both streamlined base\u0002level access and merit review for applications going beyond \nbase-level access. In the long run, the NRC should invest \nin a pilot to develop public computing infrastructure. Even \nwith public infrastructure, it will be critical to meet “burst \ndemand” (to expand resources when compute demand \npeaks). The success of the initial investments should guide \nthe prospective model as to whether to rely on publicly \nor privately owned infrastructure in the longer term. We \nnote that in order to scale successfully to either resource \nwill require building institutional capacity at academic \ninstitutions. \nCOMMERCIAL CLOUD\nThe greatest advantage of using commercial cloud \nservices for the NRC is that significant infrastructure \nalready exists.33 Under this model, the NRC would simply \nsubsidize credits for using commercial cloud services \n(similar to NSF’s CloudBank program). Thus, rather \nthan spending years building new computing resources, \npolicymakers could launch the NRC soon after they \ndetermine the program’s administrative details. (We note, \nhowever, that there may still be significant GPU shortages \nin the short run; with the contemplated scale of the NRC, \nsignificant infrastructure would need to be built.) Since \nmany researchers already use commercial cloud services \nfor their AI research, the transition into the NRC program \ncould be relatively seamless. Furthermore, commercial \ncloud platforms offer the NRC greater flexibility to change \nthe size and scope of the program. Commercial cloud \nplatforms charge for the amount of compute actually \nused.34 Thus, the size of the NRC could expand or retract \nin line with shifting demand. In contrast, a dedicated HPC \nsystem would have a set amount of hardware that costs \nthe same, no matter how effectively it’s being used.\nWorking directly with commercial cloud providers also \noffers several advantages for the NRC. The commercial \ncloud services market is highly competitive and features \nnumerous providers capable of meeting the NRC’s needs. \nThe NRC would have the option of using one provider or \nmultiple providers. If opting to use just one provider, the \ngovernment’s bargaining power may be at its strongest \nin helping to drive down prices for the NRC. Alternatively, \nusing multiple providers gives the NRC greater flexibility in \navailable services and hardware. Either way, policymakers \nwould have the opportunity to negotiate contracts and \nprices with commercial cloud providers every few years, \nwhich will be critical to cost containment.35 The NRC would \nalso not be locked into using the same provider or set of \nproviders for the duration of the program. Rather, NRC \nstaff could reevaluate which commercial cloud provider’s \ninfrastructure would best meet the NRC’s needs at the \nstart of each new contract.\nA Blueprint for the National Research Cloud 29\nCHAPTER 2\nKEY TAKEAWAYS\n Federally-funded \ninfrastructure: \nXSEDE is an NSF\u0002funded initiative \nthat integrates and \ncoordinates shared \nsupercomputing and \ndata analysis resources \nwith researchers.\n Tiered access to \ncompute: For baseline \naccess to compute, \nXSEDE leverages a \nfast, low-hurdle review \nprocess. For access \nbeyond the baseline, \nXSEDE has its own \nresource allocations \ncommittee that \nreviews applications \nevery quarter.\n “Campus Champions \nProgram:” XSEDE \npartners with \nemployees and \naffiliates at colleges, \nuniversities, and \nresearch institutions \nto help researchers \nget access to compute \nresources.\n Collaboration: XSEDE \ncollaborates with \nthe private sector in \nacquiring, operating, \nand managing \ncompute resources.\nCASE STUDY: XSEDE\nThe Extreme Science and Engineering Discovery \nEnvironment (XSEDE) is an NSF-funded organization that \nintegrates and coordinates the sharing of advanced digital \nservices such as supercomputers and high-end visualization \nand data analysis resources.36 XSEDE is a collaborative \npartnership of 19 institutions, or “Service Providers,” many \nof which are nonprofits or supercomputing centers at \nuniversities and provide computing facilities for XSEDE \nresearchers.37 XSEDE supports work from a wide variety \nof fields, including the physical sciences, life sciences, \nengineering, social sciences, the humanities, and the \narts.38 XSEDE allocations are available to any researcher \nor educator at a U.S. academic, nonprofit research, or \neducational institution, not including students.39 However, \nresearchers can share their allocations by establishing user \naccounts with other collaborators, including students.40\nResearchers have two different paths to requesting \nallocations: Startup Allocation and Research Allocation. \nStartup Allocations apportion XSEDE resources for small\u0002scale computational activities.41 Startup Allocations are one \nof the fastest ways to gain access to and start using XSEDE \nresources, as requests are typically reviewed and awarded \nwithin two weeks.42 Startup Allocation requests also \nrequire minimal documentation: the project’s abstract and \nthe researchers’ curriculum vitae (CV).43 Startup Allocations \ntypically last for one year, but requests supported by merit\u0002reviewed grants can ask for allocations that last up to three \nyears. Researchers can also submit renewal requests if their \nwork needs ongoing low-level resources.44\nFor research needs that go beyond the computational \nlimits under a Startup Allocation, researchers must \nsubmit a Research Allocation request.45 XSEDE strongly \nencourages its users to request a Startup Allocation prior \nto requesting a Research Allocation, in order to obtain \nbenchmark results and more accurately document their \nresearch needs in the Research Allocation.46 Research \nAllocation requests must include a host of documents, \nsuch as a resource-use plan, a progress report, code \nperformance calculations, CVs, and references.47\nRequests are accepted and reviewed quarterly by the \nXSEDE Resource Allocations Committee (XRAC), which \nassesses the proposals’ appropriateness of methodology, \nappropriateness of research plan, efficient use of \nresources, and intellectual merit.48\nXSEDE abides by a “one-project rule,” whereby each \nresearcher only has one XSEDE allocation for their research \nactivities.49 For instance, if a researcher has several grants \nthat require computational support, those lines of work \nshould be combined into a \nsingle allocation request. This \nminimizes the effort required \nby the researcher to submit \nrequests and reduces the \noverhead in reviewing those \nrequests.\nXSEDE also uses \na “Campus Champion \nProgram” to streamline \naccess to resources.50 The \nCampus Champion Program \nis a group of over 700 \nCampus Champions who \nare employees or affiliates \nat over 300 U.S. colleges, \nuniversities, and research\u0002focused institutions.51\nThese Campus Champions \nfacilitate and support use of \nXSEDE-allocated resources \nby researchers, educators, \nand students on their \ncampuses. For instance, the \nCampus Champions host \nawareness sessions and \ntraining workshops for their \ninstitutions’ researchers while \nalso capturing information \non problems and challenges \nthat need to be addressed by \nXSEDE resource owners.52\nFinally, XSEDE \nwelcomes collaboration \nopportunities with other \nmembers of the research \nand scientific community.53\nFor example, XSEDE assists \nother organizations in \nacquiring and operating \ncomputing resources and \nhelps to allocate and manage \naccess to those resources. \nRecently, XSEDE worked \nwith academics and private \nindustry to form the COVID-19 \nHigh Performance Computing Consortium, which \nprovides researchers with powerful computing resources \nto better understand COVID-19 and develop treatments \nto address infections.54 \nA Blueprint for the National Research Cloud 30\nCHAPTER 2\nCommercial cloud platforms also provide other \nadvantages to the NRC. The labor of managing, \nmaintaining, and upgrading the hardware behind the \nNRC would be handled by private parties that already \nhave expertise in running cloud services at scale and \nhave invested billions of dollars into doing it. This \narrangement allows researchers access to a greater \nvariety of hardware that is constantly being expanded \nand upgraded.55 With a strong economic incentive to keep \nimproving cloud offerings, commercial cloud services \noffer an assortment of instance types—i.e., the various \npermutations and combinations of GPU/CPU, memory, \nstorage, and networking specifications that constitute a \ncompute instance—with different hardware at a range of \nprice points. Thus, researchers would have the flexibility \nto choose both what hardware would best fit the needs of \ntheir projects and how best to allocate their limited cloud \ncredits. Researchers could also have access to cutting-edge \ntechnology specially designed for AI research, such as \nchips optimized for training and inference, developed and \nexclusively used by commercial cloud providers.\nUsing commercial cloud services for the NRC comes \nwith significant tradeoffs, however. While the initial costs \nof subsidizing cloud credits might be less than building \npublic infrastructure, many studies show that relying on \ncommercial cloud services would likely be much more \nexpensive in the long term.56 For example, a study of \nPurdue University’s Community Cluster Program shows \nthat the amortized cost of its on-premises cluster over five \nyears is 2.73 times cheaper than using AWS, 3.24 times \ncheaper than using Azure, and 5.54 times cheaper than \nusing Google Cloud.57 A similar study at Indiana University \nestimates that the total investment into its locally owned \nsupercomputer, Big Red II, is about $10.1 million, while the \ntotal cost of a three-year reservation on AWS about $24.9 \nmillion.58 Cost comparisons in other studies are even more \ndramatic. For instance, a study of the Advanced Research \nComputing clusters at Virginia Tech shows that the five\u0002year cost for its on-premises cloud is about $15.5 million, \nwhile the five-year cost for reserved AWS instances using \nthe same workloads would be about $136.3 million.59\nWhat explains these cost disparities? Estimates \ncomparing commercial cloud services to a dedicated HPC \ncluster show that commercial cloud services are more \nexpensive per compute cycle.60 At least in part, this is due \nto the fact that commercial services are optimized for \ncommercial applications. Compute Canada, for example, \nfound that building their own infrastructure was cheaper \nthan using commercial services, because they did not \nhave the same core use needs as commercial customers, a \ntradeoff that gained their system more computing power \nat the expense of availability.61 Although the analysis was \npublished in 2016, Compute Canada’s own benchmarking \nof costs concluded:\nCurrently, it is far more cost effective for the \nCompute Canada federation to procure and \noperate in-house cyberinfrastructure than to \noutsource to commercial cloud providers. . . . \nCloud-based costs ranged from 4x to 10x more \nthan the cost of owning and operating our own \nclusters. Some components were dramatically \nmore expensive, notably persistent storage which \nwas 40x the cost of Compute Canada’s storage.62\nUltimately, the cost difference between commercial \ncloud services and HPC systems depends on how often \nand how efficiently the HPC system is used. We provide a \ncost calculation that updates Compute Canada’s below, \narriving at cost differentials of comparable magnitude. \nCommercial cloud instances with comparable hardware \nunder constant usage, even with substantial discounts, \nWhile the initial costs of subsidizing \ncloud credits might be less than \nbuilding public infrastructure, \nmany studies show that relying on \ncommercial cloud services would \nlikely be much more expensive in the \nlong term.\nA Blueprint for the National Research Cloud 31\nCHAPTER 2\nwould be significantly more expensive over time for \nthe NRC than a dedicated HPC system. Bringing the \ncost of commercial cloud services under that of an HPC \nsystem would require policymakers to either negotiate \nexceptionally high discounts with commercial cloud \nproviders or make major sacrifices in hardware speed or \noverall scale of the NRC. A similar cost calculation is also \nwhat led Stanford University to simultaneously invest in \nboth on-premises hardware and a commercial cloud-based \nsolution for its Population Health Sciences initiative (see \nbox case study in Chapter 3). The most common practice \nacross NSF centers, such as the XSEDE initiative (see box \ncase study below), is also to build infrastructure instead \nof relying on commercial cloud credits, due to these cost \nconsiderations. \nFinally, relying on the commercial cloud may raise \nquestions about industry consolidation. There are two \nmain answers to this question. One is that building a \ndedicated, publicly owned HPC clusters would require \npurchasing sophisticated hardware from existing industry \nplayers, which also exist in concentrated industries. In \nother words, it is difficult to imagine no involvement \nof private industry under either option. Another major \nconstraint lies in time: A fully mature, public infrastructure \nNRC could not be stood up overnight. Moreover, a publicly \nowned cloud would still likely require a major technology \ncompany to build the infrastructure under contract, as is \nthe case for National Labs, or using a grant, as is the case \nfor XSEDE.\nPUBLIC INFRASTRUCTURE\nBuilding a new HPC cluster would be a bespoke \nsolution, tailored to fit the NRC’s specific compute needs. \nThis approach would be relatively well-explored territory \nfor the federal government.63 The U.S. Department of \nEnergy (DOE) and the U.S. Department of Defense (DOD) \nalready regularly contract with a handful of companies \nto build HPC clusters every few years.64 The DOE itself \nalready uses two of the three fastest HPC clusters in the \nworld and recently funded the development of two new \nsupercomputers that, when completed, will be the world’s \nfastest by a significant margin.65 The National Science \nFoundation commonly issues grants for the construction \nof high-performance computing infrastructure.66 Given this \nfamiliarity, policymakers would have reasonable estimates \nfor how much a new HPC cluster for the NRC would cost \nand would already have relationships with the companies \nthat would submit bids for the contract.\nThe hardware cost for such compute scale \nare, of course, substantial.67 For example, the IBM \nsupercomputer used at Oak Ridge National Laboratory \n(ORNL)—known as “Summit”—cost $200 million.68 At \nthe time of its completion in 2018, Summit was the \nfastest supercomputer in the world and, as of 2020, \nis still the second fastest.69 Frontier, the new Cray \nsupercomputer being built at ORNL in 2021, cost $500 \nmillion. When completed, it is anticipated to be the fastest \nsupercomputer in the world at “up to 50 times” faster \nthan Summit.70 Nonetheless, these large up-front costs \ncould come with the benefit of computing infrastructure \nspecifically designed for AI research and the NRC’s needs. \nSuch a system would be more efficient in cost per cycle \nover the long term than subsidizing commercial cloud \nservices. The NRC could also expand and upgrade multiple \nclusters over time to meet the changing needs and scope \nof the program.\nIn addition, a dedicated cluster for the NRC has the \nadvantage of giving the federal government greater control \nover computational resources (e.g., reducing uncertainty \nover the products and platforms, such as the sudden \ndeprecation of required APIs). This level of control over \nthe hardware also allows policymakers greater flexibility \nwith NRC operations. Taking the public infrastructure \napproach (i.e., “making” not “buying”) comes with several \nsignificant trade-offs to weigh against the policy goals \nof the NRC. First, building a new HPC cluster would take \nabout two years, in addition to the time it takes to solicit \nand evaluate proposals from potential contractors.71 If \nthe NRC hopes to quickly stimulate and help democratize \nAI research in the U.S., such a timeline for the program \nwould not be ideal, given how quickly AI discoveries \nadvance. Of course, contracting with cloud vendors or \nissuing grants for the construction of supercomputers \nwould also require a process. Yet, building a cluster could \nraise more challenging contracting issues, such as budget \noverruns and project delays.72 Contractors’ experience with \nbuilding this type of hardware may help mitigate some \nof these concerns, as well as their self-interest in being \nA Blueprint for the National Research Cloud 32\nCHAPTER 2\nconsidered for future government contracts. But the risks \nare nonetheless still present. \nSecond, the usability and the feature set of the software \nstack for public infrastructure is by no means proven. One \nof the most common hurdles to researcher adoption of \ncloud computing lies in the usability of systems,73 and \npublic infrastructure has less of a track record of easing that \nonboarding path at the contemplated scale. This is why we \nrecommend a pilot to assess whether a national HPC center \ncan be administered in a way to ensure the ease of cloud \ntransition and software stack that researchers have become \naccustomed with private providers. \nThird, policymakers would also need to account for \ncosts of maintaining and administering the system.74\nThey would need to find facilities to house and manage \nthe hardware and to account for the high energy costs \nof running an HPC cluster, as well as disaster prevention \nand recovery cost.75 These costs are significant. In 2021, \nthe Oak Ridge Leadership Computing Facility requested \n$225 million to operate all of its systems.76 The Argonne \nLeadership Computing Facility, in turn, requested $155 \nmillion.77 Furthermore, the lifecycle of DOE HPC systems \nhas traditionally been about seven years, after which new \nsystems are built and old ones decommissioned.78 While \nit is uncertain what the lifespan of newer systems will be, \nthis seven-year figure would lead us to argue that the NRC \nshould expect to either upgrade its systems or build new \nones with some degree of regularity. \nLast, giving the federal government greater control \nover the computing resources would not immediately \nmake the NRC safe from attacks.79 As with using \ncommercial cloud infrastructure, security will primarily be \ncontingent on the NRC’s implemented data access model.80\nWe discuss security issues in depth in Chapter 8.\nKEY TAKEAWAYS\n Significant compute \npower: Fugaku \nwas the fastest \nsupercomputer in \n2020.\n New application \nprocess for \ncompute power:\nApplications were \nsolicited to test out \nthe supercomputer \non a host of tasks \nand have control \nover who received \ncompute power. \nCASE STUDY: FUGAKU\nIn 2014, Japan’s Ministry of Education, Culture, Sports, Science and \nTechnology launched a public-private partnership between the government\u0002funded Riken Institute, the Research Organization for Information Science and \nTechnology (RIST), and Fujitsu to create the supercomputer successor to the K \ncomputer that supports a wide range of scientific and societal applications.81 The \nresult was Fugaku, which was named the world’s fastest supercomputer in 2020.82\nThe technical aim of Fugaku was to be 100 times faster than the previous \nK computer, with a performance of 442 petaFLOPS in the TOP500’s FP64 high \nperformance LINPACK benchmark.83 It currently runs 2.9 times faster than the \nnext fastest system (IBM Summit)84 and is composed of slightly over 150,000 \nconnected CPUs, with each CPU using ARM-licensed computer chips.85 Despite \nhaving around 1.9 times more parts than its K computer predecessor, Fugaku \nwas finished in three fewer months.86 The six-year budget for Fugaku was around \n$1 billion.87\nRIST solicited proposals for usage through the “Program for Promoting \nResearch on the Supercomputer Fugaku.” Under the program, Fugaku has already \nbeen used to study the effect of masks and respiratory droplets in order to inform \nJapanese policy during the COVID-19 pandemic.88 For FY 2021, 74 public and \nindustrial projects were selected for full-scale access to Fugaku.89 Currently, RIST \nis still requesting proposals that fall under specific categories of usage, and any \ninterested researcher may apply.90\nA Blueprint for the National Research Cloud 33\nCHAPTER 2\nCOST COMPARISON\nTo conclude this chapter, we provide a rough cost \ncomparison between a leading commercial cloud \nservice and a dedicated government HPC system (IBM \nSummit) (see Appendix A for details). We refer the reader \nto substantial work that has been published on the \neconomics of cloud computing for a fuller analysis, much \nof which emphasizes the variance in computing demand.91\nBuilding standalone public infrastructure is projected \nto be less expensive than implementing the NRC through \na vendor contracting arrangement over five years. At a \n10 percent discount on standard rates over five years, \nand under constant usage, AWS’s more powerful cloud\u0002computing option (known as P3 instances) could cost 7.5 \ntimes as much as Summit’s total estimated costs, using \ncomparable hardware. We use a 10 percent discount that \nwas negotiated by a major research university with a \ncommercial cloud provider. In contrast, the government \nwould need to negotiate an 88 percent discount for AWS \nto be cost-competitive with a dedicated HPC cluster in the \nlong run. Even in a scenario where NRC usage fluctuates \ndramatically, commercial cloud computing could cost 2.8 \ntimes Summit’s estimated cost. (While variability in usage \nfactors heavily into these estimates, the use of schedulers \ncan contribute to a leveling out of demand.92) \nThese cost estimates have important limitations. First, \ngovernment may be able to negotiate the cost down. We \nhave used as a benchmark one major university’s enterprise \nagreement with AWS, which provides a 10 percent discount, \nrelative to market rates. But, unless the negotiated discount \nis orders of larger magnitude, the commercial cloud will \nremain significantly more expensive. Second, these cost \nestimates primarily focus on computing.93 As Compute \nCanada’s analysis showed, the cost difference in storage \nwas even greater. Third, the use of commercial rates is \nlikely more favorable to cloud vendors, as government \nsecurity standards typically increase rates due to \nregulatory requirements. For instance, a “data sovereignty” \nrequirement for data and hardware to reside within the \nUnited States, or private cloud requirements for certain \nagency datasets, may increase the cost of commercial cloud \ncomputing significantly. Fourth, this simple cost comparison \nis static, and does not reflect changes in hardware costs \nand pricing structures that are likely to occur over a five\u0002year period under rapidly changing market conditions. \nBut, if the NRC in fact scales, systems would be procured \nincrementally over time, upgrading available resources \nand providing options at different price points, similar to \ncurrent commercial options. Last, as noted above, these \ncost estimates take into account maintenance as budgeted \nfor the Summit, but may not take into account all such \nnon-hardware costs, which is why we recommend a pilot \nto explore the ability to open up government computing \nfacilities to NRC users. \nIn short, we offer this simple comparison to highlight \nsome of the salient cost considerations to the make-or-buy \ndecision, which arrives at a very similar conclusion to the \nanalysis done by Compute Canada. \nA Blueprint for the National Research Cloud 34\nCHAPTER 2\nCASE STUDY: COMPUTE CANADA\nCompute Canada formed in 2006 as a partnership between Canada’s regional \nacademic HPC organizations to share infrastructure across Canada.94 The \norganization’s stated mission is to “enable excellence in research and innovation \nfor the benefit of Canada by effectively, efficiently, and sustainably deploying a \nstate-of-the-art advanced research computing network supported by world-class \nexpertise.”95\nCompute Canada’s infrastructure includes five HPC systems that are hosted \nat research universities across Canada.96 From 2015-2019, Compute Canada \nused about $125 million (CAD) in funding to build four of these systems.97 They \nalso investigated using commercial cloud resources instead of building these \nnew systems.98 However, they ultimately concluded that relying on commercial \ncloud providers would be significantly more expensive and could not provide \nthe desired latency for large-scale, data-intensive research.99 In 2018, Compute \nCanada requested $61 million (CAD) to fund its operations, budgeting $41 million \n(CAD) for operating its HPC systems and $20 million (CAD) on support, training, \nand outreach.100 Demand for Compute Canada’s HPC resources far exceeds the \ninfrastructure’s current capacity and is expected to keep growing.101 In 2018, \nCompute Canada estimated they would need about $90 million (CAD) per year \nover five years to invest in expanding infrastructure to the point where it could \nmeet projected demands.102\nAbout 16,000 researchers from all scientific disciplines use Compute Canada’s \ninfrastructure to support their work.103 Compute Canada distributes its resources \nin two ways. First, Principal Investigators and sponsored users may request a \nscheduler-unprioritized resource allocation for their research group.104 Compute \nCanada finds that many research groups can meet their compute needs this way.105\nAlternatively, researchers who need more or prioritized resources may submit a \nproject proposal to the annual “Research Allocation Competitions.”106 Submitted \nproposals go through a scientific peer review and a technical staff review to \nrate their merits.107 Scientific review examines the scientific excellence and \nfeasibility of the specific research project, the appropriateness of the resources \nrequested to achieve the project’s objectives, and the likelihood that the resources \nrequested will be efficiently used.108 This review is conducted on a volunteer \nbasis by 80 discipline-specific experts from Canadian academic institutions.109\nTechnical review is conducted by Compute Canada staff itself, who verify the \naccuracy of the computational resources needed for each project, based on the \ntechnical requirements outlined in the application, and makes recommendations \nabout which resources should be allocated to meet the project’s needs.110 In \n2021, Compute Canada received 651 applications to the Research Allocation \nCompetition and fully reviewed all applications in the span of five months.111\nKEY TAKEAWAYS\n Default access \nwith tiers: All \nPIs are eligible \nfor access to \na scheduler\u0002unprioritized \ncompute resource \nallocation with \nan application \nprocess built in for \nrequesting more. \nMost researchers \nfind the default \nallocation sufficient \nfor their needs.\n Widely used \nand increasing \ndemand: Compute \nCanada’s \ninfrastructure \nis widely used \nacross academic \ndisciplines, with \ndemand constantly \nexceeding \nresources. \nCompute Canada \nintends to \ninvest heavily in \ninfrastructure to \nmeet increasing \ndemands.\nA Blueprint for the National Research Cloud 35\nCHAPTER 3\nAfter compute resources, the next critical design decision for the NRC is how to \nboth store and provide its users access to datasets: the “data access” goal of the NRC. \nIndeed, as articulated in the original NRC call to action, government agencies should \n“redouble their efforts to make more and better quality data available for public \nresearch at no cost,” as it will “fuel” unique breakthroughs in research.1 Investigating \nsome of the most socially meaningful problems hinges on large but inaccessible \ndatasets in the public sector. From climate data housed by the National Oceanic and \nAtmospheric Administration (NOAA), health data from the country’s largest integrated \nhealthcare system in the Department of Veterans Affairs (VA), or employment data \nin the Department of Labor (DOL), such data could fuel both fundamental research \nusing AI and refocus efforts away from consumer-focused projects (e.g., optimizing \nadvertising) to more socially pressing topics (e.g., climate change). \nAs noted in the congressional charge, facilitating broad data access is a crucial \npillar of the NRC. Importantly, as we discuss below, we limit the scope of our \nrecommendations to facilitating access to public sector government data, which as \na condition of accessing government administrative data, NRC researchers should \nonly use for academic research purposes. NRC users should also be able to compute \non any private dataset available to them. There are available mechanisms for sharing \nsuch datasets, but we identify the NRC’s major challenge as providing access to \npreviously unavailable government data. \nGovernment data is intentionally decentralized. By design of the Privacy Act of \n1974, there is no centralized repository for U.S. government data or a core method \nfor linking data across government agencies.2\n The result is a sprawling, decentralized \ndata infrastructure with widely varying levels of funding, expertise, application of \nstandards, and access and sharing of policies. Thus, the NRC will have to develop a \nunified data strategy that can work with a wide range of agencies, unevenly adopted \nsecurity standards, and within existing data privacy legislation.\nPrevious efforts have sought to improve access to and sharing of federal data, \nboth between agencies and with external researchers, but there are still significant \nbarriers to enabling AI research access of the kind that the NRC demands.3\n By linking \ndata governance policies with access to compute, building on existing successful \nmodels, and working with agencies to create interoperable systems that satisfy \nsecurity and privacy concerns, the NRC can enable increased access to data that will \naid AI researchers in answering pressing scientific and social questions and increase \nAI innovation.4\nChapter 3: \nSecuring Data Access\nKEY \nTAKEAWAYS\n The NRC should adopt a tiered \nmodel for access to and storage \nof federal agency datasets. \nTiers should correspond to the \nsensitivity of the data.\n The NRC can help to harmonize \nthe fragmented federal data\u0002sharing landscape.\n The NRC should consider \nincentivizing agency \nparticipation by granting \nagencies that contribute data \nthe right to use NRC compute \nresources. \n The NRC should strategically \nsequence data acquisition \nby focusing first on low- to \nmoderate-risk datasets that are \ncurrently inaccessible.\n Due to legal constraints and \nmany outside options, the \nNRC should focus its efforts \non streamlining access \nto government datasets. \nResearchers should still \nbe permitted to use NRC \ncompute resources on \nprivate datasets, as long as \nresearchers certify they have \nrights to use such data.\nA Blueprint for the National Research Cloud 36\nCHAPTER 3\nWe will first explain why the NRC should focus its efforts \non facilitating federal government data sharing rather than \nprivate sector data sharing. We then examine how and why \nthe status quo for federal data sharing fails to realize the \nmassive potential of government data. While the concept \nof centralizing disparate data sources to unlock research \ninsights is not new,5 there are unique challenges for doing so \nwithin the context of the NRC. We will also discuss the key \nelements of our proposed model: (1) the use of FedRAMP as \na system for categorizing datasets based on their sensitivity, \nand for modifying access to them through tiered credentials \nfor NRC users; (2) promotion of interagency standardization \nand harmonization efforts to modernize data-sharing \npractices; and (3) strategic considerations regarding how \nto sequence efforts in streamlining access to particular \ndatasets. \nThe case studies included throughout this White \nPaper were chosen as exemplars of successful data\u0002sharing initiatives6\n and to illustrate the range of available \ndesign decisions. While each case study provides a unique \nglimpse into different approaches, some common themes \nemerge. First, many of the data-sharing entities we studied \nnot only have a single point of entry for researchers to \nrequest access, but also allow government agencies to \nretain some control over access requirements to their data. \nAs we discuss below, this conception of the NRC as a data \nintermediary would provide real benefits in streamlining \ndata access while still maintaining trust among agencies \nthat wish to protect their data. Second, some initiatives \nuse funding and personnel training as carrots to incentivize \nagencies to engage in data sharing. The NRC can learn \nfrom these initiatives in formulating its own set of \nincentives for agencies. \nPRIVATE DATA SHARING\nShould the NRC affirmatively facilitate private dataset \nsharing? While there are definite benefits to providing \nresearchers with access to private data,7 the NRC will \nhave its largest impact by focusing its efforts first on \nmechanisms to access and share government data. \nAs an initial matter, a variety of mechanisms for \ngeneral data sharing already exist.8 Private sector \nstakeholders, moreover, can and have often built their own \nin-house platforms to allow access to approved datasets \nwhile minimizing intellectual property concerns,9\n or \nprovide access to their application programming interfaces \n(APIs) to make open-source data more easily accessible.10\nBy focusing on providing access to public sector \ndata, notably administrative data that is traditionally \ninaccessible to most researchers,11 the NRC would play a \nunique and pertinent role for researchers across disciplines \nwithout having to deal with complex private-sector data \nconcerns or the need to incentivize participation by \nnongovernment actors. \nComplex intellectual property \nconcerns would arise from the NRC \npermitting, facilitating, or even \nrequiring private sector stakeholders \nand independent researchers to \nshare their private data freely \nalongside public sector data. \nComplex intellectual property concerns would arise \nfrom the NRC permitting, facilitating, or even requiring, \nprivate sector stakeholders and independent researchers \nto share their private data freely alongside public sector \ndata. First, this would involve complex questions regarding \nwhat licenses should be available or mandated for \nNRC users in order to encourage data sharing, despite \napprehensions of how such sharing may affect future \nprofitability and commercialization. While mandating \nan open-source (e.g., Creative Commons) license would \nbenefit researchers most by providing the broadest access \nto data and would benefit NRC administrators by removing \nsome possible IP infringement concerns, private sector \nstakeholders may feel deterred from uploading as a result. \nConversely, if users have a choice to adopt a license that \nA Blueprint for the National Research Cloud 37\nCHAPTER 3\nallows them to preserve their IP rights, private sector \nstakeholders may feel more comfortable sharing their \ndata, but this would shift some liability to users—or to the \nNRC itself—by relying on users to abide by the license. This \nwould involve an emphasis on enforcement, ranging from \nexplanations and user disclaimers to the industry standard \nof a full-blown notice-and-takedown system. \nData owners may want to prevent the uploading \nof copyrighted works by, for instance, having the NRC \nitself assess whether private data is already protected \nby copyright. Industry standards for conducting data \ndiligence, using manual or automated tools, would either \nbe very labor intensive12 or prohibitively expensive.13 Even \nif these industry standards were met, researchers may find \nan NRC data-sharing platform duplicative. \nNone of the above would prevent researchers from \nusing NRC compute resources on their own private \ndatasets. Like current cloud providers, the NRC can \nstipulate in an End User Licensing Agreement (EULA) that \nresearchers must agree they own the intellectual property \nrights on the data they are using.14 This EULA can also \nassign liability to the end user, rather than the NRC, for any \nuse of data that is encumbered by existing IP provisions. \nAdditionally, the discussion above pertains to whether \nresearchers should be required to share their private data, \nnot to whether researchers should be required to share the \noutputs of their research conducted on the NRC. The latter \npoint is discussed in Chapter 9.\nTHE CURRENT PATCHWORK \nSYSTEM FOR ACCESSING \nFEDERAL DATA\nThe NRC could play a pivotal role streamlining \naccess to government data in a system that is currently \ndecentralized.15 In some cases, agencies may simply \nlack a standardized method for sharing data.16 Due to \nperceived legal constraints, risks, or security concerns, \nagencies often have little practical incentive to share their \ndata.17 Successful examples of researchers gaining access \nto government data from individual agencies frequently \nrely on the researchers having personal relationships \nwith administrators, and a willingness on the part of the \nadministrator to push against these constraints in service \nof the research project.18 While this relationship-based \nprocess has produced some successes,19 the far more \ncommon outcome is that data is simply not shared or \naccessed by researchers.20 Indeed, one government official \nindicated that overcoming the obstacles to making certain \ngovernment data available for research was the greatest \nchallenge in a lengthy career.\nOne government official \nindicated that overcoming the \nobstacles to making certain \ngovernment data available \nfor research was the greatest \nchallenge in a lengthy career.\nAgencies typically require the recipient of the data \nto abide by a data-use agreement (DUA). These DUAs \nprescribe such limitations on data usage as the duration \nof use, the purpose of use, and guarantees on the privacy \nand security of data.21 However, DUAs suffer from a \ncentral problem: The process for negotiating DUAs is \nhighly fragmented and inconsistent across government \nagencies, drastically increasing the complexity in obtaining \napprovals for them.22 Some agencies have a designated \noffice or process to handle DUAs, but other agencies rely \non extemporaneous processes and ad hoc, quid pro quo \narrangements.23 One such example is the Research Data \nAssistance Center, a centralized unit within the Centers \nfor Medicare & Medicaid Services (CMS) dedicated to \nsupporting data access requests.24 In contrast, DUAs within \nthe Department of Housing and Urban Development and \nthe Department of Education are handled in decentralized \nbusiness units, each with different routing channels and \nlegal teams, which can confuse reviewers when multiple \nA Blueprint for the National Research Cloud 38\nCHAPTER 3\ndata requests between the same parties are routed \nsimultaneously but separately.25 Indeed, university DUA \nnegotiators in one survey complained that the process was \na game of “bureaucratic hot potato” and wondered, “Why \nisn’t there just one template for everything?”26 Ultimately, \nthe lack of standardization means that DUAs often require \nextensive review and revision, creating substantial delays.\nAgency-by-agency requirements also impede data \nsharing. These requirements can range from mandating \nthat researchers only access data at an onsite facility, using \ngovernment-authorized equipment, to capping the amount \nof computational cycles that can be used to analyze data, \nor restricting the amount of data available simultaneously.27\nThese restrictions are particularly problematic, given that \nmodern AI models can require massive amounts of data and \ncomputation to be most effective. \nBroadly, the reasons for this dysfunction range \nfrom valid concerns about security and liability to the \nmundane and prosaic. Information technology systems \nwithin some agencies operate literally decades behind the \ntechnological frontier; a 2016 report from the Government \nAccountability Office (GAO) detailed examples of these \nlegacy systems, discussing how several agencies were \ndependent on hardware and software that were no longer \nupdateable, and required specialized staff to maintain.28\nA lack of incentives, a risk-averse culture, and an agency’s \nstatutory authority also play an important role in enabling \nor obstructing data sharing.29\nWe are by no means the first observers to note \nthese problems. Advocates have been working for years \nto standardize and modernize government practices \naround data and technology.30 For example, the Federal \nData Strategy is the culmination of a multiyear effort to \npromulgate uniform data-sharing principles to address \nthe fact that the United States “lacks a robust, integrated \napproach to using data to deliver on mission, serve the \npublic, and steward resources.”31 However, substantial \nchallenges remain, particularly since the bulk of the efforts \nfocused on opening access to government data have \nnot been undertaken with the specific needs of machine \nlearning and AI in mind. \nTIERED DATA ACCESS AND STORAGE\nThe decentralized nature of government data has \ncascading implications across many aspects of the \ngovernment data ecosystem. One key area that will affect \nthe NRC is a lack of consistent storage and authentication \naccess protocols across government agencies. \nBecause many government datasets contain sensitive \ndata (e.g., high risk due to individual privacy concerns),32\na crucial component of the NRC’s data model will consist \nof a tiered storage taxonomy that distinguishes between \ndatasets based on their sensitivity and correspondingly \nrestricts access to different research groups. Interpreting \ntiered storage and access as two sides of the same coin, \nwe reference existing models that are based on dataset \nrisk levels and propose a framework for the NRC that \naims to achieve the dual goals of streamlining the process \nof enabling research access to government data while \nmaintaining privacy and security. \nFedRAMP: A tiered framework for data storage \non the cloud\nOne type of tiered storage taxonomy already exists for \nthird party government cloud services in one of the federal \ngovernment’s major cybersecurity frameworks, the Federal \nRisk and Authorization Management Program (FedRAMP).33\nEnacted in 2011, the framework was designed to govern all \nfederal agency cloud deployments, with certain exceptions \ndetailed in Chapter 8 of this White Paper. FedRAMP offers \ntwo paths for cloud services providers to receive federal \nauthorization. First, an individual agency may issue what \nis known as an authority-to-operate (ATO) to a cloud \nservice provider after the provider’s security authorization \npackage has been reviewed by the agency’s staff and \nthe agency has identified any shortcomings that need \nto be addressed.34 These types of ATOs are valid for each \nvendor across multiple agencies, as other agencies are \npermitted to reuse an initial agency’s security package \nin granting ATOs. The second option available to cloud \nservices providers is to obtain a provisional ATO from the \nFedRAMP Joint Authorization Board, which consists of \nrepresentatives from the Department of Defense (DOD), \nthe Department of Homeland Security (DHS), and the \nA Blueprint for the National Research Cloud 39\nCHAPTER 3\nGeneral Services Administration (GSA). These provisional \nATOs offer assurances to agencies that DHS, DOD, and the \nGSA have reviewed security considerations, but before \nany specific agency is allowed to use a vendor’s services, \nthat agency must issue its own ATO.28 In both the first and \nsecond cases, FedRAMP categorizes systems into low, \nmoderate, or high impact levels (see Table 2).\nBecause FedRAMP requirements apply to all federal \nagencies when federal data is collected, maintained, \nprocessed, disseminated, or disposed of on the cloud, \nthe NRC itself will need to be compliant with FedRAMP \nsecurity standards irrespective of the organizational form \nit takes.35 Every dataset brought on to the NRC would \nneed to be reviewed under FedRAMP with appropriate \naccess levels. If a cloud service has already been evaluated \nunder FedRAMP because it was used in the past to house \nfederal data, the service can inherit the same FedRAMP \ncompliance level in the NRC without an additional \nevaluation.36\nBesides classifying datasets, the other function of \nFedRAMP is to identify a comprehensive set of “controls,” \ni.e., requirements and mechanisms that the cloud service \nproviders must implement before the government dataset \ncan be housed on them.37 They are based on the National \nInstitute of Standards and Technology (NIST) Special \nPublication 800-53, which provides standards and security \nrequirements for information systems used by the federal \ngovernment.38\nThese controls range widely and include requirements \nsuch as ensuring that the organization requesting \ncertification “automatically disables inactive accounts,” \n“establishes and administers privileged user accounts \nin accordance with a role-based access scheme that \norganizes system access and privileges into roles,” \n“provides security awareness training on recognizing \nand reporting potential indicators of insider threat,” or \ndevelops regular security plans in the event of a breach.39\nRequirements get more strenuous for FedRAMP “high \nimpact” data (e.g., creating system level air-gaps to protect \nsensitive data).40\nLEVEL TYPE OF DATA IMPACT OF DATA BREACH NUMBER OF \nCONTROLS\nLow-impact risk\n- Low baseline\n- Low-impact SaaS\nData intended \nfor public use\nLimited adverse effects; preserves the safety, \nfinances, reputation, or mission of an agency 125\nModerate-impact risk\n- E.g., personally \nidentifiable information\nControlled \nunclassified data \nnot available to \nthe public\nCan damage an agency’s operations 325\nHigh-impact risk\n- E.g., law enforcement, \nhealthcare, emergency \nservices\nSensitive federal \ninformation\nCatastrophic impacts such as shutting down \nan agency’s operations, causing financial ruin, \nor threatening property or life\n421\nTable 2: FedRAMP levels are designated based on the degree of risk associated with the breach of an information system. The security baseline levels \nare based on confidentiality, availability, and integrity, as defined in Federal Information Processing Standard 199.41\nA Blueprint for the National Research Cloud 40\nCHAPTER 3\nThere can be significant costs with obtaining these \ncertifications and creating compliance plans, even if the \nunderlying technical specifications can be addressed or \nalready exist. A key issue for structuring the NRC is that the \nprincipal burdens of ensuring FedRAMP compliance should \nfall with NRC institutional staff, not originating agencies or \nindividual academic researchers. As part of the FedRAMP \ncertification process, NRC staff will have to consider how to \ngive access to PIs in compliance with FedRAMP rules, but \nthat process can and should avoid requiring originating \nagencies or individual universities to incur substantial \nexpenses associated with hiring consultants and attorneys \nto certify FedRAMP compliance.42\nWhile FedRAMP sets out common standards for \ncloud storage of government data within agencies,43\nit is an exception to an otherwise balkanized federal \ndata-sharing standards landscape,44 though it does not \nfacilitate data exchange. The NRC needs to maintain \ncompliance with not only FedRAMP requirements but \nalso the requirements of any agency it is partnering with \nfor data access.45 Advocates interested in increasing \ngovernment data availability have long fought to establish \na universal FedRAMP equivalent across different agencies \nthat provides shared standards for data sharing based on \ndata sensitivity.46 As we discuss in Chapter 8, establishing \nsuch universal, “centralized” security standards not only \nensures internal uniformity but also removes barriers to \ndata sharing.\nThe NRC’s implementation of FedRAMP standards can \nalso provide partnering agencies an important opportunity \nto reexamine their own standards and share best practices \nwith one another.47 This could involve raising or lowering \nrequirements that are out of date,48 given the current \nthreat to the environment and research needs. The NRC \ncan take inspiration from agencies’ best practices, as well \nas from FedRAMP to develop a common NRC standard for \ndetermining data to be high, moderate, or low risk, as well \nas what consequences should flow from that assessment. \nIn the later section on strategic considerations, we discuss \nhow to enable this process by incentivizing agencies to \nparticipate in the NRC and selecting datasets that present \na lower privacy and security risk. \nIn addition, given the diversity of data types and \nsources that could be stored on the platform, NRC \npolicy should ensure that standards and protections \nexist for data storage in areas where FedRAMP has \nblind spots. FedRAMP is in part animated by risks from \nmalicious actors like cybercriminals or adversarial foreign \ngovernments, but as we discuss in Chapter 6, privacy \nrisks may arise even for the intended use case of analysis \nby NRC researchers. Of particular concern are instances \nwhere disparate datasets are combined, which may allow \nnew inferences that make previously anonymous data \nindividually identifiable, even when the data itself did not \ncontain identifiable information.49 Such combinations \nmay also alter the original risk level of the data, creating \nan output that merits a higher risk classification. \nFurthermore, machine-learning models and \nrepresentations may unintentionally reveal properties of \nthe data used to train them,50 and dissemination of these \nmodels could pose privacy risks.\nThis is not a challenge unique to the NRC; the U.S. \nCensus Bureau and other government agencies engaged \nin data linkage have also had to develop means to \naddress this issue.51 One solution involves applying \nmethods of additional noise to the data (differential \nprivacy) in order to obfuscate individual data while \npreserving the data’s utility for research. We discuss it \nand other privacy-enhancing technologies in greater \ndetail in Chapter 7.\n52 However, privacy-enhancing \ntechnologies are no panacea, and depending on the \nnature of the particular dataset, the goals of ensuring \nanonymization, while also enabling researchers to access \nfine-grained data can conflict. \nThe NRC can also draw from the “Five Safes” data \nsecurity framework used by the UK Data Service,53 the \nFederal Statistical Research Data Centers Network, \nand the Coleridge Initiative, a model centered on data, \nprojects, people, access settings, and outputs.54 The \nimplementation of the 2019 Evidence Act is already \nusing a similar Five Safes framework in making \ndeterminations around data linkage.55 Through a \ncombined framework, the NRC could place different \nanonymization requirements on datasets, depending \non the circumstances of their access and the privacy \nA Blueprint for the National Research Cloud 41\nCHAPTER 3\nagreements through which they were collected. Similarly, \nthe NRC could control the dissemination scope of models, \ncode, and data, depending on the sensitivity. Theoretical \nidentifiability is less likely to be a concern when access \nand dissemination is restricted and the data is of a less \nsensitive nature or is not about individuals at all.56\nFacilitating Researcher Access with a Tiered \nAccess Model\nHow should researchers gain access to specific data \nresources? Currently, approval proceeds on an agency\u0002by-agency basis.57 Just as the value of the NRC for \nsupporting AI research will depend in part on the extent \nto which it can bring together datasets from different \nagencies, it will also depend on the extent to which it can \nstreamline the process for accessing data. One way to \nachieve this streamlining will be through a tiered access \nsystem for the NRC users, similar to FedRAMP’s tiered \nsystem for storing federal data on the cloud, where higher \ntiers would enable access to higher-risk data, subject to \nthe other requirements on compute and data use. We \ndiscuss this access system in more depth in Chapter 7.\nChapter 2 made the case that compute access should \nstart with PIs at academic institutions. This authorization \ncan also serve as the baseline, where all NRC-registered \nPIs can freely access and use low-risk datasets on the \nNRC. Additional tiers would impose more requirements, \nsuch as citizenship, security clearance, distribution \nrestrictions, or compute and system restrictions. These \naccess tiers will be similar to those used for determining \nFedRAMP classification for data storage, but while access \nand storage sensitivity may invoke similar considerations; \nthey might not necessarily be the same. \nA Blueprint for the National Research Cloud 42\nCHAPTER 3\nKEY TAKEAWAYS\n Balances providing \nconsistent restricted \ndata access with agency \nrequirements: The ADRF \nbalances building in each \nrestricted data set’s access \nand export review form in \na consistent manner into \nthe data portal with agency \nrequirements for data \naccess and export. This \nallows agencies to control \naccess to their data, while \nproviding a single point \nof entry for researchers. \nCurrently, the platform only \nsupports data consistent \nwith a FedRAMP Moderate \ncertificate. \n Standardized for both users \nand data stewards: Along \nwith each data access for \nusers, a point of contact at \nthe agency providing the \ndata is given access to the \nplatform as well. This allows \neasy access to approve and \ntrack projects, and work \nwith the ADRF on access \nrequirements. \n Five Safes’ data security \nframework: The ADRF \nensures data security by \nfocusing on five aspects—\ndata, projects, people, \nsettings of access, and \noutputs.\n Training as a core function: \nThe ADRF hosts workshops \nand trains government \nemployees and other \nresearchers on data use.\nCASE STUDY: COLERIDGE INITIATIVE \n(ADMINISTRATIVE DATA RESEARCH \nFACILITY)\nIn partnership with the Census Bureau and funding from the Office \nof Management and Budget, the Coleridge Initiative, a nonprofit \norganization, launched the Administrative Data Research Facility \n(ADRF), a secure computing platform for governmental agencies to \nshare and work with agency micro-data.58 The ADRF is available on \nthe Federal Risk and Authorization Management Program (FedRAMP) \nMarketplace and has a FedRAMP Moderate certification. Currently, the \nplatform supports over 100 datasets from 50 agencies.59\nThe ADRF provides access to agency-sponsored researchers and \nagency-affiliated researchers going through the ADRF training programs \nfor free. Over the past three years, over 500 employees from approximately \n100 agencies have gone through ADRF training programs.60\nThe ADRF provides a shared workspace for projects and the \nData Explorer, a tool to view an overview and metadata (name, field \ndescription, and data type) of available datasets on the ADRF.61 In \norder to access restricted data, users must meet review requirements \nset by the agency providing the data. In order to export data, users \nmust go through a unique “Export Review” process.62 The ADRF has a \nhighly involved default review process, requiring researchers to submit \nall code and output for the project for approval to the data steward \nand generating additional charges, if requesting export of more than \n10 files.63 The agency providing the data can also amend the default \nreview process, if it wishes to do so. \nPrior to transferring data files, the ADRF provides an application for \ndata hashing to safely transmit data.64 The ADRF also follows the “Five \nSafes” security model used by other government agencies, such as the \nUK Data Service.65\nData stewardship for the ADRF is defined in compliance with \nthe Title III of the Evidence-Based Policymaking Act of 2018.66 Once \na restricted dataset is shared with the ADRF, one person within the \nagency will be assigned the data steward for all project requests. \nFrom there, procedures are developed with the agency, in terms of \nexpectations for how the data will be protected, authorized users, and \naudit procedures for continued compliance. \nData stewards have access to an online portal in the ADRF. All \nproject requests for specific data are routed to the data steward \nthrough this proposal. Once access has been granted, the data steward \nalso has options to monitor the project for compliance. \nA Blueprint for the National Research Cloud 43\nCHAPTER 3\nThe NRC can \nemulate both the \nColeridge Initiative \nand Stanford’s \nCenter for Population \nHealth Sciences \n(PHS), for instance, \nwhich serve as data \nintermediaries, in \nfacilitating access to \ngovernment data. \nKEY TAKEAWAYS\n Mixed data architecture, yet consistent user \nexperience: PHS utilizes a mix of on-premises \nand cloud data services, but still seeks to \nprovide a consistent user experience.\n Restricted data access has a single point \nof entry: The PHS Data Portal standardizes, \ncentralizes, and simplifies data access \nrequirements and trainings, rather than pointing \nusers to a time-consuming process working with \neach data steward directly.\n Reduces costs and time associated with \nprocuring data: PHS leverages existing \nrelationships with agencies to consolidate \ndatasets in a single portal, saving researchers \nthe time and money necessary to gain access \nthrough individual agency requests.\nExisting models for researcher access to sensitive datasets can help paint a \npicture of how the NRC might maintain and monitor a tiered access system. The \nNRC can emulate both the Coleridge Initiative and Stanford’s Center for Population \nHealth Sciences (PHS),67 for instance, which serve as data intermediaries, in \nfacilitating access to government data. Indeed, these intermediaries have been \ndocumented as effective means to overcome barriers to data-sharing because they, \nat their core, negotiate and streamline relationships between data contributors and \nusers.68 For example, as a trusted intermediary, the NRC could centralize the DUA \nintake process by promulgating a universal standard form for agency DUAs.69\nFurthermore, similar to the Coleridge Initiative example, a designated \nrepresentative(s) within the agency could be assigned as the data steward for all \nproject requests for a certain restricted dataset. Any project requiring access to data \nin higher tiers could commence only after its proposal was reviewed and approved \nby a relevant representative. Because NRC access begins with PIs, researchers would \nalso have to obtain approval from their university Institutional Review Boards (IRBs), \nas needed. After project approval and NRC researcher clearance, data would be \nmade available through the NRC’s secure portal. Any violations of the terms of use \nor subject privacy could result in penalties ranging from a demotion of access tier to \nremoval of NRC privileges or professional, civil, or criminal penalties, as relevant.\nCASE STUDY: STANFORD \nCENTER FOR POPULATION \nHEALTH SCIENCES\nThe Stanford Center for Population Health \nSciences (PHS) provides a growing set of population \nhealth-related datasets and access methods to \nStanford researchers and affiliates.70 The PHS Data \nEcosystem hosts high-value datasets, data linkages \nand filters, and analytical tools to aid researchers. \nThe PHS partners with a wide range of public, \nnonprofit, and private entities to license population\u0002level datasets for university researchers, ranging from \nlow-risk, public datasets to restricted data containing \nProtected Health Information (PHI) and Personally\u0002Identifiable Information (PII), such as Medicare, \ncommercial claims such as Optum and Marketscan \ndata,71 and electronic medical records. \nA Blueprint for the National Research Cloud 44\nCHAPTER 3\nCASE STUDY: STANFORD CENTER FOR POPULATION \nHEALTH SCIENCES (CONT’D)\nIn addition to secure data storage and computational tools for researchers, PHS provides standardized \nand well-documented data access and management protocols, which increases data proprietor comfort \nwith sharing data. PHS also has full-time staff who cultivate and maintain relationships with organizations \nholding data. This allows PHS to work with these groups to centralize data hosting and provide secure \naccess to a wide array of researchers. \nThe PHS Data Portal is hosted on a third-party platform that enables data discovery, exploration, and \nclearly delineated, standardized steps for data access. The third-party platform, Redivis, utilizes a four-tier \naccess system: (1) overview of data and basic documentation; (2) metadata access, including definitions, \ndescriptions, and characteristics; (3) a 1 or 5 percent sample of the dataset; and (4) full data access.72\nIf data is classified as public, researchers can access it using specialized software, or simply download \nit directly.73 For restricted data, the portal has forms integrated to easily apply for access.74 After identifying \nthe dataset, the researcher must apply for membership in the organization hosting the data.75 An \nadministrator of the organization owning the dataset can set member and study requirements that must \nbe met, including training and institutional qualification, in order to access the data. Member applications \ncan be set to auto-approval or require administrative approval. Once access has been granted to a data \nset, researchers can manipulate the data using specialized software. Usage restrictions are also specified \nindividually on each dataset to control whether full, partial, or no output can be exported, and what review \nlevel is required for exporting. All applications for data and export are handled directly on the Data Explorer \nplatform.\nCurrently, the PHS Data Portal is primarily for Stanford faculty, staff, students, or other affiliates.76 Even \nwith affiliate status, certain commercial datasets may require further data rider agreements for access. \nNon-Stanford collaborators must complete all of the same access requirements as Stanford affiliates, plus \nany requirements imposed by their own institution. Additionally, a “data rider” agreement on the original \nDUA is frequently necessary.77 \nTo work with restricted data, the PHS provides two computing services for high-risk data: (1) Nero, \nwith both an on-premises and Google Cloud Platform (GCP) platform versions; and (2) PHS-Windows \nServer cluster.78 Both are managed by the Stanford Research Computing Center (SRCC). Both services are \nHIPAA compliant.79 Unrestricted data can be used on any of Stanford’s other computational environments \n(Sherlock, Oak) or simply downloaded to the researcher’s local machine.\nA Blueprint for the National Research Cloud 45\nCHAPTER 3\nPROMOTING INTERAGENCY \nHARMONIZATION AND ADOPTION \nOF MODERN DATA ACCESS \nSTANDARDS\nThe federal data-sharing landscape suffers from \ndivergent standards and practices, and individual \nagencies, left alone, have traditionally faced high hurdles \nto harmonizing and modernizing their data access \nstandards.80 As we have discussed, this state of affairs \npresents formidable barriers to AI R&D from a researcher \nperspective, but is also problematic both from an agency \nand societal perspective. As a report by the Administrative \nConference of the United States finds from surveying the \nuse of AI in the federal government, nearly half of agencies \nhave experimented with AI to improve decision-making \nand operational capabilities, but they often lack the \ntechnical infrastructure and data capacity to use modern \nAI techniques and tools.81 The lack of a modern, uniform \nstandard for data sharing in AI research, therefore, makes it \nharder for agencies to realize gains in accuracy, efficiency, \nand accountability, which subsequently impacts citizens \ndownstream, who are affected by agency decisions.82\nThe lack of a modern, uniform \nstandard for data sharing in \nAI research makes it harder \nfor agencies to realize gains \nin accuracy, efficiency, and \naccountability, which subsequently \nimpacts citizens downstream, who \nare affected by agency decisions.\nThe NRC can help overcome agency reluctance to \nshare data by enabling access to agencies to compute \non their own data. This would solve at least two crucial \nproblems for government agencies. First, access to the \nNRC’s collective computing resources would overcome \nsome difficulties that agencies have traditionally faced \nin setting up their own compute resources.83 Second, \nfacilitating agency access to modern data and compute \nresources would attract and build further in-house \ngovernment AI expertise.84 From a societal perspective, \nthis could increase the government’s capabilities in the \nresponsible adoption of AI, help reduce the cost of core \ngovernance functions, and increase agency efficiency, \neffectiveness, and accountability.85\nThe NRC can also learn from and align with other \ninitiatives to harmonize and modernize standards. \nThe Evidence Act—which requires agencies to appoint \nchief data and evaluation officers—is one example. The \nlegislation authorizing the creation of the NRC could \nprovide a federal mandate to encourage adoption of \nsharing best practices.86 However, as we discuss in \nChapter 5, a federal mandate alone, without any additional \naid or incentives, may not be enough to incentivize \nharmonization of data access and sharing standards.87\nThe Task Force should therefore consider bundling the \nmandate with additional benefits, such as providing \nfunding to assist agencies in expanding their technical or \nstaff capabilities in furtherance of the NRC and the national \nAI strategy. The NRC is aligned with the existing bipartisan \ncase for the National Secure Data Service (NSDS) \n(described in the case study below), a service that would \nfacilitate researcher access to data with enhanced privacy \nand transparency, recommended by the Commission on \nEvidence-Based Policymaking in 2018. Both the NRC and \nNSDS are complementary data-sharing initiatives that \nhave the potential to considerably improve public service \noperational effectiveness. We elaborate further on the \nNSDS proposal in Chapter 5. Lastly, training programs are \npromising avenues to increase NRC adoption and agency \nsupport. For example, as described in the case study \nabove, the Coleridge Initiative has hosted workshops to \ntrain over 500 employees from approximately 100 agencies \non data use over the past three years. \nA Blueprint for the National Research Cloud 46\nCHAPTER 3\nCASE STUDY: THE EVIDENCE ACT\nIn pursuit of greater, more secure access to and linkage of \ngovernment administrative data, a bipartisan Commission on Evidence\u0002Based Policymaking was set up by Congress in March 2016. The \ncommission’s final report88 included 22 recommendations for the federal \ngovernment to build infrastructure, privacy-protecting mechanisms,89\nand institutional capacity to provide secure access to public data for \nstatistical and research purposes. One recommendation was to create \na “National Secure Data Service” (NSDS) to facilitate access to data \nfor the purpose of building evidence, while maintaining privacy and \ntransparency. Through this service, the NSDS could help researchers by \ntemporarily linking existing data and providing secure access, without \nitself creating a data clearinghouse.\nThe Foundations for Evidence-Based Policymaking Act of \n201890 created some of the legislative footing for the commission’s \nrecommendations. In particular, it created new roles for chief data, \nevaluation, and statistical officials, and sought to increase access and \nlinkage of datasets previously within the scope of the Confidential \nInformation Protection and Statistical Efficiency Act (CIPSEA).91\nFinally, the 2020 Federal Data Strategy and associated Action \nPlan92 sought to put those legislative provisions into action. The \nstrategy included plans to improve data governance, to make data more \naccessible, to improve government use of data, and to boost the use and \nquality of data inventories, metadata, and data sensitivity.\nThe central remaining step envisioned by the initial Evidence-Based \nPolicymaking Commission is a National Secure Data Service (NSDS) \nmodeled on the UK’s Data Service.93 The UK’s Data Service provides \naccess to a range of public surveys, longitudinal studies, UK census \ndata, international aggregate data, business data, and qualitative \ndata. Alongside access, it provides guidance and training for data use, \ndevelops best practices and standards for privacy, and has specialized \nstaff who apply statistical control techniques to provide access to data \nthat are too detailed, sensitive, or confidential to be made available \nunder standard licenses.\nKEY TAKEAWAYS\n Priority data sharing where \nthere is a public service \noperational case: There is \nprecedent in large-scale \nadministrative data-sharing \ninitiatives justified on \ngrounds of improvements \nto public service \noperational efficiency and \neffectiveness.\n National Secure Data \nService (NSDS) initiative: \nThe NSDS is bolstered by \nbipartisan support.\n Institutional models \nthat balance external, \ninnovative talent and \ninternal, cross-agency \ninfluence: A Federally \nFunded Research and \nDevelopment Center \n(FFRDC) model, housed \nwithin an existing agency \n(NSF), may balance the \nability to bring in external \ntalent with internal agency \ninfluence.\nA Blueprint for the National Research Cloud 47\nCHAPTER 3\nSEQUENCING INVESTMENT INTO \nDATA ASSETS\nGiven the significant hurdles in negotiating data \naccess, the NRC will need to strategically sequence which \nagencies and datasets to focus on for researcher use. The \nfederal government collects petabytes of data,94 each with \nvarying degrees of restrictions or openness. In considering \nwhich datasets to prioritize, the NRC can draw from the \nexample of other data-sharing initiatives, as well as focus \non data sets in the short term that do not pose complex \nchallenges with regards to data privacy or sharing. One \nprivate sector example is Google Earth Engine, which \naggregated petabytes (approximately 1 million gigabytes) \nof satellite images and geospatial datasets, and then \nlinked that access to Google’s cloud-computing services \nto allow scientists to answer a variety of crucial research \nquestions.95 This process of aggregating complex data \nand hosting it in a friendly computing infrastructure to \nfacilitate research, demonstrates the compelling value \nof coupling compute and data. As another example, \nADR UK identifies specific areas of research that are of \npressing policy interest, such as “world of work,”96 and \nprioritizes data access for researchers working on those \ntopics. The UK Data Service offers datasets derived \nfrom survey, administrative and transaction sources, \nincluding productivity data from the Annual Respondents \nDatabase,97 innovation data from the UK Innovation \nSurvey,98 geospatial data from the Labour Force Survey,99\nUnderstanding Society,100 and sensitive data about \nchildhood development.101\nWhen prioritizing datasets and agencies for NRC \npartnership, we recommend the following criteria: \n• Data that is valuable to AI researchers, but is not \ncurrently available in a convenient form. For example, \nin a July 2019 request for comments, the Office of \nManagement and Budget (OMB) asked members \nof the public to provide input on characteristics of \nmodels that make them well-suited to AI R&D, what \ndata is currently restricted, and how liberation of \nsuch data would accelerate high-quality AI R&D.102\nIn one response, the Data Coalition argued that \ncontrolled release of private but structured indexed \ndata in data.gov would be valuable for research.103\nThe Data Coalition also urged agencies to consider \nreleasing raw, unstructured datasets, such as agency \ncall center logs, consumer inquiries and complaints, \nas well as regulatory inspection and investigative \nreports.104 Another example of data that is currently \nchallenging to access, but is a matter of public \nrecord, is electronic court records housed in a system \nby the Administrative Office of the U.S. Courts.105\n• Data housed within agencies that have statutory \nauthority to share data and/or that have previous \ndata-sharing experience. The Census Bureau, for \ninstance, has greater existing statutory interagency \nlinkage than other agencies, and has preexisting \nsubstantial in-house data analysis expertise.106\nThe U.S. Bureau of Labor Statistics has an existing \nprocess for sharing restricted datasets (in the \ncategories of employment and unemployment, \ncompensation and working conditions, and prices \nand living conditions) with researchers.107\n• Data with limited privacy implications. For example, \nagencies whose data concerns natural phenomena, \nrather than individuals, may be easier to manage \nfrom a privacy perspective—e.g., NASA, the US \nGeological Service, and the National Oceanic and \nAtmospheric Administration. Datasets like those \nhoused in NASA’s Planetary Data System,108 but that \nare not easily available to researchers, may serve \nas a valuable starting point for the NRC. Increasing \nthe availability and interoperability of datasets from \nthese agencies would advance the core mission of \nthe NRC and could be done without jeopardizing \nindividual privacy.\nA Blueprint for the National Research Cloud 48\nCHAPTER 4\nChapter 4: \nOrganizational Design\nWhat institutional form should the NRC take? Two overarching considerations \nare: (1) ease of access to data; and (2) ease of coordination with compute resources.1 \nAs we discussed in Chapter 3 and will detail in depth in Chapter 5, the federal \ndata-sharing landscape among agencies is highly fragmented, with many agencies \nreluctant to or legally constrained from sharing their data. The NRC will need to \ncoordinate between the entities supplying compute infrastructure and researchers \nthemselves. As the NRC’s goal is to provide researchers with access to government \ndata and high-performance computing power, one without the other will fall short \nof achieving the NRC’s mission.\nDrawing on extensive work in support of the Evidence Act, we recommend \nthe use of Federally Funded Research and Development Centers (FFRDCs) and \nprivate-public partnerships (PPPs) as possible organizational forms for the NRC. We \nrecommend the creation of an FFRDC at affiliated government agencies in the short \nterm, as we believe this path allows for the easiest facilitation of both the compute \ninfrastructure and access to government data. In the longer term, the establishment \nof a PPP could facilitate greater data sharing and access between the public and \nprivate sectors. Importantly, other options include creating an entirely new federal \nagency or bureau within an existing agency. While these options might simplify \ncoordination with compute resources, both pose challenges, with respect to data \naccessibility and interagency data sharing. \nFEDERALLY FUNDED RESEARCH AND \nDEVELOPMENT CENTER\nFFRDCs are quasi-governmental nonprofit corporations sponsored by a \nfederal agency but operated by contractors, including universities, other nonprofit \norganizations, and private-sector firms.2 The FFRDC model confers the benefits of \na close agency relationship, alongside independent administration, in facilitating \naccess to data. Due to their intimate subcontracting relationships with their parent \nagency, all FFRDCs benefit from data access that goes “beyond that which is \ncommon to the normal contractual relationship, to Government and supplier data, \nincluding sensitive and proprietary data.”3\nA recent report by Hart and Potok on the National Secure Data Service (NSDS) \n(see case study in Chapter 3) also supports the FFRDC model as an optimal way \nto facilitate access to and linkage of government administrative data.4\n The report \nconsidered FFRDCs, alongside such other institutional forms, as creating an entirely \nnew agency, housing the NSDS in an existing agency, and developing a university\u0002KEY \nTAKEAWAYS\n In the short term, the NRC \nshould be instituted as a \nFederally Funded Research \nand Development Center \n(FFRDC), which would reduce \nthe significant costs of \nsecuring data from federal \nagencies.\n In the longer term, a well\u0002designed, public-private \npartnership (PPP), governed \nby officers from Affiliated \nGovernment Agencies, \nacademic researchers, and \nrepresentatives from the \ntechnology sector, could \nincrease the quantity and \nquality of R&D, and reduce \nmaintenance costs. \n Instituting the NRC as a \nstandalone federal agency or \nbureau would face numerous \nchallenges, notably in securing \naccess to data housed in other \nagencies. \nA Blueprint for the National Research Cloud 49\nCHAPTER 4\nled, data-sharing service, but the report ultimately \nrecommended the FFRDC model for several reasons. An \nFFRDC can scale quickly, because it can access government \ndata and high-quality talent more easily than other \noptions.5 An FFRDC can also leverage existing government \nexpertise. The NSF, for instance, already sponsors five \nseparate FFRDCs and has extensive experience cultivating \nand maintaining networks of researchers.6\nHowever, the FFRDC model comes with a few \nlimitations. First, an FFRDC’s role is restricted to research \nand development for their sponsoring agency that “is \nclosely associated with the performance of inherently \ngovernmental functions.”7 Thus, it would be important to \nensure alignment during the contracting phase with the \nNRC’s core functions. \nSecond, the success of an FFRDC model for the NRC \nwill depend on the ability of the sponsoring agency to gain \ncooperation across the federal government to provide \ndata needed for research. One way to do this would be \nfor multiple agencies to co-sponsor the FFRDC, reducing \ncontracting friction for datasets.8 Another option would \nbe to create multiple FFRDCs housed in different agencies, \nincentivizing each of those agencies to share their data \nwith the respective FFRDC. An analogous example could \ninclude the National Labs as a network, where each \nNational Lab would be an instantiation of the NRC within \nits own relevant agency.9\nThird, multiple FFRDCs would require separate \nprocesses for compute resources. In the short term, \nthe NRC may alleviate this problem by contracting for \ncommercial cloud credits, which is likely already the short\u0002term solution for the NRC to provide compute access. As \ndiscussed earlier, private sector cloud providers already \nhave extensive experience in providing compute resources \nto the government10 and to academic institutions.11\nFamiliarity with these private cloud providers may reduce \nthe friction in allocating compute among researchers at \nmultiple FFRDCs.\nIn the longer term, the FFRDC model may not be the \nmost efficient. From a cost and sustainability perspective, \nFFRDCs have traditionally suffered from significant \noverruns, as they “operate under an inadequate, \ninconsistent patchwork of federal cost, accounting and \nauditing controls, whose deficiencies have contributed \nto the wasteful or inappropriate use of millions of federal \ndollars.”12 Another concern is that, historically, FFRDC \ninfrastructure has not been routinely updated. A 2017 \nDepartment of Energy report highlighted that FFRDC \ninfrastructure was inadequate to meet the mission.13 \nNASA’s Inspector General also highlighted that more \nthan 50 percent of the Jet Propulsion Laboratory (a NASA \nFFRDC) equipment was at least 50 years old.14 If an FFRDC \nversion of the NRC experiences these same challenges, \nwe recommend that the NRC, in the long run, switch to a \npublic-private partnership model.\nA Blueprint for the National Research Cloud 50\nCHAPTER 4\nKEY TAKEAWAYS\n Multiple agency \nco-sponsors: While \nSTPI’s primary \nsponsor is the \nNational Science \nFoundation, a \nnumber of other \nagencies also \nco-sponsor STPI, \nreducing difficulties \nin accessing data \nacross agencies.\n Expertise: While \nSTPI is staffed with \nits own employees, \nit can also tap into \nexpertise from \nthe hundreds of \nemployees at the \nInstitute for Defense \nAnalyses (IDA), the \norganization that \nmanages STPI. As \nan FFRDC, STPI can \nalso contract for \nadditional expertise \nas required.\nCASE STUDY: SCIENCE & TECHNOLOGY \nPOLICY INSTITUTE (STPI)\nSTPI is an FFRDC chartered by Congress in 1991 to provide rigorous objective \nadvice and analysis to the Office of Science and Technology Policy and other \nexecutive branch agencies.15 STPI is managed by the Institute for Defense Analyses \n(IDA), a nonprofit organization that also manages two other FFRDCs: the Systems \nand Analyses Center and the Center for Communications and Computing.16 IDA has \nno other lines of business outside the FFRDC framework.17\nSTPI’s primary federal sponsor is the National Science Foundation, but \nresearch at STPI is also co-sponsored by other federal agencies, including the \nNational Institute of Health (NIH), Department of Energy (DOE), Department of \nTransportation (DOT), Department of Defense (DOD), and Department of Health and \nHuman Services (HHS).18 Due to the “unique relationship” between an FFRDC and its \nsponsors, STPI “enjoys unusual access to highly classified and sensitive government \nand corporate proprietary information.”19\nNSF appropriations provide the majority of funding for STPI, including $4.7 \nmillion in FY 2020,20 but a limited amount of funding is also provided from other \nfederal agencies.21 STPI has approximately 40 full-time employees and has access \nto the expertise of IDA’s approximately 800 other employees.22 As an FFRDC, STPI \nmay also contract for expertise, as required for a particular project.23 The statute \nspecifying STPI’s duties also directs it to consult widely with representatives from \nprivate industry, academia, and nonprofit institutions, and to incorporate those \nviews in STPI’s work to the maximum extent practicable.24\nSTPI is also required to submit an annual report to the president on its \nactivities, in accordance with requirements prescribed by the president,25 which \nprovides additional accountability for the FFRDC. According to STPI’s 2020 report, \nSTPI worked across multiple federal agencies, supporting them on 48 separate \ntechnology policy analyses throughout 2020.26\nA PUBLIC-PRIVATE PARTNERSHIP (PPP)\nA Public-Private Partnership (PPP) would create a \npartnership between federal agencies and private-sector \norganizations to jointly house and manage data-sharing \nefforts and run compute infrastructure. Because different \nagencies and private-sector members may have different \ncontracting preferences, intellectual property goals, and \nsecurity allowances for data access, creating a data-sharing \npartnership within this patchwork framework could be \nchallenging in the immediate future. Nonetheless, PPPs \ncan provide a number of long-term benefits, as they have \nbeen used successfully as data clearinghouses to produce, \nanalyze, and share data between the public and private \nsector.27 Indeed, recognizing the benefits of the PPP model, \nthe European Union has launched a new initiative called \nthe Public Private Partnerships for Big Data that will offer \na secure environment for cross-sector collaboration and \nexperimentation using both commercial and public data.28\nIn general, PPPs for data-sharing can increase the quality \nand quantity of R&D, increase the value and efficiency \nof sharing public sector data, and reduce the long-run \ncost necessary to manage and maintain the data-sharing \ninfrastructure.29\nA Blueprint for the National Research Cloud 51\nCHAPTER 4\nKEY TAKEAWAYS\n Utilizing joint ventures: \nThe ADP PPP uses \na joint venture \nagreement to set the \nterms and conditions \nfor the partnership, \nwhereby one partner \nis the custodian for \ngovernment data, \nand the other is the \noperator that builds and \nmaintains the software \nthat facilitates data\u0002sharing.\n Revenue-sharing \nagreements: A shared \nrevenue model assures \ncontributions from, \nand realization of value \nto, each stakeholder. \nThe ADP subsequently \nreinvests its profits into \nimproving the system.\n Significant efficiencies: \nAccording to the \nADP, there are lower \ncosts to creating \nand maintaining the \nADP than under a \nconventional approach. \nCASE STUDY: ALBERTA DATA PARTNERSHIPS \n(ADP)\nFounded in 1997, the ADP PPP is designed to provide long-term \nmanagement of comprehensive digital data sets for the Alberta market.30\nThe PPP is structured as a joint venture between ADP, a nonprofit, and Altalis \nLtd. whereby the ADP is the “custodian” of government data and Altalis is \nthe “operator.”31 More specifically, geospatial data is owned by the provincial \ngovernment, but exclusive licensing arrangements are granted to ADP to \nallow for sales.32 Meanwhile, Altalis, under the direction and oversight of \nADP, builds software to securely load and distribute these provincial spatial \ndatasets to users. Altalis also provides training to end users and is responsible \nfor cleaning, updating, and standardizing datasets.33\nIn choosing its “operating partner” (i.e., Altalis) for the joint venture, the \nADP board initially issued a “Request for Information” that solicited proposals \nfrom private-sector companies whose core business was the improvement, \nmaintenance, management, and distribution of spatial data.34 The ADP board \nultimately chose Altalis, not only because it had the superior offering and \nexisting capabilities, but also because Altalis was willing to take on all the \ninvestment required, at its own risk, to build and operate the ADP system in \naccordance with ADP specifications.35\nToday, all Altalis and ADP costs are covered by the operations of the joint \nventure.36 The joint venture earns revenues through, for instance, through \ndirected project funding and data access fees from stakeholders, which \ninclude municipalities, regulatory agencies, energy, forestry, and mining \norganizations.37 Any profits from the joint venture are split roughly 80/20 \nbetween Altalis and ADP, respectively, and ADP subsequently uses its profit \nshare to reinvest in data and system improvements.38\nThe ADP PPP claims to have generated efficiencies for data sharing. For \ninstance, the ADP estimates that a traditional government-only approach to \nmaintaining and distributing datasets would have ranged between $65 million \nand $120 million cumulatively since ADP’s inception, and ADP claims to have \nprovided its users with $6.8 million in cost savings.39\nA Blueprint for the National Research Cloud 52\nCHAPTER 4\nA PPP model could reduce the friction of coordination \nbetween data and compute. One example of using a PPP \nfor compute resources is the COVID-19 High-Performance \nComputing Consortium, spearheaded by the Office of \nScience and Technology Policy, DOE, NSF, and IBM.40\nDrawing on the experience of XSEDE, the consortium \nhas 43 members from the public and private sectors that \nvolunteer free compute resources to researchers with \nCOVID-19-related research proposals.41 The voluntary \nnature of compute provisioning, in this instance, provides \nbenefits to both the researchers, who gain immediate \naccess to compute, and the consortium members, who \ncontribute to innovation and reap public relations benefits. \nWe also acknowledge that the evidence around the \nefficacy of PPPs is contested.42 Indeed, there is no one\u0002size-fits-all PPP model; PPPs differ vastly, according to the \nresponsibilities allocated between the private sector and \nthe public sector, and the success of a PPP can depend \non its structure.43 According to a RAND Report of 30 case \nstudies of successful public-private data clearinghouses, \nthese clearinghouses have widely different organizations, \naccess requirements, and strategies for managing data \nquality.44 Such decision points are crucial. For example, \nsome scholars emphasize the need for a trusted \nenvironment for the private and public sectors to handle \nprivacy and ethics violations in sensitive industries.45\nSimilarly, in the siloed federal data-sharing context, a PPP \nmust consider how to divide functions in tackling these \nadditional considerations in privacy, ethics, security, and \nintellectual property.\nTHE NRC AS A GOVERNMENT \nAGENCY\nThe NRC could also be constructed as a new \ngovernment agency or bureau. The main advantages \nto this model would be the development of a distinct \npublic-sector institution, devoted to AI compute and \ndata. The NRC could be to cloud and data what the U.S. \nDigital Service is to government information technology. \nSuch an agency would have to be established by statute \nor executive mandate. Enabling legislation could create \ndedicated, professional staff to build and develop the NRC, \nvest the NRC with authority to mandate interagency data \nsharing, and create a long-term plan that is informed by \nthe National AI Strategy. \nThere are, however, significant disadvantages to \ncreating a new agency or bureau. First, the NRC could \nlay claim to no government datasets at all, and could \nsubsequently encounter significant headwinds with having \nto negotiate with each originating agency for data, not to \nmention the constraints under the Privacy Act, discussed \nin Chapter 5. That said, enabling legislation could \nexempt the agency from the Privacy Act’s data linkage \nprohibitions and transfer litigation risk for data leakages \nto the new agency. Second, a new agency may face greater \nchallenges in recruiting top-flight talent.46 According to \nthe 2020 Survey on the Future of Government Service, a \nmajority of respondents at federal agencies agreed that \nthey often lose good candidates because of the time it \ntakes to hire, and less than half agreed that their agencies \nhave enough employees to do a quality job.47 Moreover, \nmany respondents highlighted inadequate career growth \nopportunities, inability to compete with private-sector \nsalaries, and lack of a proactive recruiting strategy as \nmajor factors contributing to an inadequately skilled \nworkforce in federal agencies.48 FFRDCs, in contrast, can be \nnegotiated with existing organizations, making the startup \ncosts potentially lower. Third, while national laboratories \nhave expertise contracting with entities to construct high\u0002performance computing facilities, it is unclear how a new \nfederal agency/office would approach such a task. It is \none thing for an entity like the U.S. Digital Service to help \ndevelop IT platforms for U.S. agencies; it is another to \nsimultaneously build a very large supercomputing facility \nand solve longstanding challenges with data access. \nFinally, it will be important to isolate the research mission \nof the NRC from political influence. To the extent that a \nnew agency might provide less isolation from changes \nin presidential administrations and politically appointed \nadministrators, this is an important consideration.\nWhile these disadvantages are considerable, \nambitious legislative action could, in fact, make a new \ngovernment agency a viable option.\nA Blueprint for the National Research Cloud 53\nCHAPTER 5\nChapter 5: Data \nPrivacy Compliance\nThe vision motivating the NRC is to support academic research in AI by opening \naccess to both compute and data resources. Federal data can fuel basic AI research \ndiscoveries and reorient efforts from commercial domains toward public and social \nones. As stated in the NRC’s original call, “Researchers could work with agencies to \ndevelop and test new methods of preserving data confidentiality and privacy, while \ngovernment data will provide the fuel for breakthroughs from healthcare to education \nto sustainability.”1\nBut is an NRC seeded with public sector data, particularly administrative data \nfrom U.S. government agencies, even possible given the legal constraints? Research \nproposals that sweep broadly across agencies for personally identifiable or otherwise \nsensitive data2 will rightly trigger concerns about potential privacy risk. The Privacy \nAct of 1974, the chief federal law governing data collected by government agencies, \nfundamentally challenges the notion of an NRC as a one-stop shop for federal data. \nIts research exceptions leave some uncertainty about open-ended research endeavors \nthat go beyond statistical research or policy evaluation supporting an agency’s core \nmission. Even if agencies deemed such research possible, researchers would be \nsubject to access constraints and the data itself may potentially require technical \nprivacy treatments.\nWe make the following recommendations regarding data privacy and the NRC. \nFirst, agencies may be able to share anonymized administrative data with the NRC \nwithin the boundaries of the Privacy Act for the purposes of AI research, based on the \nAct’s statistical research exemptions. Second, the NRC will require a staff of privacy \nprofessionals that include roles tasked with legal compliance, oversight, and technical \nexpertise. These professionals should build relationships with peers across agencies \nto facilitate data access. Third, the NRC should explore the design of virtual “data safe \nrooms” that enable researchers to access raw administrative microdata in a secure, \nmonitored, and cloud-based environment. Fourth, we recommend the NRC Task Force \nengage the policy and statistical research communities, and consider coordination \nwith proposals for a National Secure Data Service, which has grappled extensively \nwith these issues. \nThis chapter proceeds as follows. We first review the existing laws that apply to \ngovernment agencies and the restrictions they impose on data access and sharing. We \nthen describe current agency practices for sharing data with researchers and agencies \nunder the Privacy Act. Last, we assess the implications of current legal constraints on \nNRC data sharing and the most important cognate proposal to promote data sharing \nunder the Evidence Act. \nKEY \nTAKEAWAYS\n Agencies may share \nanonymized administrative \ndata with the NRC under \nthe statistical research \nexemption of the Privacy \nAct.\n An agency’s willingness and \nability to share data may \ndepend on the extent to \nwhich a proposed research \nproject aligns with an \nagency’s core purpose.\n The NRC will require a staff \nof privacy professionals for \nlegal compliance, oversight, \nand technical expertise. \n Individually identifiable \nor sensitive data will face \nobstacles to release and \nmay warrant technical \nprivacy and/or tiered access \nmeasures. \nA Blueprint for the National Research Cloud 54\nCHAPTER 5\nEven when authorized or \nmandated to share data in limited \ncircumstances, federal agencies \nare often reluctant to do so due to a \nmyriad of factors, most prominently \na lack of adoption of consistent \ndata security standards, as well \nas difficulties with measuring and \nassessing privacy risks.\nWe note at the outset that this chapter largely takes \nexisting statutory constraints as a given. At a macro level, \nhowever, the challenges in data sharing also suggest that \nan ambitious legislative intervention could overcome many \nexisting constraints, such as by statutorily (a) exempting \nthe NRC from the Privacy Act’s prohibition on data linkage; \n(b) granting the NRC the power to assume agency liabilities \nfor data breaches; (c) mandating that agencies transfer any \ndata that has been shared under a data use agreement or \nFreedom of Information Act (FOIA) request to the NRC; and \n(d) requiring IT modernization plans to include provisions \nfor data-sharing plans with the NRC.3\nTHE PRIVACY ACT\nData privacy issues are at the core of debates about \nsharing data, and the NRC will be no exception. Most data \nprivacy debates in the U.S. today focus on the consumer \ndata sector where data protection laws in the U.S. are \nlimited to nonexistent. In contrast, many U.S. government \nagencies are subject to a robust privacy law, the Privacy \nAct of 1974, that was passed in response to concerns \nabout government abuses of power.4 For nearly 50 years, \nthis legislation has been effective in its primary goal of \npreventing the U.S. government from centralizing and \nbroadly linking data about individuals across agencies. \nHowever, this approach has come at a cost, which is that \nmost government agencies are prevented from freely \nsharing and linking data across agency boundaries, \nwhich in turn hampers agency operational and research \nefforts.5 According to one government privacy expert, even \nwhen authorized or mandated to share data in limited \ncircumstances, federal agencies are often reluctant to do \nso due to a myriad of factors, most prominently a lack of \nadoption of consistent data security standards, as well as \ndifficulties with measuring and assessing privacy risks.6 To \nthat end, many agencies see promise in adopting technical \nprivacy measures, such as differential privacy, or the \ncreation of synthetic datasets as proxies for actual data, as \na necessary precursor for enabling data sharing for both \nresearch purposes and interagency goals.7 \nIn the nearly 50 years since the Privacy Act’s \npassage, there have been periodic efforts to address \nthe government’s approach to data management \nwhile preserving data privacy. Examples include the \nE-Government Act of 2002, 8 the Confidential Information \nProtection and Statistical Efficiency Act of 2002,9 and most \nrecently, the Foundations for Evidence Based Policymaking \nAct10 and the National Data Strategy.11 Most of these efforts \nhave been aimed at sharing government data for statistical \nanalysis and policy evaluation, and the scope of provisions \nmay need to be broadened to support AI research. We view \nthese efforts to be complementary: The NRC should build \non these efforts, while bringing increased attention to the \ncompute resources that enable AI development as well as \nadvanced data analysis.\nSTATUTORY CONSTRAINTS ON \nDATA SHARING\nOne vision of the NRC is for it to act as a data \nwarehouse for all government data. But that vision collides \nwith fundamental constraints from laws designed to \nhamper broad and unconstrained data sharing between \nU.S. government agencies. Lacking an overarching, \ncomprehensive privacy regime, similar to the European \nUnion’s General Data Protection Regulation (GDPR), the \nUS landscape is fragmented between a mix of sector\u0002specific consumer laws and certain government-specific \nlaws, such as the Privacy Act of 197412 and limited-scope \nfederal guidance, such as the Fair Information Practice \nA Blueprint for the National Research Cloud 55\nCHAPTER 5\nPrinciples.13 In particular, the Privacy Act, which focuses \nbroadly on data collection and usage by federal agencies, \nand restricts sharing between them, poses challenges \nto the ambitions of the NRC’s goal to make otherwise \nrestricted government datasets more widely available.\nExisting efforts, buttressed by such bills as the \nE-Government Act of 2002 and the Foundations of \nEvidence-Based Policymaking Act, have attempted to \nincrease access by researchers to government data assets. \nYet, these approaches were animated by the primary \npurposes of policy evaluation, not basic AI research. Nor \ndo they consider any ambitions on the part of agencies \nthemselves to pursue AI research and development.14\nApplication of these laws and regulations to the NRC, \nin part, hinge on three factors: (1) the institutional form of \nthe NRC, as we discuss in Chapter 4; (2) whether NRC users \ncan invoke the Privacy Act’s existing statistical research \nexception; and (3) whether researchers are accessing data \nfrom multiple federal agencies. Here we briefly discuss \nthe legal obligations of federal agencies. Even if the NRC \ndoes not take the form of a new standalone federal agency, \nagencies contributing data will remain subject to these \nconstraints. \nTHE PRIVACY ACT’S LIMITATIONS AND EXEMPTIONS\nThe Privacy Act was enacted in response to growing \nanxiety about digitization, as well as the Watergate \nscandal during the Nixon presidency. The Act was \nmotivated by concerns about the government’s ability \nto broadly collect data on citizens and centralize it into \ndigital databases, an emergent practice at the time. It \nis the primary limiting regulation for government data \nsharing, and has consequences for the NRC and, more \ndirectly, for any government agency wishing to share data \nwith the NRC. \nData Linkage\nThe Privacy Act applies to systems of records, which \nare defined as “a group of any records under the control of \nany agency from which information is retrieved by the name \nof the individual or by some identifying number, symbol, \nor other identifying particular assigned to the individual.”15\nImportantly, the Act places strict limits on “record \nmatching,” or linking between agencies, for the purposes of \nsharing information about individuals.16 Matching programs \nare only allowed when there is a written agreement in place \nbetween two agencies defining the purpose, legal authority, \nand the justification for the program; such agreements \ncan last for 18 months, with the option of renewal.17 These \nlimits were put in place in order to prevent the emergence \nof a centralized system of records that could track U.S. \ncitizens or permanent residents across multiple government \ndomains, as well as to limit the uses of data for the purposes \nit was collected. Indeed, while linkage across datasets may \nbe important for AI research,18 it could potentially enable \nabuse, surveillance, or the infringement of such rights such \nas free speech by enabling persecution across the many \nareas in which a U.S. citizen or resident interacts with the \nfederal system.19\nBecause the restriction on data linkages applies \nto linkages between agencies, the restriction applies in \ntwo particular scenarios for the NRC. First, if the NRC is \ninstituted as a federal agency, then agency data-sharing \nwith the NRC would run against the data linkages \nlimitation of the Privacy Act. Second, federal agency \nstaff access to the NRC could raise questions about \ninteragency data linkage under the Privacy Act. However, \nthe recommendation in Chapter 3 is focused on granting \nagencies streamlined access to the computing resources \non the NRC and their own agency data, not to any multi\u0002agency data hosted on the NRC. If the NRC is not designed \nas a federal agency and does not grant agency members \naccess to interagency data, the Privacy Act’s restrictions on \ndata linkages may not apply. \nWe note that this approach to data management \nis both unusual and out of step with the private sector, \nas well as AI research specifically. The ability for both \nindustry20 and researchers21 to associate multiple data \nsources and data points with a specific (anonymized) \nindividual is common practice outside of government. In \nfact, this limitation is not one that many governments22\nor U.S. states23 place on their data systems. However, \nthe Privacy Act’s restrictions on data linkage remains \nuncontested, even in the various reform efforts we discuss \nA Blueprint for the National Research Cloud 56\nCHAPTER 5\nbelow. It is worth noting that the federal government’s \nbroad bar against data linkages does incur welfare costs. \nFor example, during the COVID-19 pandemic, the inability \nto share and link public health data created difficulties \ntracking the spread and severity of the virus.24 While \nprojects like Johns Hopkins’ Coronavirus Research Center25\nand the COVID Tracking Project26 attempted to aggregate \navailable data, the lack of data integration slowed \nimportant operational and research responses.27 Other \ncountries, for instance, integrated immigration and travel \nrecords to triage cases and prevent hospital outbreaks.28\nWe acknowledge the potential for data linkage \nto tackle important societal problems without \nrecommending wholesale, unencumbered data linkage. \nBroad or unrestricted data linkage raises legitimate \nconcerns about both individual privacy and widespread \ngovernment surveillance,29 made concrete by the \ndisclosures of government whistle-blower Edward \nSnowden,30 among others. An initiative to link Federal \nAviation Administration (FAA) data with other agency \ndata for COVID-19 response, for instance, would meet \nresistance from the Privacy Act. The Task Force should \nappreciate these tensions and tradeoffs. Indeed, agencies \nview technical measures for privacy preservation \na necessary component of any government data \nstrategy, as methods such as multiparty computation or \nhomomorphic encryption (which we discuss in \nChapter 8) may allow for some forms of data linkages \nbetween agencies, without violating the Privacy Act.\nNo Disclosure Without Consent\nAnother core restriction of the Privacy Act is the \n“No Disclosure Without Consent” rule, which prohibits \ndisclosure of records to any agency or person without \nprior consent from the individual to whom the record \npertains.31 Because the NRC would disclose federal agency \ndata to researchers (i.e., to “person[s]”), this rule—unlike \nthe restriction on record linkage—is legally relevant and \nunavoidable. \nThe Privacy Act, however, contains a number of \nexceptions to this rule. Most pertinent to the NRC’s data\u0002sharing efforts are exemptions for: (1) “routine use”; (2) \nspecified agencies; and (3) statistical research. Under \nthe first exemption, the Privacy Act permits agencies to \ndisclose personally identifiable administrative data when \nsuch disclosure is among one of the “routine uses” of the \ndata.32 A dataset’s “routine use” is defined on an agency\u0002to-agency basis, and is simply a specification filed with \nthe Federal Register on the agency’s plan to use and share \nits data.33 As a result, the more broadly an agency defines \n“routine use” of its data, the more broadly that agency can \nshare its data with other agencies without disclosure.34\nWhile courts have limited how broadly an agency can \ndescribe “routine uses,”35 a large number of use cases can \nstill be covered by a short, general statement.36 Further \nresearch should be conducted on the conditions for when \ndata sharing for research purposes constitutes routine use. \nImplications for Data Sharing with Researchers\nMuch will rest on the interpretation of the “statistical \nresearch” exception, as applied to AI research. Despite \nthe Privacy Act’s constraints on data sharing, researchers \nhave conventionally been able to access data directly from \nagencies, based on the statistical research exception to \nthe Privacy Act. This exception allows disclosure of records \n“to a recipient who has provided the agency with advance \nadequate written assurance that the record will be used \nsolely as a statistical research or reporting record, and the \nrecord is to be transferred in a form that is not individually \nidentifiable.”37 Doing so requires either access to an approved \nresearch dataset, or for the researcher to negotiate an MOU \ndirectly with the agency, a role we suggest the NRC may be \nable to fill as an intermediary, acting as a negotiating partner \nto facilitate access requests between multiple researchers \nand agencies (discussed in Chapter 3). \nWhile the Privacy Act does not define “statistical \nresearch,” subsequent laws and policies have elaborated \non the definition. For example, the E-Government Act \ndefines “statistical purpose” to include the development \nof technical procedures for the description, estimation, or \nanalysis of the characteristics of groups, without identifying \nthe individuals or organizations that comprise such \ngroups.38 Meanwhile, a “nonstatistical purpose” includes \nthe use of personally identifiable information for any \nadministrative, regulatory, law enforcement, adjudicative, \nor other purpose that affects the rights, privileges or \nbenefits of any individual.39 That is, while researchers may \nA Blueprint for the National Research Cloud 57\nCHAPTER 5\nuse personally identifiable data for the broad purpose of \nanalyzing group characteristics, they cannot use such data \nfor targeted purposes to aid agencies with, for instance, \nspecific adjudicative or enforcement functions.\nThe precise meaning of “statistical purpose,” however, \nremains “obscure and the evaluation criteria may be \ndifficult to locate.”40 Yet, “statistical purpose” may well \nencompass data sharing for certain AI applications. The Act \nexplicitly designates the Bureau of Labor Statistics, Bureau \nof Economic Analysis, and the Census Bureau as statistical \nagencies that have heightened data-sharing powers for \nstatistical purposes.41 These agencies regularly use AI in \nconducting their statistical activities.42 While definitions \nof AI are themselves contested, statistical research may \nencapsulate at least some forms of machine learning and \nAI, if such research analyzes group characteristics43 and \ndoes not identify individuals. \nTo be sure, the NRC should not enable researchers or \nagencies to conduct an end run around the Privacy Act. \nTo that end, the NRC will require staff devoted to privacy \ncompliance and oversight to ensure compliance. Key \nquestions regarding individual identifiability, sensitivity of \nthe data, or the potential for linkage and reidentification \nwill need to be assessed by such staff. \nImplications for Agency Data Sharing with the NRC\nNotwithstanding the above avenues, agencies may \nnonetheless be reluctant to share data with the NRC and \nits researchers. Instances abound where federal agencies \nface constraints to sharing data, even if it is entirely legal \nor even federally mandated. For example, the Uniform \nFederal Crime Reporting Act of 1988 requires federal law \nenforcement agencies to report crime data to the FBI.44\nYet, no federal agencies appear to have shared their data \nwith the FBI under this law.45 Similarly, the Census Bureau \nis enabled by legislation that authorizes it to obtain \nadministrative data from any federal agency and requires \nit to try to obtain data from other agencies whenever \npossible.46 However, the statute does not similarly require \nthe program agencies to provide their data to the Census \nBureau. That is, although the Census Bureau is required \nto ask other agencies for data, those agencies are not \nrequired to, and often do not, provide it.47\nFailure to engage in data sharing, even in the face of \na statutory authorization, can stem from risk aversion. \nAccording to a GAO report, agencies choose not to share \ndata because they tend to be “overly cautious” in their \ninterpretation of federal privacy requirements.48 Because \nlegal provisions authorizing or mandating data sharing \nare often ambiguous,49 agencies may err on the side of \ncaution and choose not to share their data for fear of the \ndownside risk that recipient use of the data may violate \nprivacy or security standards.50 To make matters worse, \nbecause agencies need to devote significant resources \nto facilitate data sharing, they may simply choose not to \nprioritize data sharing at all. The lack of resources poses a \nsignificant problem; according to a Bipartisan Policy Center \nstudy on agency data sharing, about half of agencies cited \ninadequate funding or inability to hire appropriate staff as \ntheir “most critical” barrier to data sharing.51\nThe NRC may overcome these hurdles by clarifying \nlegal provisions, ensuring that the benefits to agencies of \ndata sharing outweigh the risks and costs, and advocating \nfor resources. For instance, O’Hara and Medalia describe \nhow the Census Bureau was able to obtain food stamp and \nwelfare data from state agencies. In the face of ambiguous \nstatutes authorizing the U.S. Department of Agriculture \n(USDA) and the U.S. Department of Health and Human \nServices (HHS) to perform data linkages across federally \nsponsored programs, states originally arrived at different \nstatutory interpretations. Some states agreed to share their \ndata only after (1) the Office of General Counsel at both the \nUSDA and HHS issued a memo clarifying that data sharing \nwith the Census Bureau for statistical purposes was legal \nand encouraged; and (2) the states were convinced that data \nsharing would enable evidence building that could help \nthem administer their programs.52\n[T]he NRC should not enable \nresearchers or agencies to \nconduct and end run around \nthe Privacy Act.\nA Blueprint for the National Research Cloud 58\nCHAPTER 5\nBroader data sharing with the NRC that combines \nmultiple agency or external data sources may be facilitated \nby the passage of additional laws requiring agencies to \nshare their data, subject to specific limitations on how \nthat data is used by the NRC. Even then, the effect of that \nrequirement is hardly a foregone conclusion. More is \nneeded by way of both clarifying the extent to which data \nsharing is permitted and providing benefits that incentivize \nagencies to share their data.\nCASE STUDY: ADMINISTRATIVE \nDATA RESEARCH UK\nAdministrative Data Research UK (ADR UK) is a new body, set up \nin July 2018, to facilitate secure, wide access to linked administrative \ndatasets from across government for the purpose of public research.53\nADR UK was set up as a central, coordinating point between four \nnational partnerships—ADR England, ADR Northern Ireland, ADR \nScotland, and ADR Wales—as well as the UK-wide national statistics \nagency, Office for National Statistics (ONS). ADR UK labels itself as \na “UK-wide strategic hub”: a central point that promotes the use of \nadministrative data for research, engages with government departments \nto facilitate secure access to data, and funds public good research that \nuses administrative data.54\nFunding for ADR UK came from a research council (Economic and \nSocial Research Council, ESRC) and was initially committed from July \n2018 to March 2022. A total of £59 million was provided.55\nADR UK serves three core functions. First, the promotion of the value \nand availability of government administrative datasets for research. ADR \nUK acts as a general advocate for the use of administrative datasets \nfrom across the British government. It also acts as a specific driver of \nresearch for public good: It has identified specific areas of research that \nare of pressing policy interest (e.g., “world of work”56), and is focusing \non creating access to linked datasets for researchers who tackle those \npriority themes.\nThe second core function is serving as a coordination point \nto encourage government data sharing, standards, and linkage of \nadministrative datasets. Especially for its research calls, ADR UK is able \nto highlight multiple datasets, often spanning different government \ndepartments’ scope areas that can be linked and used in research. \nIn doing so, ADR UK plays an important role in facilitating research.\nFinally, to ensure compliance with the Privacy Act, as \nwell as to facilitate the NRC’s role as a data intermediary, \nthe NRC will require a staff of privacy professionals that \ninclude positions tasked with legal compliance, oversight, \nand technical methods expertise. These professionals \nshould build relationships with peers across agencies to \nfacilitate data access.\nKEY TAKEAWAYS\n Proactive advocacy for data \nuse and linkage: Given the \nrange of agencies and data \nsources in government, \nhaving a single, coordinated \nvoice of advocacy for data \nuse and linkage of public \ndatasets for public good is an \nimportant function.\n Bringing external talent \ninto government data use: \nADR UK has two schemes—\nResearch Fellowships and \nMethod Development \nGrants—that target \nexceptional, external talent \nwith the intention of building \nawareness and use of public \ndatasets in cutting-edge \nresearch.\n Small grant funding to \naccelerate research methods \nthat use large datasets: \nBy putting out calls for \nresearch that answers broad \nthemes, ADR UK is able to \ncorral a range of datasets \nin answering research \nquestions and avoids a single \ndisciplinary focus.\nA Blueprint for the National Research Cloud 59\nCHAPTER 5\nThird, ADR UK has a strategic funding approach to further the use of administrative datasets in research \nthat has three categories of funding:\n• Building new research datasets: ADR UK’s Strategic Hub Fund initially solicited invitation\u0002only bids for researchers who would build new research datasets of public significance in the \ncourse of their work.57 These new, research-ready datasets are now accessible to a wide range of \nresearchers.58\n• Research Fellowship Schemes: A major funding focus now is on funding research through \ncompetitive open-bid invitations under a Research Fellowship Scheme.59 Specific researchers are \nidentified through the competition. They are accredited for secure data access and placed right \nat the heart of government (with 10 Downing Street), with access to linked datasets to answer \nquestions of public significance.60\n• Methods Development Grants: Separately, ADR UK invites research proposals that further \nmethodological progress for the use of large-scale administrative datasets, such that the wider \nsocial science community can draw on developed methods in research.61\nPrivacy and Security\nThe UK’s 2017 Digital Economy Act62 created a legal gateway for research access to secure government \ndata. Deidentified data held by a public authority in connection with the authority’s functions could be \ndisclosed for research, under the assurance that individual identities would not be specified.\nAny data shared with researchers is anonymized: Personal identifiers are removed, and checks are \nmade to protect against re-identification.63 A rigorous accreditation process—for both the researcher and \nproposed research—is undertaken to ensure public benefit. Data access primarily takes place via a secure \nphysical facility, or a secure connection to that facility, provided by ADR UK’s constituent partners.64 There is \nclose monitoring of researcher activity and outputs, and any output is checked before release.65\nFrom a researcher’s point of view, access to ADR UK datasets requires the following steps:66\n• Researcher submits proposal for project to ADR UK.\n• Project is approved by relevant panels.\n• Researcher engages in training and may take assessment (e.g., access to linked data held by ONS \nrequired accreditation to ONS’ Secure Research Service,67 and can access data either in person or, \nwhere additionally accredited, through remote connection).\n• Required data is determined by ADR UK (through one of the four regional partners, or ONS), then \ningested by the relevant data center.\n• De-identified data is made available through a secure data service (either at the ONS, or one of the \nfour regional partners).\n• Researcher conducts analysis; activity and outputs are monitored.\n• Outputs are checked for subject privacy. Research serving the public good is published.\nCASE STUDY: ADMINISTRATIVE DATA RESEARCH UK (CONT’D)\nA Blueprint for the National Research Cloud 60\nCHAPTER 5\nCOMPLEMENTARY EFFORTS TO \nIMPROVE THE FEDERAL APPROACH \nTO DATA MANAGEMENT\nThe barriers to data sharing created by the Privacy \nAct have long posed a challenge to researchers interested \nin using government data to evaluate or inform policy.68\nThe policy and statistical research communities, both \nwithin and outside the federal government, have engaged \nin admirable reform efforts to facilitate data sharing for \npolicy evaluation.69\nThe Foundations for Evidence Based Policymaking \nAct (EBPA) of 2018, which enacted reforms to improve \ndata access for evidence-based decision-making, is a key \nachievement of these efforts to date. However, several of \nthe provisions in the Act that helped to address some of \nthe barriers to data linking and sharing were not passed \nby Congress. These provisions—known collectively as \nthe National Secure Data Service (NSDS)—remain a high \npriority for facilitating further progress for sharing data \nfor research purposes. According to the nonprofit Data \nFoundation, one of the major supporters of the NSDS, its \npassage will “create the bridge across the government’s \ndecentralized data capabilities with a new entity that \njointly maximizes data access responsibilities with \nconfidentiality protections.”70\nThe NSDS is envisioned as an independent legal \nentity within the federal government that would have \nthe legal authority to acquire and use data. However, this \nauthority is currently conceived of as emanating from the \nEBPA, which focuses on using statistical data for evidence\u0002building purposes. A broader source of authority may be \nnecessary for AI research purposes under the NRC, which \nmay be distinct from agency obligations. One clear area \nof overlap is the proposal’s call for the NSDS to facilitate \nits own computing resources, which could be harmonized \nwith the compute needs of the NRC. Similar to Chapter 4’s \ndiscussion of organizational options, NSDS supporters \nidentify a fundamental need for both a reliable funding \nsource as well as thoughtful placement of the NSDS either \nwithin an existing agency or as an independent agency or \nFFRDC. The areas of common ground between the NRC \nand NSDS, as well as the expertise and momentum behind \nthe proposal, strongly suggest that the NRC engage and \ncoordinate with these efforts. \nAnother complementary initiative is the Federal Data \nStrategy (FDS), launched in 2018 by the executive branch \nand led by the OMB. FDS is a government-wide effort to \nreform how the entire federal government manages its \ndata. The plan calls out the need for “safe data linkage” \nthrough technical privacy techniques,71 and incorporates \na directive from the 2019 Executive Order on Maintaining \nAmerican Leadership in Artificial Intelligence to “[e]\nnhance access to high-quality and fully traceable federal \ndata, models, and computing resources to increase the \nvalue of such resources for AI R&D, while maintaining \nsafety, security, privacy, and confidentiality protections, \nconsistent with applicable laws and policies.”72 The FDS \ndirects OMB to “identify barriers to access and quality \nlimitations” and to “[p]rovide technical schema formats \non inventories,” with a focus on open data sources (i.e., \nnon-sensitive or individually identifying data).73 Datasets \nidentified by this process could be key candidates for \npopulating the NRC. \nWhile both the NSDS and the FDS may promote data \nsharing, these efforts are presently focused primarily on \nfurthering policy evaluation purposes. Fortunately, there \nis much overlap and complementarity between these \ninitiatives and the NRC, illustrating the broad importance \nof more effective mechanisms to share federal data \nsecurely and in a privacy-protecting way. \nA Blueprint for the National Research Cloud 61\nCHAPTER 6\nChapter 6: Technical \nPrivacy and Virtual \nData Safe Rooms\nWe now discuss the role of technical privacy methods for the NRC. In the past \nseveral decades, researchers have devised a variety of computational methods that \nenable data analysis while preserving privacy. These methods hold considerable \npromise for enabling the sharing of government data for research purposes. We note \nat the outset that technical methods are merely one mechanism to strengthen privacy \nprotections. While effective, such methods may be neither sufficient nor universally \nappropriate. The application of any particular method does not obviate the need \nto inquire into whether the data itself adheres to articulated privacy standards. The \nmethods discussed here are not “replacements” for the recommendations discussed \nearlier and never themselves justify the collection of otherwise problematic data. \nUse of data from the NRC introduces two threats to individual privacy. The \nfirst type involves accidental disclosure by agencies (agency disclosure): An agency \nuploads a dataset to the NRC which lacks sufficient privacy protection and contains \nidentifying information about an individual. A researcher—either analyzing this \ndataset alone or in conjunction with other NRC datasets—discovers this information \nand re-identifies the individual.1\n The second type involves accidental disclosure by \nresearchers (researcher disclosure). Here, a researcher releases products computed on \nrestricted NRC data (e.g., trained machine learning models, publications). However, \nthe released products lack sufficient privacy protection, and an outside consumer of \nthe research product learns sensitive information about an individual or individuals in \nthe original dataset used by the researcher.2\nWe recommend that, due to the infancy and uncertainty surrounding uses of \nprivacy-enhancing technologies, privacy should primarily be approached via access \npolicies to data. While there will be circumstances that suggest, or even mandate, \ntechnical treatments, access policies, discussed in Chapter 3, are the primary line of \ndefense: They ensure sensitive datasets are protected by controlling who can access \nthe data. We recommend a tiered access policy, with more sensitive datasets placed \nin more restricted tiers. For instance, highly restricted access data may correspond \nto individual health data from the VA, while minimally restricted access data may \ncorrespond to ocean measurements from NOAA. Proposals requesting access to highly \nrestricted data would face heightened standards of review, and researchers may \nbe limited to accessing only one restricted access dataset at a time. This approach \nmirrors current regimes where researchers undergo special training to work with \ncertain types of data.3\nKEY \nTAKEAWAYS\n Technical privacy measures \nare useful, but not substitutes \nfor securing data privacy \nthrough access policies. \n In some instances, the NRC or \nagencies may wish to make \naccess to data conditional on \nthe use of technical privacy \nmeasures.\n Contributing agencies and \nthe NRC should collaborate to \ndetermine technical privacy \nmeasures based on dataset \nsensitivity, dataset utility, and \nequity implications. \n The NRC must have technical \nprivacy staff to administer \ntechnical privacy treatments, \nas well as to support \nadversarial privacy research.\n The NRC should explore \nadopting virtual “data \nsafe rooms” that enable \nresearchers to access \nraw administrative data \nor microdata in a secure, \nmonitored, and cloud-based \nenvironment.\nA Blueprint for the National Research Cloud 62\nCHAPTER 6\nTechnical treatments are a different line of defense: \nThey significantly reduce the chances of deanonymizing a \ndataset. There are a range of technical methods that can \nenable analysis while ensuring privacy:\n• Techniques like k-anonymity and ℓ-diversity \nattempt to offer group-based anonymization by \nreducing the granularity of individual records in \ntabular data.4 While effective in simple settings and \neasy to implement, both methods are susceptible \nto attacks by adversaries who possess additional \ninformation about the individuals in the dataset.\n• One of the most popular techniques is differential \nprivacy,5\n which provides provable guarantees \non privacy, even when an adversary possesses \nadditional information about records in the dataset. \nHowever, differential privacy requires adding \nrandom amounts of statistical “noise” to data and \ncan sometimes compromise the accuracy of data \nanalyses. Although differential privacy has become \na point of contention with respect to the Census \nBureau’s new disclosure avoidance system,6\n the \ntechnique remains a powerful defense against bad \nactors seeking to take advantage of public data for \nthe purposes of re-identification.\n• Researchers have also identified other promising \nmethods. Recent work has demonstrated that \nmachine learning can be used to generate \n“synthetic” datasets, which mirror real world \ndatasets in important ways but consist of entirely \nsynthetic examples.7\n Other work has focused on \nthe incorporation of methods from cryptography, \nincluding secure multiparty computation8 and \nhomomorphic encryption.9\nMethods that obscure data introduce fundamental \ntensions with the way machine-learning researchers \ndevelop models. For example, when considering \nquestions of algorithmic fairness, in some instances \nprivacy protections can undercut the power to assess \nwhether such a technical method as differential privacy \nresults in demographic disparities, particularly for small \nsubgroups.10 Similarly, “error analysis”—the study of \nsamples over which a machine-learning model performs \npoorly—is central to how researchers improve models. It \nrequires understanding the attributes and characteristics \nof the data in order to better understand the deficiencies \nof an algorithm. Therefore, such methods as differential \nprivacy, which make raw data more opaque, will invariably \nimpede the process of error analysis. Synthetic data \ntypically captures relationships between variables only \nif those relationships have been intentionally included \nin the statistical model that generated the data,11 and \nthus, may be poorly suited to certain AI models that \ndiscover unanticipated relationships among data. \nWhile homomorphic encryption may not require similar \nassumptions on data structure, existing methods are \ncomputationally expensive.\nWhile promising, understanding and applying these \nmethods is an evolving scientific process. The NRC is \npoised to contribute to their evolution by directly \nsupporting research into their application. \nCRITERIA AND PROCESS FOR \nADOPTION\nThe NRC will contain a rich array of datasets, each \npresenting unique privacy implications over different \ntypes of data formats (e.g., individual tabular records, \nunstructured text, images). Including a dataset on the \nNRC raises a question of choice: Which technical privacy \ntreatment should be applied (e.g., k-anonymity vs. \ndifferential privacy), and how should it be applied? This \nquestion often requires technical determinations about \ndifferent algorithmic settings, but such technical choices \ncan also have important substantive consequences.12\nFirst, we recommend that these determinations are \nmade with respect to the following factors: \n• Dataset sensitivity: Different datasets will \npose privacy risks that range in type and \nmagnitude. Health records, for instance, are \nmore sensitive than weather patterns. The \nprivacy method chosen should reflect this \nsensitivity. As we discuss in Chapter 3, these \nprivacy methods should correspond and be tiered \nA Blueprint for the National Research Cloud 63\nCHAPTER 6\nto the appropriate FedRAMP classification for the \ndataset.\n• Dataset utility: As discussed above, applying \na privacy method can distort the original data, \ndiminishing the accuracy and utility of analysis. \nBecause different methods affect different levels \nof distortion, the choice of method should be \ninformed by the perceived utility of the data. \nHigh-utility datasets—where accurate analyses \nare highly important (e.g., medical diagnostic \ntools)—may necessitate methods that produce \nless distortion. \n• Equity: Certain privacy measures can \ndisproportionately impact underrepresented \nsubgroups in the data.13 In determining which \nmethod to apply, the presence of sensitive \nsubgroups and their relation to the objectives of \nthe dataset should be evaluated. \nFor any given dataset, we recommend that agencies \nproviding the data collaborate with NRC staff to identify and \nrecommend any privacy treatments. Originating agencies \nand NRC staff will possess domain and research expertise \nto make evaluations on the balance of privacy, utility, and \nequity, but agencies should consult with NRC staff and \nresearchers on the most appropriate treatments. Given the \ncost of review, such privacy treatments should be much less \nwidely considered for low-risk datasets. \nVIRTUAL DATA SAFE ROOMS \nFor individual research proposals that would be greatly \nhampered by technical privacy measures, the NRC should \nexplore the use of virtual “data-safe rooms” that enable \nresearchers to access raw administrative microdata in a \nsecure, monitored environment. Currently, the Census \nBureau implements these safe rooms in physical locations \nand moderates access to raw interagency data through \nits network of Federal Statistical Research Data Centers \n(FSRDCs). However, the NRC should not adopt the FSRDC \nmodel wholesale. Indeed, the barriers to using FSRDCs \nare high, and “only the most persistent researchers are \nsuccessful.”14 For instance, applying for access and gaining \napproval to use an FSRDC takes at least six months, requires \nobtaining “Special Sworn Status,” which involves a Level \nTwo security clearance, and is limited to applicants who \nare either U.S. citizens or have been U.S. residents for \nthree years.15 To further complicate matters, agencies \nhave different review and approval processes for research \nprojects that wish to access agency data using an FSRDC.16\nFinally, even after approval is granted, researchers can only \naccess the data in person by going to secure locations, such \nas the FSRDC itself.17 \nTo be clear, some of these restrictions are unique to \nthe Census Bureau. U.S. law provides that any Census \ndatasets that do not fully protect confidentiality may only \nbe used by Census staff.18 Researchers trying to access such \ndata therefore must go through the rigorous process of \nbecoming a sworn Census contractor. The extent to which \nthese restrictions apply to the NRC will depend on whether \nthe NRC institutionally houses itself in the Census Bureau, \nwhich we ultimately do not recommend.19 Other problems, \nhowever, such as the lack of interagency uniformity in \ngranting access to datasets is not a problem unique to \nCensus, but a common problem throughout the federal \ngovernment (see Chapter 3).\nAnother common problem—not necessarily tied to \nFSRDCs or the Census Bureau—is the use of a physical data \nroom to access raw microdata. The NRC should explore a \nvirtual safe room model, whereby researchers can remotely\naccess such microdata. For instance, in the private sector, \nthe nonpartisan and objective research organization, NORC, \nlocated at the University of Chicago, is a confidential, \nprotected environment where authorized researchers can \nsecurely store, access, and analyze sensitive microdata \nremotely.20 Some federal government agencies have also \nimplemented their own virtual data safe rooms. The Center \nfor Medicare and Medicaid Services’ Virtual Research Data \nCenter (VRDC), for instance, grants researchers direct \naccess to approved data files through a Virtual Private \nNetwork.21 In a 2019 Request for Information (RFI), the \nNational Institutes of Health also solicited input for its own \nadministrative data enclave and whether such an enclave \nshould be physical or virtual.22 As articulated in responses \nto the RFI from the American Society for Biochemistry and \nMolecular Biology and the Federation for of American \nA Blueprint for the National Research Cloud 64\nCHAPTER 6\nSocieties for Experimental Biology, a virtual enclave would \ngreatly facilitate researcher access to data and can be \ndesigned and administered in a way to preserve privacy \nand security.23\nA National Research Cloud cannot function effectively \nif access to certain datasets is ultimately tied to a National \nResearch Room. \nCASE STUDY: \nCALIFORNIA POLICY LAB \nThe California Policy Lab (CPL) is a University \nof California research institute that provides \nresearch and data support to help California state \nand local governments craft evidence-based \npublic policy.24 CPL offers a variety of services to \ngovernments, including data analysis services and \nsecure infrastructure for hosting and linking the vast \namounts of data collected by government entities.25\nThese services help bridge the gap between \nacademia and government by helping policymakers \ngain access to researchers and providing researchers \na secure way to access administrative data. CPL \naims to build trusting partnerships with government \nentities and enable them to make empirically \nsupported policy decisions.\nCPL enters data-use agreements with various \ngovernment entities around California, including, for \nexample, the California Department of Public Health \nand Los Angeles Homeless Services Authority.26\nThese agreements allow CPL to store administrative \ndata in a linkable format, promoting broad \nlongitudinal analyses across various public sector \ndomains.\nTo help manage the requirements of the various \ndata-use agreements and simplify compliance, \nCPL applies the strictest requirements for any \nindividual data across all data it stores.27 Each set of \nadministrative data is thus subject to strict technical \nrestrictions and thorough audits.28 CPL manages the \ndata in an on-premises data hub at UCLA. This data \nhub uses “virtual enclaves” modeled after air-gapped \nclean rooms typically used for sensitive government \ndata.29 Virtual enclaves are virtual machines that \nforbid any outbound connections.\nCPL creates a \nnew virtual enclave for \neach research project \nand only gives specific \nresearchers access \nto specific datasets \nfor each project.30\nResearchers can only \nwork with the data in \nthe enclave and can \nonly use tools provided \nin the environment. \nData access processes \nvary, based on the \nrequirements for the \ngovernment entities, \nand most of CPL’s data\u0002use agreements are \npurpose limited and thus \nrequire approval from \nthe relevant government \nentity before being used \nin a project.31\nGenerally, CPL helps researchers understand how \nto gain access to various types of administrative data. \nFor some datasets, CPL has formalized applications \non its website.32 CPL prescreens project proposals and \nsends promising projects to its government partners \nfor final approval. Researchers then conduct these \napproved projects on CPL’s secure infrastructure. For \nother datasets without formalized access processes, \nCPL directs researchers toward individuals within \nthe government entities.33 CPL can then take over \nmanagement of approved projects that aim to use \ndata stored on its hub under their standing data-use \nagreements. Alternatively, the government entities \nand researchers themselves may craft new data-use \nagreements for specific projects.\nKEY TAKEAWAYS\n Virtual enclaves: \nThe CPL heavily \nutilizes secure \nvirtual enclaves \nfor researchers to \naccess, work with, \nand perform data \nlinkages across \nsensitive datasets. \n Acting as an \nintermediary: CPL \nfacilitates and \nstreamlines access \nto administrative \ndata by acting as \nan intermediary \nbetween researchers \nand relevant state \nagencies.\nA Blueprint for the National Research Cloud 65\nCHAPTER 6\nIMPLICATIONS FOR THE NRC\nDEDICATED STAFF\nAs discussed above, it will be critical for the NRC to \nmaintain a dedicated professional staff who specialize in \nprivacy technologies. First, not all agencies or departments \nthat seek to place data into the NRC will have the expertise \nto both determine the privacy method that meets data \nutility expectations and data privacy demands, and apply \nit to the dataset of interest. Specialized NRC staff will be \nessential to assisting such agencies and departments. \nSecond, even where agencies and departments do \npossess the requisite expertise, NRC staff will bring a \nunique perspective from their collaborations across the \ngovernment. Where a specific department’s staff may \nonly foresee risks specific to the dataset, NRC staff will \nbe able to foresee instances where the presence of other \ndata in the NRC may raise other concerns. In fact, by \nworking with Affiliated Government Agencies and agency \nrepresentatives, the NRC staff can also help these agencies \ninternalize such benefits as helping them understand the \nfull range of privacy risks with respect to their data.34 Such \ncollaborative governance will be necessary to ensure that \nprivacy assessments consider the full implications of access \nand privacy technologies. Finally, it must not be overlooked \nthat while data management in general requires technical \nexpertise, these various privacy-enhancing technologies \nalso require very specific, highly skilled expertise. Using \nsynthetic data sets as an example, NRC staff could be asked \nto build synthetic data on an agency’s behalf, or need \nto validate the work performed at an agency to ensure \nit is done properly and well. Whatever the task, there \nare cascading effects downstream through the research \necosystem if not carefully managed and executed.\nA FOCUS ON EVALUATING AND RESEARCHING PRIVACY\u0002ENHANCING TECHNOLOGIES\nIt will be necessary to continually evaluate the \nstate of privacy protections on the NRC, either by NRC \nstaff members or by supporting privacy and security \nresearchers at academic institutions. Technical privacy \nand security research is by nature adversarial: Researchers \nadopt the posture of adversaries in order to probe the \nweaknesses of a system/dataset. In the context of the \nNRC, this will require simulating attacks as researchers \ntry to reidentify individuals within specific NRC datasets. \nThis type of research is necessary to advance the field, \nand the NRC may be specially positioned to support a \nresearch center devoted to researching privacy-enhancing \ntechnologies. Doing so would allow the research \ncommunity to build stronger privacy methods to ensure \nanonymity, identify flaws, and self-regulate an evolving \ndata ecosystem.\nA National Research Cloud cannot \nfunction effectively if access to \ncertain datasets is ultimately tied \nto a National Research Room. \nA Blueprint for the National Research Cloud 66\nCHAPTER 7\nChapter 7: Safeguards \nfor Ethical Research \nThe pace of advances in AI has sparked ample debate about the principles that \nshould govern its development and implementation. Despite the technology’s promise \nfor economic growth and social benefits, AI also poses serious ethical and societal risks. \nFor example, studies have demonstrated AI systems can propagate disinformation,1\nharm labor and employment,2 demonstrate algorithmic bias along age, gender, race, and \ndisability,3 and perpetuate systemic inequalities.4\nThis chapter considers how the NRC should ensure its resources are deployed \nresponsibly and ethically. A growing body of research on AI fairness, accountability, and \ntransparency has raised serious and legitimate questions about the values implicated by \nAI research and its impact on society.5 The NRC’s focus on increasing access to sources of \npublic data and fostering noncommercial AI research is intended to help address these \nconcerns by enabling broader opportunities for academic research. At the same time, \nbroadening access to resources is not enough to assure that academic AI research does \nnot exacerbate existing inequalities or perpetuate systematic biases. In addition, the NRC \nmust also be prepared to handle and act upon complaints of unethical research practices \nby researchers. \nWhile there is an abundance of proposed ethics frameworks for AI (see Appendix C for \nthose published by federal agencies), there is not a set of accepted principles enshrined \ninto law, like the Common Rule for human subjects research, that clearly establishes the \nboundaries for ethical research with AI.6 Lacking such guidance, a core question for the \nNRC is how to institutionalize the consideration of ethical concerns. This chapter starts by \ndiscussing two potential approaches for research proposals: ex ante review at the proposal \nstage for access to NRC resources (e.g., compute, dataset), and ex post review after \nresearch has concluded. Separately, we discuss guidance for the NRC on issues related to \nresearch practices. One of the virtues of starting with access by Principal Investigator (PI) \nstatus (Chapter 2) is that researchers will (a) often have undergone baseline training by \ntheir home institutions in research compliance, privacy, data security, and practices for \nresearch using human subjects; and (b) be subject to research standards and peer review \n(e.g., through IRB review when applicable). These mechanisms are insufficient to cover \nmany AI research projects, such as when human subjects review is deemed inapplicable. \nThus, we tailor our recommendations to the institutional design of the NRC. \nFirst, we recommend that the NRC require including an ethics impact statement for PIs \nrequesting access beyond base-level compute, or for research using restricted datasets. \nThis provides a layer of ethical review for the highest resource projects that are already \nrequired to undergo a custom application process. Second, for other categories of research \n(e.g., research conducted under base-level compute access, where no custom review is \nKEY \nTAKEAWAYS\n Researchers requesting \naccess to compute \nbeyond the default \nallocation and/or \nrestricted data (i.e., \nthose undergoing a \ncustom application \nprocess) should be \nrequired to provide an \nethics impact statement \nas part of their \napplication.\n The NRC should \nestablish a process \nto handle complaints \nabout unethical research \npractices or outputs. \n Eligibility based on \nPrincipal Investigator \nstatus will ensure \nsome review under the \nCommon Rule as well \nas through peer review, \nbut we recommend \nuniversities consider \nmore comprehensive \nmodels for assessing the \nethical implications of AI \nresearch.\nA Blueprint for the National Research Cloud 67\nCHAPTER 7\ncontemplated), we recommend that the NRC establish \na process for handling complaints that may arise out of \nunethical research practices and outputs. Third, given \nthe limitations of the prior mechanisms, we recommend \nthe exploration of a range of measures to address ethical \nconcerns in AI compute, such as the approach taken by the \nNational Institutes of Health to incentivize the embedding \nof bioethics in ongoing research. \nETHICS REVIEW MECHANISMS\nEX ANTE\nEx ante review assesses research yet to be performed.7\nFunding agencies and research councils worldwide rely on \nex ante peer reviews to evaluate the intellectual merit and \npotential societal impact of research proposals, based on \nset criteria.8 Institutional Review Boards (IRBs) commonly \nassess academic research involving human subjects prior \nto its initiation.9 However, much AI-related research may \nnot fall under IRB oversight, as the research may not use \nhuman subjects or rely on existing data (not collected by \nthe proposers) about people that is publicly available,10\nused with permission from the party that collected the \ndata, or is anonymized. Potential ethical issues may, \ntherefore, escape IRB review.11\nCreating an across-the-board ex ante ethics review \nprocess, however, would be challenging. First, as we \ndiscuss in Chapter Two, we recommend against case-by\u0002case review for all PI requests for access to NRC compute \nand data, as such a process would require substantial \nadministrative overhead. At the stage when researchers \nare simply applying for compute access, the research \nmay be so varied and early stage, that there is not much \nconcrete to review. And to the extent that every PI would \nrequire project-specific review, such a process would be \nonerous. \nSecond, ex ante review is unlikely to grapple with the \nmany ethical implications of design decisions that take \nplace after research commences.12 Research design can \nchange substantially from initial proposals as projects \nprogress. Ex ante review could identify some concerns, \nbut unlikely all.13 The nature of machine learning is \ninherently uncertain—and predictions can be challenging \nto explain—as well as highly dependent on the data used \nto build and train models.14 Ex ante proposal review alone \nmay not be sufficient to identify biased outcomes, and \nmay in fact require extensive documentation and review \nof the data used in a specific project to assess with any \nreliability.15\nThird, there are unique academic speech concerns \nabout government assessment of research. Authorizing the \ngovernment to conduct an ethics review (separate from \nIRB review under the Common Rule, which is typically \ndelegated to academic institutions) with vague standards \nmay implicate academic speech concerns, as well as \nsubject proposals to politically driven evaluation that can \nshift from administration to administration.\nIf the NRC were to create a process for ex ante \nreview of research proposals for ethical concerns, such \na board would likely need to be composed of scientific \nand ethics experts, similar to how the NSF conducts their \nprocess, though perhaps with the addition of members \nfrom civil society organizations that focus on countering \nAI harms. The NSF convenes groups of experts from \nacademia, industry, private companies, and government \nagencies as peer reviewers, led by NSF program officers \nand division directors.16 However, the scope and range \nof NRC research proposals are likely to be both broad, \nand highly interdisciplinary in nature, making ethics \nassessments challenging. \nEX POST\nEx post evaluations provide an assessment after \nresearch has concluded.17 In academia, researchers \nsubmit research results to journals or conferences for ex \npost peer review; it is at this pre-publication stage that \nethical issues not identified by ex ante processes may be \nsurfaced by reviewers or editors. In the public sector, for \nexample, the Privacy and Civil Liberties Oversight Board \n(PCLOB) conducts ex post reviews on counterterrorism \npractices by executive branch departments and agencies \nto ensure they are consistent with governing laws, \nregulations, and policies regarding privacy and civil \nliberties.18 PCLOB has also recently begun to evaluate the \nA Blueprint for the National Research Cloud 68\nCHAPTER 7\nuse of new technologies in foreign intelligence collection \nand analysis,19 and to identify legislative proposals that \nstrengthen its oversight of AI for counterterrorism.20\nRECOMMENDATIONS\nWhile we do not recommend across-the-board ex \nante review of research proposals, we do recommend that \nthe NRC establish a process to handle complaints about \nethical research practices and outputs. On that point, \nwe recommend the NRC collaborate with the Office of \nResearch Integrity (ORI) at the Department of Health and \nHuman Services to model their processes and procedures \nfor managing issues of research misconduct.21 The ORI \nhas substantial experience overseeing concerns about \nethical research practices. Parties could petition the NRC \nto revoke access when research is shown to manifestly \nviolate general ethical research standards or practices \napplicable to a researcher’s disciplinary domain. We note \nthat the NRC may want to adopt a high standard for such \na violation, given the academic speech considerations. For \nexample, federal agencies or external parties that wish to \nrevoke compute or data access from PIs would need to file \na written complaint with supporting evidence. Decisions \nto revoke access should require input from NRC executive \nleadership and legal counsel. \nFor PIs requesting access beyond base-level compute \nor for restricted datasets, we recommend requiring the \ncompletion of ethics impact statements to be submitted \nwith research proposals. A recent proposal to address \nthe lack of “widely applied professional ethical and \nsocietal review processes” in computing piloted such a \nrequirement in a grant process, requiring a description of \nthe potential social and ethical impacts and mitigation \nefforts by researchers.22 We limit this approach to \nproposals for compute access beyond default allocation \nor requests for access to restricted datasets, as the \nadministrability concerns are weaker for researchers who \nare already applying for compute or data access beyond \nthe default levels. For those applications, a review \nprocess of a specific proposal will already occur by an \nexternal review panel of experts (Chapter 2), and, much \nlike the NSF requires statements of “Broader Impacts;”23\nstatements about the ethical considerations of the work \ncould easily be included. It is important to note that \nethics impact statements would be only one component \nof NRC applications and should be weighed in \nconjunction with other application materials. In addition \nto requiring researchers to carefully think through and \ndocument the potential impacts of their own work, the \nstatements may also serve as useful documentation of \npotential negative impacts and be of use to NRC staff \nwhen determining whether to provide access to specific \ntypes of data. Such assessments may also be helpful for \njournals, conferences, or universities addressing ex post \nconcerns about ethical impacts. \nNext, we recommend that the NRC employ a \nprofessional staff devoted to ethics oversight, similar \nto what we propose regarding data privacy in Chapters \n5 and 6. In addition to staff devoted to handling legal \ncompliance issues, the NRC needs staff with specialized \ntraining in AI ethics (as well as expertise in other \nsubdomains) to provide expert internal consulting to \nNRC applicants, as well as to aid in evaluating ethics \nimpact statements. Similarly, data privacy experts can \nidentify ethical privacy issues specifically related to data, \nsuch as whether consent has been properly obtained \nand documented. To ensure that decisions are based on \nthe merits, the NRC staff overseeing these issues must \noperate independently of other federal agencies and be \ninsulated from political interference. \n[E]mbedded ethics approaches \nmay . . . identify[] and address[] \nissues as the research proceeds, \nin contrast to ex ante review, \nwhere it may be too early to spot \nan issue, and ex post review, \nwhich may be too late.\nA Blueprint for the National Research Cloud 69\nCHAPTER 7\nWe acknowledge that these ethics review mechanisms \nmay not identify all instances where researchers use \nthe NRC in a way to conduct research that raises ethical \nquestions. Few review mechanisms could, particularly \nin light of the considerable ambiguity present in \nethics standards (see Appendix C). Nonetheless, these \nmechanisms can augment key academic checkpoints (IRB \nreview and peer review) in an administrable fashion that \ndoes not raise serious concerns about academic speech. \nLastly, we recommend that non-NRC parties explore \na range of measures to address ethical concerns in AI \ncompute. These may include an ethics review process or \napproaches widely deployed in bioethics by the National \nInstitutes of Health, namely to incentivize the embedding \nof ethicists in research projects.24 Such embedded \nethics approaches may have the particular advantage \nof identifying and addressing issues as the research \nproceeds, in contrast to ex ante review, where it may be \ntoo early to spot an issue, and ex post review, which may \nbe too late. We expect this to be an active area of inquiry \nas new approaches are validated. The NRC, potentially in \nconjunction with the NSF, should consider offering funding \nfor projects that embed ethics domain experts into teams, \nin order to support this proposal.\nA Blueprint for the National Research Cloud 70\nCHAPTER 8\nChapter 8: Managing \nCybersecurity Risks \nWhile the NRC has the potential to level the \nplaying field for AI research, it will also create an \nalluring target for a vast array of bad actors. \nCybersecurity—the effort to protect \nsystems against incidents that may compromise \noperations or cause harm to relevant assets \nand parties—will be a critical focus of the NRC. \nIt will require a cybersecurity framework that \nmanages potential incidents throughout their \nlifecycle, spanning: (1) preparation; (2) detection \nand analysis; (3) containment, eradication, and \nrecovery; and (4) post-incident activity, which \ncollectively encompasses incident monitoring, \ndetection, recovery, and reporting.1\n Effective \ncybersecurity practices complement risk \nassessment based on impact, immediacy, and \nlikelihood, and will help gain the trust of users and thwart subversion and interference \nfrom foreign actors or other adversarial parties. Careful administrative design of the \nNRC with cybersecurity at the forefront will set a high standard as information systems \nbecome more central to our national infrastructure.\nIn this chapter we address these cybersecurity concerns. We first provide an \noverview of common types of vulnerabilities and attacks, and assess their relevance \nto the NRC. Next, we provide an overview of the federal government’s regulatory \nlandscape, as it pertains to cybersecurity, with a special focus on the FISMA and \nFedRAMP frameworks. Finally, we close with a discussion of the security and system \ndesign measures best suited to ensure that the integrity of the NRC is not compromised.\nMOTIVATIONS FOR POTENTIAL ATTACKS\nPossible attacks against the NRC could take a number of approaches, each of \nwhich would entail substantial consequences for the NRC.2 First, adversaries could \nlaunch an attack against the NRC with the intention of disrupting its operations or its \nability to aid research. For example, adversaries could attack the NRC’s infrastructure \ndirectly by disabling or interfering with NRC servers. As a result, researchers would be \nunable to access NRC servers or effectively utilize them. By launching such attacks, \nadversaries may throttle the NRC, thereby raising costs for the federal government.3\nAlternatively, adversaries could seek to attack specific research projects on the NRC, \nKEY \nTAKEAWAYS\n Deterring malicious actors \nfrom attacking the NRC will \nrequire more than adhering \nto current FISMA and \nFedRAMP standards.\n The NRC should centralize \nsecurity responsibilities for \ndatasets with the program’s \nstaff rather than deferring \nto originating agencies.\n Technical measures the \nNRC should investigate \ninclude confidential \nclouds, federated learning, \nand cryptography\u0002based measures such as \nhomomorphic encryption \nand secure multiparty \ncomputation.\nWhile the NRC \nhas the potential \nto level the \nplaying field for AI \nresearch, it will also \ncreate an alluring \ntarget for a vast \narray of bad actors.\nA Blueprint for the National Research Cloud 71\nCHAPTER 8\nthereby slowing the pace of that research or compromising \nthe quality of the research findings. They may also initiate \n“data-poisoning” attacks on NRC datasets, thereby \ncompromising the quality of research findings. \nSecond, bad actors could also launch cyber operations \nagainst the NRC, intending to steal computational \nresources. In this case, the purpose would not be to disrupt \nthe NRC, but to repurpose computational power toward \nillicit purposes (e.g., cryptocurrency mining).4 For instance, \nindividuals could pretend to be researchers, claiming to \nuse cloud credits for legitimate research purposes while \nactually using them for alternative ends. Individuals \ncould also infiltrate the NRC’s network, siphoning off \ncomputational resources from other projects and reducing \nthe functionality for legitimate users. \nThird, adversaries might pose a threat to the NRC out \nof a desire to steal or make use of the data and research \nproducts housed within the system. The NRC promises \nto be an attractive target because it will house data from \na range of different agencies. If an adversary wanted to \nsteal equivalent data from the agencies themselves, they \nwould need to break into each agency independently. \nHowever, the potential combination of datasets on the \nNRC, including researcher-owned datasets, may increase \nthe potential gains from accessing this information. \nAdditionally, adversaries may attempt to break into \nthe NRC in order to steal products generated by NRC \nresearchers. This could include trained machine-learning \nmodels or specific research findings.\nRelatedly, bad actors could determine that executing \nintrusions into the NRC is an effective way to target \nAffiliated Government Agencies. Because a participation \nincentive for agencies is the computing support that \nthe NRC will offer, one of the biggest cyber risks is of \nmalicious actors attempting to use the NRC to hack into \ntheir systems. For that reason, the cybersecurity risk to \nthe government may be substantial. On the other hand, \nas we discussed in Chapter 3, the NRC also presents an \nopportunity to enhance and harmonize security standards \ncompliance, as agencies move into the cloud. \nA range of other motivations may exist. Successful \noperations against the NRC, as a federal entity, would \ncarry symbolic value and capture attention. Ransomware \nattacks could result in significant payoffs. The NRC could \nalso be a target for espionage, both on the part of nation\u0002state actors seeking to acquire sensitive datasets (e.g., \nenergy grid infrastructure) and on the part of private sector \nentities looking to steal intellectual property or to monitor \nthe latest technological advances.\nIf successful, any attack could undermine the NRC. \nFor example, researchers would be deterred from using \nthe NRC and may invest their efforts in alternate private \nclouds. This could occur because researchers believe \nthe NRC would be ineffective to use (e.g., on account of \nfrequent server outages), or because they believe their \nresearch products would be inadequately protected. \nFederal agencies and departments could be deterred from \nentrusting the NRC with sensitive datasets. Federal entities \ncould risk embarrassment and face obstacles executing \ntheir policy objectives if datasets were accidentally leaked. \nIf the NRC is insufficiently secure, such entities may choose \nto avoid sharing data altogether.\nFISMA, FEDRAMP, AND EXISTING \nFEDERAL STANDARDS\nAs a federal entity, the NRC will be subject to federal \nstandards and regulations. In this section, we provide a \nhigh-level overview of the two most relevant regulations: \nthe Federal Information Systems Management Act (FISMA) \nand the Federal Risk and Authorization Management \nProgram (FedRAMP).5 FISMA traditionally applies to non\u0002cloud systems that support a single agency, whereas \nFedRAMP authorization is required for cloud systems.6\nWe finish by discussing critiques of these regulations. \nFISMA \nThe Federal Information Systems Management Act \n(FISMA) was first passed in 2002, with the purpose of \nproviding a comprehensive framework for ensuring the \neffectiveness of security controls for federal information \nsystems.7\n The law was later amended in 2014, and has \nA Blueprint for the National Research Cloud 72\nCHAPTER 8\nsince been augmented through other individual legislative \nand executive actions, and our discussion focuses on the \ncollective impact of FISMA compliance regulations.8\nFISMA applies to all federal agencies, contractors, \nor other sources that provide information security for \ninformation systems that support the operations and \nassets of the agency.9 It invests responsibility in several \ndifferent entities. First, the National Institute of Standards \nand Technology (NIST) is tasked with developing uniform \nstandards and guidelines for implementing security \ncontrols, evaluating the riskiness of different information \nsystems and other methodologies.10 Second, the Office of \nManagement and Budget (OMB) is tasked with overseeing \nagency compliance with FISMA and reporting to Congress \non the state of FISMA compliance.11 Third, the Department \nof Homeland Security is tasked with administering the \nimplementation of agency information security policies \nand practices.12 Finally, federal agencies are required \nto develop and implement a risk-based information \nsecurity program in compliance with NIST standards and \nOMB policies.13 Agencies are further required to conduct \nperiodic assessments to ensure continued efficiency and \ncost effectiveness.14\nSeveral NIST requirements are worth mentioning \nhere. Pursuant to NIST SP 800-18, agencies are required \nto identify relevant information systems falling under the \npurview of FISMA. Agencies must also categorize each of \nthese systems into a risk level, following the guidance laid \nout in FIPS 199 and NIST 800-60.15 NIST 800-53 outlines \nboth the security controls that agencies should follow \nand the manner in which agencies should conduct risk \nassessments.16 Agencies must further summarize both \nthe security requirements and implemented controls \nin “security plans,” as outlined in NIST 800-18.17 Finally, \norganization officials are required to conduct annual \nsecurity reviews in accordance with NIST 800-37. \nFEDRAMP \nIn the late 2000s, federal agencies began expressing \nsecurity concerns as a barrier to cloud computing \nadoption.18 In response, Congress passed the 2011 \nFederal Risk and Authorization Management Program \n(FedRAMP) to provide a cost-effective, risk-based approach \nfor the adoption and use of cloud services by the federal \ngovernment.19 FedRAMP approval is exempted where: \n(i) the cloud is private to the agency; (ii) the cloud is \nphysically located within a federal facility; and (iii) the \nagency is not providing cloud services from the cloud\u0002based information system to any external entities.20 Like \nFISMA, FedRAMP security requirements are governed by \nNIST standards, including NIST SP 800-53, FIPS 199, NIST \n800-37, and others.21 However, unlike FISMA, FedRAMP’s \ntwo tracks to receiving an authority-to-operate means \nthat vendors working with multiple agencies do not \nnecessarily need to undergo the full approval process with \neach agency. This means that cloud services providers and \nagencies alike are able to save significant time and money.\nCRITICISMS OF FISMA AND FEDRAMP \nThese regulations are not without fault. Most notably, \ncritics point to the fact that despite their existence, cyber \nintrusions on government infrastructure are common \nand accelerating.22 A 2019 report by the U.S. Senate \nCommittee on Homeland Security and Governmental \nAffairs investigating eight agencies noted that the \nfederal government is failing its legislative mandate \nfrom FISMA.23 The errors identified included a failure to \nprotect personally identifiable information, inadequate \nIT documentation, poor remediation of bugs, a failure \nto upgrade legacy systems, and inadequate authority \nvested in agency chief information officers.24 Reports by \nthe Government Accountability Office (GAO) have reached \nsimilar conclusions.25 In turn, some have criticized the \ngovernment’s approach to cybersecurity wholesale, \narguing it places too much emphasis on merely detecting \nintrusions.26 They argue for a framework of “zero trust,” \nwhich assumes that intruders will penetrate a network and \ninstead focus on security controls limiting the ability of \nthose intruders to navigate the network.27\nFedRAMP faces its own criticisms. A recent study \nnoted that securing authorization can be time-consuming \nand expensive—taking up to two years and costing \nmillions of dollars in some cases.28 Even though parts of \nFedRAMP are designed to be reusable across agencies, \nagencies often delay the process by imposing separate, \nA Blueprint for the National Research Cloud 73\nCHAPTER 8\nadditional requirements. A variety of reasons for these \ndeficiencies have been noted, including an understaffed \nJoint Authorization Board, a lack of trust between agencies \nwith regards to Authorization to Operate (ATOs), and an \noverly complex authorization process that leads to errors \nby agencies and Cloud Services Providers.29 Proposed \nrecommendations to address these deficiencies include \nincreased funding for FedRAMP’s Joint Authorization \nBoard, incentives to encourage reuse of ATOs, and \nmechanisms to improve the efficiency of the authorization \nprocess.30\nOn May 12, 2021, the Biden administration released \nan Executive Order (EO) on Improving the Nation’s \nCybersecurity,31 and OMB published a draft federal strategy \nfor public comment on September 7, 2021.32 Signed in the \naftermath of the breach of the software vendor SolarWinds, \nand the ransomware attack on Colonial Pipeline, the EO \npresents several new initiatives. First, it calls on the federal \ngovernment to embrace “zero-trust architecture” and \nimprove post-attack investigation processes. Second, it \nseeks to improve collaboration between the public and \nprivate sectors by improving disclosure requirements and \nestablishing a private-public Cybersecurity Safety Review \nBoard (modeled after the National Transportation Safety \nBoard). Finally, it seeks a more cohesive government-wide \napproach to cybersecurity, calling for the creation of a \nplaybook to standardize cyber response across federal \nagencies, alongside a government-wide detection and \nresponse system for attacks. \nThough it is too soon to determine whether the EO \nand the proposed strategy will be effective, it appears to \naddress deficiencies identified in the existing landscape. \nIt seeks to improve documentation and responsiveness \nto attacks and suggests a shift in cybersecurity thinking. \nIt is unclear, however, whether it will address the \nunderlying procurement issues and lack of interagency \ntrust that critics believe have hampered the effectiveness \nof FedRAMP. But given the potential for highly sensitive \ndata to be stored on the NRC, embracing a zero-trust \narchitecture at the outset is a crucial consideration for \nensuring its integrity.\nNRC SECURIT Y STANDARDS AND \nSYSTEM DESIGN MEASURES\nHere, we present recommendations on cybersecurity \npolicy for the NRC informed by the landscape of the \nexisting federal regulations and unique considerations that \na national research cloud will pose.\nPROCESS FOR RISK AND SECURITY DETERMINATIONS\nUnder the current regulatory landscape, agencies \nare responsible for determining the appropriate risk \ncategorizations and security controls for the datasets \nlocated on their servers. However, this raises a potential \nchallenge as agencies begin to share their data with the \nNRC—making it unclear who will maintain authority for \ncategorizing the risk of these datasets and determining \nappropriate security controls. \nOn the one hand, agencies themselves could continue \nto retain discretion over the security classification \nand controls for datasets they place into the NRC. In \nthis decentralized approach, much of the security \nresponsibilities assigned by FISMA would remain with \nthe agencies, irrespective of whether the data existed \non NRC servers. On the other hand, the NRC could take \nresponsibility for all security decisions. Datasets added to \nthe NRC would then be classified according to the NRC’s \nassessment of risk, and protected with controls that the \nNRC staff deems appropriate. This approach “centralizes” \nsecurity responsibilities by vesting it with the NRC after the \nonetime negotiation for each dataset.\nThough both approaches have their merits, we \nrecommend the centralized approach for several reasons. \nFirst, the centralized approach ensures internal uniformity. \nThe paradox of federal cybersecurity regulation is that \nalthough NIST has articulated a set of standards pertaining \nto risk and controls, agencies interpret these standards \ndifferently, leading to discrepancies in implementation and \nclassification across the federal government. Following \neach agency’s security classifications for data on the NRC \nwould produce unnecessarily complex and incoherent \nclassifications for a single system. This threatens \nto diminish the usability of the NRC, and the added \nA Blueprint for the National Research Cloud 74\nCHAPTER 8\ncomplexity could arguably weaken security by increasing \nthe likelihood of errors. Permitting the NRC to impose its \nown classifications allows for uniformity within the NRC \nand alignment with the access tiers suggested in \nChapter 3 of this White Paper. This approach may also \nsimplify managing security practices across a potential mix \nof cloud compute providers.\nSecond, the NRC represents a valuable opportunity \nto harmonize federal cybersecurity standards across \ndifferent agencies. The assessments and implementations \nadopted by the NRC must generalize to the full diversity of \nfederal datasets. Hence, the NRC’s practices can serve as \na template for NIST’s guidelines, which any agency is free \nto adopt. \nThird, the centralized approach will remove hurdles \nfor data sharing. Security concerns often impede agency \ndata sharing. In a scheme where agencies retain control \nover all security determinations, agencies could demand \nsecurity classifications that are excessively high or \nimpractical to implement. The centralized approach \nwould place the burden on agencies to articulate with \nspecificity why the NRC’s security policies or classification \nguidelines are inadequate for a particular dataset. \nFinally, researchers should also have a voice in \ndetermining the appropriate security controls, since a \npublic resource of this magnitude that cannot attract users \nis bound to fail. As security controls implicate usability, \nthe NRC should not opt for controls that substantially \ninhibit or disincentivize researchers from leveraging its \nresources. The NRC needs to strike the right balance \nbetween usability and security.\nTECHNICAL CONSIDERATIONS\nThe federal government already possesses a range of \ntechnical options and countermeasures to cyberattacks. \nCybersecurity threats and defenses are, of course, actively \nevolving, so we discuss these only as a starting point—\nrobust, long-term cybersecurity comes through continued \nvigilance and prioritization that recognizes the shifting \nnature of the field. \nDATA STORAGE\nData storage mechanisms should ensure proper \nprotection from outside access. Encryption can be used \nto protect sensitive data at rest, to be later unencrypted \nwhen needed. Physical isolation through air-gapped \nenvironments is another design feature that can remove \nthe possibility of wireless network interfaces from being \nused to connect the data to malicious outside threats. \nHowever, even air gapping is not a foolproof solution: There \nare ways to “jump” air gaps such as through hiding in USB \nthumb drives (which is allegedly how the Stuxnet malware \nfamously compromised Iranian nuclear centrifuges).33 More \nrecent attacks bypass the need for electronic transmission \naltogether by leveraging other signals that leak data, \nsuch as FM frequencies, audio, heat, light, and magnetic \nfields. These kinds of threats bring home the need for a \ncomprehensive and evolving approach to cybersecurity. \nNETWORKING PROTOCOLS\nData packets sent over networks are transmitted \naccording to a set of internationally standardized internet \nprotocols. Following the Open Systems Interconnection \n(OSI) model, the conceptual layers involved in computer \nnetworking can be categorized into seven dimensions: \nphysical, data link, network, transport, session, \npresentation, and application layers.34 \nRUNTIME SECURITY\nWhen considering runtime security technologies, \nthree design features that are relevant for the cloud \nenvironments are the use of confidential clouds, \nfederated learning, and cryptography-based measures \nsuch as homomorphic encryption and secure multiparty \ncomputation. A growing number of vendors offer \n“confidential cloud” options as an emerging technical \nsolution to fully cyber secure cloud computation that is \nsecure throughout execution.35 Confidential clouds offer \nhigh-security, end-to-end, isolated operation by executing \nworkloads within trusted execution environments. For \nexample, virtualization enables an operating system \nto run another operating system within it as a virtual \nenvironment with additional firewall or other network \nA Blueprint for the National Research Cloud 75\nCHAPTER 8\nbarriers, effectively simulating another device within the \nhost computer. \nDISTRIBUTED COMPUTING AND FEDERATED LEARNING\nAnother computing paradigm, known as distributed \ncomputing or federated learning, considers situations \nwhere multiple parties have individual shards of data they \nare interested in leveraging in aggregate, without sharing \noutright. Federated learning addresses this situation, \nfor example, demonstrating how users’ mobile phones \ncan send information—possibly differentially private—to \ncentral servers without exposing the precise details of \nany one individual’s information. A second scenario more \nrelevant to the large-scale decentralized nature of the \nNRC is distributed computing—in which many institutions \ncollectively share compute, akin in some respects to \ncrowd-sourced computing. These approaches enable \nmultiple parties to leverage existing computational \ninfrastructure, while retaining some guarantees on \nprivacy. \nCRYPTOGRAPHY-BASED MEASURES\nFinally, there are two types of cryptography-based \nmeasures worth noting. \nCryptography researchers have developed ways of \ncomputing mathematical operations over encrypted data, \nknown as homomorphic encryption. This impressive \nfeat has valuable implications because it obviates the \nneed for decryption, which can potentially expose the \nintermediate values of computation, and grant access to \npublic and secret encryption keys during computation. \nInitially, only partially homomorphic encryption schemes \nthat supported limited arithmetic operations like addition \nand multiplication were possible. But fully homomorphic \nencryption schemes have recently been developed that \nenable what is known as “arbitrary” computation for \npromising use cases in predictive medicine and machine \nlearning. That said, standardization is still underway to \nbroader adoption, and homomorphic encryption (by \ndesign) is malleable—a property in cryptography that \nis usually undesirable as it allows attackers to modify \nencrypted ciphertexts without needing to know their \ndecrypted value. These and other limitations of any \ntechnical approach are worth taking into account when \nconsidering which technologies to adopt and for what \npurpose. \nComplementing the distributed, decentralized \ncomputing model discussed throughout this White Paper \nis the subfield known as secure multiparty computation \n(also known as privacy-preserving computation), \nwhich presents methods for multiple parties to jointly \ncompute a function over all their respective inputs, while \nkeeping those inputs private from other parties. These \nmethods have matured in their origins from a theoretical \ncuriosity to techniques with practical application in \nstudies on tax and education records, cryptographic key \nmanagement for the cloud, and more.36 This makes secure \nmultiparty computation methods a potential candidate \nfor applications pertaining to secure, distributed \ncomputation. \nUltimately, it will be central for the NRC to \ncontinuously learn about the most effective security \nstandards (including such other creative strategies as red \nteaming or bug bounties37 to identify vulnerabilities) in this \nrapidly evolving space.\nA Blueprint for the National Research Cloud 76\nCHAPTER 9\nChapter 9: \nIntellectual Property\nWho should own the IP rights to outputs developed using NRC resources?1 When \nprivate research is funded, subsidized, or influenced by the federal government, the \nlaws and rules have evolved, so that both the researcher and the government have \ncertain rights in the intellectual property developed under the research. While IP \nprotection is theoretically designed to incentivize research and innovation, some signs \nindicate that AI researchers in particular are already amenable to sharing the fruits \nof their research. Indeed, over 2,000 researchers signed a 2018 petition to boycott \na new machine intelligence journal started by Nature, because it promised to place \nits articles behind a paywall.2 The Open Science and Open Research movements \nhave also encouraged AI researchers to make their machine-learning software and \nalgorithms publicly available.3 Furthermore, as we discuss below, the advancement of \ntechniques like transfer learning depend on researchers being able to distribute the \nfruits of their research freely. \nThis chapter surveys the existing IP-sharing agreements between researchers and \nthe government, and explores whether and to what extent the government should \nretain IP rights over researchers’’ outputs, as a condition of using the NRC.4 While the \nevidence on optimal IP rights varies, we recommend that: (1) Academic researchers \nand universities should retain the same IP rights as the Bayh-Dole Act provides for \npatents developed under federally funded research; (2) The government should retain \nits copyrights and data rights under the Uniform Guidance, but contract around \nthose rights where applicable to incentivize NRC usage and AI innovation; and (3) \nThe government should consider conditions for requiring researchers to share their \nresearch outputs under an open-access license.\nKEY \nTAKEAWAYS\n To harmonize with the \nfederal grant process, the \nNRC should adopt the same \napproach to allocating \npatent rights, copyrights, \nand data rights to NRC \nusers as applies to federal \nfunding agreements.\n The NRC should contract \naround government \nintellectual property \nrights where applicable to \nincentivize NRC usage and \nAI innovation.\n The NRC should consider \nconditions for requiring \nresearchers to share \noutputs under an open\u0002source license.\n[A]cademic researchers and universities should retain \nthe same IP rights as the Bayh-Dole Act provides for \npatents developed under federally funded research.\nPATENTS RIGHTS \nA core question is whether NRC users should retain patent rights in inventions \nsupported by the NRC. The Bayh-Dole Act regulates patent rights for inventions \ndeveloped under federal funding agreements and its applicability depends on the \nA Blueprint for the National Research Cloud 77\nCHAPTER 9\nnature of NRC access; for instance, if cloud credits are \napportioned using federal grants, as described in \nChapter 2, they may be considered federal funding \nagreements.5 In such cases, Bayh-Dole Act permits \nresearchers to hold the title to the patent and to license \nthe patent rights.6 However, these patent rights come with \ncertain restrictions: For example, the funding agency has \na free, nonexclusive license to use the invention “for or on \nbehalf of the United States,” and the agency may use “[m]\narch-in rights” to grant additional licenses.7\nThe broader policy question about the government’s \nexercise of its patent rights is whether and how patents \nstimulate innovation in AI. Some commentators have \nargued that the U.S. suffers from over-patenting in \nsoftware,8\n and AI is no exception.9 The total number of AI \npatent applications received annually by the U.S. Patent \nand Trademark Office more than doubled from 30,000 in \n2002 to over 60,000 in 2018,10 and some argue that this \nproliferation of broad AI patents, especially those filed by \ncommercial companies, is hindering future innovation.11\nIn the Bayh-Dole context, researchers have also found that \nthe benefits of university patenting may justify the costs \nonly where industry licensees need exclusivity to justify \nundertaking the costs of commercialization, as, for instance, \nin the pharmaceutical context.12 For the substantial portion \nof university patenting, including AI, this rationale may not \ncarry much weight.13\nSome research shows that patents actually may not \nactually have any net effect on the amount or quality of \nAI research conducted in the university context. In an \nempirical study of faculty at the top computer science and \nelectrical engineering universities in the United States, \nresearch has found that the prospect of obtaining patent \nrights to the fruits of their research does not motivate \nresearchers to conduct more or higher-quality research.14\nEighty-five percent of professors reported that patent \nrights were not among the top four factors motivating their \nresearch activities, and 57 percent of professors reported \nthat they did not know whether or how their university \nshares licensing revenue with inventors.15 The patent \nscheme adopted by the NRC, therefore, may not have a \nstrong influence on researcher adoption. \nThat said, as a practical matter, there is a virtue \nto treating innovations stemming from NRC usage in a \nfashion that is consistent with Bayh-Dole. Particularly \nif cloud credits are awarded through the expansion of \nprograms like NSF CloudBank, it would be confusing to \nhave distinct patent rights out of the research and cloud \ngrant. In addition, many university tech transfer offices \nappear to have strong preferences for patent rights.16 To \nthe extent that universities view retaining patent rights \nas a condition for using the NRC, aligning NRC patent \nrights with Bayh-Dole may be preferred, but the evidence \nunderpinning this recommendation is not strong. \nCOPYRIGHT, DATA RIGHTS, AND \nTHE UNIFORM GUIDANCE\nThe Uniform Guidance (2 C.F.R. § 200) streamlines and \nconsolidates government requirements for receiving and \nusing federal awards to reduce administrative burden.17\nGrants.gov describes it as a “government-wide framework \nfor grants management,” a groundwork of rules for federal \nagencies in administering federal funding.18 The Uniform \nGuidance includes provisions on, for instance, cost \nprinciples, audit requirements, and requirements for the \ncontents of federal awards.19\nThe Uniform Guidance is applicable to “federal \nawards,”20 but IP provisions do not require the government \nto assert their rights over researcher outputs.21 Whether \nand how the government allocates its IP rights under the \nUniform Guidance is therefore an important question.\nThis section first covers government copyright \nand data rights to IP under the Uniform Guidance and \nThe government should . . . \nconsider conditions for requiring \nNRC researchers to disclose or \nshare their research outputs under \nan open-access license.\nA Blueprint for the National Research Cloud 78\nCHAPTER 9\ndiscusses how sharing copyright and data rights might \nimpact the AI innovation landscape. We then examine \nthe extent to which the government should retain its \nrights to research generated using the NRC. While the \nevidence is mixed, we ultimately recommend that the \ngovernment retain its copyrights and data rights under \nthe Uniform Guidance, but contract around those rights \nwhere applicable, to incentivize NRC usage and further AI \ninnovation. \nCOPYRIGHT\nUnder U.S. copyright law, NRC researchers can obtain \ncopyrights over various aspects of their work. For instance, \nNRC researchers may wish to copyright the software they \nused to build the model, since software is considered a \nliterary work under the Copyright Act.22 Researchers may \neven obtain copyrights over various aspects of the model, \nincluding the choices of training parameters, model \narchitectures, and training labels, if they can show that \nthose choices required creativity.23 Many scholars have \neven opined, without reaching consensus, on whether \noutputs such as text and art that are artificially generated \ncan be copyrighted.24\nUnder the Uniform Guidance,25 the recipient of \nfederal funds may copyright any work that was developed \nor acquired under a federal award. However, even \nif researchers are permitted to maintain copyrights, \nthe federal awarding agency reserves a “royalty-free, \nnonexclusive and irrevocable right to reproduce, publish, \nor otherwise use the work for federal purposes, and to \nauthorize others to do so.”26 Notably, this right is limited to \n“federal purposes,” meaning that third parties who acquire \nlicenses to the researchers’ copyrighted works cannot use \nthem for exclusively commercial purposes.27\nIt is unclear to what extent copyrights over NRC \noutputs should be fully vested in the researcher to \nstimulate basic AI research. One class of AI research \nand development output that has received significant \nacademic attention has been whether AI-generated \ncreative works, like music from OpenAI’s Jukebox,28 can \nor should receive copyright protection.29 However, the \ntechnology and copyright community has hardly reached \na consensus on whether the public interest in AI research \nrequires granting copyright in these scenarios. On one \nhand, in a survey of AI scientists, tech policy experts, and \ncopyright scholars, roughly 54 percent of respondents \nagreed that copyright protection is an important incentive \nfor authors to make their work commercially available, \nand 63 percent agreed that an increase in the number \nof commercially available AI-produced works would \nstimulate further AI growth and research.30 On the other \nhand, in the same survey approximately 56 percent of \nrespondents agreed that the U.S. Copyright Office should \ndeny copyright protection to creative works produced \nindependently by AI without creative intervention from a \nhuman author.31\nNotwithstanding the prominent debate about \ncopyright over creative works generated by AI models, such \nworks are only a subset of possible copyright protection \nin the AI context. As discussed above, researchers could \ntheoretically seek additional copyright protection over, \namong other things, their code, architecture, or model. \nHere, AI innovation may depend on sharing these \ncopyrightable elements. For instance, transfer learning \nuses existing ML models and “fine-tunes” those models for \na related target task,32 and various fine-tuning approaches \nhave emerged to perform transfer learning on different \nclasses of tasks.33\nDATA RIGHTS\nUnder the Uniform Guidance, when “data” is \n“produced” under a federal award, the government \nreserves the right to: (1) obtain, reproduce, publish or \notherwise use such data; and (2) authorize others to \nreceive, reproduce, publish or otherwise use such data.34\nNotably, this does not limit the use of such data for \nfederal government purposes. In other words, such data \ncan be promulgated for any use. The outstanding question, \ntherefore, is whether this “data,” which is not explicitly \ndefined in the Uniform Guidance, covers data generated \nfor AI and machine-learning purposes. Below, we examine \ntwo classes of data generated for AI purposes—synthetic \ndata and data labels—and how sharing this data could \nimpact AI innovation. \nA Blueprint for the National Research Cloud 79\nCHAPTER 9\nOne class of data generated for AI purposes is \nsynthetic data. Researchers have turned to deep \ngenerative models such as Variational Autoencoders35 and \nGenerative Adversarial Networks36 to generate synthetic \ndata to train their machine learning models. As noted by \nthe World Intellectual Property Organization, synthetic \ndata is an entirely new class of data that does not fit \nneatly under existing IP law.37 While a researcher may seek \ncopyright protection over the subset of synthetic data that \nis “creative,” therefore implicating the copyright provisions \nof the Uniform Guidance (described above), the broad \nclass of synthetic data, whether “creative” or not, may \nalso implicate the data rights provision. On the one hand, \ntraining data is often carefully guarded,38 so requirements \nto share synthetic data, which is often used to train AI \nmodels, may be a non-starter for NRC users. On the other \nhand, many scholars have written about the promise of \nsynthetic data to actually enable further data sharing by \npreserving privacy and researchers’ trade secrets.39 In fact, \nsharing synthetic datasets would spur additional research \nand innovation in fields such as healthcare, where data \nsharing has been limited.40\n Another class of data generated for AI is labeled data, \nnamely data that has been tagged and classified to provide \nground truth for supervised machine learning models.41\nWhile techniques have been developed to decrease the \ncosts associated with data labeling,42 it nevertheless \nremains a resource and time-intensive task. For example, \nCognilytics Research reports that 25 percent of the total \ntime spent building machine learning models is devoted to \ndata labeling.43 Researchers using the NRC may therefore \nseek to protect their investment in data labeling by opting \nnot to share their labels with others, especially if the \nunderlying data is proprietary.44 However, recognizing the \ndifficulty of data labeling, some researchers have built \nonline platforms for sharing data labels.45 In the case \nof ImageTagger, a data labeling and sharing platform \nfor RoboCup Soccer, the developers wanted to solve \nthe problem that no single team, acting alone, could \neasily build its own high-quality training sets.46 Similarly, \nin the NRC’s case, the sharing of labeled government \ndata—where labeling may have been augmented by NRC \nresources47—could act as a rising tide that lifts all boats, \nimproving the quality of not only the government data \nas a training dataset, but also all subsequent research \nusing that data. Furthermore, sharing data labels could \nbe instrumental in conducting bias and fairness of NRC \nresearch outputs where necessary, as discussed in \nChapter 7.\n48\nRETAINING IP RIGHTS IN THE UNIFORM GUIDANCE\nAs the preceding discussion suggests, sharing AI \nresearch output covered by copyrights and data rights \ncould be beneficial to AI innovation. We therefore \nrecommend that the NRC at least retain the same rights to \ncopyrights and data rights as under the Uniform Guidance, \nyielding several additional benefits. First, similar to our \nrecommendation in Chapter 3 that federal agencies \nshould be allowed to use the NRC’s compute resources, \nretaining the same Uniform Guidance IP allocation scheme \ncould produce welfare benefits by improving government \ndecision-making using AI. For instance, federal agencies \ncan reduce the cost of core governance functions and \nincrease agency efficiency and effectiveness by using \ndata labels shared by NRC researchers or by fine-tuning \nmodels generated by NRC researchers. Second, retaining \nthe Uniform Guidance IP allocation scheme would result \nin more consistency across the federal award landscape. \nIndeed, as mentioned above in the patent context, it \ncould be confusing to diverge from the Uniform Guidance, \nespecially if the cloud credit grant is apportioned through \nprograms like CloudBank but the research grant is \nadministered as a federal award.\nIn sum, we recommend that the government at least \nretain its copyrights and data rights under the Uniform \nGuidance. However, we also reiterate that the Uniform \nGuidance serves merely as a helpful framework, not as an \nimmutable rule. Where the Uniform Guidance IP allocation \nwould dissuade researchers from using the NRC or hinder \nAI innovation in specific scenarios, the government \ncan and should explicitly modify its rights and contract \nseparately with researchers on what rights the government \nretains, if any.\nA Blueprint for the National Research Cloud 80\nCHAPTER 9\nCONSIDERATIONS FOR OPEN\u0002SOURCING\nShould the government go beyond its rights and \nmandate that researchers share their NRC research \noutputs with others under an open-source license? As \nan initial matter, we note that agencies can modify the \nIP allocation schemes under the Uniform Guidance, but \nnot under the Bayh-Dole Act. Some federal agencies \nsupplement and/or replace the IP rights set out in the \nUniform Guidance with restrictions that are more specific \nto the IP being developed for that particular agency or \nunder a specific award.49 For instance, the Department of \nLabor requires that intellectual property developed under \na federal award must not only comply with the terms \nspecified in the Uniform Guidance, but also be available \nfor open licensing to the public.50 NSF grantees are also \nexpected to share their data with others.51 However, \nthe government cannot change the allocation of patent \nownership under the Bayh-Dole Act, unless the Act itself is \nmodified or unless the NRC isn’t administered as a federal \naward, rendering the Act inapplicable. \nRequiring researchers to open-source their research \noutputs may be possible, but the considerations \naround it are complex. On the one hand, an open\u0002source requirement could negatively affect downstream \ncommercialization, given the wide range of potential AI \nresearch.52 While the NRC might protect commercialization \nto some degree by adopting a restrictive open-source \nlicense,53 the mere divergence from the Uniform Guidance \nor the Bayh-Dole Act could be confusing for researchers \nin navigating federal awards and understanding open\u0002source licensing interactions across multiple situations.54\nFurthermore, requiring researchers to share research \noutputs comes with its own host of privacy and \ncybersecurity issues.55 If researchers are permitted to use \nthe NRC to conduct classified research,56 for instance, then \nkeeping research outputs proprietary would serve the \nnational interest.57 In this case, however, the NRC should \nconsider limiting any open-source requirement to research \nthat has fewer privacy and security implications.\nOn the other hand, as discussed, sharing research \noutputs with other NRC researchers could be beneficial, \nand many scholars argue that AI researchers should \nopen-source their software to stimulate innovation.58 A \nrequirement to open-source software code, which can \nbe the subject of both copyrights and patent rights,59\nmay contravene Bayh-Dole and face challenges from \nuniversities that seek to retain their patent rights, but \nsoftware patent disclosures alone are often limited and \nover-broad, and fail to enhance social welfare.60 Requiring \nfuller disclosure of code generated on the NRC can \ntherefore decrease the risk of over-patenting and increase \nAI innovation. The growth of the robust open-source and \nopen science movements also suggests that an open\u0002sourcing requirement for the NRC would not be a complete \nbarrier to NRC usage.61\nA strong argument for mandating open-sourcing \nalso comes from the increasing private- sector reliance \non trade secrets for IP protection in AI.62 Some argue that \nthis heightened emphasis on trade secret protection \nconstitutes “artificial stupidity,”63 as it has stifled \ninnovation in AI by preventing disclosure, providing \nprotection for a potentially unlimited duration, and \nattaching immediately and broadly to any output with \nperceivable economic value.64 The reliance on secrecy, \ntherefore, contravenes many of the principles described \nabove—which argue that sharing code and data is crucial \nin AI—and results in significant AI industry consolidation \nand suboptimal levels of AI innovation.65 This harkens back \nto the goal of the NRC discussed in Chapter 1: To address \nproblems with AI research being concentrated in the hands \nof a few private-sector players. Because the NRC should \nexplicitly avoid replicating these private-sector challenges, \nthis lends additional support to a recommendation that \nthe NRC should contemplate requiring researchers to share \ntheir research outputs.\nIn sum, while AI raises a host of novel IP issues (e.g., \nwhether AI output is itself eligible for IP protection), \nwe think the government can steer clear of many of \nthese complications by tracking Bayh-Dole and the \nUniform Guidance. The government should also consider \nconditions for requiring NRC researchers to disclose or \nshare their research outputs under an open-access license.\nA Blueprint for the National Research Cloud 81\nConclusion \nAs we have articulated in this White Paper, the ambitious call for an NRC has transformative potential for the AI \nresearch landscape.\nIts biggest promise is to ensure more equitable access to core ingredients for AI research: compute and data. \nLeveling this playing field could shift the current ecosystem from one that focuses on narrow commercial problems to \none that fosters basic, noncommercial AI research to ensure long-term national competitiveness, to solve some of the \nmost pressing problems, and to rigorously interrogate AI models.\nAs we have spelled out in this White Paper, the NRC does raise a host of policy, legal, and normative questions. How \ncan such compute resources be provided in a way that is expeditious and user-friendly, but does not preclude the \npotential cost savings from a publicly owned resource? How can the NRC be designed to adhere to the Privacy Act of \n1974, which was animated by concerns about a national system of records that surveils its citizens? How can we ensure \nthat NRC mitigates, rather than heightens, concerns about the unethical use of AI? And how can one prevent the NRC \nfrom becoming the biggest target for cyberattacks?\nThese are tough questions, and we hope to have sketched out our initial attempt at answers above. We are hopeful, \nif designed well, the NRC could help to realign the AI innovation space from one that is fixated with short-term private \nprofit to one that is infused with long-term public values.\nA Blueprint for the National Research Cloud 82\nADP Alberta Data Partnerships\nADR UK Administrative Data Research UK\nADRF Administrative Data Research Facility\nAI artificial intelligence\nAPI application programming interface\nARPA Advanced Research Projects Agency\nARPANET Advanced Research Projects Agency \nNetwork\nATO authority-to-operate\nATO Authorization to Operate\nAWS Amazon Web Services\nCaaS Compute as a Service\nCIPSEA Confidential Information Protection\nand Statistical Efficiency Act\nCMS Centers for Medicare & Medicaid Services\nCPL California Policy Lab\nCPU central processing unit\nDFARS Defense Federal Acquisition Regulation \nSupplement\nDHS U.S. Department of Homeland Security\nDOD U.S. Department of Defense\nDOE U.S. Department of Energy\nDOT U.S. Department of Transportation\nDUA Data Use Agreement\nEBPA Foundations for Evidence Based \nPolicymaking Act or Evidence Act\nEO Executive Order\nESRC Economic and Social Research Council\nEULA End-User Licensing Agreement\nFAA Federal Aviation Administration\nFAR Federal Acquisition Regulation\nFDS Federal Data Strategy\nFedRAMP Federal Risk and Authorization Management \nProgram\nFFRDC Federally-Funded Research and Development \nCenter\nFIPS Federal Information Processing Standards\nFISMA Federal Information Security Modernization \nAct\nFSRDC Federal Statistical Research Data Center\nGAO U.S. Government Accountability Office\nGCP Google Cloud Platform\nGDPR General Data Protection Regulation\nGPS Global Positioning System\nGPU graphics processing unit\nGSA U.S. General Services Administration\nHAI Stanford Institute for Human-Centered \nArtificial Intelligence\nHECToR High-End Computing Terascale Resource\nHHS U.S. Department of Health and Human \nServices\nHIPAA Health Insurance Portability and \nAccountability Act\nHPC high-performance computing\nHTTP Hypertext Transfer Protocol\nHTTPS Hypertext Transfer Protocol Secure\nIC U.S. Intelligence Community\nIDA Institute for Defense Analyses\nIPTO Information Processing Techniques Office\nIRB Institutional Review Board\nJV joint venture\nLIDAR Light Detection and Ranging\nML machine learning\nMOU memorandum of understanding\nGlossary of Acronyms\nA Blueprint for the National Research Cloud 83\nNAIRR National Artificial Intelligence Research \nResource Task Force Act\nNASA National Aeronautics and Space \nAdministration\nNDAA National Defense Authorization Act\nNIH National Institutes of Health\nNISE NSF’s Directorate for Computer and \nInformation Science and Engineering\nNIST National Institute of Standards and \nTechnology\nNIST SP NIST Special Publications\nNOAA National Oceanic and Atmospheric \nAdministration\nNORC National Opinion Research Center\nNRC National Research Cloud\nNSCAI National Security Commission on Artificial \nIntelligence\nNSDS National Secure Data Service\nNSF National Science Foundation\nODNI Office of the Director of National Intelligence\nOECD Organization for Economic Co-operation and \nDevelopment\nOMB U.S. Office of Management and Budget\nONS Office for National Statistics\nORNL Oak Ridge National Laboratory\nOLCF Oak Ridge Leadership Computing Facility\nOSI Open Systems Interconnection\nOT Other Transaction\nPCLOB Privacy and Civil Liberties Oversight Board\nPHI protected health information\nPHS Stanford Center for Population \nHealth Sciences\nPI principal investigator\nPII personally identifiable information\nPPP public-private partnership\nR&D research and development\nRFI Request for Information\nRFP Request for Proposal\nRIST Research Organization for Information \nScience and Technology\nSDSC San Diego Supercomputer Center\nSRCC Stanford Research Computing Center\nSSL Secure Sockets Layer\nSTPI Science & Technology Policy Institute\nTLS Transport Layer Security\nUC Berkeley University of California, Berkeley\nUC San Diego University of California, San Diego\nUCLA University of California, Los Angeles\nUSDA U.S. Department of Agriculture\nVRDC CMS’ Virtual Research Data Center\nWIPO World Intellectual Property Organization\nA Blueprint for the National Research Cloud 84\nAppendix\nA. COMPUTING INFRASTRUCTURE \nCOST COMPARISONS\nThis Appendix provides a sample cost-estimate \ncomparison between a commercial cloud service, AWS, and \na dedicated government HPC system, Summit. In sum, our \nestimations show that AWS P3 instances with comparable \nhardware to Summit would be 7.5 times as expensive \nas estimated costs under constant usage, and 2.8 times \nSummit’s estimated costs under fluctuating demand.\nTable 3 lists the three infrastructure models used \nin this comparison. Summit was used as the reference \ngovernment HPC system because it is one of the DOE’s \nnewest systems and has hardware well-suited for AI \nresearch.1 The other infrastructure model used is AWS EC2 \nP3.2 Both are commonly used in AI research and general \nHPC applications. Other commercial cloud platforms, \nsuch as GCP or Azure, could also feasibly provide the \ninfrastructure for the NRC. AWS EC2 P3 was used here \nbecause AWS has a robust cost calculator that allows for \nvariable workloads. \nThe number of AWS instances were set such that those \nmodels would have the exact same number of GPUs as \nSummit. GPUs were the fixed variable because GPUs are \nthe most important hardware for AI research applications, \nspecifically deep learning. Both Summit and AWS P3 \ninstances use NVIDIA V100 GPUs.\nWe conduct our cost comparison for the two \ninfrastructure models over five years, as Summit’s initial \nRFP documents include a five-year maintenance contract. \nAWS, however, only provides one-year or three-year pricing \nplans, so we extrapolated the five-year cost based on its \nthree-year plan.\nFor the cost estimate of Summit, we based our \ncalculation on the budget details in the original \nDepartment of Energy (DOE) Request for Proposal \n(RFP) in January 2014.3\n The RFP includes a $155 million \nmaximum budget for building Summit, an expected $15 \nmillion maximum for the non-recurring engineering cost,4\nand around $15 million for five-year maintenance,5 plus \ninterest based on the U.S. Treasury securities at five-year \nconstant maturity as specified in the price schedule.6 Upon \ncalculation, we estimated Summit costs around $192 \nmillion in total, which is consistent with public reporting of \nthe cost of Summit.7\nFor the cost estimate of AWS, we used the AWS pricing \ncalculator, choosing U.S. East (N. Virginia) as the data \ncenter and publicly available rates under the cheapest \npossible pricing plan (EC2 Instance Savings Plans). To \napproximate a negotiated discount, we applied a 10 \npercent discount based on the negotiated rate of one \nmajor university.\nSince commercial cloud platform costs scale with \nhow many instances are actually in use, two costs were \ncalculated for each AWS model representing usage \nextremes: (1) with the infrastructure under constant usage; \n(2) with the infrastructure under dramatically fluctuating \nusage each day. For the daily spike traffic calculation, \nwe set the model to run five days a week with 8.4 hours \neach day at peak performance. The maximum number of \ninstances used is the same as one would use for constant \nuse while the minimum number is zero. This workload \nsetting is based on the assumption that GPUs used for \ntraining AI models sit idle 30 percent of the time.8\n These \nestimates should provide hard upper and lower bounds on \ncosts for using each instance type. \nFigure 1 plots costs on the y-axis over a five-year \nperiod on the x-axis. The turquoise line indicates the \ncost to a Summit-like system and the purple and blue \nlines indicate the cost of the same AWS instances under \nvariable and constant usage. Overall, this simple analysis \ncorroborates the analysis conducted by Compute Canada, \nwhich found that commercial cloud “ranged from 4x to \n10x more than the cost of owning and operating our own \nclusters.”9\n Over five years and under constant usage, \nAWS P3 instances with comparable hardware to Summit \nwould be 7.5 times as expensive as estimated costs. Under \nfluctuating demand, AWS P3 instances would cost 2.8 \ntimes Summit’s estimated costs.\nA Blueprint for the National Research Cloud 85\nWe note that this simple analysis omits many potential factors (see discussion in Chapter 2), but provides a starting \npoint to understanding the considerable cost implications for the make-or-buy decision. \nFIGURE 1: ESTIMATED COST OF AWS INSTANCES COMPARED TO SUMMIT OVER 3 YEARS\nTABLE 3: SUMMIT & AWS COMPARISON\nSummit IBM AC922\nAWS p3dn.24xlarge\n(3456 nodes)\nGPUs RAM Network Bandwidth\n27,648\n(NVIDIA Volta V100)\n27,648\n(NVIDIA Volta V100)\n2.8 PB\n2.6 PB\n200 Gb/s\n100 Gb/s\n$1,600\n$1,400\n$1,200\n$1,000\n$800\n$600\n$400\n$200\n$0\n0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60\nMillions\nMonths\nAWS P3 (Constant Usage) AWS P3 (Daily Spike Traffic) Summit\nA Blueprint for the National Research Cloud 86\nB. FACILITATING PRIVATE DATASET \nSHARING\nUnique IP challenges arise if researchers are permitted \nto share their own private datasets with the NRC. Indeed, \nresearchers who “upload” proprietary data may be \nconcerned about how other NRC users utilize that data.10\nThrough interviews conducted for this White Paper, \ncorporate stakeholders representing the entertainment \nindustry, as well as other creative industries, have further \nexpressed fear that researchers may upload and share data \nto which they do not hold rights. However, if the NRC does \ndecide to facilitate private data-sharing, it should consider \nadopting two requirements to address these concerns: (1) \nThe NRC should require all users to affirm they either have \nthe original IP rights to the data or the data is already in \nthe public domain; and (2) The NRC should have a scheme \nfor its users to license their data.\n(a) NRC users must own IP rights to the data they are \nuploading\nResearchers uploading data need to agree that they \nown the intellectual property rights to the data prior to \nupload, or that the data is already in the public domain. \nThis should be the case whether researchers share the data \nbroadly with other researchers or simply use their data for \ntheir own private use. \nOf course, despite mandating that uploaders \nguarantee legitimate ownership or public domain status \nof their uploaded IP, uploaders may nevertheless upload \ndata they don’t own the IP rights to. This may happen \nbecause computer engineers and researchers are not \ninformed about IP law, anticipate that fair use will excuse \ntheir behavior, or simply hope not to get caught.11 Industry \nstakeholders were also concerned that AI researchers \nwould pull out “facts” from a copyrighted work (e.g., \ncertain melodies in the chorus of a song) or apply certain \nalgorithms to the work and “wrongly” claim a copyright \nover the transformed work. Whatever the case may be, this \nassembly of protected input data represents the “clearest \ncopyright liability in the machine learning process” \nbecause assembling protected data violates the right to \nreproduction, and any preprocessing on the data could \nviolate the right to derivative works.12\nIn interviews, corporate stakeholders expressed a \ndesire to stymie the upload of copyrighted works by having \nthe NRC itself assess whether uploaded data is already \nprotected by copyright. Diligencing data can be completed \nmanually, or by using such automated systems as Content \nID, which is also used by corporations such as YouTube.\n13\nThe former option would be very labor intensive,14 whereas \nthe latter may be prohibitively expensive,15 so the value of \naddressing these concerns must be weighed against these \nburdensome costs.\nFinally, it is unclear the extent to which uploading and \nsharing copyrighted data for machine learning amounts \nto fair use.16 The most analogous case is Author’s Guild \nv. Google Books.17 In that case, Google scanned over 20 \nmillion books, many of which were copyright-protected, \nand assembled a corpus of machine-readable texts to \npower its Google Books service.18 The 2nd Circuit held that \nGoogle Books’ unauthorized reproductions of copyrighted \nworks was transformative fair use, largely because Google \nBooks provided information about books through small \nsnippets, without threatening the rights-holders’ core \nprotectable expression in the books.19 While some have \nopined that the Author’s Guild holding categorically \nprotects using copyrighted material in datasets for \nmachine learning purposes,20 many legal scholars are not \nso sure about such a broad holding, especially because fair \nuse is so fact-intensive.21 Indeed, while Google Books used \ncopyrighted works for a nonexpressive purpose, Sobel \nnotes that machine learning models may increasingly be \nable to glean value from a work’s expressive aspects.22\nTherefore, until courts and legislators provide more clarity \non the applicability of fair use in the machine learning \ncontext, the NRC should still require data uploaders to \nattest that they own the rights to the data.\n(b) Users must be able to license their data to other \nusers. \nIf the NRC enabled private data sharing, users would \nneed to make clear what rights other NRC users have over \nthe uploaders’ shared data. The NRC would have two basic \noptions for creating IP licensing schemes: (1) The NRC \nA Blueprint for the National Research Cloud 87\ncould permit researchers to use whatever IP license they \nwish when sharing their private data; or (2) The NRC could \nmandate a uniform license across the board for all data \nthat is uploaded.\n(1) Researcher’s Choice of License \nAllowing researchers to craft their own IP licensing \nagreements when sharing private data with other \nresearchers would be the most frictionless solution from \nthe perspective of the uploader; it would allow them to \nshare exactly what they want and restrict use to only \ncertain contexts. This choice of license seems to be \nimportant to data sharers.23 Indeed, many data scientists \nand engineers have written guides advising members \nof the open-source community on how they should go \nabout choosing specific licenses for their work.24 GitHub, \nan open-source code-sharing platform, permits its users \nto choose from dozens of licenses,25 and FigShare, a data\u0002sharing platform for researchers, likewise supports a host \nof different Creative Commons licenses.26 Some datasets \neven have their own custom IP licensing agreements. \nThe Twitter academic dataset, for instance, is licensed \naccording to Twitter’s own developer agreement and \nnoncommercial use policies, not to an existing open\u0002source license.27\nHowever, there are disadvantages to such flexibility. \nJust because different licenses might be allowed doesn’t \nmean these licenses will be fully understood by all users. \nAdopting multiple licenses may result in increased \naccidental infringement. Indeed, a study conducted by \nthe Institute of Electrical and Electronics Engineers found \nthat “although [software] developers clearly understood \ncases involving one license, they struggled when multiple \nlicenses were involved,”28 and in particular, were found \nto “lack the knowledge and understanding to tease apart \nlicense interactions across multiple situations.”29\nIn particular, researchers unfamiliar with the \nallowances provided by different data licenses, in \ncontexts where more than one license is implemented, \nmay lead to certain licenses being violated. For example, \nwhen researchers were surveyed regarding their \nunderstanding of copyright transfer agreements in the IP \ncommercialization process, they only demonstrated an \naverage 33 percent score on a knowledge-testing survey.30\n(2) Uniform Licensing Agreement \nThe second option available to the NRC would be to \nmandate that all private data be licensed under a single \nuniform license. For the NRC administration itself, this \nmay be the more straightforward option, since users \ncould be notified upon login about the appropriate use \nof data. The disadvantage of this strategy is that it may \ndeter would-be researchers who would share data under \na narrower license.31 Given the desire to allow researchers \nto innovate freely, there may be concerns about adopting \na restrictive licensing agreement. Nonetheless, several \noptions of licensing agreements would still be available \nfor adoption, and this pathway would require choosing a \nuniform agreement from these options, with the possibility \nof allowing an opt-out of this default license. \nIf the NRC were to implement a uniform license, \nit could look to the licensing agreements leveraged \nby institutional research clouds, such as the Harvard \nDataverse as an analogy in determining best practices \nfor its own licensing agreements. The model adopted by \nthe Dataverse is a default use of the CC0 Public Domain \nDedication “because of its name recognition in the \nscientific community” and its “use by repositories as well \nas scientific journals that require the deposit of open \ndata.”32 Like an unrestricted Creative Commons or Open \nData license, a public domain license would allow the data \nit governs to be used in any context, even commercial \nones, and would also allow reproduction and creation of \nderivatives from the data. \nAlternatively, the NRC could have a default open \nlicense while also permitting researchers to choose from \na handful of more restrictive licenses if they wish. For \nexample, the Harvard Dataverse notably allows uploaders \nto optout of the CC0 if needed and specify custom terms \nof use. The Australian Research Data Commons and \ndata-sharing platform FigShare33 also use a default CC0 \nlicense but nevertheless permit researchers to use a \nconditioned Creative Commons license. These conditioned \nlicenses can, for instance, require attribution to the \nA Blueprint for the National Research Cloud 88\noriginal owner, prevent exact reproduction, or only allow \nuse for noncommercial contexts. This may also help \naccommodate researchers who seek to upload datasets \nincorporating third-party data that holds a more restrictive \nlicense, since a “combined dataset will adopt the most \nrestrictive condition(s) of its component parts.”34\nIf the NRC goes down this route of giving users the \nchoice of a narrower license, it would also shift some \nliability to users—or to the NRC itself—by relying on \nusers to abide by the license. Approaches to enforcement \nwould vary, depending on the amount of responsibility \nin enforcement, and by extension liability the NRC seeks \nto take on. For example, in the Harvard Dataverse, if an \nuploader decides to opt out of a default open license \nand pursue their own custom licensing agreement \nover uploaded data, the Dataverse’s General Terms of \nUse absolve this particular cloud from resource-heavy \nenforcement responsibilities by stating that it “has no \nobligation to aid or support either party of the Agreement \nin the execution or enforcement of the Data Use \nAgreement’s terms.”35\nC. CURRENT STATE OF AI ETHICS \nFRAMEWORKS\nAI ethics frameworks (or principles, guidelines) \nattempt to address the ethical concerns related to \nthe development, deployment, and use of AI within \nprospective organizations. We briefly discuss the current \nlandscape of AI ethics frameworks, while noting that this is \nstill an emergent topic without broad consensus.\nBetween 2015 and 2020, governments, technology \ncompanies, international organizations, professional \norganizations, and researchers around the world have \npublished some 117 documents related to AI ethics.36\nThese frameworks aim to tackle the disruptive potential \nof AI technologies by producing normative principles \nand “best practice” recommendations.37 Due to the \nprominence of essentially contested concepts in AI ethics—\ni.e., words such as fairness, equity, privacy that have \ndifferent meanings for different audiences38—as well as \nthe lack of binding professional history and accountability \nmechanisms, those frameworks are often high level and \nself-regulatory, posing little threat to potential breaches to \nethical conduct.39\nFederal Frameworks\nIn the United States, there is no central guiding \nframework on the responsible development and \napplication of AI across the federal government. Some \ngovernment agencies have adopted or are in the process \nof adopting their own AI framework, while others have not \npublished such guidelines. The following are published \nfederal AI ethical frameworks as of August 2021:\n• After 15 months of deliberation with leading \nAI experts, the Department of Defense (DOD) \nadopted a series of ethical principles for the use \nof AI in February 2020 that align with the existing \nDOD mission and stakeholders.40\n• The General Services Administration (GSA), \ntasked by the Office of Management and Budget \n(OMB) in the Federal Data Strategy 2020 Action \nPlan, developed a Data Ethics Framework in \nFebruary 2020 to help federal personnel make \nethical decisions as they acquire, manage, and \nuse data.41\n• The Government Accountability Office (GAO) \ndeveloped an AI accountability framework \nin June 2020 for federal agencies and other \nentities involved in the design, development, \ndeployment, and continuous monitoring of \nAI systems to help ensure accountability and \nresponsible use of AI.42\n• The Office of the Director of National Intelligence \n(ODNI) released the Principles of AI Ethics for \nthe Intelligence Community in July 2020 to \nguide the intelligence community’s (IC) ethical \ndevelopment and use of AI to solve intelligence \nproblems.43\n• The National Security Commission on Artificial \nIntelligence (NSCAI) published a set of best \npractices in July 2020 (later revised and \nA Blueprint for the National Research Cloud 89\nintegrated into the Commission’s 2021 Final \nReport) for agencies critical to national security \nto implement as a paradigm for the responsible \ndevelopment and fielding of AI systems.44\nWhile these frameworks can help guide the NRC’s \napproach to ethics, we refrain from recommending a \nspecific framework for several reasons. First, despite \ngrowing calls for applied ethics in the AI community, \ndeveloping an AI ethics framework is still an emerging \narea. The lack of a unified government standard poses \nchallenges to the establishment of the NRC’s ethics review \nprocess.\nSecond, there are, in fact, significant differences \namong ethics frameworks published by various federal \nagencies. For example, NSCAI laid out differences between \nits recommended practices and those by DOD and IC.45\nMoreover, among the five frameworks above, the GSA \nFramework focused only on the ethical conduct of federal \nemployees when dealing with data while others focused \non the ethical development and application of AI systems \nspecifically. \nThird, the ethics framework for adopting AI technology \nmay be different from a framework for assessing research. \nMost federal agencies develop frameworks to guide the \nuse of AI-driven solutions for agency-specific tasks. For \nexample, DOD’s ethical principles only apply to defense\u0002specific combat or noncombat AI systems.46 In the absence \nof a central federal guideline, the NRC should not adopt \na framework by a particular agency because these \nframeworks are not necessarily designed for the wide \nrange of research contemplated for the NRC. The work on \nframeworks may nonetheless provide a useful starting \npoint for NRC’s ethics process.\nD. STAFFING AND EXPERTISE\nAs noted throughout this White Paper, the success of \nthe NRC will depend on human resources—both within \nthe NRC as well as across government— to resolve the \nmany challenges the NRC promises to tackle. While we \nrefrain from providing an organizational chart, we list the \ndimensions where staffing and expertise will be critical \nto the success of the NRC. This list is not meant to be \nexhaustive, but to highlight the vital importance of human \nresources.\nHuman Resource Areas\n• Computing\n°\n System administrators\n°\n Data center engineers\n°\n Research software engineers\n°\n Research application developers\n• Data\n°\n Data officers\n°\n Agency liaisons\n°\n Data architects\n°\n Data scientists\n• Grant administrators\n• Contracting officers\n• Support and training staff \n• Privacy staff (technical and legal)\n• Ethics staff\n• Cybersecurity staff\nA Blueprint for the National Research Cloud 90\nExecutive Summary\n1 Klaus Schwab, The Fourth Industrial Revolution (2016).\n2 Tae Yano & Moonyoung Kang, Taking Advantage of Wikipedia in Natural Language Processing, Carnegie Mellon U. (2008), https://www.cs.cmu.\nedu/~taey/pub/wiki.pdf.\n3 See, e.g., Anthony Alford, Google Trains Two Billion Parameter AI Vision Model, InfoQ (June 22, 2021), https://www.infoq.com/news/2021/06/google\u0002vision-transformer/; Anthony Alford, OpenAI Announces GPT-3 AI Language Model with 175 Billion Parameters, InfoQ (June 2, 2020), https://www.infoq.\ncom/news/2020/06/openai-gpt3-language-model/.\n4 AlphaGo, DeepMind (2021), https://deepmind.com/research/case-studies/alphago-the-story-so-far/. \n5 Benjamin F. Jones & Lawrence H. Summers, A Calculation of the Social Returns to Innovation (Nat’l Bureau of Econ. Research, Working Paper No. \n27863, 2020); J.G. Tewksbury, M.S. Crandall & W.E. Crane, Measuring the Societal Benefits of Innovation, 209 Sci. Mag. 658-62 (1980); see also National \nAcademies of Sciences, Engineering, and Medicine, Returns to Federal Investments in the Innovation System (2017)\n6 Stuart Zweben & Betsy Bizot, 2019 Taulbee Survey: Total Undergrad CS Enrollment Rises Again, but with Fewer New Majors; Doctoral Degree \nProduction Recovers From Last Year’s Dip (2019). \n7 Jathan Sadowski, When Data is Capital: Datafication, Accumulation, and Extraction, 2019 Big Data & Soc’y 1 (2019). \n8 Amy O’Hara & Carla Medalia, Data Sharing in the Federal Statistical System: Impediments and Possibilities, 675 Annals Am. Acad. Pol. & Soc. Sci. 138, \n140-41 (2018).\n9 Nat’l Security Comm’n on Artificial Intelligence, Final Report 186 (2021).\n10 Stan. U. Inst. for Human-Centered Artificial Intelligence, 2021 Artificial Intelligence Index Report 118 (2021).\n11 Id.\n12 Id.\n13 Neil C. Thompson, Shuning Ge & Yash M. Sherry, Building the Algorithm Commons: Who Discovered the Algorithms that Underpin Computing in the \nModern Enterprise?, 11 Global Strategy J. 17-33 (2020).\n14 See, e.g., U.S. Gov’t Accountability Office, Federal Agencies Need to Address Aging Legacy Systems (2016); U.S. Gov’t Accountability Office, \nCloud Computing: Agencies Have Increased Usage and Realized Benefits, but Cost and Savings Data Need To Be Better Tracked (2019).\n15 David Freeman Engstrom, Daniel E. Ho, Catherine M. Sharkey & Mariano-Florentino Cuéllar, Government by Algorithm: Artificial \nIntelligence in Federal Administrative Agencies 6, 71-72 (2020).\n16 William M. (Mac) Thornberry National Defense Authorization Act for Fiscal Year 2021, Pub. L. No. 116-283, § 5106.\n17 The Biden Administration Launches the National Artificial Intelligence Research Resource Task Force, The White House (June 10, 2021), https://www.\nwhitehouse.gov/ostp/news-updates/2021/06/10/the-biden-administration-launches-the-national-artificial-intelligence-research-resource-task-force/. \n18 William M. (Mac) Thornberry National Defense Authorization Act for Fiscal Year 2021, Pub. L. No. 116-283, § 5107 (g).\n19 Nat’l Security Comm’n on Artificial Intelligence, supra note 9, at 191.\n20 See, e.g., Cloudbank, https://www.cloudbank.org; Fact Sheet: National Secure Data Service Act Advances Responsible Data Sharing in Government, \nData Coalition (May 13, 2021), https://www.datacoalition.org/fact-sheet-national-secure-data-service-act-advances-responsible-data-sharing-in\u0002government/. \n21 Steve Lohr, Universities and Tech Giants Back National Cloud Computing Project, N.Y. Times (June 30, 2020), https://www.nytimes.com/2020/06/30/\ntechnology/national-cloud-computing-project.html; John Etchemendy & Fei-Fei Li, National Research Cloud: Ensuring the Continuation of American \nInnovation, Stan. U. Inst. for Human-Centered Artificial Intelligence, (Mar. 28, 2020), https://hai.stanford.edu/news/national-research-cloud\u0002ensuring-continuation-american-innovation.\n22 Jennifer Villa & Dave Troiano, Choosing Your Deep Learning Infrastructure; The Cloud vs. On-Prem Debate, Determined AI (July 30, 2020), https://\ndetermined.ai/blog/cloud-v-onprem/; Is HPC Going to Cost Me a Fortune?, InsideHPC (last visited July 23, 2021), https://insidehpc.com/hpc-basic\u0002training/is-hpc-going-to-cost-me-a-fortune/.\n23 See, e.g., US Plans $1.8 Billion Spend on DOE Exascale Supercomputing, HPCwire (Apr. 11, 2018), https://www.hpcwire.com/2018/04/11/us-plans-1-\n8-billion-spend-on-doe-exascale-supercomputing/; Federal Government, Advanced HPC (last visited July 23, 2021), https://www.advancedhpc.com/\npages/federal-government; United States Continues to Lead World In Supercomputing, U.S. Dep’t. Energy (Nov. 18, 2019), https://www.energy.gov/\narticles/united-states-continues-lead-world-supercomputing.\n24 See NSF Funds Five New XSEDE-Allocated Systems, Nat’l Sci. Found. (Aug. 10, 2020), https://www.xsede.org/-/nsf-funds-five-new-xsede-allocated\u0002systems.\n25 Cloudbank, supra note 20.\n26 See, e.g., National Data Service, http://www.nationaldataservice.org; The Open Science Data Cloud, https://www.opensciencedatacloud.org; Harvard \nDataverse, https://dataverse.harvard.edu; FigShare, https://figshare.com. \n27 FedRAMP, https://www.fedramp.gov.\n28 See Fact Sheet: National Secure Data Service Act Advances Responsible Data Sharing in Government, Data Coalition (May 13, 2021), https://www.\ndatacoalition.org/fact-sheet-national-secure-data-service-act-advances-responsible-data-sharing-in-government/.\n29 See Administrative Data Research Facility, Coleridge Initiative, https://coleridgeinitiative.org/adrf/ (last visited July 26, 2021). \n30 See Landsat Data Access, U.S. Geological Survey, https://www.usgs.gov/core-science-systems/nli/landsat/landsat-data-access (last visited July 23, \n2021); Fed. Geographic Data Comm., The Value Proposition for Landsat Applications (2014); Crista L. Straub, Stephen R. Koontz & John B. Loomis, \nEconomic Valuation of Landsat Imagery (2019).\nEndnotes\nA Blueprint for the National Research Cloud 91\n31 See Bipartisan Pol’y Ctr., Barriers to Using Government Data: Extended Analysis of the U.S. Commission on Evidence-Based Policymaking’s \nSurvey of Federal Agencies and Offices 18-20 (2018); see also U.S. Dep’t of Health & Human Services, The State of Data Sharing at the U.S. \nDepartment of Health and Human Services 4 (2018) (describing how data at the agency is “largely kept in silos with a lack of organizational awareness \nof what data are collected across the Department and how to request access.”).\n32 Privacy Act, 5 U.S.C. § 552a (1974).\n33 Michael S. Bernstein et al., ESR: Ethics and Society Review of Artificial Intelligence Research, Cornell. U. (July 9, 2021), https://arxiv.org/\npdf/2106.11521.pdf.\n34 Courtenay R. Bruce et al., An Embedded Model for Ethics Consultation: Characteristics, Outcomes, and Challenges, 5 AJOB Empirical Bioethics 8 \n(2014).\nIntroduction\n1 National Research Cloud Call to Action, Stan. U. Inst. For Human-Centered Artificial Intelligence, https://hai.stanford.edu/national-re\u0002search-cloud-joint-letter.\n2 See id.; John Etchemendy & Fei-Fei Li, National Research Cloud: Ensuring the Continuation of American Innovation, Stan. U. Inst. For Human-Centered \nArtificial Intelligence (Mar. 28, 2020), https://hai.stanford.edu/news/national-research-cloud-ensuring-continuation-american-innovation.\n3 William M. (Mac) Thornberry National Defense Authorization Act for Fiscal Year 2021, Pub. L. No. 116-283, § 5106.\n4 The Biden Administration Launches the National Artificial Intelligence Research Resource Task Force, The White House (June 10, 2021), https://www.\nwhitehouse.gov/ostp/news-updates/2021/06/10/the-biden-administration-launches-the-national-artificial-intelligence-research-resource-task-force/. \n5 Privacy Act of 1974, 5 U.S.C. § 552a (2012).\n6 Foundations for Evidence-Based Policymaking Act of 2017, Pub. L. No. 115-435, 132 Stat. 5529 (2019).\n7 See Fact Sheet: National Secure Data Service Act Advances Responsible Data Sharing in Government, Data Coalition (May 13, 2021), https://www.data\u0002coalition.org/fact-sheet-national-secure-data-service-act-advances-responsible-data-sharing-in-government/.\n8 See, e.g., Facial Recognition and Biometric Technology Moratorium Act, S. 4084, 116th Cong. (2020); Bhaskar Chakravorti, Biden’s ‘Antitrust Revolution’ \nOverlooks AI—at Americans’ Peril, Wired (July 27, 2021), https://www.wired.com/story/opinion-bidens-antitrust-revolution-overlooks-ai-at-ameri\u0002cans-peril/.\nChapter 1\n1 See Stephen Breyer, Regulation and Its Reform (1982); Clifford Winston, Government Failure Versus Market Failure (2006).\n2 Largest Companies by Market Cap, Companies Market Cap (2021), https://companiesmarketcap.com.\n3 Stan. U. Inst. for Human-Centered Artificial Intelligence, 2021 Artificial Intelligence Index Report 93 (2021).\n4 See, e.g., Mary L. Gray & Siddarth Suri, Ghost Work: How to Stop Silicon Valley from Building a New Global Underclass (2019); Craig Webster & \nStanislav Ivanov, Robotics, Artificial Intelligence, and the Evolving Nature of Work 132-35 (2019); Weiyu Wang & Keng Siau, Artificial Intelligence, \nMachine Learning, Automation, Robotics, Future of Work and Future of Humanity: \nA Review and Research Agenda, 30 J. Database Mgmt. 61 (2019).\n5 AlphaFold: A Solution to a 50-Year-Old Grand Challenge in Biology, DeepMind (Nov. 30, 2020), https://deepmind.com/blog/article/alphafold-a-solution\u0002to-a-50-year-old-grand-challenge-in-biology.\n6 Tanha Talaviya et al., Implementation of Artificial Intelligence in Agriculture for Optimisation of Irrigation and Application of Pesticides and Herbicides, 4 \nArtificial Intelligence in Agriculture 58 (2020). \n7 Greg Allen & Taniel Chan, Artificial Intelligence and National Security, Harv. Kennedy Sch. Belfer Ctr. (July 2017), https://www.belfercenter.org/\npublication/artificial-intelligence-and-national-security. \n8 Stan. U. Inst. for Human-Centered Artificial Intelligence, supra note 3.\n9 Jeffrey Ding, Deciphering China’s AI Dream (2018).\n10 Fugaku is being used extensively for AI research initiatives. See Atsushi Nukariya et al., HPC and AI Initiatives for Supercomputer Fugaku and Future \nProspects, Fujitsu (Nov. 11, 2020), https://www.fujitsu.com/global/about/resources/publications/technicalreview/2020-03/article09.html.\n11 Eng’g & Physical Sciences Research Council, The Impact of HECToR (2014).\n12 Joshua New, Why the United States Needs a National Artificial Intelligence Strategy and What it Should Look Like (2018).\n13 Maggie Miller, White House Establishes National Artificial Intelligence Office, The Hill (Jan. 12, 2021), https://thehill.com/policy/cybersecurity/533922-\nwhite-house-establishes-national-artificial-intelligence-office.\n14 See Fast Track Action Comm. on Strategic Computing, National Strategic Computing Initiative Update: Pioneering the Future of Computing\n(2019), https://www.nitrd.gov/pubs/National-Strategic-Computing-Initiative-Update-2019.pdf.\n15 Nat’l Security Comm’n on Artificial Intelligence, Final Report (2021).\n16 The COVID-19 High Performance Computing Consortium, COVID-19 HPC Consortium, https://covid19-hpc-consortium.org.\n17 See Aaron L. Friedberg, Science, the Cold War, and the American State, 20 Diplomatic Hist. 107, 112 (1996); Sean Pool & Jennifer Erickson, The High \nReturn on Investment for Publicly Funded Research, Ctr. for Am. Progress (Dec. 10, 2012), https://www.americanprogress.org/issues/economy/\nreports/2012/12/10/47481/the-high-return-on-investment-for-publicly-funded-research/.\n18 Peter L. Singer, Federally Supported Innovations: 22 Examples of Major Technology Advances That Stem from Federal Research Support \n14-15 (2014).\n19 Nat’l Research Council, Government Support for Computing Research 136-55 (1999). \n20 Nat’l Security Comm’n on Artificial Intelligence, supra note 15, at 185.\n21 Philippe Aghion, Benjamin F. Jones & Charles I. Jones, Artificial Intelligence and Economic Growth, in The Economics of Artificial Intelligence: An \nAgenda 237 (2019).\nA Blueprint for the National Research Cloud 92\n22 Ian Moll, The Myth of the Fourth Industrial Revolution, 68 Theoria 1 (2021); see also Tim Unwin, 5 Problems with 4th Industrial Revolution, ICT Works \n(Mar. 23, 2019), https://www.ictworks.org/problems-fourth-industrial-revolution/.\n23 See, e.g., Geoffrey A. Manne & Joshua D. Wright, Google and the Limits of Antitrust: The Case Against the Antitrust Case Against Google, 34 Harv. J.L. & \nPub. Pol’y 1 (2011); Lina M. Khan, Amazon’s Antitrust Paradox, 126 Yale L.J. 710 (2016).\n24 David Patterson et al., Carbon Emissions and Large Neural Network Training, Cornell U. (Apr. 23, 2021), https://arxiv.org/pdf/2104.10350.pdf. To be \nclear, however, the study found that training other sophisticated but smaller NLP models such as Meena and T5 required approximately 96 and 48 tons \nof carbon dioxide, respectively. Id. Another study found that the training state-of-the-art NLP models produced approximately 626,000 pounds (313 \ntons) of carbon dioxide, five times the lifetime emissions of the average car in the United States. Emma Strubell, Ananya Ganesh & Andrew McCallum, \nEnergy and Policy Considerations for Deep Learning in NLP, Cornell U. (2019), https://arxiv.org/pdf/1906.02243.pdf.\n25 Calculate Your Carbon Footprint, The Nature Conservancy, https://www.nature.org/en-us/get-involved/how-to-help/carbon-footprint-calculator/.\n26 Economic studies in other fields also show that increasing access, supply, or quality of certain goods without appropriate pricing mechanisms or \nregulatory interventions can lead to over-use and waste. See, e.g., Chengri Ding & Shunfeng Song, Traffic Paradoxes and Economic Solutions, 1 J. Urban \nMgmt. 63 (2012) (roads and traffic congestion); Ari Mwachofi & Assaf F. Al-Assaf, Health Care Market Deviations from the Ideal Market, 11 Sultan Qaboos \nUniv. Med. J. 328 (2011) (doctors and quality of care).\n27 See Emily M. Bender et al., On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?, 2021 Proceedings ACM Conf. on Fairness, \nAccountability & Transparency 610 (2021).\n28 See Joy Buolamwini & Timnit Gebru, Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification, 81 Proceeding Machine \nLearning Res. 1 (2018); Inioluwa Deborah Raji & Joy Buolamwini, Actionable Auditing: Investigating the Impact of Publicly Naming Biased Performance \nResults of Commercial AI Products, 2019 Proceedings AAAI/ACM Conf. on AI, Ethics & Soc’y 429 (2019).\n29 See Virginia Eubanks, Automating Inequality (2018); Cathy O’Neil, Weapons of Math Destruction (2016).\n30 See Christopher Whyte, Deepfake News: AI-Enabled Disinformation as a Multi-Level Public Policy Challenge, 5 J. Cyber Pol’y 199 (2020); Jeffrey Dastin, \nAmazon Scrapes Secret AI Recruiting Tool that Showed Bias Against Women, Reuters (Oct. 10, 2018), https://www.reuters.com/article/us-amazon-com\u0002jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G; James Vincent, Google ‘Fixed’ its \nRacist Algorithm by Removing Gorillas from its Image-Labeling Tech, The Verge (Jan. 12, 2018), https://www.theverge.com/2018/1/12/16882408/google\u0002racist-gorillas-photo-recognition-algorithm-ai;.\n31 Kate Crawford, Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence 211 (2021) (“AI systems are built to see and \nintervene in the world in ways that primarily benefit the states, institutions, and corporations that they serve. In this sense, AI systems are expressions \nof power that emerge from wider economic and political forces, created to increase profits and centralize control for those who wield them.”).\n32 Elizabeth Gibney, Self-Taught AI is Best Yet at Strategy Game Go, Nature (Oct. 18, 2017), https://www.nature.com/news/self-taught-ai-is-best-yet-at\u0002strategy-game-go-1.22858.\n33 Bill Schackner, Carnegie Mellon’s Prestigious Computer Science School has a New Leader, Pittsburgh Post-Gazette (Aug. 8, 2019), https://www.post\u0002gazette.com/news/education/2019/08/08/Carnegie-Mellon-University-computer-science-Martial-Hebert-dean-artificial-intgelligence-google-robotics/\nstories/201908080096.\n34 Bipartisan Pol’y Ctr, Cementing American Artificial Intelligence Leadership: AI Research & Development (2020).\n35 Nur Ahmed & Muntasir Wahed, The De-Democratization of AI: Deep Learning and the Compute Divide in Artificial Intelligence Research, Cornell U.\n(Oct. 22, 2020), https://arxiv.org/pdf/2010.15581.pdf.\n36 Id.\n37 Fei-Fei Li, America’s Global Leadership in Human-Centered AI Can’t Come From Industry Alone, The Hill (July 6, 2021), https://thehill.com/opinion/\ntechnology/561638-americas-global-leadership-in-human-centered-ai-cant-come-from-industry?rl=1.\n38 Cade Metz, A.I. Researchers Are Making More Than $1 Million, Even at a Nonprofit, N.Y. Times (Apr. 19, 2018), https://www.nytimes.com/2018/04/19/\ntechnology/artificial-intelligence-salaries-openai.html. \n39 Stan. U. Inst. for Human-Centered Artificial Intelligence, supra note 3, at 118.\n40 Michael Gofman & Zhao Jin, Artificial Intelligence, Education, and Entrepreneurship, SSRN (Sept. 17, 2019), https://papers.ssrn.com/sol3/papers.\ncfm?abstract_id=3449440.\n41 Jathan Sadowski, When Data is Capital: Datafication, Accumulation, and Extraction, 2019 Big Data & Soc’y 1 (2019).\n42 For example, researchers have clamored for Facebook to share some of its proprietary data so they can better understand the effect of social media \non politics and societal discourse. Simon Hegelich, Facebook Needs to Share More with Researchers, Nature (Mar. 24, 2020), https://www.nature.com/\narticles/d41586-020-00828-5.\n43 Ashlee Vance, This Tech Bubble Is Different, Bloomberg (Apr. 14, 2011), https://www.bloomberg.com/news/articles/2011-04-14/this-tech-bubble-is\u0002different. \n44 Amy O’Hara & Carla Medalia, Data Sharing in the Federal Statistical System: Impediments and Possibilities, 675 Annals Am. Acad. Pol. & Soc. Sci. 138, \n140-41 (2018).\n45 Dario Amodei & Danny Hernandez, AI and Compute, Open AI (May 16, 2018), https://openai.com/blog/ai-and-compute/.\n46 See, e.g., Ahmed & Wahed, supra note 35; Ian Sample, ‘We Can’t Compete’: Why Universities Are Losing Their Best AI Scientists, The Guardian (Nov. 1, \n2017), https://www.theguardian.com/science/2017/nov/01/cant-compete-universities-losing-best-ai-scientists.\n47 Neil C. Thompson, Shuning Ge & Yash M. Sherry, Building the Algorithm Commons: Who Discovered the Algorithms that Underpin Computing in the \nModern Enterprise?, 11 Global Strategy J. 17-33 (2020).\n48 Minkyung Baek, RoseTTAFold: Accurate Protein Structure Prediction Accessible to All, U. Wash. Inst. for Protein Design (July 15, 2021), https://www.\nipd.uw.edu/2021/07/rosettafold-accurate-protein-structure-prediction-accessible-to-all/; Minkyung Baek et al., Accurate Prediction of Protein Structures \nand Interactions Using a Three-Track Neural Network, Sci. Mag. (July 15, 2021), https://science.sciencemag.org/content/sci/early/2021/07/19/science.\nabj8754.full.pdf.\n49 How Diplomacy Helped to End the Race to Sequence the Human Genome, Nature (June 24, 2020), https://www.nature.com/articles/d41586-020-\n01849-w.\n50 Joel Klinger et al., A Narrowing of AI Research?, Cornell U. (Nov. 17, 2020), https://arxiv.org/pdf/2009.10385.pdf.\n51 Id. \n52 Alex Tamkin et al., Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models, Cornell U. (Feb. 4, 2021), https://arxiv.\norg/pdf/2102.02503.pdf.\nA Blueprint for the National Research Cloud 93\n53 Those 5 were Boston, San Francisco, San Jose, Seattle and San Diego. See Robert D. Atkinson, Mark Muro & Jacob Whiton, The Case for Growth \nCenters: How to Spread Tech Innovation Across America, Brookings (Dec. 9, 2019), https://www.brookings.edu/research/growth-centers-how-to-spread\u0002tech-innovation-across-america/.\n54 Interview with Professor Erik Brynjolfsson, Director, Stanford Digital Economy Lab (2021). \n55 Solon Barocas & Andrew D. Selbst, Big Data’s Disparate Impact, 104 Cal. L. Rev. 671 (2016).\nChapter 2\n1 “Principal Investigator” status may differ from university to university, but typically represents the core faculty that are eligible to oversee research \nprojects at their home institutions.\n2 See Beth Jensen, AI Index Diversity Report: An Unmoving Needle, Stan. U. Inst. for Human-Centered Artificial Intelligence (May 3, 2021), https://hai.\nstanford.edu/news/ai-index-diversity-report-unmoving-needle.\n3 For a perspective, for instance, on the importance of modeling and simulation in physics, see Karen E. Wilcox, Omar Ghattas & Patrick Heimbach, The \nImperative of Physics-Based Modeling and Inverse Theory in Computational Science, 1 Nature Comp. Sci. 166 (2021).\n4 15 U.S.C. § 9415 (emphasis added). \n5 Id. (emphasis added). \n6 Contemporaneous accounts corroborate this core focus. The National Security Commission on AI, for instance, describes the proposal as “provid[ing] \nverified researchers and students subsidized access to scalable compute resources” with a specific reference to the “compute divide” that has left “middle\u0002and lower-tier universities [lacking] the resources necessary for cutting-edge AI research.” Nat’l Security Comm’n on A.I., Final Report 191, 197 (2021) \n(emphasis added). Upon the announcement of the NRC legislation, Jeff Dean, SVP of Google Research and Google Health, noted, “A National AI Research \nResource will help accelerate US progress in artificial intelligence and advanced technologies by providing academic researchers access to the cloud \ncomputing resources necessary for experiments at scale.” Brandi Vincent, Congress Inches Closer to Creating a National Cloud for AI Research, NextGov (July \n2, 2020), https://www.nextgov.com/emerging-tech/2020/07/congress-inches-closer-creating-national-cloud-ai-research/166624/ (emphasis added). Others \nhave suggested that “researchers” under NRC could include individuals at small businesses, start-up companies, non-profits, and certain technology firms. \nOne co-sponsor of the legislation, for instance, suggested that NRC resources should be provided to “developers” and “entrepreneurs.” Portman, Heinrich \nIntroduce Bipartisan Legislation to Develop National Cloud Computer for AI Research, Rob Portman, U.S. Senator for Ohio (June 4, 2020), https://www.\nportman.senate.gov/newsroom/press-releases/portman-heinrich-introduce-bipartisan-legislation-develop-national-cloud.\n7 Frequently Asked Questions About Small Businesses, U.S. Small Bus. Admin. Office of Advoc. (Oct. 2020), https://cdn.advocacy.sba.gov/wp-content/\nuploads/2020/11/05122043/Small-Business-FAQ-2020.pdf. \n8 Louise Balle, Information on Small Business Startups, Houston Chron., https://smallbusiness.chron.com/information-small-business-startups-2491.\nhtml. \n9 Such entities could potentially collaborate with academic partners, and the NRC would of course also need to set rules about collaborator eligibility. \n10 PI status provides a level of standardization across faculty compared to other metrics, such as tenure-track or designation as research faculty. For \nexample, the University of Michigan appoints individuals focused on full-time research as “research faculty,” which is not a tenure track position. In \ncontrast, research faculty at Purdue are eligible for tenure-track. Distinct from the categorization used by both universities, MIT designates full-time \nresearchers as “academic staff” rather than faculty. All three types of researchers, however, qualify for principal investigator status at their respective \nuniversities. Some universities go further by providing temporary PI status to non-PI status individuals affiliated with the university for a single project \n(including all three universities mentioned previously).\n11 Community & Education Resource Requests, CloudBank, https://www.cloudbank.org/training/cloudbank-community#toc-eligibilit-36nfpcrS. \n12 Apply for an Account, Compute Canada, https://www.computecanada.ca/research-portal/account-management/apply-for-an-account/. \n13 Nat’l Sci. Bd., Science & Engineering Indicators 2016, Academic Research and Development 72 (2016). \n14 Id.\n15 College Enrollment in the United States from 1965 to 2019 and Projections up to 2029 for Public and Private Colleges, Statista (Jan. 2021), https://www.\nstatista.com/statistics/183995/us-college-enrollment-and-projections-in-public-and-private-institutions/.\n16 Colaboratory – Frequently Asked Questions, Google, https://research.google.com/colaboratory/faq.html.\n17 Weekly Maximum GPU Usage, Kaggle (2019), https://www.kaggle.com/general/108481.\n18 Community & Education Resource Requests, supra note 11.\n19 Merit Review: Why You Should Volunteer to Serve as an NSF Reviewer, Nat’l Sci. Found., https://www.nsf.gov/bfa/dias/policy/merit_review/reviewer.\njsp#1.\n20 See XSEDE Campus Champions, XSEDE, https://www.xsede.org/community-engagement/campus-champions.\n21 Compute Canada, for instance, provides access to 15% of PIs to increased compute capacity based on a merit competition. In 2021, Compute \nCanada completed its review of 650 research submissions in about five months with only 80 volunteer reviewers from Canadian academic institutions \nto assess the scientific merit of the proposal. Resource Allocation Competitions, Compute Canada, https://www.computecanada.ca/research-portal/\naccessing-resources/resource-allocation-competitions/; 2021 Resource Allocations Competition Results, Compute Canada, https://www.computecanada.\nca/research-portal/accessing-resources/resource-allocation-competitions/rac-2021-results/. Compare this with CloudBank, which allocates compute \nresources by leveraging NSF’s grant administration process: In 2019, NSF needed 30,000 volunteer reviewers to handle over 40,000 proposals, with \neach proposal requiring about 10 months to process from start to finish. Nat’l. Sci. Found., Merit Review Process: Fiscal Year 2019 Digest (2020); NSF \nProposal and Award Process, Nat’l Sci. Found., https://www.nsf.gov/attachments/116169/public/nsf_proposal_and_award_process.pdf.\n22 Another boundary question will be the resource allocation to PIs that are affiliated both with universities and with private companies. As a default, \nNRC resources should go toward academic projects, and not subsidize work that is conducted in private researcher capacity. \n23 Resource Allocation Competitions, supra note 21.\n24 Simplifying Cloud Services, Sci. Node (Dec. 2, 2019), https://sciencenode.org/feature/An%20easier%20cloud.php. \n25 Frequently Asked Questions (FAQ), CloudBank, https://www.cloudbank.org/faq.\n26 Simplifying Cloud Services, supra note 25.\n27 Id.\nA Blueprint for the National Research Cloud 94\n28 Id.\n29 Frequently Asked Questions (FAQ), supra note 26.\n30 Frequently Asked Questions (FAQs) for Budgeting for Cloud Computing Resources via CloudBank in NSF Proposals, Nat’l Sci. Found., https://www.nsf.\ngov/pubs/2020/nsf20108/nsf20108.jsp.\n31 Simplifying Access to Cloud Resources for Researchers: CloudBank, Amazon Web Serv. (Nov. 16, 2020), https://aws.amazon.com/blogs/publicsector/\nsimplifying-access-cloud-resources-researchers-cloudbank/.\n32 Community & Education Resource Requests, supra note 11.\n33 Larry Dignan, AWS Cloud Computing Ops, Data Centers, 1.3 Million Servers Creating Efficiency Flywheel, ZDNet (June 17, 2016), https://www.zdnet.\ncom/article/aws-cloud-computing-ops-data-centers-1-3-million-servers-creating-efficiency-flywheel/; Rich Miller, Ballmer: Microsoft Has 1 Million \nServers, Data Ctr. Knowledge (July 15, 2013), https://www.datacenterknowledge.com/archives/2013/07/15/ballmer-microsoft-has-1-million-servers; \nDaniel Oberhaus, Amazon, Google, Microsoft: Here’s Who Has the Greenest Cloud, Wired (Dec. 18, 2019), https://www.wired.com/story/amazon-google\u0002microsoft-green-clouds-and-hyperscale-data-centers/; Russell Brandom, Mapping out Amazon’s Invisible Server Empire, The Verge (May 10, 2019), \nhttps://www.theverge.com/2019/5/10/18563485/amazon-web-services-internet-location-map-data-center.\n34 See, e.g., AWS Pricing, Amazon Web Services, https://aws.amazon.com/pricing/; Overview of Cloud Billing Concepts, Google Cloud, https://cloud.\ngoogle.com/billing/docs/concepts; Azure Pricing, Azure, https://azure.microsoft.com/en-us/pricing/#product-pricing. \n35 Large research universities already negotiate enterprise agreements with cloud providers.\n36 What We Do, XSEDE, https://www.xsede.org/about/what-we-do (last visited Sept. 19, 2021).\n37 XSEDE Overall Organization, XSEDE Wiki, https://confluence.xsede.org/display/XT/XSEDE+Overall+Organization (last visited Sept. 19, 2021).\n38 XSEDE Allocations Info & Policies, XSEDE, https://portal.xsede.org/allocations/policies (last visited Sept. 19, 2021).\n39 Id.\n40 Id.\n41 Startup Allocations, XSEDE, https://portal.xsede.org/allocations/startup (last visited Sept. 19, 2021).\n42 Id.\n43 Id.\n44 Id.\n45 Research Allocations, XSEDE, https://portal.xsede.org/allocations/research (last visited Sept. 19, 2021).\n46 Id.\n47 Id.\n48 Id.\n49 XSEDE Allocations Info & Policies, supra note 36.\n50 XSEDE Campus Champions, supra note 20.\n51 Id.\n52 Id.\n53 XSEDE as a Collaborator on Proposals, XSEDE, https://www.xsede.org/about/collaborating-with-xsede (last visited Sept. 19, 2021).\n54 COVID-19 HPC Consortium, XSEDE, https://www.xsede.org/covid19-hpc-consortium (last visited Sept. 19, 2021).\n55 Amazon, for example, introduced its P4, P3, and P2 instances in 2020, 1997, and 1996, respectively. Frederic Lardinois, AWS Launches Its Next-Gen \nGPU Instances with 8 Nvidia A100 Tensor Core GPUs, TechCrunch (Nov. 2, 2020), https://social.techcrunch.com/2020/11/02/aws-launches-its-next-gen\u0002gpu-instances/; Ian C. Schafer, Amazon Elastic Compute Cloud P3 Launched alongside NVIDIA GPU Cloud, SD Times (Oct. 26, 2017), https://sdtimes.com/\nai/amazon-elastic-compute-cloud-p3-launched-alongside-nvidia-gpu-cloud/; Jeff Barr, New P2 Instance Type for Amazon EC2 – Up to 16 GPUs, Amazon \nWeb Services (Sept. 29, 2016), https://aws.amazon.com/blogs/aws/new-p2-instance-type-for-amazon-ec2-up-to-16-gpus/. The introduction years of \nthe P4 and P3 instances line up with the release of NVIDIA’s newest general purpose data center GPUs. \n56 See, e.g., Sarah Wang & Martin Casado, The Cost of Cloud, a Trillion Dollar Paradox, Andreessen Horowitz (May 27, 2021), https://a16z.\ncom/2021/05/27/cost-of-cloud-paradox-market-cap-cloud-lifecycle-scale-growth-repatriation-optimization/.\n57 Preston Smith et al., Community Clusters or the Cloud: Continuing Cost Assessment of On-Premises and Cloud HPC in Higher Education, 2019 \nProceedings Practice & Experience Advanced Res. Computing on Rise of the Machines 1 (2019). The amortized cost includes the annual compute \ncost, subsidized hardware cost, and power costs, but does not include personnel costs, as such costs are fixed and would be recurred regardless of \nwhether a cluster existed physically on-prem or on the cloud. Id.\n58 Craig A. Stewart et al., Return on Investment for Three Cyberinfrastructure Facilities: A Local Campus Supercomputer; the NSF-Funded Jetstream Cloud \nSystem; and XSEDE, 11 Int’l Conf. on Utility & Cloud Computing 223 (2018).\n59 Srijith Rajamohan & Robert E. Settlage, Informing the On/Off-prem Cloud Discussion in Higher Education, 2020 Practice & Experience Adv. Res. \nComputing 64 (2020). The cost sources include hardware, software services, software administration, electricity, and facilities but do not include \ncomputational scientists support, scientific software licenses, and data transfer costs. The study is also limited to Virginia Tech’s particular cloud \nworkload.\n60 Jennifer Villa & Dave Troiano, Choosing Your Deep Learning Infrastructure: The Cloud vs. On-Prem Debate, Determined AI (July 30, 2020), https://\ndetermined.ai/blog/cloud-v-onprem/; Is HPC Going to Cost Me a Fortune?, insideHPC, https://insidehpc.com/hpc-basic-training/is-hpc-going-to-cost\u0002me-a-fortune/.\n61 Interview with Suzanne Talon, Regional Director, Compute Canada (Jan. 14, 2021).\n62 Compute Canada, Cloud Computing for Researchers 1 (2016), https://www.computecanada.ca/wp-content/uploads/2015/02/CloudStrategy2016-\n2019-forresearchersEXTERNAL-1.pdf.\n63 US Plans $1.8 Billion Spend on DOE Exascale Supercomputing, HPCwire (Apr. 11, 2018), https://www.hpcwire.com/2018/04/11/us-plans-1-8-billion\u0002spend-on-doe-exascale-supercomputing/; Federal Government, Advanced HPC, https://www.advancedhpc.com/pages/federal-government; United \nStates Continues To Lead World In Supercomputing, Energy.gov, https://www.energy.gov/articles/united-states-continues-lead-world-supercomputing; \nHigh Performance Computing, Energy.gov, https://www.energy.gov/science/initiatives/high-performance-computing.\n64 See, e.g., DOE Announces Five New Energy Projects at LLNL, LLNL (Nov. 13, 2020), https://www.llnl.gov/news/doe-announces-five-new-energy\u0002projects-llnl; New HPCMP System at the AFRL DSRC DoD Supercomputing Resource Center to Provide over Nine PetaFLOPS of Computing Power to \nA Blueprint for the National Research Cloud 95\nAddress Physics, AI, and ML Applications for DoD Users, DOD HPC, https://www.hpc.mil/images/hpcdocs/newsroom/21-19_TI-21_web_announcement_\nAFRL_DSRC.pdf; Public Announcement, DOD HPC, https://www.hpc.mil/images/hpcdocs/newsroom/awards_and_press/HC101321D0002_PUBLIC_\nANNOUNCEMENT_20210505.pdf.\n65 Devin Coldewey, $600M Cray Supercomputer Will Tower Above the Rest — to Build Better Nukes, TechCrunch (Aug. 13, 2019), https://social.\ntechcrunch.com/2019/08/13/600m-cray-supercomputer-will-tower-above-the-rest-to-build-better-nukes/; CORAL-2 RFP, Oak Ridge Nat’l Laboratory\n(Apr. 9, 2018), https://procurement.ornl.gov/rfp/CORAL2/.\n66 See, e.g., NSF Funds Five New XSEDE-Allocated Systems, Nat’l Sci. Found. (Aug. 10, 2020), https://www.xsede.org/-/nsf-funds-five-new-xsede\u0002allocated-systems. \n67 Timothy Prickett Morgan, Bending The Supercomputing Cost Curve Down, The Next Platform (Dec. 2, 2019), http://www.nextplatform.\ncom/2019/12/02/bending-the-supercomputing-cost-curve-down/; Ben Dickson, The GPT-3 Economy, TechTalks (Sept. 21, 2020), https://bdtechtalks.\ncom/2020/09/21/gpt-3-economy-business-model/.\n68 Elijah Wolfson, The US Just Retook the Title of World’s Fastest Supercomputer from China, Quartz (June 9, 2018), https://qz.com/1301510/the-us-has\u0002the-worlds-fastest-supercomputer-again-the-200-petaflop-summit/.\n69 November 2020, TOP500 (Nov. 2020), https://www.top500.org/lists/top500/2020/11/.\n70 U.S. Department of Energy and Cray to Deliver Record-Setting Frontier Supercomputer at ORNL, Oak Ridge Nat’l Laboratory (May 7, 2019), https://\nwww.ornl.gov/news/us-department-energy-and-cray-deliver-record-setting-frontier-supercomputer-ornl.\n71 Coury Turczyn, Building an Exascale-Class Data Center, Oak Ridge Leadership Computing Facility (Dec. 11, 2020), https://www.olcf.ornl.\ngov/2020/12/11/building-an-exascale-class-data-center/.\n72 Don Clark, Intel Slips, and a High-Profile Supercomputer Is Delayed, N.Y. Times (Aug. 27, 2020), https://www.nytimes.com/2020/08/27/technology/\nintel-aurora-supercomputer.html; Mila Jasper, 10 of 15 of DOD’s Major IT Projects Are Behind Schedule, GAO Found, Nextgov (Jan. 4, 2021), https://www.\nnextgov.com/it-modernization/2021/01/10-15-dods-major-it-projects-are-behind-schedule-gao-found/171155/.\n73 See Nattakarn Phaphoom et al., A Survey Study on Major Technical Barriers Affecting the Decision to Adopt Cloud Services, 103 J. Systems & Software 167, \n171-72 (2015) (describing data portability, integration with existing systems, migration complexity, and availability as major barriers to cloud adoption); \nAbdulrahman Alharthi et al., An Overview of Cloud Services Adoption Challenges in Higher Education Institutions, 2 Proceedings of the Int’l Workshop \non Emerging Software as a Service & Analytics 102, 107-08 (2015) (acknowledging the low rate of cloud computing adoption in higher education and \nemphasizing that bolstering both the perceived ease of use and the actual usefulness of cloud computing can increase the adoption rate).\n74 See Dep’t of Energy, FY 2021 Budget Justification Volume 4: Science (2020).\n75 Joe Weinman, Cloudonomics: The Business Value of Cloud Computing (2012).\n76 OLCF supports and manages ORNL’s supercomputing resources, including Summit and eventually Frontier. This figure accounts for “operations and \nuser support at the LCF facilities–including power, space, leases, and staff. Id. at 37-38.\n77 ACLF supports and manages Argonne National Laboratory’s computing resources, including the Theta system and, later this year, the new Aurora \ncomputer, another DOE exascale HPC system. Id. \n78 OLCF operated its Titan HPC system for 7 years. See Coury Turczyn, supra note 72. ACLF also operated its Mira HPC system for 7 years. Argonne’s Mira \nSupercomputer to Retire After Years of Enabling Groundbreaking Science, HPCwire (Dec. 20, 2019), https://www.hpcwire.com/2019/12/20/argonnes\u0002mira-supercomputer-to-retire-after-years-of-enabling-groundbreaking-science/. If still operational, these systems would rank about the 19th and 29th \nfastest in the world, respectively. Compare November 2020, supra note 70, with TOP500 List - June 2019, TOP500 (June 2019), https://www.top500.org/\nlists/top500/list/2019/06/.\n79 See, e.g., Kim Zetter, Top Federal Lab Hacked in Spear-Phishing Attack, Wired (Apr. 20, 2011), https://www.wired.com/2011/04/oak-ridge-lab-hack/; \nNatasha Bertrand & Eric Wolff, Nuclear Weapons Agency Breached amid Massive Cyber Onslaught, Politico (Dec. 17, 2020), https://www.politico.com/\nnews/2020/12/17/nuclear-agency-hacked-officials-inform-congress-447855 (last visited Mar. 2, 2021); Ryan Lucas, List Of Federal Agencies Affected By \nA Major Cyberattack Continues To Grow, NPR (Dec. 18, 2020), https://www.npr.org/2020/12/18/948133260/list-of-federal-agencies-affected-by-a-major\u0002cyberattack-continues-to-grow (last visited Mar. 2, 2021).\n80 We discuss data access models in Chapter Three.\n81 See Ongoing Projects, RIKEN Ctr. for Computational Sci., https://www.r-ccs.riken.jp/en/fugaku/research/covid-19/projects/.\n82 Fugaku Retains Title as World’s Fastest Supercomputer, HPCWire (Nov. 17, 2020), https://www.hpcwire.com/off-the-wire/fugaku-retains-title-as\u0002worlds-fastest-supercomputer/.\n83 November 2020, supra note 70.\n84 Id.\n85 Behind the Scenes of Fugaku as the World’s Fastest Supercomputer, Fujitsu (Feb. 2, 2021), https://blog.global.fujitsu.com/fgb/2021-02-02/behind\u0002the-scenes-of-fugaku-as-the-worlds-fastest-supercomputer-1manufacturing/.\n86 Id.\n87 Don Clark, Japanese Supercomputer Is Crowned World’s Speediest, N.Y. Times (June 22, 2020), https://www.nytimes.com/2020/06/22/technology/\njapanese-supercomputer-fugaku-tops-american-chinese-machines.html.\n88 Justin McCurry, Non-Woven Masks Better to Stop Covid-19, Says Japanese Supercomputer, The Guardian (Aug. 26, 2020), http://www.theguardian.\ncom/world/2020/aug/26/non-woven-masks-better-to-stop-covid-19-says-japanese-supercomputer.\n89 Fujitsu and RIKEN Complete Joint Development of Japan’s Fugaku, the World’s Fastest Supercomputer, Fujitsu (Mar. 9, 2021), https://www.fujitsu.com/\nglobal/about/resources/news/press-releases/2021/0309-02.html.\n90 Id.\n91 See, e.g., Rolf Harms & Michael Yamartino, The Economics of the Cloud (2010); Srijith Rajamohan & Robert E. Settlage, Informing the On/Off-Prem \nCloud Discussion in Higher Education, 2020 Practice & Experience in Advanced Res. Computing 64 (2020); Byung Chul Tak et al., To Move or Not To Move: \nThe Economics of Cloud Computing, 3 USENIX Conf. on Hot Topics in Cloud Computing 1 (2011); Edward Walker, Walter Brisken & Jonathan Romney, \nTo Lease or Not To Lease from Storage Clouds, 43 Computer 44 (2010).\n92 See, e.g., Di Zhang et al., RLScheduler: An Automated HPC Batch Job Scheduler Using Reinforcement Learning, Cornell U. (Sept. 2, 2020), https://arxiv.\norg/pdf/1910.08925.pdf.\n93 For instance, we have not been able to identify good estimates of electricity and cooling costs for DOE supercomputers. \n94 Hugh Couchman et al., Compute Canada — Calcul Canada: A Proposal to the Canada Foundation for Innovation – National Platforms Fund\nA Blueprint for the National Research Cloud 96\n58 (2006).\n95 About, Compute Canada, https://www.computecanada.ca/about/.\n96 National Systems, Compute Canada, https://www.computecanada.ca/techrenewal/national-systems/.\n97 Compute Canada Technology Briefing, Compute Canada (Nov. 2017), https://www.computecanada.ca/wp-content/uploads/2015/02/Technology\u0002Briefing-November-2017.pdf. \n98 Cloud Computing for Researchers, Compute Canada (Dec. 2016), https://www.computecanada.ca/wp-content/uploads/2015/02/CloudStrategy2016-\n2019-forresearchersEXTERNAL-1.pdf.\n99 Id.\n100 Budget Submission 2018, Compute Canada (2018), https://www.computecanada.ca/wp-content/uploads/2015/02/UTF-8Compute20Canada20Budg\net20Submission202018.pdf at 5.\n101 Compute Canada projected it had only met about 55% of total demand for CPU compute hours in 2018. Id.\n102 Id.\n103 Compute Canada, Annual Report 2019-2020 4 (2020).\n104 Rapid Access Service, Compute Canada, https://www.computecanada.ca/research-portal/accessing-resources/rapid-access-service/.\n105 Id.\n106 Resource Allocation Competitions, supra note 21.\n107 Id.\n108 Id.\n109 Id.\n110 Id.\n111 2021 Resource Allocations Competition Results, supra note 21.\nChapter 3\n1 National Research Cloud Call to Action, Stan. U. Inst. for Human-Centered Artificial Intelligence (2020), https://hai.stanford.edu/national\u0002research-cloud-joint-letter. \n2 We discuss the Privacy Act and privacy considerations in more detail in Chapter Five.\n3 Amy O’Hara & Carla Medalia, Data Sharing in the Federal Statistical System: Impediments and Possibilities, 675 Annals Am. Acad. Pol. & Soc. Sci. 138, \n140-41 (2018); see also President’s Mgmt. Agenda, Federal Data Strategy 2020 Action Plan (2020).\n4 Improved data access would, as we describe below, also promote evidence-based policymaking and improve trust in science (as data access makes \nreplication efforts much easier). \n5 See, e.g., Nick Hart & Nancy Potok, Modernizing U.S. Data Infrastructure: Design Considerations for Implementing a National Secure Data \nService to Improve Statistics and Evidence Building (2020).\n6 These initiatives are successful in that they are sustainable and have been used by researchers to access multi-agency government data. The only \nexception is the National Secure Data Service (NSDS), which has not yet been implemented. We discuss the NSDS alongside the Census Bureau and \nthe Evidence-Based Policy-Making Act of 2018 below. Importantly, our focus in these case studies is not to evaluate their efforts or measure their exact \nlevels of success but to identify and understand some of the differences and similarities in the range of data-sharing efforts.\n7 For instance, private sector data may facilitate research regarding social media use, internet behavior, or fill in gaps for federal statistics research \nthrough big data analysis. See Robert M. Groves & Brian A. Harris-Kojetin, Using Private-Sector Data for Federal Statistics, Nat’l Ctr. for Biotechnology \nInfo. (Jan. 12, 2017), https://www.ncbi.nlm.nih.gov/books/NBK425876/.\n8 See, e.g., National Data Service, Nat’l Data Serv., http://www.nationaldataservice.org; The Open Science Data Cloud, Open Sci. Data Cloud, https://\nwww.opensciencedatacloud.org, Harvard Dataverse, Harv. Dataverse, https://dataverse.harvard.edu, FigShare, https://figshare.com. \n9 Facebook Data for Research provides access to a variety of libraries, via in-house platforms. See, e.g., Facebook Data For Good, Facebook (2020), https://\ndataforgood.fb.com/; What is the Facebook Ad Library and How do I Search it?, Facebook (2021), https://www.facebook.com/help/259468828226154; \nFacebook Disaster Maps Methodology, Facebook (May 15, 2019), https://research.fb.com/facebook-disaster-maps-methodology/.\n10 For example, Twitter has a Developer Portal that provides access to their API to allow researchers to use user data for noncommercial purposes. \nSee Twitter Developers, Twitter (2021), https://developer.twitter.com/en/portal/petition/academic/is-it-right-for-you; Take Your Research Further with \nTwitter Data, Twitter (2021), https://developer.twitter.com/en/solutions/academic-research. Thus, uploading Twitter data to a separate Cloud may \nprovide few incentives to researchers who can use the API route.\n11 See Nat’l Acad. of Sci., Innovations in Federal Statistics 31-42 (2017).\n12 See Jennifer M. Urban, Joe Karaganis & Brianna M. Schofield, Notice & Takedown in Everyday Practice 39 (2017) (illustrating the difficulty \nthat online service providers face in manually evaluating a large volume of data for potential infringement; for example, one online service provider \nexplained that “out of fear of failing to remove infringing material, and motivated by the threat of statutory damages, its staff will take “six passes to try \nto find the [identified content].”); see also Letter from Thom Tillis, Marsha Blackburn, Christopher A. Coons, Dianne Feinstein et. al, to Sundar Pichai, \nChief Executive Officer, Google Inc. (Sept. 3, 2019), https://www.ipwatchdog.com/wp-content/uploads/2019/09/9.3-Content-ID-Ltr.pdf (“We have \nheard from copyright holders who have been denied access to Content ID tools, and as a result, are at a significant disadvantage to prevent repeated \nuploading of content that they have previously identified as infringing. They are left with the choice of spending hours each week seeking out and \nsending notices about the same copyrighted works, or allowing their intellectual property to be misappropriated.”).\n13 To illustrate the costs of implementing Content ID on a large-scale platform, Google announced in a report in 2016 that YouTube had invested more \nthan $60 million in Content ID. See Google, How Google Fights Piracy 6 (2016).\n14 See, e.g., AWS Customer Agreement, Amazon (Nov. 30, 2020), https://aws.amazon.com/agreement/. \n15 For instance, across the 29 distinct agencies in the Department of Health and Human Services (HHS), data “are largely kept in silos with a lack of \norganizational awareness of what data are collected across the Department and how to request access. Each agency operates within its own statutory \nauthority and each dataset can be governed by a particular set of regulations.” U.S. Dep’t of Health & Human Services, The State of Data Sharing at \nthe U.S. Department of Health and Human Services 4 (2018). \nA Blueprint for the National Research Cloud 97\n16 See, e.g., id. at 8 (“HHS lacks consistent and standardized processes for one agency to request data from another agency.”).\n17 O’Hara & Medalia, supra note 3, at 140-41.\n18 See id. at 142 (“Most [data-sharing] agreements rely heavily on interpersonal relationships and informal quid pro quo arrangements, handling data \nrequests in a less centralized fashion.”).\n19 Jeffrey Mervis, How Two Economists Got Direct Access to IRS Tax Records, Sci. Mag. (May 22, 2014), https://www.sciencemag.org/news/2014/05/how\u0002two-economists-got-direct-access-irs-tax-records. \n20 See Robert M. Groves & Adam Neufeld, Accelerating the Sharing of Data Across Sectors to Advance the Common Good 17 (2017).\n21 See, e.g., Data Use Agreement, Dep’t Health & Human Services, https://www.hhs.gov/sites/default/files/ocio/eplc/EPLC%20Archive%20\nDocuments/55-Data%20Use%20Agreement%20%28DUA%29/eplc_dua_practices_guide.pdf. \n22 O’Hara & Medalia, supra note 3, at 138, 141.\n23 Bipartisan Pol’y Ctr., Barriers to Using Government Data: Extended Analysis of the U.S. Commission on Evidence-Based Policymaking’s \nSurvey of Federal Agencies and Offices 18-20 (2018).\n24 See Research Data Assistance Center (ResDAC), Ctr. for Medicare & Medicaid Services (Aug. 30, 2018), https://www.cms.gov/Research-Statistics\u0002Data-and-Systems/Research/ResearchGenInfo/ResearchDataAssistanceCenter. \n25 O’Hara & Medalia, supra note 3, at 141.\n26 Michelle Mello et al., Waiting for Data: Barriers to Executing Data Use Agreements, 367 Sci. Mag. 150 (Jan. 10, 2020), https://www.\nsciencemagazinedigital.org/sciencemagazine/10_january_2020/MobilePagedArticle.action?articleId=1552284#articleId1552284. \n27 Interview with Amy O’Hara, Executive Director, Georgetown Federal Statistical Research Data Center (Apr. 22, 2021); see also Special Sworn Research \nProgram, Bureau of Econ. Analysis, https://www.bea.gov/research/special-sworn-researcher-program; Nat’l Ctr. for Educ. Stat., Restricted-Use \nData Procedures Manual (2011). \n28 See U.S. Gov’t Accountability Office, Federal Agencies Need to Address Aging Legacy Systems (2016).\n29 O’Hara & Medalia, supra note 3, at 140-41.\n30 See, e.g., id.; U.S. Gov’t Accountability Office, supra note 28. \n31 President’s Mgmt. Agenda, supra note 3, at 11.\n32 Groves & Neufeld, supra note 20, at 12-13. For a precise definition of sensitive data, see Glossary: Sensitive Information, Nat’l Inst. Standards & \nTech., https://csrc.nist.gov/glossary/term/sensitive_information. \n33 Shanna Nasiri, FedRAMP Low, Moderate, High: Understanding Security Baseline Levels, Reciprocity (Sept. 24, 2019), https://reciprocity.com/fedramp\u0002low-moderate-high-understanding-security-baseline-levels/.\n34 Michael McLaughlin, Reforming FedRAMP: A Guide to Improving the Federal Procurement and Risk Management of Cloud Services, Info. Tech. & \nInnovation Found. (June 15, 2020), https://itif.org/publications/2020/06/15/reforming-fedramp-guide-improving-federal-procurement-and-risk\u0002management.\n35 Frequently Asked Questions, FedRAMP, https://www.fedramp.gov/faqs. \n36 Do Once, Use Many - How Agencies Can Reuse a FedRAMP Authorization, FedRAMP (May 7, 2020), https://www.fedramp.gov/how-agencies-can-reuse\u0002a-fedramp-authorization/. \n37 FedRAMP, FedRAMP Low, Moderate, and High Security Control Baselines (2021).\n38 Security and Privacy Controls for Information Systems and Organizations, Nat’l Inst. Standards & Tech. (Sept. 23, 2020), https://csrc.nist.gov/\npublications/detail/sp/800-53/rev-5/final.\n39 See, e.g., id.; NIST Risk Management Framework AC-2: Account Management, Nat’l Inst. Standards & Tech., https://csrc.nist.gov/Projects/risk\u0002management/sp800-53-controls/release-search#!/control?version=4.0&number=AC-2; NIST Risk Management Framework AC-3: Access Enforcement, \nNat’l Inst. Standards & Tech., https://csrc.nist.gov/Projects/risk-management/sp800-53-controls/release-search#!/control?version=5.1&number=AC-3.\n40 See Mark Bergen, Google Engineers Refused to Build Security Tool to Win Military Contracts, Bloomberg (June 21, 2018), https://www.bloomberg.com/\nnews/articles/2018-06-21/google-engineers-refused-to-build-security-tool-to-win-military-contracts.\n41 See Nat’l Inst. Standards & Tech., Standards for Security Categorization of Federal Information and Information Systems (2004).\n42 Partnering with FedRAMP, FedRAMP, https://www.fedramp.gov/cloud-service-providers/. While it may cost cloud service providers between $365,000 \nand $865,000 and take 6-12 months to receive FedRAMP compliance, Adam Isles, Securing Your Cloud Solutions: Research and Analysis on Meeting \nFedRAMP/Government Standards 21 (2017), such costs are borne by the cloud service providers themselves, not the providers’ customers. Indeed, \nFedRAMP uses a “do once, use many” model: Once a cloud service provider obtains an authorization to operate (ATO), that ATO can be leveraged and \nreciprocated across multiple customers, eliminating duplicative efforts and inconsistencies that would come from requiring multiple re-authorizations. \nId. at 11.\n43 Even within FedRAMP there are substantial amounts of variation in how different organizations ensure compliance with the relevant controls \nand standards, with many of the controls written broadly enough to give room for substantial interpretation. However, it does lay out a variety of \nconsiderations and requirements that are consistent across domains and allows a degree of predictability and reliance that is not present in other \naspects of federal data governance. \n44 O’Hara & Medalia, supra note 3, at 141 (“Data sharing is taking place on a mandatory or voluntary basis, and data requests are managed through a \ndesignated staff/process or diffusely through an organization.”).\n45 Bipartisan Pol’y Ctr., supra note 23, at 17 (“The lack of standard procedures or guidelines for sharing data across federal agencies that fund \nresearch makes efforts to link and share data difficult or inefficient.”).\n46 See, e.g., Amy O’Hara, US Federal Data Policy: An Update on The Federal Data Strategy and The Evidence Act, 5 Int’l J. Population Data Sci. 5 (2020).\n47 While existing federal efforts and initiatives are already aimed at harmonizing data sharing best practices, see, e.g., 2020 Action Plan, Federal Data \nStrategy (May 14, 2020), https://strategy.data.gov/action-plan/, the NRC can accelerate these efforts. Indeed, the development of clear, consistent \nstandards is crucial in facilitating data-sharing. David Crotty, Ida Sim & Michael Stebbins, Open Access to Federally Funded Research Data 7 (2020). \n48 These requirements are inconsistent and out-of-date due to difficulties in defining risk as well as risk aversion on the parts of agencies. See O’Hara \n& Medalia, supra note 3, at 140-41; see also David S. Johnson et al., The Opportunities and Challenges of Using Administrative Data Linkages to Evaluate \nMobility, 657 Annals Am. Acad. Pol. & Soc. Sci. 252-53 (2015).\n49 For a discussion of inference threats, see Nat’l Acad. of Sci., Eng’g & Med., Federal Statistics, Multiple Data Sources, and Privacy Protection: \nNext Steps 68 (2017).\n50 Congzheng Song & Ananth Raghunathan, Information Leakage in Embedding Models, Cornell U. (Mar. 31, 2020), https://arxiv.org/abs/2004.00053. \n51 See, e.g., Statistical Safeguards, Census Bureau (July 1, 2021), https://www.census.gov/about/policies/privacy/statistical_safeguards.html. \nA Blueprint for the National Research Cloud 98\n52 Alexandra Wood et al., Differential Privacy: A Primer for a Non-Technical Audience, 21 Vand. J. Ent. & Tech. L. 209 (2018).\n53 Regulating Access to Data, UK Data Serv., https://www.ukdataservice.ac.uk/manage-data/legal-ethical/access-control/five-safes.\n54 Administrative Data Research Facility, Coleridge Initiative, https://coleridgeinitiative.org/adrf/. \n55 See O’Hara, supra note 46. \n56 For additional discussion of the privacy implications of the NRC, see Chapter Five. \n57 See U.S. Office of Mgmt. & Budget, Barriers to Using Administrative Data for Evidence-Building 7 (2016).\n58 Administrative Data Research Facility, supra note 54.\n59 Id.\n60 Training, Coleridge Initiative, https://coleridgeinitiative.org/training/.\n61 ADRF User Guide: Data Explorer, Coleridge Initiative, https://coleridgeinitiative.org/adrf/documentation/using-the-adrf/data-explorer/. \n62 ADRF User Guide: Exporting Results, Coleridge Initiative, https://coleridgeinitiative.org/adrf/documentation/using-the-adrf/exporting-results/.\n63 Id.\n64 ADRF User Guide: Data Hashing Application, Coleridge Initiative, https://coleridgeinitiative.org/adrf/documentation/adrf-overview/data-hashing\u0002application/. \n65 ADRF User Guide: Security Model and Compliance, Coleridge Initiative, https://coleridgeinitiative.org/adrf/documentation/adrf-overview/security\u0002model-and-compliance/. \n66 Overview for Collaborators, Coleridge Initiative, https://coleridgeinitiative.org/collaborators/. \n67 Data, Stan. Med. Ctr. for Population Health Sci., https://med.stanford.edu/phs/data.html. \n68 Stanford Ctr. for Philanthropy & Civ. Soc’y, Trusted Data Intermediaries 2-3 (2018).\n69 Others have also recognized the benefit of universal DUA templates. See Mello et al., supra note 26, at 150; Guidance for Providing and Using \nAdministrative Data for Statistical Purposes, Office of Mgmt. & Budget (Feb. 14, 2014), https://obamawhitehouse.archives.gov/sites/default/files/omb/\nmemoranda/2014/m-14-06.pdf. \n70 Data | Center for Population Health Sciences | Stanford Medicine, Stan. Med. Ctr. for Population Health Sci., https://med.stanford.edu/phs/data.\nhtml.\n71 See Stanford PHS – Datasets, Redivis, https://redivis.com/StanfordPHS/datasets?orgDatasets-tags=109.medicare.\n72 Access Levels, Redivis (July 2020), https://docs.redivis.com/reference/data-access/access-levels.\n73 Step 1: Getting Access, Stan. Med. PHS Documentation, https://phsdocs.developerhub.io/start-here/getting-data-access.\n74 Id.\n75 Id.\n76 PHS Data-Use Workflow, Stan. Med. PHS Documentation, https://phsdocs.stanford.edu/start-here/phs-data-use-workflow.\n77 Id.\n78 PHS Computing Environment, Stan. Med. PHS Documentation, https://phsdocs.stanford.edu/computing-environment.\n79 Id.\n80 See U.S. Gov’t Accountability Office, Federal Agencies Need to Address Aging Legacy Systems 15 (2016) (noting that from 2010-2015, many \nfederal agencies increased their spending on operations and maintenance due to legacy systems).\n81 David Freeman Engstrom, Daniel E. Ho, Catherine M. Sharkey & Mariano-Florentino Cuéllar, Government by Algorithm: Artificial \nIntelligence in Federal Administrative Agencies 6, 71-72 (2020).\n82 Id. at 6-7.\n83 Id. at 71-72.\n84 Id. at 73.\n85 Id. at 6.\n86 See Results for America, The Promise of the Foundations for Evidence-Based Policymaking Act and Proposed Next Steps (2019) \n87 For example, the Uniform Federal Crime Reporting Act of 1988 requires federal law enforcement agencies to share crime data with the FBI. See 34 \nU.S.C. §§41303(c)(2), (3), (4). Unfortunately, though, no federal agencies apparently currently share their data with the FBI under this law. Nat’l Acad. of \nSci., supra note 11, at 41 (2017).\n88 Katharine G. Abraham & Ron Haskins, The Promise of Evidence-Based Policymaking; Comm’n on Evidence-Based Policymaking (2018).\n89 These privacy-preserving mechanisms are especially important in light of ongoing legal and political challenges in differential privacy application to \nFederal data. See, e.g., Dan Bouk & danah boyd, Democracy’s Data Infrastructure (2021).\n90 Foundations for Evidence-Based Policymaking Act of 2018, Pub. L. No. 115-435.\n91 Confidential Information Protection and Statistical Efficiency Act of 2002, Pub. L. No. 107-347.\n92 Overview, Federal Data Strategy (2020), https://strategy.data.gov/overview/.\n93 UK Data Service, UK Data Serv., https://www.ukdataservice.ac.uk/ (last visited Jun. 21, 2021).\n94 For example, the Social Security Administration alone has over 14 petabytes of data, stored in roughly 200 databases. Engstrom, Ho, Sharkey & \nCuéllar, supra note 81, at 72.\n95 Google Earth Engine, Google Earth Engine, https://earthengine.google.com (last visited Aug. 15, 2021). \n96 World of Work, ADR UK, https://www.adruk.org/our-work/world-of-work/.\n97 Annual Respondents Database, 1973-2008: Secure Access, UK Data Serv. (2020), https://beta.ukdataservice.ac.uk/datacatalogue/studies/\nstudy?id=6644. \n98 UK Innovation Survey, UK Data Serv. (2021), https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=6699. \n99 Quarterly Labour Force Survey, 1992-2021: Secure Access, UK Data Serv. (2021), https://beta.ukdataservice.ac.uk/datacatalogue/studies/\nstudy?id=6727.\n100 Understanding Society: Waves 1-10, 2009-2019 and Harmonised BHPS: Waves 1-18, 1991-2009: Secure Access, UK Data Serv. (2021), https://beta.\nukdataservice.ac.uk/datacatalogue/studies/study?id=6676.\n101 These datasets have helped researchers tackle some specific, public good questions. See, e.g., Francisco Perales, Why Does the Work Women Do Pay \nLess Than the Work Men Do?, UK Data Serv. (Dec. 8, 2011), https://beta.ukdataservice.ac.uk/impact/case-studies/case-study?id=62; Eva-Maria Bonin, Do \nParenting Programmes Reduce Conduct Disorder?, UK Data Serv. (Apr. 4, 2012), https://beta.ukdataservice.ac.uk/impact/case-studies/case-study?id=93. \n102 Identifying Priority Access or Quality Improvements for Federal Data and Models for Artificial Intelligence Research and Development (R&D), and \nTesting; Request for Information, 84 Fed. Reg. 32962 (July 10, 2019). \nA Blueprint for the National Research Cloud 99\n103 Nick Hart, Data Coalition Comments on AI Data and Model R&D RFI, Data Coalition (Aug. 9, 2019), http://www.datacoalition.org/wp-content/\nuploads/2019/09/Comment.RFI_.OMB_.2019-14618.DataCoalition.pdf. \n104 Id.\n105 See Adam R. Pah et al., How to Build a More Open Justice System, 369 Sci. 134 (2020); see also Seamus Hughes, The Federal Courts Are Running an \nOnline Scam, Politico (Mar. 20, 2019), https://www.politico.com/magazine/story/2019/03/20/pacer-court-records-225821/.\n106 Legal Authority and Policies for Data Linkage at Census, Census Bureau (Apr. 4, 2018), https://www.census.gov/about/adrm/linkage/about/\nauthority.html.\n107 BLS Restricted Data Access, U.S. Bureau of Lab. Stat., https://www.bls.gov/rda/restricted-data.htm (last updated May 20, 2021).\n108 Welcome to the PDS, NASA, https://pds.nasa.gov.\nChapter 4\n1 While we believe that these are the primary axes for consideration, some secondary considerations include organizational clout, talent retention, and \nbureaucratic overhead.\n2 Congressional Research Serv., Federally Funded Research and Development Centers (FFRDCs): Background and Issues for Congress 1 (2020). \n3 Id. See also About IDA, Inst. Defense Analyses, https://www.ida.org/about-ida (emphasizing that IDA, the private sector subcontractor that oper\u0002ates the Science & Technology Policy Institute and several other FFRDCs, “enjoys unusual access to classified government information and sensitive \ncorporate proprietary information.”); U.S. Gov’t Accountability Office, Federally Funded Research and Development Centers: Improved Oversight \nand Evaluation Needed for DOD’s Data Access Pilot Program 6 (2020) (discussing how the Department of Defense was able to establish a three-year \npilot program that allowed its FFRDC researchers to forgo having to obtain nondisclosure agreements with each data owner in order to streamline the \ndata-access process).\n4 Nick Hart & Nancy Potok, Modernizing U.S. Data Infrastructure: Design Considerations for Implementing a National Secure Data Service to \nImprove Statistics and Evidence Building (2020).\n5 Id. at 26.\n6 Id. at 26-27, 29-30.\n7 U.S. Gov’t Accountability Office, supra note 3, at 6. Note that while the FFRDC must operate to serve its sponsors, in establishing an FFRDC, the \nsponsor must ensure that it operates with substantial independence; the FFRDC must be “operated, managed, or administered by an autonomous or\u0002ganization or as an identifiably separate operating unit of a parent organization.” See Federal Acquisition Regulations [hereinafter “FAR”] § 35.017(a)(2).\n8 One example of this is the Science & Technology Policy Institute, which we discuss in a case study below.\n9 U.S. Dep’t of Energy, The State of the DOE National Laboratories 11-13 (2020).\n10 See, e.g., More Federal Agencies Head to the Cloud With Azure Government, Applied Info. Sci. (Feb. 23, 2018), https://www.ais.com/more-federal\u0002agencies-head-to-the-cloud-with-azure-government/; see also AWS GovCloud, Amazon, https://aws.amazon.com/govcloud-us/. Microsoft was also \npreviously awarded a $10 billion contract from the Pentagon. See Kate Conger, Microsoft Wins Pentagon’s $10 Billion JEDI Contract, Thwarting Amazon, \nN.Y. Times (Sept. 4, 2020), https://www.nytimes.com/2019/10/25/technology/dod-jedi-contract.html. However, this contract was recently canceled “due \nto evolving requirements, increased cloud conservancy and industry advances.” Ellie Kaufman & Zachary Cohen, Pentagon Cancels $10 Billion Cloud \nContract Given to Microsoft Over Amazon, CNN (July 6, 2021), https://www.cnn.com/2021/07/06/tech/defense-department-cancels-jedi-contract-am\u0002azon-microsoft/index.html. The Pentagon will now instead seek new bids for an updated Joint Warfighting Cloud Capability (JWCC) contract from \nAmazon and Microsoft. Id.\n11 See, e.g., Bram Bout, Helping Universities Build What’s Next with Google Cloud Platform, Google (Oct. 25, 2016), https://blog.google/outreach-ini\u0002tiatives/education/helping-universities-build-whats-next-google-cloud-platform; Cloud Computing for Education, Amazon, https://aws.amazon.com/\neducation/. \n12 Congressional Research Serv., supra note 2, at 11-12 (2020).\n13 U.S. Dep’t of Energy, Annual Report on the State of the DOE National Laboratories 87 (2017).\n14 Congressional Research Serv., supra note 2, at 19.\n15 Congressional Research Serv., Office of Science and Technology Policy (OSTP): History and Overview 9 (2020). STPI’s duties are also specified \nin 42 U.S.C. § 6686.\n16 What are FFRDCs?, Inst. Defense Analyses, https://www.ida.org/ida-ffrdcs. \n17 Id.\n18 Sponsors, Inst. Defense Analyses, https://www.ida.org/en/about-ida/sponsors. \n19 Id.\n20 Congressional Research Serv., supra note 15, at 9-10.\n21 For instance, from 2008-2012, these other federal agencies contributed a total of $9.8 million of funding to STPI while NSF contributed about $24 \nmillion. U.S. Gov’t Accountability Office, Federally Funded Research Centers: Agency Reviews of Employee Compensation and Center Perfor\u0002mance 43-44 (2014).\n22 Congressional Research Serv., supra note 15, at 9-10.\n23 Id.\n24 42 U.S.C. § 6686(d).\n25 42 U.S.C. § 6686(e).\n26 Sci. & Tech. Pol’y Inst., Report to the President Fiscal Year 2020 (2020).\n27 See, e.g., Open Government, Millennium Challenge Corp., https://www.mcc.gov/initiatives/initiative/open; Nat’l Geospatial Advisory Comm., \nAdvancing the National Spatial Data Infrastructure Through Public-Private Partnerships and Other Innovative Partnerships (2020); Nat’l \nAeronautics & Space Admin., Public-Private Partnerships for Space Capability Development 33-36 (2014).\n28 Big Data Value Public-Private Partnership, European Comm’n (Mar. 9, 2021), https://digital-strategy.ec.europa.eu/en/library/big-data-value-pub\u0002lic-private-partnership.\n29 RAND, Public-Private Partnerships for Data-Sharing: A Dynamic Environment 33, 99 (2000).\n30 See Homepage - Alberta Data Partnerships, Alberta Data Partnerships, http://abdatapartnerships.ca (last visited Aug. 15, 2021). \nA Blueprint for the National Research Cloud 100\n31 Alberta Data Partnerships, A P3 Success Story 1 (2017).\n32 Id.\n33 Id. at 19, 35.\n34 Id. at 15.\n35 Id.\n36 Id. at 1.\n37 Nat’l Geospatial Advisory Comm., Public-Private Partnership Use Case: Alberta Data Partnerships 1 (2020).\n38 Alberta Data Partnerships, supra note 31, at 15.\n39 Id. at 16.\n40 The COVID-19 High Performance Computing Consortium, COVID-19 HPC Consortium, https://covid19-hpc-consortium.org. \n41 Id.\n42 See, e.g., David Hall, Why Public-Private Partnerships Don’t Work (2015); Disadvantages and Pitfalls of the PPP Option, APMG Int’l, https://\nppp-certification.com/ppp-certification-guide/54-disadvantages-and-pitfalls-ppp-option. \n43 Graeme A. Hodge, Carsten Greve & Anthony E. Boardman, International Handbook on Public–Private Partnerships, 187-90 (2012).\n44 For example, on one end of a spectrum, the California Teale Data Center creates, owns, maintains, and archives its own datasets for private sector \nuse. In contrast, the Pennsylvania Spatial Data Access houses metadata, requiring users to ask the actual data sources for access. RAND, supra note 29, \nat 102-03. We encourage the Task Force to examine this comprehensive report to assess the various organizational options for a PPP data clearinghouse \nmodel. \n45 Angela Ballantyne & Cameron Stewart, Big Data and Public-Private Partnerships in Healthcare and Research, 11 Asian Bioethics R. 315, 315 (2019).\n46 See Gov’t Accountability Office, Human Capital: Improving Federal Recruiting and Hiring Efforts; see also Catch and Retain: Improving Recruit\u0002ing and Retention at Government Agencies, Salesforce, https://www.salesforce.com/solutions/industries/government/resources/government-recruit\u0002ment-software/.\n47 Partnership for Public Service, Survey on the Future of Government Service 2 (2020).\n48 Id.\nChapter 5\n1 National Research Cloud Call to Action, Stan. U. Inst. for Human-Centered Artificial Intelligence (2020), https://hai.stanford.edu/national-re\u0002search-cloud-joint-letter.\n2 Sensitive information, as defined by the National Institute of Standards and Technology, is information where the loss, misuse, or unauthorized \naccess or modification could adversely affect the national interest or the conduct of federal programs, or the privacy to which individuals are entitled \nunder 5 U.S.C. § 552a (the Privacy Act); that has not been specifically authorized under criteria established by an Executive Order or an Act of Congress \nto be kept classified in the interest of national defense or foreign policy. See Glossary: Sensitive Information, Nat’l Inst. Standards & Tech., \nhttps://csrc.nist.gov/glossary/term/sensitive_information.\n3 We thank Mark Krass for these insights. \n4 Agencies covered by the Act include “any Executive department, military department, Government corporation, Government controlled corporation, \nor other establishment in the executive branch of the [federal] Government (including the Executive Office of the President), or any independent regula\u0002tory agency.” 5 U.S.C. § 552(f)(1).\n5 U.S. General Accounting Office, Record Linkage and Privacy: Issues in Creating New Federal Research and Statistical Information 10 (2001). \n6 Interview with Marc Groman, Former Senior Advisor for Privacy, White House Office of Management and Budget (Feb. 18, 2021); see also Bipartisan \nPol’y Ctr., Barriers to Using Government Data: Extended Analysis of the U.S. Commission on Evidence-Based Policymaking’s Survey of Federal \nAgencies and Offices 10 (2018).\n7 See Joseph Near & David Darais, Differentially Private Synthetic Data, Nat’l Inst. Standards & Tech. (May 3, 2021), https://www.nist.gov/blogs/cyber\u0002security-insights/differentially-private-synthetic-data; see also Steven M. Bellovin et al., Privacy and Synthetic Datasets, 22 Stan. L. Rev. 1 (2019).\n8 E-Government Act of 2002, Pub. L. No. 107-347. \n9 Confidential Information Protection and Statistical Efficiency Act of 2002, 44 U.S.C. § 3501 (2012).\n10 Foundations for Evidence-Based Policymaking Act of 2017, Pub. L. No. 115-435, 132 Stat. 5529 (2019).\n11 President’s Mgmt. Agenda, Federal Data Strategy 2020 Action Plan (2020).\n12 Privacy Act of 1974, 5 U.S.C. § 552a (2012).\n13 There are many versions of the Fair Information Practice Principles, and the U.S. government has not institutionalized a specific version, though the \nversion used by the Department of Homeland Security is commonly referenced (available at: https://www.dhs.gov/publication/privacy-policy-guid\u0002ance-memorandum-2008-01-fair-information-practice-principles). The Organisation for Economic Cooperation and Development produced an influen\u0002tial version of them in 1980 (revised in 2013), which remains an authoritative source. OECD Guidelines on the Protection of Privacy and Transborder Flows \nof Personal Data, OECD (2013), https://www.oecd.org/digital/ieconomy/oecdguidelinesontheprotectionofprivacyandtransborderflowsofpersonaldata.\nhtm.\n14 See David Freeman Engstrom, Daniel E. Ho, Catherine M. Sharkey & Mariano-Florentino Cuéllar, Government by Algorithm: Artificial Intelli\u0002gence in Federal Administrative Agencies (2020) (documenting present use of AI by government agencies).\n15 5 U.S.C. § 552a (a)(5).\n16 5 U.S.C. §§ 552a(a)(8)(A)(i)(I), (II).\n17 The Privacy Act of 1974, Elec. Privacy Info. Ctr., https://epic.org/privacy/1974act/ (last visited Aug. 15, 2021).\n18 See Fact Sheet: National Secure Data Service Act Advances Responsible Data Sharing in Government, Data Coalition (May 13, 2021), https://www.\ndatacoalition.org/fact-sheet-national-secure-data-service-act-advances-responsible-data-sharing-in-government/; U.S Gov’t Accountability Office, \nRecord Linkage and Privacy: Issues in Creating New Federal Research and Statistical Information (2001).\n19 It is no small irony that private companies in the U.S. have fulfilled that mission today. In fact, the U.S. government now approaches private industry, \neither through legal process or through procurement, when it requires data about individuals that the government itself does not collect. Senator Ron \nA Blueprint for the National Research Cloud 101\nWyden has proposed legislation to prevent the government from making these purchases. Wyden, Paul and Bipartisan Members of Congress Introduce \nThe Fourth Amendment Is Not For Sale Act, Ron Wyden U.S. Senator for Or. (Apr. 21, 2021), https://www.wyden.senate.gov/news/press-releases/\nwyden-paul-and-bipartisan-members-of-congress-introduce-the-fourth-amendment-is-not-for-sale-act-.\n20 See, e.g., World Econ. Forum, The Next Generation of Data-Sharing in Financial Services (2019).\n21 See, e.g., Stacie Dusetzina et al., Linking Data for Health Services Research: A Framework and Instructional Guide, Agency for Healthcare Research & \nQuality (Sept. 1, 2014), https://www.ncbi.nlm.nih.gov/books/NBK253315/.\n22 See, e.g., European Comm’n, A European Strategy for Data (2020) (arguing for cross-border data aggregation and linkage of both private and pub\u0002lic sector data); M Sanni Ali et al., Administrative Data Linkage in Brazil: Potentials for Health Technology Assessment, 10 Frontiers in Pharmacology 984 \n(2019); Data Linkage, Australian Inst. of Health & Welfare (Jan. 4, 2020), https://www.aihw.gov.au/our-services/data-linkage. \n23 See, e.g., Elsa Augustine, Vikash Reddy & Jesse Rothstein, Linking Administrative Data: Strategies and Methods (2018) (describing tips for \nconducting data linkages in California); see also U.S. Dep’t of Health & Human Services, Status of State Efforts to Integrate Health and Human \nServices Systems and Data (2016).\n24 Ben Moscovitch, How President Biden Can Improve Health Data Sharing For COVID-19 And Beyond, Health Affairs (Mar. 1, 2021), https://www.\nhealthaffairs.org/do/10.1377/hblog20210223.611803/full/.\n25 Home, Johns Hopkins Coronavirus Resource Ctr., https://coronavirus.jhu.edu/.\n26 The COVID Tracking Project, https://covidtracking.com/. \n27 Fred Bazzoli, COVID-19 Emergency Shows Limitations of Nationwide Data Sharing Infrastructure, Healthcare IT News (June 2, 2020), https://www.\nhealthcareitnews.com/news/covid-19-emergency-shows-limitations-nationwide-data-sharing-infrastructure.\n28 See, e.g., C. Jason Wang et al., Response to COVID-19 in Taiwan: Big Data Analytics, New Technology, and Proactive Testing, JAMA (Mar. 3, 2020), https://\njamanetwork.com/journals/jama/fullarticle/2762689/; Fang-Ming Chen et al., Big Data Integration and Analytics to Prevent a Potential Hospital Outbreak \nof COVID-19 in Taiwan, 54 J. Microbiology, Immunology & Infection 129-30 (2020).\n29 See, e.g., Q&A on the Pentagon’s “Total Information Awareness” Program, Am. C.L. Union, https://www.aclu.org/other/qa-pentagons-total-informa\u0002tion-awareness-program; The Five Problems with CAPPS II: Why the Airline Passenger Profiling Proposal Should Be Abandoned, Am. C.L. Union, https://\nwww.aclu.org/other/five-problems-capps-ii. \n30 See, e.g., Barton Gellman, Dark Mirror: Edward Snowden and the American Surveillance State (2020); Edward Snowden, Permanent Record \n(2019).\n31 5 U.S.C. § 552(b).\n32 5 U.S.C. § 552(b)(3).\n33 The Privacy Act also contains specific carve-outs for disclosures to the Census Bureau and to the National Archives and Records Administration. \nHowever, the carve-outs for these two agencies require that the disclosures be made for the purposes of a census survey and of recording historical \nvalue, respectively. Because the NRC’s explicit purpose is to democratize AI innovation, it is unlikely that the NRC can take advantage of this existing \nexception to dataset disclosures under the Privacy Act.\n34 For example, the Federal Emergency Management Agency’s list of routine uses includes broad disclosure “[t]o an agency or organization for the \npurpose of performing audit or oversight operations as authorized by law, but only such information as is necessary and relevant to such audit or over\u0002sight function.” Privacy Act of 1974; Department of Homeland Security Federal Emergency Management Agency-008 Disaster Recovery Assistance Files \nSystem of Records, 78 Fed. Reg. 25282 (May 30, 2013). \n35 See, e.g., Britt v. Naval Investigative Service, 886 F.2d 544 (3d Cir. 1989).\n36 The Privacy Act of 1974, supra note 17. \n37 5 U.S.C. § 552(b)(5). \n38 5 U.S.C. §§ 552a(a)(8)(B)(i), (ii) (emphasis added).\n39 44 U.S.C. § 3561(8), (12).\n40 U.S. Dep’t of Health & Human Services, The State of Data Sharing at the U.S. Department of Health and Human Services 16 (2018).\n41 44 U.S.C. § 3575(4).\n42 See Engstrom, Ho, Sharkey & Cuéllar, supra note 14, at 16 (finding that the Bureau of Labor Statistics is one of the top ten agencies that use \nartificial intelligence); Machine Learning, Census Bureau (Apr. 17, 2019), https://www.census.gov/topics/research/data-science/about-machine-learn\u0002ing.html (asserting that the Census Bureau “needs” machine learning capabilities); Bureau of Econ. Analysis, 2020 Strategic Action Plan 7 (2020) \n(highlighting the importance of artificial intelligence and machine learning to BEA’s strategy).\n43 Group level data analyses also have inherent privacy risks and harms. See, e.g., Linnet Taylor, Safety in Numbers? Group Privacy and Big Data Analyt\u0002ics in the Developing World, in Group Privacy: New Challenges of Data Technologies 13 (2017).\n44 See 34 U.S.C. §§ 41303(c)(2), (3), (4).\n45 Nat’l Acad. of Sci., Innovations in Federal Statistics 41 (2017).\n46 See 13 U.S.C. § 6. \n47 Nat’l Acad. of Sci., supra note 45, at 40.\n48 According to the study, “an agency’s legal counsel may advise against sharing data as a precautionary measure rather than because of an explicit \nprohibition.” U.S. Gov’t Accountability Office, Sustained and Coordinated Efforts Could Facilitate Data Sharing While Protecting Privacy 1 \n(2013).\n49 See Amy O’Hara & Carla Medalia, Data Sharing in the Federal Statistical System: Impediments and Possibilities, 675 Annals Am. Acad. Pol. & Soc. Sci.\n138, 141 (2018).\n50 Robert M. Groves & Adam Neufeld, Accelerating the Sharing of Data Across Sectors to Advance the Common Good 12 (2017). \n51 Bipartisan Pol’y Ctr., supra note 6, at 18-20.\n52 See O’Hara & Medalia, supra note 49, at 141.\n53 Administrative Data Research UK, ADR UK, https://www.adruk.org.\n54 About ADR UK, ADR UK, https://www.adruk.org/about-us/about-adr-uk/.\n55 Id.\n56 See World of Work, ADR UK, https://www.adruk.org/our-work/world-of-work/.\n57 Funding Opportunities, ADR UK, https://www.adruk.org/news-publications/funding-opportunities/.\n58 Id.\nA Blueprint for the National Research Cloud 102\n59 Id.\n60 Funding Opportunity: A Unique Chance to Shape Data Science at the Heart of UK Government, ADR UK (Apr. 8, 2021), https://www.adruk.org/\nnews-publications/news-blogs/funding-opportunity-a-unique-chance-to-shape-data-science-at-the-heart-of-uk-government-384/. \n61 Funding Opportunities, supra note 57.\n62 Digital Economy Act 2017 (Gr. Br.).\n63 ADR UK, Trust, Security and Public Interest: Striking the Balance 28 (2020).\n64 Id.\n65 Id.\n66 How Do We Work with Researchers?, ADR UK, https://www.adruk.org/our-mission/working-with-researchers/.\n67 Accessing Secure Research Data as an Accredited Researcher, Off. for Nat’l Stat., https://www.ons.gov.uk/aboutus/whatwedo/statistics/requesting\u0002statistics/approvedresearcherscheme. \n68 See Nick Hart & Nancy Potok, Modernizing U.S. Data Infrastructure: Design Considerations for Implementing a National Secure Data Service \nto Improve Statistics and Evidence Building 17, 21 (2020).\n69 Id.\n70 Id. at 15.\n71 President’s Mgmt. Agenda, supra note 11, at 9.\n72 Id. at 31.\n73 See What is Open Data?, Open Data Handbook, https://opendatahandbook.org/guide/en/what-is-open-data/. \nChapter 6\n1 See Keeping Secrets: Anonymous Data Isn’t Always Anonymous, Berkeley Sch. of Info. (Mar. 15, 2014), https://ischoolonline.berkeley.edu/blog/anony\u0002mous-data/; Arvind Narayanan & Vitaly Shmatikov, How to Break Anonymity of the Netflix Prize Dataset, Cornell U. (Nov. 22, 2007), https://arxiv.org/pdf/\ncs/0610105.pdf.\n2 Matt Fredrikson, Somesh Jha & Thomas Ristenpart, Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures, 22 Pro\u0002ceedings of the ACM Special Interest Group on Security, Audit & Control 1322 (2015); Nicholas Carlini et al., Extracting Training Data from Large \nLanguage Models, Cornell U. (June 15, 2021), https://arxiv.org/pdf/2012.07805.pdf. \n3 See, e.g., HIPAA Training, Certification, and Compliance, HIPAA Training, https://www.hipaatraining.com/; Research Data Management, UK Data Serv., \nhttps://ukdataservice.ac.uk/learning-hub/research-data-management/.\n4 Ashwin Machanavajjhala et al., L-Diversity: Privacy Beyond K-Anonymity, 22 Int’l Conf. Data Eng’g 24 (2006). \n5 Cynthia Dwork & Aaron Roth, The Algorithmic Foundations of Differential Privacy (2014). \n6 See, e.g., Tara Bahrampour & Marissa J. Lang, New System to Protect Census Data May Compromise Accuracy, Some Experts Way, Wash. \nPost (June 1, 2021), https://www.washingtonpost.com/local/social-issues/2020-census-differential-privacy-ipums/2021/06/01/6c\u000294b46e-c30d-11eb-93f5-ee9558eecf4b_story.html; Kelly Percival, Court Rejects Alabama Challenge to Census Plans for Redistricting and Privacy, \nBrennan Ctr. (June 30, 2021), https://www.brennancenter.org/our-work/analysis-opinion/court-rejects-alabama-challenge-census-plans-redistrict\u0002ing-and-privacy.\n7 See, e.g., Leonard E. Burman et al., Safely Expanding Research Access to Administrative Tax Data: Creating a Synthetic Public Use File and a \nValidation Server (2018); see also The Synthetic Data Vault, https://sdv.dev. \n8 Valerie Chen, Valerio Pastro & Mariana Raykova, Secure Computation for Machine Learning with SPDZ, Cornell U. (Jan. 2, 2019), https://arxiv.org/\npdf/1901.00329.pdf. \n9 Louis J. M. Aslett et al., A Review of Homomorphic Encryption and Software Tools for Encrypted Statistical Machine Learning, Cornell U. (Aug. 26, 2015), \nhttps://arxiv.org/pdf/1508.06574.pdf. \n10 See Hongyan Chang & Reza Shokri, On the Privacy Risks of Algorithmic Fairness, Cornell U. (Apr. 7, 2021), https://arxiv.org/pdf/2011.03731.pdf.\n11 Ruggles et al., Differential Privacy and Census Data: Implications for Social and Economic Research, 109 Am. Econ. Ass’n Papers & Proceedings 403, \n406 (2019).\n12 In Computer Science literature, such algorithmic settings are often referred to as hyperparameters. For instance, k is a hyperparameter for k-ano\u0002nymity. By setting k to different values (e.g., 5, 10, 100), practitioners can modulate the amount of anonymity afforded to records in the data. As we note \nhowever, the choice of hyperparameters controls both the privacy effected on a dataset as well as the fidelity of that data. \n13 See Differential Privacy for Census Data Explained, Nat’l Conf. of State Legislatures (July 1, 2021), https://www.ncsl.org/research/redistricting/dif\u0002ferential-privacy-for-census-data-explained.aspx; Hongyan Chang & Reza Shokri, On the Privacy Risks of Algorithmic Fairness, Cornell U. (Apr. 7, 2021), \nhttps://arxiv.org/pdf/2011.03731.pdf. Some scholars even find that the incorporation of differential privacy into machine learning algorithms can have \ndisparate impact on underrepresented groups. See Eugene Bagdasaryan & Vitaly Shmatikov, Differential Privacy Has Disparate Impact on Model Accura\u0002cy, Cornell U. (Oct. 27, 2019), https://arxiv.org/pdf/1905.12101.pdf.\n14 Steven Ruggles, Differential Privacy and Census Data: Implications for Social and Economic Research 17.\n15 Id. at 18-19.\n16 Nat’l Acad. of Sci., Innovations in Federal Statistics 86 (2017). The fragmented FSRDC review process is similar to the fragmented data access \nregime we discussed in Chapter Three.\n17 Special Sworn Researcher Program, Bureau of Econ. Analysis, https://www.bea.gov/research/special-sworn-researcher-program (last updated July \n23, 2021).\n18 13 U.S.C. § 9.\n19 The institutional form of the NRC is discussed in depth in Chapter Four.\n20 NORC Data Enclave, NORC, https://www.norc.org/PDFs/BD-Brochures/2016/Data%20Enclave%20One%20Sheet.pdf. \n21 CMS Virtual Research Data Center (VRDC), Research Data Assistance Ctr., https://resdac.org/cms-virtual-research-data-center-vrdc. \n22 Request for Information (RFI) Seeking Stakeholder Input on the Need for an NIH Administrative Data Enclave, Nat’l Inst. of Health (Mar. 1, 2019), \nhttps://grants.nih.gov/grants/guide/notice-files/NOT-OD-19-085.html. \nA Blueprint for the National Research Cloud 103\n23 See FASEB Response to NIH Request for Information (RFI): Seeking Stakeholder Input on the Need for an NIH Administrative Data Enclave, Fed’n of Am. \nSocieties for Experimental Biology (2019), https://www.faseb.org/Portals/2/PDFs/opa/2019/FASEB_Response_Data_Enclave_RFI_NOT-OD-19-085.\npdf; Am. Soc’y of Biochemistry & Molecular Biology (May 30, 2019), https://www.asbmb.org/getmedia/e3401ed5-3210-4ed2-a82a-7363cb86071d/\nASBMB-Response-to-NIH-RFI-NOT-09-19-085.pdf. \n24 What We Do, Cal. Pol’y Lab, https://www.capolicylab.org/what-we-do/.\n25 Id.\n26 CPL Roadmap to Government Administrative Data in California, Cal. Pol’y Lab, https://www.capolicylab.org/data-resources/california-data-road\u0002map/.\n27 Interview with Evan White, Executive Director, California Policy Lab (Apr. 29, 2021).\n28 Id.\n29 Id.; see, e.g., Policy Evaluation and Research Linkage Initiative (PERLI), Cal. Pol’y Lab, https://www.capolicylab.org/data-resources/perli/; University \nof California Consumer Credit Panel, Cal. Pol’y Lab, https://www.capolicylab.org/data-resources/university-of-california-consumer-credit-panel/.\n30 Interview with Evan White, supra note 27.\n31 Id.\n32 See, e.g., Life Course Dataset, Cal. Pol’y Lab, https://www.capolicylab.org/life-course-dataset/.\n33 See CPL Roadmap to Government Administrative Data in California, supra note 26.\n34 We note that it is possible that the organizational form could affect the authority of NRC staff to speak to the legality of data transfers. \nChapter 7\n1 Christopher Whyte, Deepfake News: AI-Enabled Disinformation as a Multi-Level Public Policy Challenge, 5 J. Cyber Pol’y 199 (2020); Don Fallis, What Is \nDisinformation?, 63 Libr. Trends 601 (2015). \n2 Mary L. Gray & Siddharth Suri, Ghost Work: How to Stop Silicon Valley From Building a New Global Underclass (2019); Science Must Examine \nthe Future of Work, Nature (Oct. 19, 2017), \nhttps://www.nature.com/articles/550301b. \n3 David Danks & Alex John London, Algorithmic Bias in Autonomous Systems, 26 Int’l Joint Conf. on Artificial Intelligence 4691 (2017); Joy Buolam\u0002wini & Timnit Gebru, Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification, 81 Proceeding of Machine Learning Res.\n1 (2018); Ben Hutchinson et al., Unintended Machine Learning Biases as Social Barriers for Persons with Disabilities, 125 ACM SIGACCESS Accessibility & \nComputing 1 (2020).\n4 Oscar H. Gandy Jr., The Panoptic Sort: A Political Economy of Personal Information (1993); Virginia Eubanks, Automating Inequality (2018); \nRashida Richardson, Racial Segregation and the Data-Driven Society: How Our Failure to Reckon with Root Causes Perpetuates Separate and Unequal \nRealities, 36 Berkeley Tech. L. J. 101 (2021). \n5 For approaches to improve machine learning practices, see Timnit Gebru et al., Datasheets for Datasets, Cornell U. (Mar. 19, 2020), https://arxiv.org/\npdf/1803.09010.pdf; Margaret Mitchell et al., Model Cards for Model Reporting, 2019 Proceedings ACM Conf. on Fairness, Accountability & Transpar\u0002ency 220 (2019); Kenneth Holstein et al., Improving Fairness in Machine Learning Systems: What do Industry Practitioners Need?, 2019 CHI Conf. on Hum. \nFactors in Computing Sys. 1 (2019); Michael A Madaio et al., Co-Designing Checklists to Understand Organizational Challenges and Opportunities Around \nFairness in AI, 2020 CHI Conf. on Hum. Factors in Computing Sys. 318 (2019). The literature on AI’s societal impacts and fairness, accountability, and \ntransparency of AI is vast, but see Michael Kearns & Aaron Roth, The Ethical Algorithm: The Science of Socially Aware Algorithm Design (2019); \nEubanks, supra note 4; Solon Barocas, Moritz Hardt & Arvind Narayanan, Fairness and Machine Learning (2019); Cathy O’Neil, Weapons of Math \nDestruction (2016).\n6 45 C.F.R §§ 46.101-124.\n7 J. Britt Holbrook & Robert Frodeman, Peer Review and the Ex Ante Assessment of Societal Impacts, 20 Res. Evaluation 239 (2011). \n8 Id.\n9 Institutional Review Boards (IRBs) and Protection of Human Subjects in Clinical Trials, U.S. Food & Drug Admin., https://www.fda.gov/about-fda/cen\u0002ter-drug-evaluation-and-research-cder/institutional-review-boards-irbs-and-protection-human-subjects-clinical-trials (last updated Sept. 11, 2019).\n10 There are crucial questions with regards to consent even with data considered “publicly” available. See generally Casey Fiesler & Nicholas Proferes, \n“Participant” Perceptions of Twitter Research Ethics, 4 Social Media + Society 1 (2018); Sarah Gilbert, Jessica Vitak & Katie Shilton, Measuring Americans’ \nComfort with Research Uses of Their Social Media Data, 7 Social Media + Society 1 (2021). \n11 Sara R. Jordan, Future of Privacy Forum, Designing an Artificial Intelligence Research Review Committee (2019), https://fpf.org/wp-content/\nuploads/2019/10/DesigningAIResearchReviewCommittee.pdf.\n12 Agatta Feretti et al., Ethics Review of Big Data Research: What Should Stay and What Should Be Reformed?, 22 BMC Medical Ethics 1, 6 (2021); Kathryn \nM. Porter et al., The Emergence of Clinical Research Ethics Consultation: Insights from a National Collaborative, 2018 Am. J. Bioethics 39 (2018). \n13 Feretti et al., supra note 12.\n14 See, e.g., Mark Diaz et al., Addressing Age-Related Bias in Sentiment Analysis, 2018 Proceedings CHI Conf. on Hum. Factors in Computing Sys. 1 \n(2018); Buolamwini & Gebru, supra note 3.\n15 See, e.g., Timnit Gebru et al., Datasheets for Datasets, Cornell U. (Mar. 19, 2020), https://arxiv.org/pdf/1803.09010.pdf; Margaret Mitchell et al., Model \nCards for Model Reporting, 2019 Proceedings ACM Conf. on Fairness, Accountability & Transparency 220 (2019); Emily M. Bender et al., On the Dan\u0002gers of Stochastic Parrots: Can Language Models Be Too Big?, 2021 Proceedings ACM Conf. on Fairness, Accountability & Transparency 610 (2021); \nChristo Wilson et al., Building and Auditing Fair Algorithms: A Case Study in Candidate Screening, 2021 Proceedings ACM Conf. on Fairness, Account\u0002ability & Transparency 666 (2021); Pauline T. Kim, Auditing Algorithms for Discrimination, 166 U. Pa. L. Rev. Online 189 (2017).\n16 Phase II: Proposal Review and Processing, Nat’l Sci. Found., https://www.nsf.gov/bfa/dias/policy/merit_review/phase2.jsp#select.\n17 Harvey A. Averch, Criteria for Evaluating Research Projects and Portfolios, in Evaluating R&D Impacts: Methods and Practice 263 (1993).\n18 Nat’l Security Comm’n on Artificial Intelligence, Final Report 141-54 (2021). \n19 History and Mission, U.S. Privacy & Civil Liberties Oversight Bd., https://www.pclob.gov/About/HistoryMission.\n20 AI in Counterterrorism Oversight Enhancement Act of 2021, H.R. 4469, 117th Cong. (2021). \n21 In instances where a researcher is using data obtained from one of the agencies that falls under ORI’s oversight, it may make sense to have ORI adju-\nA Blueprint for the National Research Cloud 104\ndicate those cases directly. For more information about the ORI, see ORI, Office Of Research Integrity, https://ori.hhs.gov/.\n22 Michael S. Bernstein et al., ESR: Ethics and Society Review of Artificial Intelligence Research, Cornell U. (July 9, 2021), https://arxiv.org/\npdf/2106.11521.pdf.\n23 Nat’l Sci. Found., Broader Impacts, https://www.nsf.gov/od/oia/special/broaderimpacts/.\n24 See, e.g., Notice of Special Interest: Administrative Supplement for Research and Capacity Building Efforts Related to Bioethical Issues, Nat’l Inst. of \nHealth (Nov. 17, 2020), https://grants.nih.gov/grants/guide/notice-files/NOT-OD-21-020.html; Notice of Special Interest: Administrative Supplement \nfor Research on Bioethical Issues, Nat’l Inst. of Health (Dec. 30, 2019), https://grants.nih.gov/grants/guide/notice-files/NOT-OD-20-038.html; see also \nCourtenay R. Bruce et al., An Embedded Model for Ethics Consultation: Characteristics, Outcomes, and Challenges, 5 AJOB Empirical Bioethics 8 (2014); \nSharon Begley, In a Lab Pushing the Boundaries of Biology, an Embedded Ethicist Keeps Scientists in Check, Stat (Feb. 23, 2017), https://www.statnews.\ncom/2017/02/23/bioethics-harvard-george-church/. Private foundations also promote the use of embedded bioethicists. See, e.g., Making a Differ\u0002ence Request for Proposals – Fall 2021, The Greenwall Found. (2021), https://greenwall.org/making-a-difference-grants/request-for-proposals-MAD\u0002fall-2021.\nChapter 8\n1 Paul Cichonski et al., Computer Security Incident Handling Guide (2012). \n2 Putative attacks could include the deployment of ransomware, phishing schemes, gaining root access (the highest level of privilege available which \ngives users access to all commands and files by default), exposure of secret credentials, data poisoning, data exfiltration, as well as other types of \nunauthorized network intrusions.\n3 Karen Hao, AI Consumes a Lot of Energy. Hackers Could Make it Consume More, MIT Tech. R. (May 6, 2021), https://www.technologyreview.\ncom/2021/05/06/1024654/ai-energy-hack-adversarial-attack/.\n4 Catalin Cimpanu, Vast Majority of Cyber-Attacks on Cloud Servers Aim to Mine Cryptocurrency, ZDNet (Sept. 14, 2020), https://www.zdnet.com/article/\nvast-majority-of-cyber-attacks-on-cloud-servers-aim-to-mine-cryptocurrency/.\n5 We note that the NRC will likely need to comply with data specific security regulations as well. For instance, medical data security will need to comply \nwith HIPAA, and financial data will need to comply with The Gramm-Leach-Bliley Act.\n6 Ray Dunham, FISMA Compliance: Security Standards & Guidelines Overview, Linford & Co. (Nov. 29, 2017), https://linfordco.com/blog/fisma-compliance/.\n7 Amy J. Frontz, Review of the Department of Health and Human Services Compliance with the Federal Information Security Modernization Act \nof 2014 for Fiscal Year 2020 (2021). \n8 U.S. Senate Comm. on Homeland Security & Governmental Affairs, Federal Cybersecurity: America’s Data at Risk 18 (2019). \n9 Federal Information Security Modernization Act (FISMA) Background, Nat’l Inst. Standards & Tech., https://csrc.nist.gov/projects/risk-management/\nfisma-background (last updated Aug. 4, 2021).\n10 Dunham, supra note 6.\n11 U.S. Senate Comm. on Homeland Security & Governmental Affairs, supra note 8, at 19.\n12 Id. at 18.\n13 Id. at 19.\n14 Id. at 20.\n15 Kevin Stine, et al., Guide for Mapping Types of Information and Information Systems to Security Categories (2008). Specifically, FISMA defines \ncompliance in terms of three levels: low impact, moderate impact, and high impact. Low impact indicates that the loss of confidentiality, integrity, or \navailability of the system will have a limited adverse effect, while high impact indicates that such losses will have severe or catastrophic effects. See Sar\u0002ah Harvey, 3 FISMA Compliance Levels: Low, Moderate, High, KirkpatrickPrice (Apr. 24, 2020), https://kirkpatrickprice.com/blog/fisma-compliance-lev\u0002els-low-moderate-high/.\n16 Nat’l Inst. Standards & Tech., Security and Privacy Controls for Information Systems and Organizations (2020). \n17 Marianne Swanson et al., Guide for Developing Security Plans for Federal Information Systems (2006). \n18 Michael McLaughlin, Reforming FedRAMP: A Guide to Improving the Federal Procurement and Risk Management of Cloud Services, Info. Tech. & Innova\u0002tion Found. (June 15, 2020), https://itif.org/publications/2020/06/15/reforming-fedramp-guide-improving-federal-procurement-and-risk-management.\n19 Program Basics, FedRAMP, https://www.fedramp.gov/program-basics/; see also Steven VanRoekel, Security Authorization of Information Sys\u0002tems in Cloud Computing Environments (2011). \n20 FISMA vs. FedRAMP and NIST: Making Sense of Government Compliance Standards, Foresite, https://foresite.com/fisma-vs-fedramp-and-nist-making\u0002sense-of-government-compliance-standards/. However, we note that FedRAMP approval is exempted for certain types of cloud models: (i) where the \ncloud is private to the agency, (ii) where the cloud is physically located within a Federal facility, (iii) where the agency is not providing cloud services \nfrom the cloud-based information system to any external entities. See VanRoekel, supra note 19.\n21 FedRAMP, FedRAMP Security Assessment Framework 5 (2017).\n22 Doina Chiacu, White House Warns Companies to Step Up Cybersecurity: ‘We Can’t Do it Alone’, Reuters (June 3, 2021), https://www.reuters.com/tech\u0002nology/white-house-warns-companies-step-up-cybersecurity-2021-06-03/; see also Significant Cyber Incidents, Ctr. Strategic & Int’l Studies, https://\nwww.csis.org/programs/strategic-technologies-program/significant-cyber-incidents (last visited Aug. 19, 2021).\n23 U.S. Senate Comm. Homeland Security & Governmental Affairs, supra note 8, at 5.\n24 Id. at 6.\n25 Frontz, supra note 7.\n26 Jonathan Reiber & Matt Glenn, The U.S. Government Needs to Overhaul Cybersecurity. Here’s How., Lawfare (Apr. 9, 2021), https://www.lawfareblog.\ncom/us-government-needs-overhaul-cybersecurity-heres-how.\n27 Nat’l Security Agency, Embracing a Zero Trust Security Model (2021).\n28 McLaughlin, supra note 18.\n29 Id.\n30 Id.\n31 Exec. Order No. 14,028, 86 Fed. Reg. 26633 (May 17, 2021).\n32 U.S. Office of Mgmt. & Budget, Moving the U.S. Government Towards Zero Trust Cybersecurity Principles (2021).\nA Blueprint for the National Research Cloud 105\n33 See, e.g., David Kushner, The Real Story of Stuxnet, IEEE Spectrum (Feb. 26, 2013), https://spectrum.ieee.org/the-real-story-of-stuxnet.\n34 HTTP is the protocol at the highest level of abstraction targeting the application layer, and its secure variant HTTPS additionally encrypts the data \nusing an encryption protocol. Without encryption, HTTP is insecure and should not be used. The encryption protocol in original use was SSL but this \nhas since been deprecated in the realm of network security in favor of its newer version, TLS. Both SSL and TLS rely on public key certificates signed by \na trusted certificate authority. When these certificates have expired, the websites providing them can no longer necessarily be trusted. Although these \nmeasures have their own limitations, not adopting them can only be less secure.\n35 See, e.g., Azure Confidential Computing, Microsoft, https://azure.microsoft.com/en-ca/solutions/confidential-compute/; Nataraj Nagaratnam, \nConfidential Computing, IBM (Oct. 16, 2020), \nhttps://www.ibm.com/cloud/learn/confidential-computing; Confidential Computing, Google Cloud, https://cloud.google.com/confidential-computing.\n36 David Archer et al., From Keys to Databases—Real-World Applications of Secure Multi-Party Computation, 61 Computer J. 1749 (2018). \n37 Amit Elazari Bar On, We Need Bug Bounties for Bad Algorithms, Motherboard (May 3, 2018) https://www.vice.com/en/article/8xkyj3/we-need-bug\u0002bounties-for-bad-algorithms.\nChapter 9\n1 Importantly, this chapter discusses the extent to which researchers should be required to share their research outputs, not the extent to which re\u0002searchers should be required to share their private data. The latter was discussed in Chapter Three.\n2 Dan Robitzski, AI Researchers Are Boycotting A New Journal Because It’s Not Open Access, Futurism (May 3, 2018), https://futurism.com/artificial-intel\u0002ligence-journal-boycot-open-access.\n3 Mikio L. Braun & Cheng Soon Ong, Open Science in Machine Learning (2014).\n4 Since researchers using the NRC are not “contractors” under FAR/DFARS, and since evidence is lacking on the value of Other Transactions to AI re\u0002searchers, we do not cover FAR/DFARS and Other Transactions in this section. \n5 Under the Bayh-Dole Act, Aa “federal funding agreement” is defined as “any contract, grant, or cooperative agreement entered into between any \nFederal agency, other than the Tennessee Valley Authority, and any contractor for the performance of experimental, developmental, or research work \nfunded in whole or in part by the Federal Government.” 35 U.S.C. § 201.\n6 35 U.S.C. § 202.\n7 35 U.S.C. § 203.\n8 See, e.g., Mark A. Lemley & Julie E. Cohen, Patent Scope and Innovation in the Software Industry, 89 Cal. L. Rev. 1 (2001); Mark A. Lemley, Software \nPatents and the Return of Functional Claiming, 2013 Wis. L. Rev. 905 (2013).\n9 Jeremy Gillula & Daniel Nazer, Stupid Patent of the Month: Will Patents Slow Artificial Intelligence?, Elec. Frontier Found. (Sept. 29, 2017), https://\nwww.eff.org/deeplinks/2017/09/stupid-patent-month-will-patents-slow-artificial-intelligence.\n10 U.S. Patent & Trademark Off., Inventing AI: Tracing the Diffusion of Artificial Intelligence with Patents 2 (2020). \n11 See, e.g., Mike James, Google Files AI Patents, I Programmer (July 8, 2015), https://www.i-programmer.info/news/105-artificial-intelli\u0002gence/8765-google-files-ai-patents.html. This is especially problematic because companies represent 26 out of the top 30 AI patent applicants world\u0002wide, while only four are universities or public research organizations. World Intell. Prop. Org., Artificial Intelligence 7 (2019).\n12 Lisa Ouellette & Rebecca Weires, University Patenting: Is Private Law Serving Public Values?, 2019 Mich. St. L. Rev. 1329 (2019).\n13 Id. at 1331; see also Arti Kaur Rai, Regulating Scientific Research: Intellectual Property Rights and the Norms of Science, 94 Nw. U. L. Rev. 77, 136 (1999).\n14 See Brian J. Love, Do University Patents Pay Off? Evidence From a Survey of University Inventors in Computer Science and Electrical Engineering, 16 Yale \nJ. L & Tech. 285 (2014).\n15 See id. at 286.\n16 See, e.g., Tech Transfer FAQ, U. Mich., https://techtransfer.umich.edu/for-inventors/resources/inventor-faq/ (“We carefully review the commercial \npotential for an invention before investing in the patent process. However, because the need for commencing a patent filing usually precedes finding a \nlicensee, we look for creative and cost-effective ways to seek early protections for as many promising inventions as possible”); What is Technology Trans\u0002fer, Princeton U., https://patents.princeton.edu/about-us/what-technology-transfer (“[T]echnologies and everyday products are possible because of \ntechnology transfer . . . Because the discoveries emerging from university research tend to be early-stage, high-risk inventions, successful university \ntechnology transfer transactions require a patent system that protects such innovations.”).\n17 The Uniform Guidance for intellectual property is laid out in 2 C.F.R. § 200.315.\n18 Uniform Administrative Requirements, Cost Principles, and Audit Requirements for Federal Awards, Grants.gov, https://www.grants.gov/learn-grants/\ngrant-policies/omb-uniform-guidance-2014.html (last visited Aug. 27, 2021).\n19 See Key Sections of the Uniform Guidance, AICPA.org, https://www.aicpa.org/interestareas/governmentalauditquality/resources/singleaudit/uni\u0002formguidanceforfederalrewards/key-sections-uniform-guidance.html.\n20 2 C.F.R. § 200.315. A “federal award” under the Uniform Guidance includes, among other things, “the federal financial assistance that a recipient \nreceives directly from a Federal awarding agency or indirectly from a pass-through entity;” or “the cost-reimbursement contract under the Federal \nAcquisition Regulations;” or a “grant agreement, cooperative agreement, [or] other agreement [for federal financial assistance].” 2 C.F.R. § 200.1.\n21 2 C.F.R. §§ 200.315(b), (c). These provisions specify that the government merely “reserves” its “right” to copyright and data rights over research \nproduced under the federal award. \n22 U.S. Copyright Office, Compendium of U.S. Copyright Office Practices 35 (2021, 3d ed.). \n23 Wil Michiels, How Do You Protect Your Machine Learning Investment?, EETimes (Mar. 31, 2020), https://www.eetimes.com/how-do-you-protect-your\u0002machine-learning-investment-part-ii/. \n24 See, e.g., Tabrez Y. Ebrahim, Data-Centric Technologies: Patent and Copyright Doctrinal Disruptions, 43 Nova L. Rev. 287, 304; Daryl Lim, AI & IP: Innova\u0002tion & Creativity in an Age of Accelerated Change, 52 Akron L. Rev. 813, 835 (2018)\n25 2 C.F.R. § 200.315(b).\n26 Id.\n27 For a comprehensive report on how artificial intelligence is used in various government agencies, see David Freeman Engstrom, Daniel E. Ho, Cath\u0002erine M. Sharkey & Mariano-Florentino Cuéllar, Government by Algorithm: Artificial Intelligence in Federal Administrative Agencies (2020).\nA Blueprint for the National Research Cloud 106\n28 Jukebox, OpenAI (Apr. 30, 2020), https://openai.com/blog/jukebox/.\n29 See, e.g., Shlomit Yanisky-Ravid, Generating Rembrandt: Artificial Intelligence, Copyright, and Accountability in the 3A Era--the Human-Like Authors \nare Already Here--a New Model, 27 Mich. St. L. Rev. 659 (2017); Kalin Hristov, Artificial Intelligence and the Copyright Dilemma, 57 J. Franklin Pierce Ctr. \nIntell. Prop. 431 (2017). \n30 Kalin Hristov, Artificial Intelligence and the Copyright Survey, 16 J. Sci. Pol’y & Governance 1, 14-15 (2020).\n31 Id. at 16.\n32 See What is Transfer Learning?, TensorFlow (Mar. 31, 2020), https://www.tensorflow.org/js/tutorials/transfer/what_is_transfer_learning. \n33 See, e.g., Yunhui Guo et al., SpotTune: Transfer Learning Through Adaptive Fine-Tuning, Cornell U. (Nov. 2018), https://arxiv.org/pdf/1811.08737.pdf. \n34 2 C.F.R. § 200.315(d).\n35 See Zhiqiang Wan, Yazhou Zhang & Haibo He, Variational Autoencoder Based Synthetic Data Generation for Imbalanced Learning, IEEE (2017).\n36 See Noseong Park, Mahmoud Mohammadi & Kshitij Gorde, Data Synthesis Based on Generative Adversarial Networks, 11 Proc. VLDB Endowment\n1071 (2018).\n37 See Ron Bakker, Impact of Artificial Intelligence on IP Policy 12. \n38 See Marta Duque Lizarralde, A Guideline to Artificial Intelligence, Machine Learning and Intellectual Property 4-7 (2020). \n39 Steven M. Bellovin et al., Privacy and Synthetic Datasets, 22 Stan. Tech. L. Rev. 1, 2-3 (2019); see also Fida K. Dankar & Mahmoud Ibrahim, Fake It Till \nYou Make It: Guidelines for Effective Synthetic Data Generation, 5 Applied Sci. 11 (2021); but see Theresa Stadler et al., Synthetic Data – Anonymisation \nGroundhog Day, Cornell U. (July 8, 2021), https://arxiv.org/pdf/2011.07018.pdf.\n40 See, e.g., Daniel S. Quintana, A Synthetic Dataset Primer for the Biobehavioural Sciences to Promote Reproducibility and Hypothesis Generation, 9 eLife\n1 (2020).\n41 Yuji Roh et al., A Survey on Data Collection for Machine Learning, Cornell U. (Aug. 12, 2019), https://arxiv.org/pdf/1811.03402.pdf.\n42 See, e.g., Hang Qiu et al., Minimum Cost Active Labeling, Cornell U. (June 24, 2020), https://arxiv.org/pdf/2006.13999.pdf; Eric Horvitz, Machine \nLearning, Reasoning, and Intelligence in Daily Life: Directions and Challenges, 18 Proceeding of the Conf. on Uncertainty in Artificial Intelligence 3 \n(2007).\n43 Cognilytics Research, Data Engineering, Preparation, and Labeling for AI 2019 3 (2019).\n44 See Wil Michiels, How Do You Protect Your Machine Learning Investment?, EETimes (Mar. 26, 2020), https://www.eetimes.com/how-do-you-protect\u0002your-machine-learning-investment/. In fact, in the European Union, labeled datasets are awarded with database rights protections. Mauritz Kop, \nMachine Learning & EU Data Sharing Practices, Stan.-Vienna Transatlantic Tech. L. F.(Mar. 24, 2020), https://ttlfnews.wordpress.com/2020/03/24/\nmachine-learning-eu-data-sharing-practices/.\n45 See, e.g., Niklas Fiedler et al., ImageTagger: An Open Source Online Platform for Collaborative Image Labeling, 11374 Lecture Notes in Computer Sci. \n162 (2019).\n46 Id. at 162.\n47 Researchers may, for instance, use NRC data and compute resources to implement active learning strategies, procedures to manually label a subset \nof available data and infer the remaining labels automatically using a machine learning model. See, e.g., Oscar Reyes et al., Effective Active Learning \nStrategy for Multi-Label Learning, 273 Neurocomputing 494 (2018). Similarly, researchers may augment existing public sector data with valuable labels. \n48 See, e.g., Pedro Saleiro et al., Aequitas: A Bias and Fairness Audit Toolkit, Cornell U. (Apr. 29, 2019), https://arxiv.org/pdf/1811.05577.pdf; Florian \nTramèr et al., FairTest: Discovering Unwarranted Associations in Data-Driven Applications, Cornell U. (Aug. 16, 2019), https://arxiv.org/pdf/1510.02377.\npdf.\n49 While we do not discuss the idiosyncratic modifications to the Uniform Guidance that vary from agency-to-agency, we encourage the task force to \nassess these modifications if it decides to implement the NRC through a particular agency. If the NRC is administered through multiple agencies, the \ncomplex amalgam of agency-specific IP rules may increase the friction in using the NRC if researchers must context-switch from one set of regulations \nto the next depending on the funding agency.\n50 2 C.F.R. § 2900.13. Previously, the Department of Labor explicitly required IP generated under a federal award to be licensed under a Creative \nCommons Attribution license, but this rule was changed in April 2021 to replace the proprietary term “Creative Commons Attribution license” with the \nindustry-recognized standard “open license.” 86 Fed. Reg. 22107 (Apr. 27, 2021).\n51 Dissemination and Sharing of Research Results - NSF Data Management Plan Requirements, Nat’l Sci. Found., https://www.nsf.gov/bfa/dias/policy/\ndmp.jsp. \n52 See, e.g., Aidan Courtney et al., Balancing Open Source Stem Cell Science with Commercialization, Nature Biotechnology (Feb. 7, 2011), https://\nwww.nature.com/articles/nbt.1773.\n53 See Klint Finley, When Open Source Software Comes with a Few Catches, Wired (July 31, 2019), https://www.wired.com/story/when-open-source-soft\u0002ware-comes-with-catches/; Guide to Open Source Licenses, Synopsys (Oct. 7, 2016), https://www.synopsys.com/blogs/software-security/open-source-li\u0002censes/.\n54 See Daniel A. Almeida et. al, Do Software Developers Understand Open Source Licenses?, 25 IEEE Int’l Conf. on Program Comprehension 1 (2017) \n(finding that software developers “struggle[] when multiple [open-source] licenses [are] involved” and “lack the knowledge and understanding to tease \napart license interactions across multiple situations.”).\n55 See, e.g., Alexandra Theben et al., Challenges and Limits of an Open Source Approach to Artificial Intelligence 14 (2021); Stadler et al., supra \nnote 39; Milad Nasr et al., Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Feder\u0002ated Learning, Cornell U. (June 6, 2020), https://arxiv.org/abs/1812.00910.pdf.\n56 Some universities have decided to eliminate classified research. See, e.g., At the Hands of Radicals, Stan. Mag. (Jan. 2009), https://stanfordmag.org/\ncontents/at-the-hands-of-the-radicals.\n57 See Donald Kennedy, Science and Secrecy, 289 Sci. 724 (2000); Peter J. Westwick, Secret Science: A Classified Community in the National Laboratories, \n38 Minerva 363 (2000).\n58 See Braun & Ong, supra note 3; Sören Sonnenburg et al., The Need for Open Source Software in Machine Learning, 8 J. Machine Learning Res. 2443 \n(2007); see also Katie Malone & Richard Wolski, Doing Data Science on the Shoulders of Giants: The Value of Open Source Software for the Data Science \nCommunity, HDSR (May 31, 2020), https://hdsr.mitpress.mit.edu/pub/xsrt4zs2/release/4. \n59 See Laura A. Heymann, Overlapping Intellectual Property Doctrines: Election of Rights Versus Selection of Remedies, 17 Stan. Tech. L. Rev. 239, 240 \n(2013); Oracle Am. Inc. v. Google Inc., 750 F.3d 1339 (Fed. Cir. 2014) (accepting that software is both patentable and copyrightable).\nA Blueprint for the National Research Cloud 107\n60 Robert E. Thomas, Debugging Software Patents: Increasing Innovation and Reducing Uncertainty in the Judicial Reform of Software Patent Law, 25 \nSanta Clara Computer & High Tech. L.J. 191, 222-23 (2008).\n61 See, e.g., Joaquin Vanschorin et al., OpenML: Networked Science in Machine Learning, Cornell U. (Aug. 1, 2014), https://arxiv.org/pdf/1407.7722.pdf \n(developing a collaboration platform through which scientists can automatically share, organize and discuss machine learning experiments, data, and \nalgorithms); see also Sarah O’Meara, AI Researchers in China Want to Keep the Global-Sharing Culture Alive, Nature (May 29, 2019), https://www.nature.\ncom/articles/d41586-019-01681-x; Shuai Zhao et al., Packaging and Sharing Machine Learning Models via the Acumos AI Open Platform, 17 ICMLA (2018).\n62 Jeanne C. Fromer, Machines as the New Oompa-Loompas: Trade Secrecy, the Cloud, Machine Learning, and Automation, 94 N.Y.U. L. Rev. 706, 712 \n(2019); Jordan R. Raffe et al., The Rising Importance of Trade Secret Protection for AI- Related Intellectual Property 1, 5-6 (2020); Jessica \nM. Meyers, Artificial Intelligence and Trade Secrets, Am. Bar Ass’n (Feb. 2019), https://www.americanbar.org/groups/intellectual_property_law/pub\u0002lications/landslide/2018-19/january-february/artificial-intelligence-trade-secrets-webinar/; AIPLA Comments Regarding “Request for Comments on \nIntellectual Property Protection for Artificial Intelligence Innovation”, Am. Intell. Prop. L. Ass’n (Jan. 10, 2020), https://www.uspto.gov/sites/default/files/\ndocuments/AIPLA_RFC-84-FR-58141.pdf.\n63 Clark D. Asay, Artificial Stupidity, 61 Wm. & Mary L. Rev. 1187, 1197, 1241-42 (2020).\n64 See id.; Am. Intell. Prop. L. Ass’n, supra note 62, at 16.\n65 See Asay, supra note 63, at 1242.\nAppendix\n1 Department of Energy Awards $425 Million for Next Generation Supercomputing Technologies, Energy.gov (Nov. 14, 2014), https://www.energy.gov/\narticles/department-energy-awards-425-million-next-generation-supercomputing-technologies.\n2 Amazon EC2 P3 Instances, Amazon, https://aws.amazon.com/ec2/instance-types/p3/ (last visited Sept. 9, 2021).\n3 CORAL Request for Proposal B604142, Lawrence Livermore Nat’l Laboratory (2014), https://web.archive.org/web/20140816181824/ https://asc.llnl.\ngov/CORAL/. We note that we were not able to locate the final award documents, nor is Summit budgeted in sufficient detail to back out cost from the \nDOE budget statements. Our cost estimates here, however, are comparable to publicly reported estimates for the total cost of the Summit system. \n4 This is based on a $30M maximum in the DOE Office of Science contract for non-recurring engineering (NRE) costs for the systems at Argonne National \nLaboratory and Oak Ridge National Laboratory. \n5 This is based on the difference in the RFP terms between the inclusion of maintenance under the Lawrence Livermore National Laboratory system \n(with a maximum budget of $170M) and the exclusion of maintenance under the systems for the Oak Ridge National Laboratory and the Argonne \nNational Laboratory (with a maximum budget for the build contract of $155M). This is likely an upper bound on maintenance, given that the difference \nreflects the combination of NRE and 5-year maintenance. \n6 See CORAL Price Schedule, Lawrence Livermore Nat’l Laboratory (2014), https://web.archive.org/web/20140816181824/ https://asc.llnl.gov/CORAL/\nRFP_components/04_CORAL_Price_Schedule_ANL_ORNL_tabs.xlsx. We used 1.62% as the interest rate to calculate the cost over 60 months. It is the \n5-year Treasury constant maturity rate on November 14, 2014, see Selected Interest Rates (Daily) – H.15, Fed. Res., https://www.federalreserve.gov/\nreleases/H15/default.htm, when DOE announced the award of the HPC system, see Department of Energy Awards $425 Million for Next Generation Super\u0002computing Technologies, supra 1.\n7 For instance, this estimate is in line with the cost of $200M reported by the New York Times. Steve Lohr, Move Over, China: U.S. is Again Home to World’s \nSpeediest Supercomputer, N.Y. Times (June 8, 2018), https://www.nytimes.com/2018/06/08/technology/supercomputer-china-us.html. Some reporting \nconflates the procurement of multiple systems that occurred contemporaneously. \n8 Research shows that for training compute-intensive deep learning models, such as ResNet-101, the GPU utilization is around 70%. Jingoo Han et \nal., A Quantitative Study of Deep Learning Training on Heterogeneous Supercomputers, 2019 IEEE Conf. on Cluster Computing 1, 5 (2019). However, \nResNet-50 has a GPU utilization of approximately 40%, see id., and other accounts report that GPUs are utilized only 15-30% of the time, see, e.g., Lukas \nBiewald, Monitor and Improve GPU Usage for Training Deep Learning Models, Towards Data Sci. (Mar. 27, 2019), https://towardsdatascience.com/mea\u0002suring-actual-gpu-usage-for-deep-learning-training-e2bf3654bcfd; Janet Morss, Giving Your Data Scientists a Boost with GPUaaS, CIO (June 2, 2020), \nhttps://www.cio.com/article/3561090/giving-your-data-scientists-a-boost-with-gpuaas.html.\n9 Compute Canada, Cloud Computing for Researchers 1 (2016), https://www.computecanada.ca/wp-content/uploads/2015/02/CloudStrate\u0002gy2016-2019-forresearchersEXTERNAL-1.pdf.\n10 Jennifer Shkabatur, The Global Commons of Data, 22 Stan. Tech. L.R. 407, 407-09 (2019).\n11 Benjamin Sobel, Artificial Intelligence’s Fair Use Crisis, 41 Colum. J.L. & Arts 61 (2017).\n12 Id.\n13 See Protecting What We Love About the Internet: Our Efforts to Stop Online Piracy, Google Pub. Pol’y Blog (Nov. 7, 2019), https://www.blog.google/\noutreach-initiatives/public-policy/protecting-what-we-love-about-internet-our-efforts-stop-online-piracy/. \n14 See Jennifer M. Urban, Joe Karaganis & Brianna M. Schofield, Notice & Takedown in Everyday Practice 39 (2017) (illustrating the difficulty that \nonline service providers face in manually evaluating a large volume of data for potential infringement; for example, one online service provider ex\u0002plained that “out of fear of failing to remove infringing material, and motivated by the threat of statutory damages, its staff will take “six passes to try to \nfind the [identified content].”); see also Letter from Thom Tillis, Marsha Blackburn, Christopher A. Coons, Dianne Feinstein et. al, to Sundar Pichai, Chief \nExecutive Officer, Google Inc. (Sept. 3, 2019), https://www.ipwatchdog.com/wp-content/uploads/2019/09/9.3-Content-ID-Ltr.pdf (“We have heard from \ncopyright holders who have been denied access to Content ID tools, and as a result, are at a significant disadvantage to prevent repeated uploading of \ncontent that they have previously identified as infringing. They are left with the choice of spending hours each week seeking out and sending notices \nabout the same copyrighted works, or allowing their intellectual property to be misappropriated.”).\n15 See Google, How Google Fights Piracy 6 (2016). To illustrate the costs of implementing Content ID on a large-scale platform, Google announced in \na report in 2016 that YouTube had invested more than $60 million in Content ID. \n16 See Sobel, supra note 11, at 66-79.\n17 See Authors Guild v. Google Inc., 804 F.3d 202 (2d Cir. 2015).\n18 Id.\n19 Id. at 216-17.\n20 Matthew Stewart, The Most Important Court Decision For Data Science and Machine Learning, Towards Data Sci. (Oct. 31, 2019), https://towardsdata-\nA Blueprint for the National Research Cloud 108\nscience.com/the-most-important-supreme-court-decision-for-data-science-and-machine-learning-44cfc1c1bcaf. \n21 See, e.g., James Grimmelmann, Copyright for Literate Robots, 101 Iowa L. Rev. 657, 661; Sobel, supra note 11, at 51-57.\n22 See Sobel, supra note 11, at 57.\n23 See Anna I. Krylov et. al. What is the Price of Open Source Software? 6 J. Physical Chemistry Letters 2751, 2753 (2015) (explaining that budding \nresearchers considering commercialization may be particularly concerned about what licenses are available, since a “strictly open-source environment \nmay furthermore disincentivize young researchers to make new code available right away, lest their ability to publish papers be short-circuited by a \nmore senior researcher with an army of postdocs poised to take advantage of any new code.”).\n24 See, e.g., A Data Scientist’s Guide to Open-Source Licensing, Towards Data Sci. (Nov. 4, 2018), https://towardsdatascience.com/a-data-scientists\u0002guide-to-open-source-licensing-c70d5fe42079; Choose an Open-Source License, https://choosealicense.com. \n25 Licensing a Repository, GitHub, https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/licensing-a-repository. \n26 What is the Most Appropriate Licence for My Data?, FigShare, https://help.figshare.com/article/what-is-the-most-appropriate-licence-for-my-data. \n27 See Developer Agreement, Twitter (Mar. 10, 2020), https://developer.twitter.com/en/developer-terms/agreement; Non-commercial Use of the Twitter \nAPI, Twitter, https://developer.twitter.com/en/developer-terms/commercial-terms.\n28 See Daniel A. Almeida et. al, Do Software Developers Understand Open Source Licenses?, 25 IEEE Int’l Conf. on Program Comprehension 1 (2017). \n29 Id. at 9.\n30 Alexandra Kohn & Jessica Lange, Confused About Copyright? Assessing Researchers’ Comprehension of Copyright Transfer Agreements, 6 J. Librarian\u0002ship & Scholarly Commc’n. 1, 9 (2018). \n31 See Will Frass, Jo Cross & Victoria Gardner, Taylor & Francis Open Access Survey June 2014 15 (2014). Note that lack of IP literacy could act as \nan additional deterrent to uploaders. The Taylor and Francis Open Access Survey of 2014 found that “63% of respondents indicated a lack of under\u0002standing of publisher policy as an important or very important factor in failing to deposit an article in an IR [Institutional Repository].” Id.\n32 Dataverse Community Norms, Harv. Dataverse, https://dataverse.org/best-practices/dataverse-community-norms.\n33 Copyright and License Policy, FigShare, https://help.figshare.com/article/copyright-and-license-policy. \n34 Australian Data Research Commons, Research Data Rights Managing Guide 6 (2019).\n35 See Harvard Dataverse General Terms of Use, Harv. Dataverse (2021), https://dataverse.org/best-practices/harvard-dataverse-general-terms-use.\n36 Stan. U. Inst. of Human-Centered Artificial Intelligence, Artificial Intelligence Index Report 2021 125-34 (2021).\n37 Thilo Hagendorff, The Ethics of AI Ethics: An Evaluation of Guidelines, 30 Minds & Machines 99 (2020). \n38 Andrew D. Selbst, An Institutional View of Algorithmic Impact Assessments, 35 Harv. J.L. & Tech. 1, 66 (forthcoming 2021).\n39 Brent Mittlestadt, Principles Alone Cannot Guarantee Ethical AI, 1 Nature Mach. Intelligence 501 (2019). \n40 DOD Adopts Ethical Principles for Artificial Intelligence, U.S. Dep’t Defense (Feb. 24, 2020), https://www.defense.gov/Newsroom/Releases/Release/\nArticle/2091996/dod-adopts-ethical-principles-for-artificial-intelligence/. \n41 President’s Mgmt. Agenda, Federal Data Strategy: Data Ethics Framework (2020).\n42 Artificial Intelligence: An Accountability Framework for Federal Agencies and Other Entities, U.S. Gov’t Accountability Office (June 30, 2021), https://\nwww.gao.gov/products/gao-21-519sp.\n43 Principles of Artificial Intelligence Ethics for the Intelligence Community, Office of the Director of Nat’l Intelligence, https://www.odni.gov/index.\nphp/features/2763-principles-of-artificial-intelligence-ethics-for-the-intelligence-community. \n44 Key Considerations for Responsible Development and Fielding of Artificial Intelligence, Nat’l Security Comm’n Artificial Intelligence (2021), https://\nwww.nscai.gov/key-considerations/. \n45 Recommended Practices, Nat’l Security Comm’n Artificial Intelligence, https://www.nscai.gov/wp-content/uploads/2021/01/Key-Consider\u0002ations-Supporting-Visuals.pdf. \n46 Defense Innovation Bd., AI Principles: Recommendations on the Ethical Use of Artificial Intelligence by the Department of Defense (2019). \n",
    "length": 431092,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Government by AI? Trump Administration Plans to Write Regulations Using Artificial Intelligence - Media Coverage - Stanford Law School",
    "url": "https://law.stanford.edu/press/government-by-ai-trump-administration-plans-to-write-regulations-using-artificial-intelligence/",
    "text": "Government by AI? Trump Administration Plans to Write Regulations Using Artificial Intelligence - Media Coverage - Stanford Law School\nlogo-footerlogo-fulllogo-stanford-universitylogomenu-closemenuClose IconIcon with an X to denote closing.Play IconPlay icon in a circular border.\n[Skip to main content] \n![Sls logo] \n# Government by AI? Trump Administration Plans to Write Regulations Using Artificial Intelligence\n## Details\nPublish Date:January 26, 2026Author(s):\n* Coburn, JesseSource:ProPublicaRelated Person(s):\n* [Daniel E. Ho],\n* [David Freeman Engstrom],\n* [Mariano-Florentino Cuéllar] \n## Summary\nDavid Engstrom, Dan Ho, and Mariano-Florentino Cuéllar&#8217;s report, &#8220;Government by Algorithm: Artificial Intelligence in Federal Administrative Agencies&#8221;, was mentioned by*ProPublica*in &#8220;Government by AI? Trump Administration Plans to Write Regulations Using Artificial Intelligence&#8221;. https://www.propublica.org/article/trump-artificial-intelligence-google-gemini-transportation-regulations\n[Read More] \n**Back to the Top",
    "length": 1055,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "AI Playground offers a safe place to explore and experiment",
    "url": "https://news.stanford.edu/stories/2025/01/ai-playground-offers-a-safe-place-to-explore-and-experiment",
    "text": "AI Playground offers a safe place to explore and experiment | Stanford Report[Skip to content] \n[![Stanford University]] \nMenuToggle SearchSearchClose\nSearch query\nClearSearch\nSubmit search\n[![Stanford Report]] [![Stanford Report]] \n# AI Playground offers a safe place to explore and experiment\n## Read next:\n[] \nPreferencesShow me...Faculty/StaffStudent\n## Along with Stanford news and stories, show me:\n* Student information\n* Faculty/Staff information\nWe want to provide announcements, events, leadership messages and resources that are relevant to you. Your selection is stored in a browser cookie which you can remove at any time using “Clear all personalization” below.\nClear all personalization\nAppearance preference:LightDark\nClose preferencesClose\n## Stanford Report cookie usage information\nWe want to provide stories, announcements, events, leadership messages and resources that are relevant to you. Your selection is stored in a browser cookie which you can remove at any time by visiting the \"Show me...\" menu at the top right of the page. For more, read our[cookie policy].\nAcceptDecline\nJanuary 9th, 2025|6min read[Institutional News] \n# Stanford’s AI Playground offers a safe place to explore and experiment\nUniversity IT began piloting the Stanford AI Playground earlier this year, providing the Stanford community a secure platform to explore various AI tools.\n![] \nThis image was generated using the DALL-E-3 plugin in the Stanford AI Playground. | Chelcey Adami\nAs artificial intelligence continues to rapidly evolve, Stanford’s[AI Playground] provides a convenient environment for faculty, staff, and students to experiment with AI technology.\nEarlier this year, University IT began piloting the AI Playground, a Stanford-hosted environment that allows users to test out and compare a range of AI tools, including ChatGPT, Google Gemini, and Wolfram. As of Oct. 1, the AI Playground is accessible to all Stanford faculty, staff, students, postdocs, and visiting scholars.\nThe project to create the AI Playground stemmed from community feedback and the need for a centralized, accessible platform where community members could safely try AI tools, explained Chief Information Officer Steve Gallagher.\n“Our goal was to put the tools into people’s hands in a more accessible way,” Gallagher said. “One of UIT’s strategic values is to innovate with purpose, and in the case of the Playground, we want to help enable the innovation happening across campus.”\nThe AI Playground is designed to be both an entry point into AI for users who are unsure where to start and a place of experimentation for those more advanced in AI usage. “The vision is to increase the collective literacy around artificial intelligence and these tools across campus,” Gallagher said.\nThe AI Playground is part of Stanford’s broader effort to address the growing role of AI in research, education, and administration. Separately, the AI at Stanford Advisory Committee, appointed by the provost to assess potential policy gaps for AI use at the university, recently[released its initial findings].\n## Time to play\nThe AI Playground includes various large language models (LLMs), a form of generative AI specializing in text-based content. Users can access additional LLM plugins for image generation, web scraping, and AI-assisted Google services.\nThis versatility enables users to compare how different AI models handle the same task or use different models simultaneously to complete tasks such as document analysis, creating graphs and websites, making recommendations, or skill assessments.\nUIT provides resources on[how to use AI responsibly],[how to use the AI Playground], and[how to use generative AI tools], which are built using algorithms that can generate text, images, video, audio, and 3D models in response to user prompts.\nJoshua Barnett, IT director and enterprise infrastructure architect, emphasized the importance of expanding AI access to more students.\n“We needed to level the playing field and make it free to use,” said Barnett, who helped assemble the Playground. “Some faculty have started mandating its use in their classes, and so we moved up access to students to allow for the start of fall quarter.”\nMost users at this time are curious how they can use AI in their existing workflow, studies, and research but are unsure where to begin, Barnett said. “A lot of people don’t know what they don’t know,” he said, adding that UIT plans to introduce more demos showcasing the platforms’ capabilities.\nFor example, a history professor and students used AI in one class to quickly analyze periodicals and handwritten journals. Staff have also leveraged AI to analyze spreadsheets or to draft emails. “It’s a good tool to help augment what we’re doing day to day and make us do our jobs better and more efficiently,” Barnett said.\nHowever, Barnett said it’s important for users to remain vigilant and remember that LLMs can “hallucinate” or introduce errors; he reminded users to always verify anything AI produces.\n“If you don’t like the output of an image or report, or the information that’s given to you is questionable, have a conversation with the language model and ask why it’s doing that and give it feedback,” Barnett said. “Ask it to cite sources. They do make stuff up sometimes, but they are a great start if you don’t know where to begin.”\nStanford’s AI Playground enables data privacy by taking advantage of Stanford’s infrastructure and vendor partnerships. Files uploaded to the Playground are not shared externally or used to train models.\nThe name “AI Playground” underscores its purpose as a space to safely experiment with and get comfortable using AI tools, said Ganesh Karkala, associate vice president for enterprise technology and chief technology officer. “We wanted people to play with their data set and ask a set of questions without worrying about how vendors will use this data,” Karkala said.\nThe university has agreements that ensure data remains in the university environment, and Stanford is also considering a data retention policy to automatically purge data after a set period.\nNevertheless, users should only share low- and moderate-risk data in the AI Playground, meaning they should not upload files with proprietary or private data. “A lot of day-to-day tasks can be covered under those low- or moderate-risk labels,” Barnett said. “Hopefully, in the near future we’ll be able to expand it all the way up to high-risk data.”\nIn the meantime, Stanford Medicine users should still use[Stanford Medicine’s Secure GPT tool.] \n## A balance\n[Russ Altman] is the chair of the AI at Stanford Advisory Committee, which has established principles for AI use at the university, including the necessity for human oversight.\nWhile there are legitimate concerns regarding AI, it’s important for Stanford to encourage people to learn about it and how it may improve people’s lives, said Altman, the Kenneth Fong Professor in the School of Engineering and professor of bioengineering, of genetics, of medicine–biomedical informatics research, and of biomedical data science, and a senior fellow at the Stanford Institute for Human-Centered Artificial Intelligence.\n“There’s a huge education required of faculty, students, and staff about what it can do,” Altman continued. “I’ve met faculty who have not tried ChatGPT or any LLM yet, and as a faculty member you need to be aware of how AI can be used. The Stanford way is to take new technologies and balance encouragement of experimentation with acknowledgement of risks.”\nUsers can join the #ai-playground Slack channel to pose questions and share their experiences using the AI Playground. In January, UIT is also launching[“AI Playground 101,”] an interactive UIT Tech Training class.\n### Related story\n[\n![] \n#### Report outlines Stanford principles for use of AI\nThe AI at Stanford Advisory Committee’s report includes recommendations for AI use in the university’s administration, education, and research.\n] \n### Related story\n[\n![one chair is not like the others] \n#### Can AI hold consistent values? Stanford researchers probe LLM consistency and bias\nNew research tests large language models for consistency across diverse topics, revealing that while they handle neutral topics reliably, controversial issues lead to varied answers.\n] \n### Related story\n[\n![] \n#### A new campus makerspace lets teachers tinker with AI\nThe AI Tinkery, which launched this fall at the Stanford Accelerator for Learning, helps educators explore the potential of artificial intelligence in the classroom.\n] \n### Related story\n[\n![teacher standing at whiteboard in front of classroom] \n#### AI+Education: How large language models could speed promising new classroom curricula\nStanford computer science scholars propose using language models to create new learning materials for K-12 students.\n] \n### Writer\nChelcey Adami\n### Campus unit\n[University IT] \n### Related topics\n[Institutional News] \n### Share this story\nCopy link\n## Subscribe to Stanford Report\nNews, insights and events delivered to your inbox each weekday morning.\nSign up\n![] \n## Research Matters\nGroundbreaking innovations that begin in Stanford labs flow freely into private industry to improve human well-being, fuel the economy, and strengthen American competitiveness.\n[Learn more about research at Stanford] \n## Stories for you\n## Popular stories\n1. [A day in the life of Cardinal hoops star Ebuka Okorie] \n2. [Two from Stanford receive 2026 Churchill Scholarships] \n3. [Stanford’s farm goes fully electric] \n4. [Howard Wolf to retire as president of the Stanford Alumni Association] \n5. [Flu season survival tips from a Stanford Medicine expert] \n## Read next\n[View allRead next] \n![Profile image of Howard Wolf.] \n## [Howard Wolf to retire as president of the Stanford Alumni Association] \n[Institutional News] \nNews\n![Mariano-Florentino Cuéllar stands in the Main Quad.] \n## [Mariano-Florentino Cuéllar to return to Stanford] \n[Awards, Honors &amp; Appointments] \nNews\n![Image of Chris Field at a Woods staff retreat.] \n## [Woods Institute Director Chris Field to step down Sept. 1] \n[Institutional News] \nNews\n![Tavita Pritchard on the field] \n## [Tavita Pritchard named Bradford M. Freeman Director of Football] \n[Institutional News] \nNews\n![Profile picture of Colin Kahl standing next to sandstone columns on the Stanford campus.] \n## [Colin Kahl named director of the Freeman Spogli Institute for International Studies] \n[Institutional News] \nNews\n![A symmetrical Spanish-style courtyard on a sunny day. A fountain in front is surrounded by flowers, and behind is the Old Union building.] \n## [Academic Integrity Working Group addresses generative AI and exam policies] \n[Institutional News] \nNews\nSlide 1Slide 2Slide 3Slide 4Slide 5Slide 6\nPreviousNext",
    "length": 10814,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Generative AI in Undergraduate Research : The Basics",
    "url": "https://guides.library.stanford.edu/c.php?g=1478643&p=11016317",
    "text": "The Basics - Generative AI in Undergraduate Research - Guides at Stanford University[Skip to Main Content] \n[Stanford University] \n* [My Account] \nSubject\\*\nIn a few words, what is your feedback about?\nMessage\\*\nYour name\\*\nYour email\\*\nLeave this field blank\nCancel\n[Stanford Libraries] \n# [Guides] \nWayfinders for our collections, tools, and services\nSearch this GuideSearch\n# Generative AI in Undergraduate Research: The Basics\nThings to consider before asking GenAI for help with your research project\n## What is Generative Artificial Intelligence?\nArtificial Intelligence (AI) is a broad term used to describe technology that attempts to simulate intelligent behavior. The more recent versions of AI use massive amounts of data, both of known and unknown provenance, to train so called &quot;models.&quot; Generative AI, also known as GenAI, uses these models to produce new content such as text, images, audio, and video.\nThis guide focuses on the use of Large Language Models (LLMs) in undergraduate writing and research, as LLMs are the most popular type of GenAI for producing text.LLMs essentially use probability to determine which words and phrases are the most likely to follow each other in a sentence, paragraph, or line of code. Since LLMs are designed for plausibility not reliability, AI-generated content needs to be carefully considered for accuracy and reliability. There is a consensus that AI-generated content must be cited (essentially treated like an author or creator), though specific citation practices are still evolving.\n## Can GenAI help me with my research?\nBefore using any GenAI tool in your coursework or research, check with your instructor to see what is permissible. Keep in mind that, absent a clear statement from an instructor, Stanford&#39;s general policy prohibits &quot;using generative AI tools to substantially complete an assignment or exam&quot; and instructs students to acknowledge the use of GenAI. (*See[Generative AI Policy Guidance], Stanford Office of Community Standards)*\nIf your instructor has allowed some use of GenAI, ask yourself:\n- Is my use of GenAI a shortcut that is inhibiting my learning? Or, is it turbocharging my learning?\n- How can I check to make sure the AI-generated content is accurate?\n- Is the information and data I am submitting to the GenAI platform free of personal or confidential content?\n- Do the benefits of GenAI outweigh the costs?\n## Possible Uses for GenAI\n- Brainstorming or refining your research topic\n- Organizing your reading or discussion notes\n- Highlighting or discovering themes or common threads within a set of texts or data sets\n- Soliciting feedback on writing\n- Hearing a counter argument to your position\n## What is GenAI not so good at?\n**Identifying references:**GenAI tools consistently hallucinate articles, books and even scholars, particularly if you are researching a topic that is less studied. We recommend starting with[Google Scholar] or our[library catalog] to get you started on finding relevant sources for your project. If you do use GenAI to discover a source, be sure to check it before citing it.\n******Accounting for Bias:**GenAI tools reflect the biases, worldviews, and inaccuracies that are present in their training data. As a result, they can replicate gender, racial, and cultural biases, all while presenting information in an authoritative and seemingly objective manner.\n**Writing:**Did you know that depending on AI to write an essay may[erode your critical thinking abilities]? Writing is how we process information, so when you outsource writing, you might outsource your thinking too!\n* * [**Next:**Tips for Prompts &gt;&gt;] \n* [The Basics] \n* [Tips for Prompts] \n* [Campus Resources] \n* [Ethical Considerations] \n* [Further Reading] \n## Need help?\n[\n![] \nPauline Lewis\n] \n[Email me] \n## Need help?\n[\n![] \nBogdana Marchis\n] \n[Email me] \n* Last Updated:Sep 10, 2025 3:42 PM\n* URL:https://guides.library.stanford.edu/c.php?g=1478643\n* [**Print Page] \n[Login to LibApps] \n[Report a problem] \n[****]",
    "length": 4035,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Generative AI in Undergraduate Research : Campus Resources",
    "url": "https://guides.library.stanford.edu/c.php?g=1478643&p=11019219",
    "text": "Campus Resources - Generative AI in Undergraduate Research - Guides at Stanford University[Skip to Main Content] \n[Stanford University] \n* [My Account] \nSubject\\*\nIn a few words, what is your feedback about?\nMessage\\*\nYour name\\*\nYour email\\*\nLeave this field blank\nCancel\n[Stanford Libraries] \n# [Guides] \nWayfinders for our collections, tools, and services\nSearch this GuideSearch\n# Generative AI in Undergraduate Research: Campus Resources\nThings to consider before asking GenAI for help with your research project\n## Campus Resources\n* [AI in Academic Research] \nLibrary guide offering researchers a practical framework for evaluating, applying, and citing Large Language Models (LLMs) in text-based research.\n* [AI Tools and Resources for Biomedical Research] \nLibrary guide from Lane Library (Stanford School of Medicine)\n* [Stanford AI Playground] \nThe Stanford AI Playground is a user-friendly platform, built on open-source technologies, that allows you to safely try various AI models from vendors like OpenAI, Google, and Anthropic in one spot. The AI Playground is being managed by University IT (UIT) as a pilot for most people at Stanford. This includes all active faculty, staff, students, postdocs, visiting scholars, and affiliates.\n* [Stanford UIT \"Responsible AI at Stanford\"] \nWith this guide, learn how to more confidently use AI tools and models while keeping Stanford's data safe.\n* [Stanford Office of Community Standards: Generative AI Policy Guidance] \nThe Stanford Board on Conduct Affairs lays out which uses of AI are and are not consistent with the Stanford Honor Code\n* [Stanford GenAI Prompt Guide] \nA guide to creating effective prompts for both text- and image-based GenAI tools\n* [Stanford's Human-Centered Artificial Intelligence] \nThe Stanford Institute for Human-Centered AI (HAI) is an interdisciplinary institute established in 2019 to advance AI research, education, policy, and practice. Stanford HAI brings together thought leaders from academia, industry, government, and civil society to shape the development and responsible deployment of AI.\n* [&lt;&lt;**Previous:**Tips for Prompts] \n* [**Next:**Ethical Considerations &gt;&gt;] \n* [The Basics] \n* [Tips for Prompts] \n* [Campus Resources] \n* [Ethical Considerations] \n* [Further Reading] \n* Last Updated:Sep 10, 2025 3:42 PM\n* URL:https://guides.library.stanford.edu/c.php?g=1478643\n* [**Print Page] \n[Login to LibApps] \n[Report a problem] \n[****]",
    "length": 2445,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Creating an AI Course Policy Workshop Kit",
    "url": "https://teachingcommons.stanford.edu/professional-development/workshops-programs/do-it-yourself-workshop-kits/creating-ai-course-policy",
    "text": "Creating an AI Course Policy Workshop Kit | Teaching Commons\n[Skip to main content] [Skip to secondary navigation] \n[Stanford University(link is external)] \n[\nStanford\nTeaching Commons\n] \nSearch this siteSubmit Search\n# Creating an AI Course Policy Workshop Kit\n## Main navigation\n[Skip Secondary Navigation] \nMain content start\nThis workshop kit will help your audience write a clear and comprehensive AI course policy statement for their students that can then be used within the course syllabus.\n## Workshop goal and objectives\nThis workshop can help participants think about if and how students might be allowed to use generative AI tools in their course and how to communicate that policy through the syllabus. The workshop provides example course policies, important pedagogic considerations, and sample language to help participants generate a draft AI course policy statement.\nAfter completing this workshop, participants should be able to:\n* Reflect on the purpose(s) of syllabi and course policies.\n* Compare different AI policy syllabus statements.\n* Determine an appropriate policy for their course.\n* Begin drafting an AI policy statement for their syllabus.## Audience\nThis workshop is intended for instructors and staff writing an AI course policy statement for their syllabi.\n## Time\nThis workshop is approximately 60 minutes in length.\n## Key strategies\nThis workshop leverages active learning strategies and is focused on the practical task of writing a course policy statement. The workshop uses a worksheet activity to structure the task of writing a draft policy statement.\nConsider the following strategies and perspectives when developing this workshop.\n* Acknowledge that every course is different and that instructors have the authority and responsibility to decide their course policies.\n* Encourage participants to align their policy to the needs of their students, whether that prohibits, supports, or limits AI use.\n* Provide a variety of example syllabi statements and language for participants to consider.## Reading list\nConsider these resources as you prepare to lead this workshop.\n**Artificial Intelligence Teaching Guide:**Web-based resources from the Stanford Teaching Commons\n* [Creating your course policy on AI] \n**Relevant Stanford campus policies**\n* [Generative AI Policy Guidance], Stanford Office of Community Standards\n* [Guidance on the Standard Sanction], Stanford Office of Community Standards\n**General resources about syllabi**\n* [CTL Syllabus Template], Stanford Center for Teaching and Learning\n* [Should You Add an AI Policy to Your Syllabus?], Chronicle of Higher Education\n**Example AI course policy statements**\n* [Classroom Policies for AI Generative Tools], created by Lance Eaton## Slide presentation with speaker notes\nHere you can find slide presentations for this workshop in different formats. The PDF version of the slides is useful for skimming and previewing the workshop. View the slide notes for additional context and sample scripts. We then recommend downloading the full PowerPoint presentation to adapt and customize. The full PowerPoint presentation includes speaker notes, sample scripts, and additional optional slides from other AI workshop kits.\nComplete this form to receive an email with links to the files. This form helps us track the kits' impact and advocate for more resources to support teaching and learning.\n## Workshop agenda\nThis suggested agenda proposes a workshop that balances content presentations with learning activities. This agenda is a starting point as you adapt it to fit your audience.\n* Introduction (10 minutes)\n* Lecture and slide presentation\n* Slides 1 to 6\n* Warm-up activity (5 minutes)\n* A small group discussion on what is currently included within our syllabi.\n* Slide 7\n* Analyzing example AI policy statements (10 minutes)\n* Lecture and slide presentation\n* Slides 8 to 13\n* Considerations for deciding your policy (15 minutes)\n* Lecture and slide presentation\n* Slides 14 to 21\n* Worksheet activity (15 minutes)\n* An activity where participants generate a draft of their policy statement using a worksheet\n* Slides 22 to 23\n* [Worksheet for creating your AI course policy] (129.75 KB)\n* Wrap-up (5 minutes)\n* Participants complete a short summative assessment and evaluation survey.\n* Slides 24 to 25## Promotional resources\nYou can use the following sample language to promote your workshop. This language was created with assistance from ChatGPT. Consider using AI tools yourself to generate other promotional material, such as promotional images, to get more practice using generative AI tools.\n## Crafting Your AI Course Policy: Professional Development Workshop\nExample 1\nJoin our 60-minute workshop designed to help university staff and instructors create clear AI course policy statements for their syllabi. You'll explore the purpose of course policies, compare AI policy examples, and draft your own policy statement, all through active learning and practical activities.\n**Workshop Highlights:**\n* **Reflect**on the purpose of syllabi and course policies.\n* **Compare**different AI policy syllabus statements.\n* **Determine**an appropriate AI policy for your course.\n* **Draft**an AI policy statement using a guided worksheet.\n## Master Your AI Course Policy: Exclusive Workshop for Instructors\nExample 2\nUnlock the secrets to crafting a standout AI course policy in our dynamic 60-minute workshop! Tailored for university staff and instructors, you'll explore essential policy elements, compare real-world examples, and draft your own statement with hands-on guidance. Elevate your syllabus and stay ahead in the AI-driven academic landscape!\n## Assessment and evaluation survey\nThis survey serves multiple functions. It gathers demographic information about the workshop participants, evaluates the workshop's effectiveness, prompts metacognitive reflection, and gathers feedback on improving the workshop. The survey was created in Google Forms. Make a copy of the survey to adapt and keep in your Google Drive.\n[Preview the example survey as a responder.] \n[Create a copy of the example survey (Stanford only)]. Log in to your Stanford Google account, then use the link to make a copy in your Google Drive to edit and distribute.\n## Feedback on this workshop kit\nPlease respond to the poll below to share your feedback on this workshop kit. Your feedback will help us improve this workshop kit and develop resources that support your teaching. Your anonymous responses will be seen only by the Teaching Commons team. Please respond to the prompt, \"What feedback do you have for the creators of the Creating an AI Course Policy Workshop Kit?\"\n## Author and attribution\nKenji Ikemoto, Associate Director for Academic Technology at the Stanford Center for Teaching and Learning, created this workshop kit in June 2024. This workshop kit is licensed under[Creative Commons BY-NC-SA 4.0] (attribution, non-commercial, share-alike). You may adapt, remix, or enhance these modules for non-commercial use. Please attribute it to the**Stanford Center for Teaching and Learning**. We ask that you also share your work under the same licensing terms.\n## Additional resources on AI and education\nIn 2025, the Center for Teaching and Learning launched the[AI Meets Education at Stanford (AIMES)] initiative. AIMES resources and programs include the[AIMES Library of Examples,] containing exemplary learning objects from Stanford courses, and the[Critical AI Literacy for Instructors] self-paced Canvas course. ## Contact\nIf you have questions about this workshop kit contact us at[TeachingCommons@stanford.edu].\nStanford community members are invited to[request a consultation] with experts at the Center for Teaching and Learning (CTL).\nBack to Top",
    "length": 7766,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "AI Policy Working Group | Stanford HAI",
    "url": "https://hai.stanford.edu/policy/student-opportunities/ai-policy-working-group",
    "text": "AI Policy Working Group | Stanford HAI\n# AI Policy Working Group\n### Stay Up To Date\nGet the latest news, advances in research, policy work, and education program updates from HAI in your inbox weekly.\n[\n**Sign Up**For Latest News\n] \n[] \n* [] \n* [] \n* [] \n* [] \n* [] \n* [] \n###### Navigate\n* [\nAbout\n] \n* [\nEvents\n] \n* [\nCareers\n] \n* [\nSearch\n] \n###### Participate\n* [\nGet Involved\n] \n* [\nSupport HAI\n] \n* [\nContact Us\n] \n[Skip to content] \n[] \n[] \n* [] \n* [] \n* [] \n* [] \n* [] \n* [] \n[] \n## Overview\nStanford HAI invites applications to join the AI Policy Working Group, a unique opportunity for**Stanford graduate students and researchers**to bridge the gap between cutting-edge AI research and pressing public policy challenges.\nMembers will work closely with HAI faculty leaders and scholars, including[Daniel E. Ho],[Sanmi Koyejo], and[Rishi Bommasani], on projects at the intersection of AI, governance, and societal impact. The Working Group provides hands-on experience in policy-relevant research and aims to train the next generation of scholars who can inform AI policymaking in the U.S. and globally.\n**Program Duration:**October 1, 2025 - June 30, 2026\n[\nApply by October 6\n] \n[See Program Requirements] \nHAI Faculty Leaders and Scholars\n![Dan Ho headshot] \n[Daniel E. Ho] \nWilliam Benjamin Scott and Luna M. Scott Professor of Law | Professor of Political Science | Professor of Computer Science (by courtesy) | Senior Fellow, Stanford HAI | Senior Fellow, Stanford Institute for Economic and Policy Research | Director of the Regulation, Evaluation, and Governance Lab (RegLab)\n![] \n[Sanmi Koyejo] \nAssistant Professor of Computer Science, Stanford University; Faculty Affiliate, Stanford HAI\n![] \n[Rishi Bommasani] \nSenior Research Scholar, Stanford HAI\n![headshot] \n[Elena Cryst] \nDirector of Policy and Society, Stanford HAI\n##### ##### ##### ##### ## Program Requirements\n**Time Commitment**: \\~10 hours per week\n**Project Collaboration**: Members will contribute to a research project that will aim to have both scientific impact through a high-quality paper and policy impact. Here are some potential projects and we welcome new ideas in your applications:\n* Improving how governments evaluate AI tools\n* Measuring how AI capabilities relate to AI economic impacts\n* Building tools to and frameworks to price data used to train AI\n**Bi-Weekly Policy Meetings**: A research meeting for members to discuss active projects and understand recent policy developments.\n## Past Work\nWhile our group’s common focus is on rigorous research that positively impacts AI policy, we produce a variety of deliverables. Some recent examples include:\n* Technical papers like the[Foundation Model Transparency Index] \n* General-audience papers like[Considerations for Governing Open Foundation Models] and[Advancing Science- and Evidence-based AI Policy] \n* Policy briefs like[Adverse Event Reporting for AI] and[Expanding Academia’s Role in Public Sector AI] \n## Benefits\n* Experience contributing to impactful policy work at the frontier of AI.\n* A vibrant network of peers across disciplines interested in AI and public policy.\n* Mentorship from faculty and experts to build knowledge in tech policy\n* A moderate research stipend\n## Eligibility\n* Currently enrolled in a masters, PhD, or postdoctoral program at Stanford (all schools and programs eligible)\n* No prior policy experience is required; we encourage applicants from all disciplines.\n## Application\n[\nStart your application\n] \n\\* Applications should be submitted by October 6th, 2025",
    "length": 3550,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "AI Playground offers a safe place to explore and experiment",
    "url": "https://uit.stanford.edu/news/stanford%E2%80%99s-ai-playground-offers-safe-place-explore-and-experiment",
    "text": "[Skip to content] \n\n## Stanford Report cookie usage information\n\nWe want to provide stories, announcements, events, leadership messages and resources that are relevant to you. Your selection is stored in a browser cookie which you can remove at any time by visiting the \"Show me...\" menu at the top right of the page. For more, read our [cookie policy].\n\nAcceptDecline\n\nJanuary 9th, 2025\\| 1 min read [Institutional News] \n\n# Stanford’s AI Playground offers a safe place to explore and experiment\n\nUniversity IT began piloting the Stanford AI Playground earlier this year, providing the Stanford community a secure platform to explore various AI tools.\n\nThis image was generated using the DALL-E-3 plugin in the Stanford AI Playground. \\| Chelcey Adami\n\nAs artificial intelligence continues to rapidly evolve, Stanford’s [AI Playground] provides a convenient environment for faculty, staff, and students to experiment with AI technology.\n\nEarlier this year, University IT began piloting the AI Playground, a Stanford-hosted environment that allows users to test out and compare a range of AI tools, including ChatGPT, Google Gemini, and Wolfram. As of Oct. 1, the AI Playground is accessible to all Stanford faculty, staff, students, postdocs, and visiting scholars.\n\nThe project to create the AI Playground stemmed from community feedback and the need for a centralized, accessible platform where community members could safely try AI tools, explained Chief Information Officer Steve Gallagher.\n\n“Our goal was to put the tools into people’s hands in a more accessible way,” Gallagher said. “One of UIT’s strategic values is to innovate with purpose, and in the case of the Playground, we want to help enable the innovation happening across campus.”\n\nThe AI Playground is designed to be both an entry point into AI for users who are unsure where to start and a place of experimentation for those more advanced in AI usage. “The vision is to increase the collective literacy around artificial intelligence and these tools across campus,” Gallagher said.\n\nThe AI Playground is part of Stanford’s broader effort to address the growing role of AI in research, education, and administration. Separately, the AI at Stanford Advisory Committee, appointed by the provost to assess potential policy gaps for AI use at the university, recently [released its initial findings].\n\n## Time to play\n\nThe AI Playground includes various large language models (LLMs), a form of generative AI specializing in text-based content. Users can access additional LLM plugins for image generation, web scraping, and AI-assisted Google services.\n\nThis versatility enables users to compare how different AI models handle the same task or use different models simultaneously to complete tasks such as document analysis, creating graphs and websites, making recommendations, or skill assessments.\n\nUIT provides resources on [how to use AI responsibly], [how to use the AI Playground], and [how to use generative AI tools], which are built using algorithms that can generate text, images, video, audio, and 3D models in response to user prompts.\n\nJoshua Barnett, IT director and enterprise infrastructure architect, emphasized the importance of expanding AI access to more students.\n\n“We needed to level the playing field and make it free to use,” said Barnett, who helped assemble the Playground. “Some faculty have started mandating its use in their classes, and so we moved up access to students to allow for the start of fall quarter.”\n\nMost users at this time are curious how they can use AI in their existing workflow, studies, and research but are unsure where to begin, Barnett said. “A lot of people don’t know what they don’t know,” he said, adding that UIT plans to introduce more demos showcasing the platforms’ capabilities.\n\nFor example, a history professor and students used AI in one class to quickly analyze periodicals and handwritten journals. Staff have also leveraged AI to analyze spreadsheets or to draft emails. “It’s a good tool to help augment what we’re doing day to day and make us do our jobs better and more efficiently,” Barnett said.\n\nHowever, Barnett said it’s important for users to remain vigilant and remember that LLMs can “hallucinate” or introduce errors; he reminded users to always verify anything AI produces.\n\n“If you don’t like the output of an image or report, or the information that’s given to you is questionable, have a conversation with the language model and ask why it’s doing that and give it feedback,” Barnett said. “Ask it to cite sources. They do make stuff up sometimes, but they are a great start if you don’t know where to begin.”\n\nStanford’s AI Playground enables data privacy by taking advantage of Stanford’s infrastructure and vendor partnerships. Files uploaded to the Playground are not shared externally or used to train models.\n\nThe name “AI Playground” underscores its purpose as a space to safely experiment with and get comfortable using AI tools, said Ganesh Karkala, associate vice president for enterprise technology and chief technology officer. “We wanted people to play with their data set and ask a set of questions without worrying about how vendors will use this data,” Karkala said.\n\nThe university has agreements that ensure data remains in the university environment, and Stanford is also considering a data retention policy to automatically purge data after a set period.\n\nNevertheless, users should only share low- and moderate-risk data in the AI Playground, meaning they should not upload files with proprietary or private data. “A lot of day-to-day tasks can be covered under those low- or moderate-risk labels,” Barnett said. “Hopefully, in the near future we’ll be able to expand it all the way up to high-risk data.”\n\nIn the meantime, Stanford Medicine users should still use [Stanford Medicine’s Secure GPT tool.] \n\n## A balance\n\n[Russ Altman] is the chair of the AI at Stanford Advisory Committee, which has established principles for AI use at the university, including the necessity for human oversight.\n\nWhile there are legitimate concerns regarding AI, it’s important for Stanford to encourage people to learn about it and how it may improve people’s lives, said Altman, the Kenneth Fong Professor in the School of Engineering and professor of bioengineering, of genetics, of medicine–biomedical informatics research, and of biomedical data science, and a senior fellow at the Stanford Institute for Human-Centered Artificial Intelligence.\n\n“There’s a huge education required of faculty, students, and staff about what it can do,” Altman continued. “I’ve met faculty who have not tried ChatGPT or any LLM yet, and as a faculty member you need to be aware of how AI can be used. The Stanford way is to take new technologies and balance encouragement of experimentation with acknowledgement of risks.”\n\nUsers can join the #ai-playground Slack channel to pose questions and share their experiences using the AI Playground. In January, UIT is also launching [“AI Playground 101,”] an interactive UIT Tech Training class.\n\n### Related story\n\n[**Report outlines Stanford principles for use of AI** \\\n\\\nThe AI at Stanford Advisory Committee’s report includes recommendations for AI use in the university’s administration, education, and research.] \n\n### Related story\n\n[**Can AI hold consistent values? Stanford researchers probe LLM consistency and bias** \\\n\\\nNew research tests large language models for consistency across diverse topics, revealing that while they handle neutral topics reliably, controversial issues lead to varied answers.] \n\n### Related story\n\n[**A new campus makerspace lets teachers tinker with AI** \\\n\\\nThe AI Tinkery, which launched this fall at the Stanford Accelerator for Learning, helps educators explore the potential of artificial intelligence in the classroom.] \n\n### Related story\n\n[**AI+Education: How large language models could speed promising new classroom curricula** \\\n\\\nStanford computer science scholars propose using language models to create new learning materials for K-12 students.] \n\n### Writer\n\nChelcey Adami\n\n### Campus unit\n\n[University IT] \n\n### Related topics\n\n[Institutional News] \n\n### Share this story\n\nCopy link\n\n## Read next\n\n[View allRead next] \n\n## [Stanford Institute for Theoretical Physics named for Leinweber Foundation gift] \n\n[Institutional News] \n\nNews\n\n## [How Stanford is simplifying decisions and processes] \n\n[Institutional News] \n\nNews\n\n## [Stanford streamlines the process of faculty appointments and promotions] \n\n[Institutional News] \n\nNews\n\n## [Three simplification projects target research support] \n\n[Institutional News] \n\nNews\n\n## [Universities remain key to U.S. discovery and innovation despite challenges] \n\n[Institutional News] \n\nNews\n\n## [Stanford names SEQ for donor gift] \n\n[Institutional News] \n\nNews\n\nPreviousNext",
    "length": 8859,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Response to NTIA’s Request for Comment on Dual Use Open Foundation Models | Stanford HAI",
    "url": "https://hai.stanford.edu/policy/response-to-ntias-request-for-comment-on-dual-use-open-foundation-models",
    "text": "Response to NTIA’s Request for Comment on Dual Use Open Foundation Models | Stanford HAI\n### Stay Up To Date\nGet the latest news, advances in research, policy work, and education program updates from HAI in your inbox weekly.\n[\n**Sign Up**For Latest News\n] \n[] \n* [] \n* [] \n* [] \n* [] \n* [] \n* [] \n###### Navigate\n* [\nAbout\n] \n* [\nEvents\n] \n* [\nCareers\n] \n* [\nSearch\n] \n###### Participate\n* [\nGet Involved\n] \n* [\nSupport HAI\n] \n* [\nContact Us\n] \n[Skip to content] \n[] \n[] \n* [] \n* [] \n* [] \n* [] \n* [] \n* [] \n[] \n[\npolicyResponse to Request\n] # Response to NTIA’s Request for Comment on Dual Use Open Foundation Models\nDate\nMarch 27, 2024\nTopics\n[\nFoundation Models\n] [\nRegulation, Policy, Governance\n] [\nPrivacy, Safety, Security\n] \n![] \n[**Read Paper**] \nabstract\nStanford scholars respond to a federal RFC on dual use foundation models with widely available model weights, urging policymakers to consider their marginal risks.\n### Executive Summary\nIn this response to the National Telecommunications and Information Administration’s (NTIA) request for comment on dual use foundation AI models with widely available model weights, scholars from Stanford HAI, the Center for Research on Foundation Models (CRFM), the Regulation, Evaluation, and Governance Lab (RegLab), and other institutions urge policymakers to amplify the benefits of open foundation models while further assessing the extent of their marginal risks.\n[**Read Paper**] \nShare\n[] [] [] \nLink copied to clipboard!\nAuthors\n* ###### Researchers from Stanford HAI\n### Related Publications\n##### [Response to OSTP&#x27;s Request for Information on Accelerating the American Scientific Enterprise] \n[Rishi Bommasani,] [John Etchemendy,] [Surya Ganguli,] [Daniel E. Ho,] [Guido Imbens,] [James Landay,] [Fei-Fei Li,] [Russell Wald] \nQuick ReadDec 26, 2025\nResponse to Request\n![] \nStanford scholars respond to a federal RFI on scientific discovery, calling for the government to support a new “team science” academic research model for AI-enabled discovery.\nResponse to Request\n![] \n#### [Response to OSTP&#x27;s Request for Information on Accelerating the American Scientific Enterprise] \n[Rishi Bommasani,] [John Etchemendy,] [Surya Ganguli,] [Daniel E. Ho,] [Guido Imbens,] [James Landay,] [Fei-Fei Li,] [Russell Wald] \n[Sciences (Social, Health, Biological, Physical)] [Regulation, Policy, Governance] Quick ReadDec 26\nStanford scholars respond to a federal RFI on scientific discovery, calling for the government to support a new “team science” academic research model for AI-enabled discovery.\n##### [Beyond DeepSeek: China&#x27;s Diverse Open-Weight AI Ecosystem and Its Policy Implications] \n[Caroline Meinhardt,] [Sabina Nong,] [Graham Webster,] [Tatsunori Hashimoto,] [Christopher Manning] \nDeep DiveDec 16, 2025\nIssue Brief\n![] \nAlmost one year after the “DeepSeek moment,” this brief analyzes China’s diverse open-model ecosystem and examines the policy implications of their widespread global diffusion.\nIssue Brief\n![] \n#### [Beyond DeepSeek: China&#x27;s Diverse Open-Weight AI Ecosystem and Its Policy Implications] \n[Caroline Meinhardt,] [Sabina Nong,] [Graham Webster,] [Tatsunori Hashimoto,] [Christopher Manning] \n[Foundation Models] [International Affairs, International Security, International Development] Deep DiveDec 16\nAlmost one year after the “DeepSeek moment,” this brief analyzes China’s diverse open-model ecosystem and examines the policy implications of their widespread global diffusion.\n##### [Response to FDA&#x27;s Request for Comment on AI-Enabled Medical Devices] \n[Desmond C. Ong,] [Jared Moore,] [Nicole Martinez-Martin,] [Caroline Meinhardt,] [Eric Lin,] [William Agnew] \nQuick ReadDec 02, 2025\nResponse to Request\n![] \nStanford scholars respond to a federal RFC on evaluating AI-enabled medical devices, recommending policy interventions to help mitigate the harms of AI-powered chatbots used as therapists.\nResponse to Request\n![] \n#### [Response to FDA&#x27;s Request for Comment on AI-Enabled Medical Devices] \n[Desmond C. Ong,] [Jared Moore,] [Nicole Martinez-Martin,] [Caroline Meinhardt,] [Eric Lin,] [William Agnew] \n[Healthcare] [Regulation, Policy, Governance] Quick ReadDec 02\nStanford scholars respond to a federal RFC on evaluating AI-enabled medical devices, recommending policy interventions to help mitigate the harms of AI-powered chatbots used as therapists.\n##### [Jen King&#x27;s Testimony Before the U.S. House Committee on Energy and Commerce Oversight and Investigations Subcommittee] \n[Jennifer King] \nQuick ReadNov 18, 2025\nTestimony\n![] \nIn this testimony presented to the U.S. House Committee on Energy and Commerce’s Subcommittee on Oversights and Investigations hearing titled “Innovation with Integrity: Examining the Risks and Benefits of AI Chatbots,” Jen King shares insights on data privacy concerns connected with the use of chatbots. She highlights opportunities for congressional action to protect chatbot users from related harms.\nTestimony\n![] \n#### [Jen King&#x27;s Testimony Before the U.S. House Committee on Energy and Commerce Oversight and Investigations Subcommittee] \n[Jennifer King] \n[Privacy, Safety, Security] Quick ReadNov 18\nIn this testimony presented to the U.S. House Committee on Energy and Commerce’s Subcommittee on Oversights and Investigations hearing titled “Innovation with Integrity: Examining the Risks and Benefits of AI Chatbots,” Jen King shares insights on data privacy concerns connected with the use of chatbots. She highlights opportunities for congressional action to protect chatbot users from related harms.",
    "length": 5586,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "The Responsible Development of Automated Student Feedback with Generative AI",
    "url": "https://scale.stanford.edu/genai/repository/responsible-development-automated-student-feedback-generative-ai",
    "text": "The Responsible Development of Automated Student Feedback with Generative AI | SCALE Initiative\n[Skip to main content] \n[![Stanford University]] \n## Search and Filter\nWhat is the application?\nTeaching –Instructional Materials[**] \nTeaching –Assessment and Feedback[**] \nTeaching –Professional Learning[**] \nLearning –Student Support[**] \nCommunicating / Social Tools[**] \nOrganizing[**] \nAnalyzing[**] \nWho is the user?\nStudent[**] \nParent/Caregiver[**] \nEducator[**] \nSchool Leader[**] \nOthers[**] \nWhich age?\n0-3 years\nElementary (PK5)\nMiddle School (6-8)\nHigh School (9-12)\nPost-Secondary\nAdult\nWhy use AI?\nEfficiency[**] \nOutcomes –Literacy[**] \nOutcomes –Numeracy[**] \nOutcomes –Other Academic[**] \nOutcomes –Differentiation[**] \nOutcomes –Social Emotional[**] \nOutcomes –Durable Skills[**] \nReimagined Schooling[**] \nOther[**] \nStudy design\nDescriptive –Implementation and Use[**] \nDescriptive –Product Development[**] \nImpact –Randomized Controlled Trial[**] \nImpact –Quasi-experimental[**] \nSystematic Review[**] \n## Submit a research study\nContribute to the repository:\n[Add a paper] \n# The Responsible Development of Automated Student Feedback with Generative AI\nAuthors\nEuan D Lindsay,\nMike Zhang,\nAditya Johri,\nand Johannes Bjerva\nDate\n02/2025\nPublisher\narXiv\nLink\n[http://arxiv.org/pdf/2308.15334v3] \nProviding rich, constructive feedback to students is essential for supporting\nand enhancing their learning. Recent advancements in Generative Artificial\nIntelligence (AI), particularly with large language models (LLMs), present new\nopportunities to deliver scalable, repeatable, and instant feedback,\neffectively making abundant a resource that has historically been scarce and\ncostly. From a technical perspective, this approach is now feasible due to\nbreakthroughs in AI and Natural Language Processing (NLP). While the potential\neducational benefits are compelling, implementing these technologies also\nintroduces a host of ethical considerations that must be thoughtfully\naddressed. One of the core advantages of AI systems is their ability to\nautomate routine and mundane tasks, potentially freeing up human educators for\nmore nuanced work. However, the ease of automation risks a ``tyranny of the\nmajority'', where the diverse needs of minority or unique learners are\noverlooked, as they may be harder to systematize and less straightforward to\naccommodate. Ensuring inclusivity and equity in AI-generated feedback,\ntherefore, becomes a critical aspect of responsible AI implementation in\neducation. The process of developing machine learning models that produce\nvaluable, personalized, and authentic feedback also requires significant input\nfrom human domain experts. Decisions around whose expertise is incorporated,\nhow it is captured, and when it is applied have profound implications for the\nrelevance and quality of the resulting feedback. Additionally, the maintenance\nand continuous refinement of these models are necessary to adapt feedback to\nevolving contextual, theoretical, and student-related factors. Without ongoing\nadaptation, feedback risks becoming obsolete or mismatched with the current\nneeds of diverse student populations [...]\nWhat is the application?\n[Teaching –Assessment and Feedback] \nWho is the user?\n[Educator] \nWho age?\n[Post-Secondary] \nWhy use AI?\n[Efficiency],\n[Outcomes –Differentiation] \nStudy design\n[Descriptive –Implementation and Use],\n[Descriptive –Product Development] \n[![Stanford University]] \n* [Stanford Home] \n* [Maps &amp; Directions] \n* [Search Stanford] \n* [Emergency Info] \n* [Privacy] \n* [Copyright] \n* [Trademarks] \n* [Non-Discrimination] \n* [Accessibility] \n©Stanford University. Stanford, California 94305.",
    "length": 3681,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Guides: Generative AI in Undergraduate Research: The Basics",
    "url": "https://guides.library.stanford.edu/c.php?g=1478643",
    "text": "The Basics - Generative AI in Undergraduate Research - Guides at Stanford University[Skip to Main Content] \n[Stanford University] \n* [My Account] \nSubject\\*\nIn a few words, what is your feedback about?\nMessage\\*\nYour name\\*\nYour email\\*\nLeave this field blank\nCancel\n[Stanford Libraries] \n# [Guides] \nWayfinders for our collections, tools, and services\nSearch this GuideSearch\n# Generative AI in Undergraduate Research: The Basics\nThings to consider before asking GenAI for help with your research project\n## What is Generative Artificial Intelligence?\nArtificial Intelligence (AI) is a broad term used to describe technology that attempts to simulate intelligent behavior. The more recent versions of AI use massive amounts of data, both of known and unknown provenance, to train so called &quot;models.&quot; Generative AI, also known as GenAI, uses these models to produce new content such as text, images, audio, and video.\nThis guide focuses on the use of Large Language Models (LLMs) in undergraduate writing and research, as LLMs are the most popular type of GenAI for producing text.LLMs essentially use probability to determine which words and phrases are the most likely to follow each other in a sentence, paragraph, or line of code. Since LLMs are designed for plausibility not reliability, AI-generated content needs to be carefully considered for accuracy and reliability. There is a consensus that AI-generated content must be cited (essentially treated like an author or creator), though specific citation practices are still evolving.\n## Can GenAI help me with my research?\nBefore using any GenAI tool in your coursework or research, check with your instructor to see what is permissible. Keep in mind that, absent a clear statement from an instructor, Stanford&#39;s general policy prohibits &quot;using generative AI tools to substantially complete an assignment or exam&quot; and instructs students to acknowledge the use of GenAI. (*See[Generative AI Policy Guidance], Stanford Office of Community Standards)*\nIf your instructor has allowed some use of GenAI, ask yourself:\n- Is my use of GenAI a shortcut that is inhibiting my learning? Or, is it turbocharging my learning?\n- How can I check to make sure the AI-generated content is accurate?\n- Is the information and data I am submitting to the GenAI platform free of personal or confidential content?\n- Do the benefits of GenAI outweigh the costs?\n## Possible Uses for GenAI\n- Brainstorming or refining your research topic\n- Organizing your reading or discussion notes\n- Highlighting or discovering themes or common threads within a set of texts or data sets\n- Soliciting feedback on writing\n- Hearing a counter argument to your position\n## What is GenAI not so good at?\n**Identifying references:**GenAI tools consistently hallucinate articles, books and even scholars, particularly if you are researching a topic that is less studied. We recommend starting with[Google Scholar] or our[library catalog] to get you started on finding relevant sources for your project. If you do use GenAI to discover a source, be sure to check it before citing it.\n******Accounting for Bias:**GenAI tools reflect the biases, worldviews, and inaccuracies that are present in their training data. As a result, they can replicate gender, racial, and cultural biases, all while presenting information in an authoritative and seemingly objective manner.\n**Writing:**Did you know that depending on AI to write an essay may[erode your critical thinking abilities]? Writing is how we process information, so when you outsource writing, you might outsource your thinking too!\n* * [**Next:**Tips for Prompts &gt;&gt;] \n* [The Basics] \n* [Tips for Prompts] \n* [Campus Resources] \n* [Ethical Considerations] \n* [Further Reading] \n## Need help?\n[\n![] \nPauline Lewis\n] \n[Email me] \n## Need help?\n[\n![] \nBogdana Marchis\n] \n[Email me] \n* Last Updated:Sep 10, 2025 3:42 PM\n* URL:https://guides.library.stanford.edu/c.php?g=1478643\n* [**Print Page] \n[Login to LibApps] \n[Report a problem] \n[****]",
    "length": 4035,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Student Perspective: Designing a National AI Research Resource | Stanford HAI",
    "url": "https://hai.stanford.edu/news/student-perspective-designing-national-ai-research-resource",
    "text": "Student Perspective: Designing a National AI Research Resource | Stanford HAI\n### Stay Up To Date\nGet the latest news, advances in research, policy work, and education program updates from HAI in your inbox weekly.\n[\n**Sign Up**For Latest News\n] \n[] \n* [] \n* [] \n* [] \n* [] \n* [] \n* [] \n###### Navigate\n* [\nAbout\n] \n* [\nEvents\n] \n* [\nCareers\n] \n* [\nSearch\n] \n###### Participate\n* [\nGet Involved\n] \n* [\nSupport HAI\n] \n* [\nContact Us\n] \n[Skip to content] \n[] \n[] \n* [] \n* [] \n* [] \n* [] \n* [] \n* [] \n[] \n[\nnews\n] # Student Perspective: Designing a National AI Research Resource\nDate\nOctober 06, 2021\n![] \nJD/MBA student Chris Wan helped lead a Stanford Law School/HAI practicum that paired his passions in public policy and technology access.\nChris Wan joined Facebook’s news credibility team as a software engineer right after the 2016 election, when the social media giant was inundated with fake news and election interference. By 2018, he was concerned that government policymakers were becoming unable to keep up with the rapid pace of technology innovation.\n“The breaking point for me was when Mark Zuckerberg testified in Congress,” Wan says. It was clear to him that these senators, while intelligent people, didn’t understand technology and their questions weren’t getting answered. “It was frustrating. My sense was that technology was moving too fast for the government to respond.”\nThat push-pull tension between business and law inspired him to pursue a JD/MBA and when he joined Stanford, Wan looked for opportunities to understand policymakers’ role in shaping the future of technology. That led him to a chance to work on an exciting new project with Stanford Law professor[Daniel E. Ho], associate director of the[Stanford Institute for Human-Centered AI].\n“Dan told me that there was this interesting project that HAI was working on around the national research cloud,” Wan says. “I jumped at the opportunity.”\nIn early 2020, Stanford HAI and universities around the country asked the federal government to create a National AI Research Resource (NAIRR), a proposal that called for a partnership between academia, government, and industry to provide researchers with affordable access to the massive and expensive compute power and data that fuels today’s advanced AI research. Those resources are currently available to only a handful of the country’s wealthiest corporations and universities.\n“The result is that the ingenuity found at many public and private research institutions —which is the engine of America’s progress —is being locked out of the future of AI,” Wan says.\nIn October of 2020, he began working with Ho to scope out the various problems an NRC might face. When, two months later, Congress passed the National AI Research Resource Task Force Act of 2020 to study how to build an NAIRR, plans were in the works at HAI for a two-quarter practicum. Titled*Creating a National Research Cloud*, the course based at[Stanford Law School] brought together 14 student researchers from law, engineering, computer science, economics, and business, all charged with figuring out how best to design and operate an NAIRR. Findings would be compiled into a report to inform both federal task force members and the public.\nWan initially set to work on the practicum’s course planning; he organized interviews, scheduled guest speakers and reading material, and met regularly with Ho and other HAI faculty. During the practicum’s first quarter, he worked as a liaison between research teams and led his own team researching intellectual property issues. Later, during the practicum’s spring quarter, he began the process of transforming the teams’ findings into a draft report. Just this week,[that report was released] to the new federal task force.\n“There was a lot of blood, sweat, and tears involved, but this entire project was both rewarding and fascinating,” Wan says. “I interviewed various major players in the media and entertainment industries regarding their thoughts and concerns on an NRC. We also brought in top-notch speakers, including Eric Horvitz, the chief scientific officer at Microsoft, and Amy O’Hara, who founded the unit at the U.S. Census Bureau that pioneered making government data more widely available for research. We spoke with policymakers and former policymakers and partners at law firms —people all across the gamut that are expert in their fields. No student normally has these sorts of opportunities to speak with people like this, but I’m thankful that HAI and the NRC project unlocked those opportunities for me and the other students in the practicum.”\nWan expects HAI’s 100-page report to be the most comprehensive received by the federal task force, addressing topics including what a research cloud is, who should get access, and how to preserve privacy, as well as issues around researcher ethics, cybersecurity, and intellectual property.\n“We hope our efforts are recognized and eventually put into practice by the task force and also that we here at HAI can continue to drive this conversation forward,” he says.\nWan’s work also solidified his interest in equalizing the playing field. “There’s a quote that captures the essence of the NRC: ‘The future is already here; it’s just not evenly distributed.’ Working on the NRC practicum really hammered that home for me. Democratizing foundational technologies and innovation to all researchers across the country is something that’s really important for our nation and for innovation in general, and it’ll unlock benefits we can’t even imagine today.”\n*Read “[Building a National AI Research Resource: A Blueprint for the National Research Cloud] ”*\n*Read[an interview with the report’s authors] on key insights and takeaways from the new report.*\nShare\n[] [] [] \nLink copied to clipboard!\nContributor(s)\n###### Beth Jensen\n### Related News\n##### [A New Economic World Order May Be Based on Sovereign AI and Midsized Nation Alliances] \n[Alex Pentland] \nFeb 06, 2026\nNews\n![close-up of a globe with pinpoints of lights coming out of all the countries] \nAs trust in the old order erodes, mid-sized countries are building new agreements involving shared digital infrastructure and localized AI.\nNews\n![close-up of a globe with pinpoints of lights coming out of all the countries] \n#### [A New Economic World Order May Be Based on Sovereign AI and Midsized Nation Alliances] \n[Alex Pentland] \nFeb 06\nAs trust in the old order erodes, mid-sized countries are building new agreements involving shared digital infrastructure and localized AI.\n##### [Smart Enough to Do Math, Dumb Enough to Fail: The Hunt for a Better AI Test] \nAndrew Myers\nFeb 02, 2026\nNews\n![illustration of data and lines] \nA Stanford HAI workshop brought together experts to develop new evaluation methods that assess AI&#x27;s hidden capabilities, not just its test-taking performance.\nNews\n![illustration of data and lines] \n#### [Smart Enough to Do Math, Dumb Enough to Fail: The Hunt for a Better AI Test] \nAndrew Myers\n[Foundation Models] [Generative AI] [Privacy, Safety, Security] Feb 02\nA Stanford HAI workshop brought together experts to develop new evaluation methods that assess AI&#x27;s hidden capabilities, not just its test-taking performance.\n##### [What Davos Said About AI This Year] \n[Shana Lynch] \nJan 28, 2026\nNews\n![James Landay and Vanessa Parli] \nWorld leaders focused on ROI over hype this year, discussing sovereign AI, open ecosystems, and workplace change.\nNews\n![James Landay and Vanessa Parli] \n#### [What Davos Said About AI This Year] \n[Shana Lynch] \n[Economy, Markets] Jan 28\nWorld leaders focused on ROI over hype this year, discussing sovereign AI, open ecosystems, and workplace change.",
    "length": 7702,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "AI and Neuroscience Grants | Wu Tsai Neurosciences Institute",
    "url": "https://neuroscience.stanford.edu/ai-and-neuroscience-grants",
    "text": "AI and Neuroscience Grants | Wu Tsai Neurosciences Institute[Skip to main content] \n[Stanfor University] \n[![Home]] \nSearch\n# AI and Neuroscience Grants\nImage\n![Wide banner graphic announcing the call for applications for Wu Tsai Neuro &amp; HAI Partnership Grants. Navy header reads “Call for Applications,” with the main title centered. A pill-shaped label says “Bridging Neuroscience and AI.” Curved teal, pink, red, and white vertical stripes border both sides, with the Wu Tsai Neuro and HAI logos at bottom.] \nThe Stanford Wu Tsai Neurosciences Institute and[Human-Centered Artificial Intelligence (HAI)] invite the Stanford research community to submit proposals for our AI and Neuroscience Partnership Grants.\nThe goal of this partnership is to support projects that have the potential to transform our understanding of the human brain using AI and advance the development of intelligent technology. Researchers with proposals that can make a persuasive case that initial results will catalyze further support from internal and external stakeholders are encouraged to apply.\nClosed\n#### Apply by Jan. 16, 2026\n* **Award:**Up to**$125,000**in direct costs for one year,less an 8% infrastructure charge\n* **Eligibility:**Teams must include**at least two independent co-PIs.**All applicants must be affiliated with**both Institutes**([Wu Tsai Neuro affiliation form];[HAI affiliation form])\n[Read the application and eligibility details here] *(Stanford log-in required)*\n[Apply on Slideroom] \nProgram sponsor\nStanford Human-Centered Artificial Intelligence (HAI), Wu Tsai Neurosciences Institute\nContact\nChiara Bertipaglia, Associate Director, Programs and Training (bertipag@stanford.edu)\n#### Grant Recipients\nWu Tsai Neurosciences Institute\nNeuro-AI Grant\n2022\n##### [The Synaptic Organization of Dendrites] \nWu Tsai Neurosciences Institute\nNeuro-AI Grant\n2022\n##### [Silent Speech Decoding Using Flexible Electronics and Artificial Intelligence] \nWu Tsai Neurosciences Institute\nNeuro-AI Grant\n2022\n##### [At-home Stroke Rehabilitation System based on Augmented Reality and Brain Computer Interface Paradigm] \nWu Tsai Neurosciences Institute\nNeuro-AI Grant\n2022\n##### [Tracking Parkinson’s Disease with Transformer Models of Everyday Looking Behaviors]",
    "length": 2262,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Getting Beyond the Sandbox: A Playbook for Developing Generative AI Solutions for Enterprise Applications | Stanford Law School",
    "url": "https://law.stanford.edu/2024/03/28/getting-beyond-the-sandbox-a-playbook-for-developing-generative-ai-solutions-for-enterprise-applications",
    "text": "Getting Beyond the Sandbox: A Playbook for Developing Generative AI Solutions for Enterprise Applications - CodeX - Stanford Law School\nlogo-footerlogo-fulllogo-stanford-universitylogomenu-closemenuClose IconIcon with an X to denote closing.Play IconPlay icon in a circular border.\n[Skip to main content] \n![Sls logo] \n# Getting Beyond the Sandbox: A Playbook for Developing Generative AI Solutions for Enterprise Applications\n* March 28, 2024\n* [Subscribe] \n* [**Share on Twitter] \n* [**Share on Facebook] \n*By Jay Mandal, Fellow and Dr. Megan Ma, Associate Director, Codex*\n**Introduction**\nCompanies are captivated by the potential of generative artificial intelligence (AI) to transform their businesses, but many don’t know exactly how.[Generative AI] is defined as artificial intelligence capable of generating text, images, video or other data using generative models. Companies in all fields are experimenting in sandbox environments of different ways this emerging technology can provide value to customers. This urgency is also partly owed to leadership and market pressures to release new generative AI-driven solutions. The main questions for companies are asking: (1) where should be the focus; and (2) how do we get started?\nThis is a generative AI playbook that provides companies a framework on how to prioritize use cases and start building. The key elements covered are:\n1. Developing solid use cases;\n2. Developing a solution and using proprietary data;\n3. Which type of generative AI model to use;\n4. Risk mitigation to enable enterprise readiness for generative AI; and\n5. Optimizing the user-experience (UX); and\n6. Conclusion\nThe focus of this paper will be on generative AI solutions that benefit external customers, and not solutions that benefit employees and improve internal company processes.\n[Access the full paper here.] \n**Back to the Top",
    "length": 1870,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Krishnaram Kenthapadi's Publications",
    "url": "http://www-cs-students.stanford.edu/~kngk/research.html",
    "text": "Krishnaram Kenthapadi's Publications## [Krishnaram Kenthapadi] 's Publications\n|\nLast revised May, 2024. Please see**[arXiv] **and**[Google Scholar] **for my recent publications and**[LinkedIn] **for my recent talks.\nKrishnaram's research interests are in the areas of Trustworthy Generative AI, Trustworthy Health AI, AI Observability, AI Safety, Fairness/Explainability/Privacy/Robustness in AI/ML Systems, Algorithms for Large Datasets and Graphs, Data Mining, Web Search, Information Retrieval, Search and Recommendation Systems, Social Network Analysis, and Computational Education.#### [Selected Projects] \n#### [Publications, Articles, Talks, and Tutorials Since 2016] \n#### [Document Understanding and Augmentation with Multimedia Content / Computational Education] \n#### [Algorithmic Identification and Ranking of Twitter Groups] \n#### [Web Search and Related Topics] \n#### [Large Graph/Data Algorithms] \n#### [Privacy] \n#### [PhD Thesis] \n**Publications, Articles, Talks, and Tutorials Since 2016:**\n* Krishnaram Kenthapadi, Mehrnoosh Sameki, Ankur Taly,**Grounding and Evaluation for Large Language Models: Practical Challenges and Lessons Learned **, Tutorial at ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD 2024).Abstract\nWith the ongoing rapid adoption of Artificial Intelligence (AI) based systems in high-stakes domains such as financial services, healthcare and life sciences, hiring and human resources, education, societal infrastructure, and national security, ensuring the trustworthiness, safety, and observability of these systems has become a crucial necessity. Hence, it is essential to ensure that the AI systems are evaluated and monitored for not only accuracy and quality related metrics, but also for robustness against adversarial attacks, robustness under distribution shift, bias and discrimination against underrepresented groups, security and privacy protection, interpretability, and other responsible AI related dimensions. Our focus is on large language models (LLMs) and other generative AI models and applications, for which there are additional challenges such as hallucinations (and other ungrounded or low-quality outputs), harmful content (such as sexual, racist, and hateful responses), jailbreaks on safety and alignment mechanisms, prompt injection attacks, misinformation and disinformation, fake, misleading, and manipulative content, and copyright infringement.\nIn this tutorial, we first highlight key harms associated with generative AI systems with a focus on ungrounded answers (hallucinations), jailbreaks and prompt injection attacks, harmful content, and copyright infringement. Then, we discuss how to effectively address potential risks and challenges, following the framework of identification, measurement, mitigation (with four mitigation layers at model, safety system, application, and positioning), and operationalization. We present real-world LLM use cases, practical challenges, best practices, lessons learned from deploying solution approaches in industry, and key open problems. Our goal is to stimulate further research on grounding and evaluation of LLMs, and enable researchers and practitioners to build more robust and trustworthy LLM applications.\n* Krishnaram Kenthapadi, Himabindu Lakkaraju, Nazneen Rajani,**[Trustworthy Generative AI] **, Tutorials at International Conference on Machine Learning (ICML 2023), ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD 2023 [[proposal]]), and ACM Conference on Fairness, Accountability, and Transparency (FAccT 2023) [[slides]] [[Video Recording]].Abstract\nGenerative AI models and applications are being rapidly developed and deployed across a wide spectrum of industries and applications ranging from writing and email assistants to graphic design and art generation to educational assistants to coding to drug discovery. However, there are several ethical and social considerations associated with generative AI models and applications. These concerns include lack of interpretability, bias and discrimination, privacy, lack of model robustness, fake and misleading content, copyright implications, plagiarism, and environmental impact associated with training and inference of generative AI models.\nIn this tutorial, we first motivate the need for adopting responsible AI principles when developing and deploying large language models (LLMs) and other generative AI models, as part of a broader AI model governance and responsible AI framework, from societal, legal, user, and model developer perspectives, and provide a roadmap for thinking about responsible AI for generative AI in practice. We provide a brief technical overview of text and image generation models, and highlight the key responsible AI desiderata associated with these models. We then describe the technical considerations and challenges associated with realizing the above desiderata in practice. We focus on real-world generative AI use cases spanning domains such as media generation, writing assistants, copywriting, code generation, and conversational assistants, present practical solution approaches / guidelines for applying responsible AI techniques effectively, discuss lessons learned from deploying responsible AI approaches for generative AI applications in practice, and highlight the key open research problems. We hope that our tutorial will inform both researchers and practitioners, stimulate further research on responsible AI in the context of generative AI, and pave the way for building more reliable and trustworthy generative AI applications in the future.\n* Gyandev Gupta, Bashir Rastegarpanah, Amalendu Iyer, Joshua Rubin, Krishnaram Kenthapadi,[Measuring Distributional Shifts in Text: The Advantage of Language Model-Based Embeddings], Preprint, 2023 [[arxiv]]Abstract\nAn essential part of monitoring machine learning models in production is measuring input and output data drift. In this paper, we present a system for measuring distributional shifts in natural language data and highlight and investigate the potential advantage of using large language models (LLMs) for this problem. Recent advancements in LLMs and their successful adoption in different domains indicate their effectiveness in capturing semantic relationships for solving various natural language processing problems. The power of LLMs comes largely from the encodings (embeddings) generated in the hidden layers of the corresponding neural network. First we propose a clustering-based algorithm for measuring distributional shifts in text data by exploiting such embeddings. Then we study the effectiveness of our approach when applied to text embeddings generated by both LLMs and classical embedding algorithms. Our experiments show that general-purpose LLM-based embeddings provide a high sensitivity to data drift compared to other embedding methods. We propose drift sensitivity as an important evaluation metric to consider when comparing language models. Finally, we present insights and lessons learned from deploying our framework as part of the Fiddler ML Monitoring platform over a period of 18 months.\n* Vijay Keswani, Elisa Celis, Krishnaram Kenthapadi, Matthew Lease,[Designing Closed-Loop Models for Task Allocation], International Conference on Hybrid Human Artificial Intelligence ([HHAI]), 2023 [[arxiv]] [[code]] [[slides]]Abstract\nAutomatically assigning tasks to people is challenging because human performance can vary across tasks for many reasons. This challenge is further compounded in real-life settings in which no oracle exists to assess the quality of human decisions and task assignments made. Instead, we find ourselves in a \"closed\" decision-making loop in which the same fallible human decisions we rely on in practice must also be used to guide task allocation. How can imperfect and potentially biased human decisions train an accurate allocation model? Our key insight is to exploit weak prior information on human-task similarity to bootstrap model training. We show that the use of such a weak prior can improve task allocation accuracy, even when human decision-makers are fallible and biased. We present both theoretical analysis and empirical evaluation over synthetic data and a social media toxicity detection task. Results demonstrate the efficacy of our approach.\n* Michael Lohaus, Matthaeus Kleindessner, Krishnaram Kenthapadi, Francesco Locatello, Chris Russell,[Are Two Heads the Same as One? Identifying Disparate Treatment in Fair Neural Networks], Advances in Neural Information Processing Systems (NeurIPS), 2022 [[arxiv]] [[slides and video]]Abstract\nWe show that deep networks trained to satisfy demographic parity often do so through a form of race or gender awareness, and that the more we force a network to be fair, the more accurately we can recover race or gender from the internal state of the network. Based on this observation, we investigate an alternative fairness approach: we add a second classification head to the network to explicitly predict the protected attribute (such as race or gender) alongside the original task. After training the two-headed network, we enforce demographic parity by merging the two heads, creating a network with the same architecture as the original network. We establish a close relationship between existing approaches and our approach by showing (1) that the decisions of a fair classifier are well-approximated by our approach, and (2) that an unfair and optimally accurate classifier can be recovered from a fair classifier and our second head predicting the protected attribute. We use our explicit formulation to argue that the existing fairness approaches, just as ours, demonstrate disparate treatment and that they are likely to be unlawful in a wide range of scenarios under US law.\n* Sanjiv Das, Michele Donini, Muhammad Bilal Zafar, John He, Krishnaram Kenthapadi,[FinLex: An Effective Use of Word Embeddings for Financial Lexicon Generation], The Journal of Finance and Data Science, 2022.**[The Outstanding Paper in FinTech Award] **[[Amazon Science]]Abstract\nWe present a simple and effective methodology for the generation of lexicons (word lists) that may be used in natural language scoring applications. In particular, in the finance industry, word lists have become ubiquitous for sentiment scoring. These have been derived from dictionaries such as the Harvard Inquirer and require manual curation. Here, we present an automated approach to the curation of lexicons, which makes automatic preparation of any word list immediate. We show that our automated word lists deliver comparable performance to traditional lexicons on machine learning classification tasks. This new approach will enable finance academics and practitioners to create and deploy new word lists in addition to the few traditional ones in a facile manner.\n* David Munechika, Zijie J Wang, Jack Reidy, Josh Rubin, Krishna Gade, Krishnaram Kenthapadi, Duen Horng Chau,[Visual Auditor: Interactive Visualization for Detection and Summarization of Model Biases], IEEE Visualization and Visual Analytics (VIS) Conference, 2022 [[arxiv]] [[code]] [[live demo]] [[demo video]]Abstract\nAs machine learning (ML) systems become increasingly widespread, it is necessary to audit these systems for biases prior to their de-ployment. Recent research has developed algorithms for effectively identifying intersectional bias in the form of interpretable, underper-forming subsets (or slices) of the data. However, these solutions and their insights are limited without a tool for visually understanding and interacting with the results of these algorithms. We propose Visual Auditor, an interactive visualization tool for auditing and summarizing model biases. Visual Auditor assists model validation by providing an interpretable overview of intersectional bias (bias that is present when examining populations defined by multiple features), details about relationships between problematic data slices, and a comparison between underperforming and overper-forming data slices in a model. Our open-source tool runs directly in both computational notebooks and web browsers, making model auditing accessible and easily integrated into current ML development workflows. An observational user study in collaboration with domain experts at Fiddler AI highlights that our tool can help ML practitioners identify and understand model biases.\n* Krishnaram Kenthapadi, Himabindu Lakkaraju, Pradeep Natarajan, Mehrnoosh Sameki,**[Model Monitoring in Practice] **, Tutorials at ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD 2022 [[proposal]]), ACM Conference on Fairness, Accountability, and Transparency (FAccT 2022), and The Web Conference (WWW 2023) [[slides]] [[Video Recording]].Abstract\nArtificial Intelligence (AI) is increasingly playing an integral role in determining our day-to-day experiences. Increasingly, the applications of AI are no longer limited to search and recommendation systems, such as web search and movie and product recommendations, but AI is also being used in decisions and processes that are critical for individuals, businesses, and society. With AI based solutions in high-stakes domains such as hiring, lending, criminal justice, healthcare, and education, the resulting personal and professional implications of AI are far-reaching. Consequently, it becomes critical to ensure that these models are making accurate predictions, are robust to shifts in the data, are not relying on spurious features, and are not unduly discriminating against minority groups. To this end, several approaches spanning various areas such as explainability, fairness, and robustness have been proposed in recent literature, and many papers and tutorials on these topics have been presented in recent computer science conferences. However, there is relatively less attention on the need for monitoring machine learning (ML) models once they are deployed and the associated research challenges.\nIn this tutorial, we first motivate the need for ML model monitoring[14], as part of a broader AI model governance[9] and responsible AI framework, from societal, legal, customer/end-user, and model developer perspectives, and provide a roadmap for thinking about model monitoring in practice. We then present findings and insights on model monitoring desiderata based on interviews with various ML practitioners spanning domains such as financial services, healthcare, hiring, online retail, computational advertising, and conversational assistants[15]. We then describe the technical considerations and challenges associated with realizing the above desiderata in practice. We provide an overview of techniques/tools for model monitoring (e.g., see [1, 1, 2, 5, 6, 8, 10-13, 18-21]. Then, we focus on the real-world application of model monitoring methods and tools [3, 4, 7, 11, 13, 16, 17], present practical challenges/guidelines for using such techniques effectively, and lessons learned from deploying model monitoring tools for several web-scale AI/ML applications. We present case studies across different companies, spanning application domains such as financial services, healthcare, hiring, conversational assistants, online retail, computational advertising, search and recommendation systems, and fraud detection. We hope that our tutorial will inform both researchers and practitioners, stimulate further research on model monitoring, and pave the way for building more reliable ML models and monitoring tools in the future.\n* Yigitcan Kaya, Muhammad Bilal Zafar, Sergul Aydore, Nathalie Rauschmayr, Krishnaram Kenthapadi,[Generating Distributional Adversarial Examples to Evade Statistical Detectors], International Conference on Machine Learning (ICML), 2022 [[slides]] [[slides and video]] [[blog]]Abstract\nDeep neural networks (DNNs) are known to be highly vulnerable to adversarial examples (AEs) that include malicious perturbations. Assumptions about the statistical differences between natural and adversarial inputs are commonplace in many detection techniques. As a best practice, AE detectors are evaluated against ’adaptive’ attackers who actively perturb their inputs to avoid detection. Due to the difficulties in designing adaptive attacks, however, recent work suggests that most detectors have incomplete evaluation. We aim to fill this gap by designing a generic adaptive attack against detectors: the ’statistical indistinguishability attack’ (SIA). SIA optimizes a novel objective to craft adversarial examples (AEs) that follow the same distribution as the natural inputs with respect to DNN representations. Our objective targets all DNN layers simultaneously as we show that AEs being indistinguishable at one layer might fail to be so at other layers. SIA is formulated around evading distributional detectors that inspect a set of AEs as a whole and is also effective against four individual AE detectors, two dataset shift detectors, and an out-of-distribution sample detector, curated from published works. This suggests that SIA can be a reliable tool for evaluating the security of a range of detectors.\n* Ana Lucic, Sheeraz Ahmad, Amanda Furtado Brinhosa, Vera Liao, Himani Agrawal, Umang Bhatt, Krishnaram Kenthapadi, Alice Xiang, Maarten de Rijke, Nicholas Drabowski,[Towards the Use of Saliency Maps for Explaining Low-Quality Electrocardiograms to End Users], ICML Workshop on Interpretable ML in Healthcare, 2022 [[arxiv]]Abstract\nWhen using medical images for diagnosis, either by clinicians or artificial intelligence (AI) systems, it is important that the images are of high quality. When an image is of low quality, the medical exam that produced the image often needs to be redone. In telemedicine, a common problem is that the quality issue is only flagged once the patient has left the clinic, meaning they must return in order to have the exam redone. This can be especially difficult for people living in remote regions, who make up a substantial portion of the patients at Portal Telemedicina, a digital healthcare organization based in Brazil. In this paper, we report on ongoing work regarding (i) the development of an AI system for flagging and explaining low-quality medical images in real-time, (ii) an interview study to understand the explanation needs of stakeholders using the AI system at OurCompany, and, (iii) a longitudinal user study design to examine the effect of including explanations on the workflow of the technicians in our clinics. To the best of our knowledge, this would be the first longitudinal study on evaluating the effects of XAI methods on end-users -- stakeholders that use AI systems but do not have AI-specific expertise. We welcome feedback and suggestions on our experimental setup.\n* David Nigenda, Zohar Karnin, Muhammad Bilal Zafar, Raghu Ramesha, Alan Tan, Michele Donini, Krishnaram Kenthapadi,[Amazon SageMaker Model Monitor: A System for Real-Time Insights into Deployed Machine Learning Models], ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD), 2022 [[arxiv]]Abstract\nWith the increasing adoption of machine learning (ML) models and systems in high-stakes settings across different industries, guaranteeing a model's performance after deployment has become crucial. Monitoring models in production is a critical aspect of ensuring their continued performance and reliability. We present Amazon SageMaker Model Monitor, a fully managed service that continuously monitors the quality of machine learning models hosted on Amazon SageMaker. Our system automatically detects data, concept, bias, and feature attribution drift in models in real-time and provides alerts so that model owners can take corrective actions and thereby maintain high quality models. We describe the key requirements obtained from customers, system design and architecture, and methodology for detecting different types of drift. Further, we provide quantitative evaluations followed by use cases, insights, and lessons learned from more than two years of production deployment.\n* Nathalie Rauschmayr, Sami Kama, Muhyun Kim, Miyoung Choi, Krishnaram Kenthapadi,[Profiling Deep Learning Workloads at Scale using Amazon SageMaker], ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD), 2022 [[Amazon Science]]Abstract\nWith the rise of deep learning (DL), machine learning (ML) has become compute and data intensive, typically requiring multi-node multi-GPU clusters. As state-of-the-art models grow in size in the order of trillions of parameters, their computational complexity and cost also increase rapidly. Since 2012, the cost of deep learning doubled roughly every quarter, and this trend is likely to continue. ML practitioners have to cope with common challenges of efficient resource utilization when training such large models. In this paper, we propose a new profiling tool that cross-correlates relevant system utilization metrics and framework operations. The tool supports profiling DL models at scale, identifies performance bottlenecks, and provides insights with recommendations. We deployed the profiling functionality as an add-on to Amazon SageMaker Debugger, a fully-managed service that leverages an on-the-fly analysis system (called rules) to automatically identify complex issues in DL training jobs. By presenting deployment results and customer case studies, we show that it enables users to identify and fix issues caused by inefficient hardware resource usage, thereby reducing training time and cost.\n* Murtuza N Shergadwala, Himabindu Lakkaraju, Krishnaram Kenthapadi,[A Human-Centric Perspective on Model Monitoring], AAAI Conference on Human Computation and Crowdsourcing (HCOMP), 2022 [[arxiv]]. Short versions appeared in[ICML 2022 Workshop on Human-Machine Collaboration and Teaming] and[NeurIPS 2022 Workshop on Challenges in Deploying and Monitoring Machine Learning Systems] [[slides and video]]Abstract\nPredictive models are increasingly used to make various consequential decisions in high-stakes domains such as healthcare, finance, and policy. It becomes critical to ensure that these models make accurate predictions, are robust to shifts in the data, do not rely on spurious features, and do not unduly discriminate against minority groups. To this end, several approaches spanning various areas such as explainability, fairness, and robustness have been proposed in recent literature. Such approaches need to be human-centered as they cater to the understanding of the models to their users. However, there is little to no research on understanding the needs and challenges in monitoring deployed machine learning (ML) models from a human-centric perspective. To address this gap, we conducted semi-structured interviews with 13 practitioners who are experienced with deploying ML models and engaging with customers spanning domains such as financial services, healthcare, hiring, online retail, computational advertising, and conversational assistants. We identified various human-centric challenges and requirements for model monitoring in real-world applications. Specifically, we found that relevant stakeholders would want model monitoring systems to provide clear, unambiguous, and easy-to-understand insights that are readily actionable. Furthermore, our study also revealed that stakeholders desire customization of model monitoring systems to cater to domain-specific use cases.\n* Kate Donahue, Alexandra Chouldechova, Krishnaram Kenthapadi,[Human-Algorithm Collaboration: Achieving Complementarity and Avoiding Unfairness], ACM Conference on Fairness, Accountability, and Transparency (FAccT), 2022 [[arxiv]] [[Research highlight in the Montreal AI Ethics Newsletter]]. Short version appeared in[NeurIPS 2021 Workshop on Human Centered AI] [[slides and video]].Abstract\nMuch of machine learning research focuses on predictive accuracy: given a task, create a machine learning model (or algorithm) that maximizes accuracy. In many settings, however, the final prediction or decision of a system is under the control of a human, who uses an algorithm’s output along with their own personal expertise in order to produce a combined prediction. One ultimate goal of such collaborative systems is complementarity: that is, to produce lower loss (equivalently, greater payoff or utility) than either the human or algorithm alone. However, experimental results have shown that even in carefully-designed systems, complementary performance can be elusive. Our work provides three key contributions. First, we provide a theoretical framework for modeling simple human-algorithm systems and demonstrate that multiple prior analyses can be expressed within it. Next, we use this model to prove conditions where complementarity is impossible, and give constructive examples of where complementarity is achievable. Finally, we discuss the implications of our findings, especially with respect to the fairness of a classifier. In sum, these results deepen our understanding of key factors influencing the combined performance of human-algorithm systems, giving insight into how algorithmic tools can best be designed for collaborative environments.\n* Emily Diana, Wesley Gill, Michael J. Kearns, Krishnaram Kenthapadi, Aaron Roth, Saeed Sharifi-Malvajerdi,[Multiaccurate Proxies for Downstream Fairness], ACM Conference on Fairness, Accountability, and Transparency (FAccT), 2022 [[arxiv]]Abstract\nWe study the problem of training a model that must obey demographic fairness conditions when the sensitive features are not available at training time —in other words, how can we train a model to be fair by race when we don’t have data about race? We adopt a fairness pipeline perspective, in which an “upstream” learner that does have access to the sensitive features will learn a proxy model for these features from the other attributes. The goal of the proxy is to allow a general “downstream” learner —with minimal assumptions on their prediction task —to be able to use the proxy to train a model that is fair with respect to the true sensitive features. We show that obeying multiaccuracy constraints with respect to the downstream model class suffices for this purpose, provide sample- and oracle efficient-algorithms and generalization bounds for learning such proxies, and conduct an experimental evaluation. In general, multiaccuracy is much easier to satisfy than classification accuracy, and can be satisfied even when the sensitive features are hard to predict.\n* Matthaus Kleindessner, Samira Samadi, Muhammad Bilal Zafar, Krishnaram Kenthapadi, Chris Russell,[Pairwise Fairness for Ordinal Regression], International Conference on Artificial Intelligence and Statistics (AISTATS), 2022 [[arxiv]] [[code]] [[slides and video]]Abstract\nWe initiate the study of fairness for ordinal regression. We adapt two fairness notions previously considered in fair ranking and propose a strategy for training a predictor that is approximately fair according to either notion. Our predictor has the form of a threshold model, composed of a scoring function and a set of thresholds, and our strategy is based on a reduction to fair binary classification for learning the scoring function and local search for choosing the thresholds. We provide generalization guarantees on the error and fairness violation of our predictor, and we illustrate the effectiveness of our approach in extensive experiments.\n* Deborah Sulem, Michele Donini, Muhammad Bilal Zafar, Francois-Xavier Aubet, Jan Gasthaus, Tim Januschowski, Sanjiv Das, Krishnaram Kenthapadi, Cedric Archambeau,[Diverse Counterfactual Explanations for Anomaly Detection in Time Series], Preprint, 2022 [[arxiv]]Abstract\nData-driven methods that detect anomalies in times series data are ubiquitous in practice, but they are in general unable to provide helpful explanations for the predictions they make. In this work we propose a model-agnostic algorithm that generates counterfactual ensemble explanations for time series anomaly detection models. Our method generates a set of diverse counterfactual examples, i.e, multiple perturbed versions of the original time series that are not considered anomalous by the detection model. Since the magnitude of the perturbations is limited, these counterfactuals represent an ensemble of inputs similar to the original time series that the model would deem normal. Our algorithm is applicable to any differentiable anomaly detection model. We investigate the value of our method on univariate and multivariate real-world datasets and two deep-learning-based anomaly detection models, under several explainability criteria previously proposed in other data domains such as Validity, Plausibility, Closeness and Diversity. We show that our algorithm can produce ensembles of counterfactual examples that satisfy these criteria and thanks to a novel type of visualisation, can convey a richer interpretation of a model's internal mechanism than existing methods. Moreover, we design a sparse variant of our method to improve the interpretability of counterfactual explanations for high-dimensional time series anomalies. In this setting, our explanation is localised on only a few dimensions and can therefore be communicated more efficiently to the model's user.\n* Fan Wu, Linyi Li, Chejian Xu, Huan Zhang, Bhavya Kailkhura, Krishnaram Kenthapadi, Ding Zhao, Bo Li,[COPA: Certifying Robust Policies for Offline Reinforcement Learning against Poisoning Attacks], International Conference on Learning Representations (ICLR), 2022 [[arxiv]] [[Leaderboard]] [[code]] [[slides and video]]Abstract\nAs reinforcement learning (RL) has achieved near human-level performance in a variety of tasks, its robustness has raised great attention. While a vast body of research has explored test-time (evasion) attacks in RL and corresponding defenses, its robustness against training-time (poisoning) attacks remains largely unanswered. In this work, we focus on certifying the robustness of offline RL in the presence of poisoning attacks, where a subset of training trajectories could be arbitrarily manipulated. We propose the first certification framework, COPA, to certify the number of poisoning trajectories that can be tolerated regarding different certification criteria. Given the complex structure of RL, we propose two certification criteria: per-state action stability and cumulative reward bound. To further improve the certification, we propose new partition and aggregation protocols to train robust policies. We further prove that some of the proposed certification methods are theoretically tight and some are NP-Complete problems. We leverage COPA to certify three RL environments trained with different algorithms and conclude: (1) The proposed robust aggregation protocols such as temporal aggregation can significantly improve the certifications; (2) Our certification for both per-state action stability and cumulative reward bound are efficient and tight; (3) The certification for different training algorithms and environments are different, implying their intrinsic robustness properties. All experimental results are available at[https://copa-leaderboard.github.io].\n* Jing Liu, Chulin Xie, Krishnaram Kenthapadi, Sanmi Koyejo, Bo Li,[RVFR: Robust Vertical Federated Learning via Feature Subspace Recovery],[NeurIPS Workshop on New Frontiers in Federated Learning: Privacy, Fairness, Robustness, Personalization and Data Ownership], 2021Abstract\nVertical Federated Learning (VFL) is a distributed learning paradigm that allows multiple agents to jointly train a global model when each agent holds a different subset of features for the same sample(s). VFL is known to be vulnerable to backdoor attacks during training, and also vulnerable to test-time attacks. However, unlike the standard horizontal federated learning, improving the robustness of VFL remains challenging. To this end, we propose RVFR, a novel robust VFL training and inference framework. The key to our approach is to ensure that with a low-rank feature subspace, a small number of attacked samples, and other mild assumptions, RVFR recovers the underlying uncorrupted features with guarantees, thus sanitizes the model against a vast range of backdoor attacks. Further, RVFR also defends against inference-time adversarial feature attack. Our empirical studies further corroborate the robustness of the proposed framework.\n* Dylan Slack, Nathalie Rauschmayr, Krishnaram Kenthapadi,[Defuse: Training More Robust Models through Creation and Correction of Novel Model Errors],[NeurIPS Workshop on eXplainable AI approaches for debugging and diagnosis], 2021 [[arxiv]] [[code]] [[slides and video]] [[Amazon Science article]]Abstract\nWe typically compute aggregate statistics on held-out test data to assess the generalization of machine learning models. However, test data is only so comprehensive, and in practice, important cases are often missed. Thus, the performance of deployed machine learning models can be variable and untrustworthy. Motivated by these concerns, we develop methods to generate and correct novel model errors beyond those available in the data. We propose Defuse: a technique that trains a generative model on a classifier's training dataset and then uses the latent space to generate new samples which are no longer correctly predicted by the classifier. For instance, given a classifier trained on the MNIST dataset that correctly predicts a test image, Defuse then uses this image to generate new similar images by sampling from the latent space. Defuse then identifies the images that differ from the label of the original test input. Defuse enables efficient labeling of these new images, allowing users to re-train a more robust model, thus improving overall model performance. We evaluate the performance of Defuse on classifiers trained on real world datasets and find it reveals novel sources of model errors.\n* Michaela Hardt, Xiaoguang Chen, Xiaoyi Cheng, Michele Donini, Jason Gelman, Satish Gollaprolu, John He, Pedro Larroy, Xinyu Liu, Nick McCarthy, Ashish Rathi, Scott Rees, Ankit Siva, ErhYuan Tsai, Keerthan Vasist, Pinar Yilmaz, Bilal Zafar, Sanjiv Das, Kevin Haas, Tyler Hill, Krishnaram Kenthapadi,[Amazon SageMaker Clarify: Machine Learning Bias Detection and Explainability in the Cloud], ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD), 2021 [**Product page:[Amazon SageMaker Clarify] **] [**[Open Source Implementation] **] [[slides]] [[Amazon AI Fairness and Explainability Whitepaper]] [Amazon Blog Posts:[Amazon SageMaker Clarify announcement];[Amazon Science Article]] [**Press Coverage**:[TechCrunch];[VentureBeat];[Yahoo Finance];[TechTarget];[GeekWire];[ZDNet];[Business Wire];[A Cloud Guru: SageMaker Clarify is the most important announcement of re:Invent 2020]]Abstract\nUnderstanding the predictions made by machine learning (ML) models and their potential biases remains a challenging and labor-intensive task that depends on the application, the dataset, and the specific model. We present Amazon SageMaker Clarify, an explainability feature for Amazon SageMaker that launched in December 2020, providing insights into data and ML models by identifying biases and explaining predictions. It is deeply integrated into Amazon SageMaker, a fully managed service that enables data scientists and developers to build, train, and deploy ML models at any scale. Clarify supports bias detection and feature importance computation across the ML lifecycle, during data preparation, model evaluation, and post-deployment monitoring. We outline the desiderata derived from customer input, the modular architecture, and the methodology for bias and explanation computations. Further, we describe the technical challenges encountered and the tradeoffs we had to make. For illustration, we discuss two customer use cases. We present our deployment results including qualitative customer feedback and a quantitative evaluation. Finally, we summarize lessons learned, and discuss best practices for the successful adoption of fairness and explanation tools in practice.\n* Valerio Perrone, Huibin Shen, Aida Zolic, Iaroslav Shcherbatyi, Amr Ahmed, Tanya Bansal, Michele Donini, Fela Winkelmolen, Rodolphe Jenatton, Jean Baptiste Faddoul, Barbara Pogorzelska, Miroslav Miladinovic, Krishnaram Kenthapadi, Matthias Seeger, Cedric Archambeau,[Amazon SageMaker Automatic Model Tuning: Scalable Black-box Optimization], ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD), 2021 [[arxiv]]Abstract\nTuning complex machine learning systems is challenging. Machine learning models typically expose a set of hyperparameters, be it regularization, architecture, or optimization parameters, whose careful tuning is critical to achieve good performance. To democratize access to such systems, it is essential to automate this tuning process. This paper presents Amazon SageMaker Automatic Model Tuning (AMT), a fully managed system for black-box optimization at scale. AMT finds the best version of a machine learning model by repeatedly training it with different hyperparameter configurations. It leverages either random search or Bayesian optimization to choose the hyperparameter values resulting in the best-performing model, as measured by the metric chosen by the user. AMT can be used with built-in algorithms, custom algorithms, and Amazon SageMaker pre-built containers for machine learning frameworks. We discuss the core functionality, system architecture and our design principles. We also describe some more advanced features provided by AMT, such as automated early stopping and warm-starting, demonstrating their benefits in experiments.\n* Sergul Aydore, William Brown, Michael Kearns, Krishnaram Kenthapadi, Luca Melis, Aaron Roth, Ankit Siva,[Differentially Private Query Release Through Adaptive Projection], International Conference on Machine Learning (ICML), 2021 (Long Presentation) [[arxiv]] [[code]] [[slides and video]]. Short version in[ICLR 2021 Workshop on Synthetic Data Generation: Quality, Privacy, Bias].Abstract\nWe propose, implement, and evaluate a new algorithm for releasing answers to very large numbers of statistical queries like k-way marginals, subject to differential privacy. Our algorithm makes adaptive use of a continuous relaxation of the Projection Mechanism, which answers queries on the private dataset using simple perturbation, and then attempts to find the synthetic dataset that most closely matches the noisy answers. We use a continuous relaxation of the synthetic dataset domain which makes the projection loss differentiable, and allows us to use efficient ML optimization techniques and tooling. Rather than answering all queries up front, we make judicious use of our privacy budget by iteratively and adaptively finding queries for which our (relaxed) synthetic data has high error, and then repeating the projection. We perform extensive experimental evaluations across a range of parameters and datasets, and find that our method outperforms existing algorithms in many cases, especially when the privacy budget is small or the query class is large.\n* Muhammad Bilal Zafar, Philipp Schmidt, Michele Donini, Cedric Archambeau, Felix Biessmann, Sanjiv Ranjan Das, Krishnaram Kenthapadi,[More Than Words: Towards Better Quality Interpretations of Text Classifiers], Preprint, 2021 [[arxiv]]Abstract\nThe large size and complex decision mechanisms of state-of-the-art text classifiers make it difficult for humans to understand their predictions, leading to a potential lack of trust by the users. These issues have led to the adoption of methods like SHAP and Integrated Gradients to explain classification decisions by assigning importance scores to input tokens. However, prior work, using different randomization tests, has shown that interpretations generated by these methods may not be robust. For instance, models making the same predictions on the test set may still lead to different feature importance rankings. In order to address the lack of robustness of token-based interpretability, we explore explanations at higher semantic levels like sentences. We use computational metrics and human subject studies to compare the quality of sentence-based interpretations against token-based ones. Our experiments show that higher-level feature attributions offer several advantages: 1) they are more robust as measured by the randomization tests, 2) they lead to lower variability when using approximation-based methods like SHAP, and 3) they are more intelligible to humans in situations where the linguistic coherence resides at a higher granularity level. Based on these findings, we show that token-based interpretability, while being a convenient first choice given the input interfaces of the ML models, is not the most effective one in all situations.\n* Muhammad Bilal Zafar, Michele Donini, Dylan Slack, Cedric Archambeau, Sanjiv Ranjan Das, Krishnaram Kenthapadi,[On the Lack of Robust Interpretability of Neural Text Classifiers], Findings at the Joint Conference of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing (ACL-IJCNLP Findings), 2021 [[arxiv]]Abstract\nWith the ever-increasing complexity of neural language models, practitioners have turned to methods for understanding the predictions of these models. One of the most well-adopted approaches for model interpretability is feature-based interpretability, i.e., ranking the features in terms of their impact on model predictions. Several prior studies have focused on assessing the fidelity of feature-based interpretability methods, i.e., measuring the impact of dropping the top-ranked features on the model output. However, relatively little work has been conducted on quantifying the robustness of interpretations. In this work, we assess the robustness of interpretations of neural text classifiers, specifically, those based on pretrained Transformer encoders, using two randomization tests. The first compares the interpretations of two models that are identical except for their initializations. The second measures whether the interpretations differ between a model with trained parameters and a model with random parameters. Both tests show surprising deviations from expected behavior, raising questions about the extent of insights that practitioners may draw from interpretations.\n* Sergul Aydore, Haipeng Chen, Edward Choi, Jamie Hayes, Mario Fritz, Rachel Cummings, Krishnaram Kenthapadi, Organizers of the[ICLR 2021 Workshop on Synthetic Data Generation: Quality, Privacy, Bias] [[Workshop Webpage]] [[Amazon Science Article]].Abstract\nDespite the substantial benefits from using synthetic data, the process of synthetic data generation is still an ongoing technical challenge. Although the two scenarios of limited data and privacy concerns share similar technical challenges such as quality and fairness, they are often studied separately. This workshop aims at the intersection of these challenges of synthetic data generation, and hopes to shine a light on the solutions that address these challenges. The 1st Synthetic Data Generation workshop will be a virtual workshop at ICLR 2021 (held on May 8, 2021). Our goal is to advance the general discussion of the topic by highlighting contributions proposing innovative approaches integrating quality, privacy and bias aspects of synthetic data generation.\n* Sanjiv Das, Michele Donini, Jason Gelman, Kevin Haas, Mila Hardt, Jared Katzman, Krishnaram Kenthapadi, Pedro Larroy, Pinar Yilmaz, Bilal Zafar,[Fairness Measures for Machine Learning in Finance], The Journal of Financial Data Science, 2021 [[PDF]]Abstract\nWe present a machine learning pipeline for fairness-aware machine learning (FAML) in finance that encompasses metrics for fairness (and accuracy). Whereas accuracy metrics are well understood and the principal ones used frequently, there is no consensus as to which of several available measures for fairness should be used in a generic manner in the financial services industry. We explore these measures and discuss which ones to focus on, at various stages in the ML pipeline, pre-training and post-training, and we also examine simple bias mitigation approaches. Using a standard dataset we show that the sequencing in our FAML pipeline offers a cogent approach to arriving at a fair and accurate ML model. We discuss the intersection of bias metrics with legal considerations in the US, and the entanglement of explainability and fairness is exemplified in the case study. We discuss possible approaches for training ML models while satisfying constraints imposed from various fairness metrics, and the role of causality in assessing fairness.\n* Vijay Keswani, Matthew Lease, Krishnaram Kenthapadi,[Designing Closed Human-in-the-loop Deferral Pipelines],[Symposium on Biases in Human Computation and Crowdsourcing (BHCC)], 2021 [[arxiv]]Abstract\nIn hybrid human-machine deferral frameworks, a classifier can defer uncertain cases to human decision-makers (who are often themselves fallible). Prior work on simultaneous training of such classifier and deferral models has typically assumed access to an oracle during training to obtain true class labels for training samples, but in practice there often is no such oracle. In contrast, we consider a \"closed\" decision-making pipeline in which the same fallible human decision-makers used in deferral also provide training labels. How can imperfect and biased human expert labels be used to train a fair and accurate deferral framework? Our key insight is that by exploiting weak prior information, we can match experts to input examples to ensure fairness and accuracy of the resulting deferral framework, even when imperfect and biased experts are used in place of ground truth labels. The efficacy of our approach is shown both by theoretical analysis and by evaluation on two tasks.\n* Vijay Keswani, Matthew Lease, Krishnaram Kenthapadi,[Towards Unbiased and Accurate Deferral to Multiple Experts], AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society (AIES), 2021 (Oral Presentation) [[arxiv]] [[code]] [[poster]] [[video]]Abstract\nMachine learning models are often implemented in cohort with humans in the pipeline, with the model having an option to defer to a domain expert in cases where it has low confidence in its inference. Our goal is to design mechanisms for ensuring accuracy and fairness in such prediction systems that combine machine learning model inferences and domain expert predictions. Prior work on \"deferral systems\" in classification settings has focused on the setting of a pipeline with a single expert and aimed to accommodate the inaccuracies and biases of this expert to simultaneously learn an inference model and a deferral system. Our work extends this framework to settings where multiple experts are available, with each expert having their own domain of expertise and biases. We propose a framework that simultaneously learns a classifier and a deferral system, with the deferral system choosing to defer to one or more human experts in cases of input where the classifier has low confidence. We test our framework on a synthetic dataset and a content moderation dataset with biased synthetic experts, and show that it significantly improves the accuracy and fairness of the final predictions, compared to the baselines. We also collect crowdsourced labels for the content moderation task to construct a real-world dataset for the evaluation of hybrid machine-human frameworks and show that our proposed learning framework outperforms baselines on this real-world dataset as well.\n* Emily Diana, Wesley Gill, Michael Kearns, Krishnaram Kenthapadi, Aaron Roth,[Minimax Group Fairness: Algorithms and Experiments], AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society (AIES), 2021 (Oral Presentation) [[arxiv]] [[code]] [[poster]] [[video]]Abstract\nWe consider a recently introduced framework in which fairness is measured by worst-case outcomes across groups, rather than by the more standard differences between group outcomes. In this framework we provide provably convergent oracle-efficient learning algorithms (or equivalently, reductions to non-fair learning) for minimax group fairness. Here the goal is that of minimizing the maximum loss across all groups, rather than equalizing group losses. Our algorithms apply to both regression and classification settings and support both overall error and false positive or false negative rates as the fairness measure of interest. They also support relaxations of the fairness constraints, thus permitting study of the tradeoff between overall accuracy and minimax fairness. We compare the experimental behavior and performance of our algorithms across a variety of fairness-sensitive data sets and show empirical cases in which minimax fairness is strictly and strongly preferable to equal outcome notions.\n* Valerio Perrone, Michele Donini, Muhammad Bilal Zafar, Robin Schmucker, Krishnaram Kenthapadi, Cedric Archambeau,[Fair Bayesian Optimization], AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society (AIES), 2021 [[arxiv]] [[slides/video]] [[poster]] [Integration with[AutoGluon]:[Fair Bayesian Optimization tutorial]] [[Amazon Science Article]]. Earlier version received the**[Best Paper Award] **at the[ICML 2020 Workshop on AutoML] [[Amazon Science Article]].Abstract\nGiven the increasing importance of machine learning (ML) in our lives, several algorithmic fairness techniques have been proposed to mitigate biases in the outcomes of the ML models. However, most of these techniques are specialized to cater to a single family of ML models and a specific definition of fairness, limiting their adaptibility in practice. We introduce a general constrained Bayesian optimization (BO) framework to optimize the performance of any ML model while enforcing one or multiple fairness constraints. BO is a model-agnostic optimization method that has been successfully applied to automatically tune the hyperparameters of ML models. We apply BO with fairness constraints to a range of popular models, including random forests, gradient boosting, and neural networks, showing that we can obtain accurate and fair solutions by acting solely on the hyperparameters. We also show empirically that our approach is competitive with specialized techniques that enforce model-specific fairness constraints, and outperforms preprocessing methods that learn fair representations of the input data. Moreover, our method can be used in synergy with such specialized fairness techniques to tune their hyperparameters. Finally, we study the relationship between fairness and the hyperparameters selected by BO. We observe a correlation between regularization and unbiased models, explaining why acting on the hyperparameters leads to ML models that generalize well and are fair.\n* Nathalie Rauschmayr, Vikas Kumar, Rahul Huilgol, Andrea Olgiati, Satadal Bhattacharjee, Nihal Harish, Vandana Kannan, Amol Lele, Anirudh Acharya, Jared Nielsen, Lakshmi Ramakrishnan, Ishaaq Chandy, Ishan Bhatt, Zhihan Li, Kohen Chia, Neelesh Dodda, Jiacheng Gu, Miyoung Choi, Balajee Nagarajan, Jeffrey Geevarghes, Denis Davydenko, Sifei Li, Lu Huang, Edward Kim, Tyler Hill, Krishnaram Kenthapadi,[Amazon SageMaker Debugger: A System for Real-time Insights into Machine Learning Model Training], The Conference on Machine Learning and Systems (MLSys), 2021 [[MLSys 2021 Proceedings]] [[slides and video]] [[Amazon Science Article]]Abstract\nManual debugging is a common productivity drain in the machine learning (ML) lifecycle. Identifying underperforming training jobs requires constant developer attention and deep domain expertise. As state-of-the-art models grow in size and complexity, debugging becomes increasingly difficult. Just as unit tests boost traditional software development, an automated ML debugging library can save time and money. We present Amazon SageMaker Debugger, a machine learning feature that automatically identifies and stops underperforming training jobs. Debugger is a new feature of Amazon SageMaker that automatically captures relevant data during training and evaluation and presents it for online and offline inspection. Debugger helps users define a set of conditions, in the form of built-in or custom rules, that are applied to this data, thereby enabling users to catch training issues as well as monitor and debug ML model training in real-time. These rules save time and money by alerting the developer and terminating a problematic training job early.\n* Zeinab S. Jalali, Krishnaram Kenthapadi and Sucheta Soundarajan,[On Measuring the Diversity of Organizational Networks], International Conference on Complex Networks (CompleNet), 2021 [[arxiv]] [[code]]Abstract\nThe interaction patterns of employees in social and professional networks play an important role in the success of employees and organizations as a whole. However, in many fields there is a severe under-representation of minority groups; moreover, minority individuals may be segregated from the rest of the network or isolated from one another. While the problem of increasing the representation of minority groups in various fields has been well-studied, diversification in terms of numbers alone may not be sufficient: social relationships should also be considered. In this work, we consider the problem of assigning a set of employment candidates to positions in a social network so that diversity and overall fitness are maximized, and propose Fair Employee Assignment (FairEA), a novel algorithm for finding such a matching. The output from FairEA can be used as a benchmark by organizations wishing to evaluate their hiring and assignment practices. On real and synthetic networks, we demonstrate that FairEA does well at finding high-fitness, high-diversity matchings.\n* Krishnaram Kenthapadi, Invited panelist in the[Roundtable on Trustworthy AI (\"Principles of Trustworthy AI: Comparing Western and Non-Western conceptions of fairness and AI ethics\")] organized by the[Indo-US Science &amp; Technology Forum] as part of the[U.S.- India Artificial Intelligence (USIAI) Initiative], July 2021.Abstract\nA principled approach to Trustworthy AI needs to revisit, and think beyond, standard outcome-based notions (e.g., demographic parity, equal opportunity) that are suited to fair classification and aligned with certain civil rights and labor laws in the US. Ethical behavior and trustworthiness are inherently human qualities whose definitions vary based on the underlying context. What is justice, in theory and practice, also varies based on the law of the land. Following the western notion of business, applications of trustworthy AI are often seen as exchange of goods and services with utilities for various stakeholders, supported by certain beliefs about labor, ownership, property, rights. This track will focus on understanding the above in different non-Western contexts, important challenges therein, and fundamental trade-offs between multiple desired properties in trustworthy AI.\n* Krishnaram Kenthapadi,[Responsible AI in Industry: Practical Challenges and Lessons Learned], Invited talks at (1)[Responsible Use of Data seminar series, The Academic Fringe Festival, Delft University of Technology] [[Talk Details]], July 2021, (2) Flipkart Data Science Conference, June 2021, (3) Syracuse University, May 2021, (4)[The University of Waterloo ML+Logic Colloquium] [[Video Recording]], April 2021, (5) The University of British Columbia, Vancouver, March 2021 [[slides]]Abstract\nHow do we develop machine learning models and systems taking fairness, accuracy, explainability, and transparency into account? How do we protect the privacy of users when building large-scale AI based systems? Model fairness and explainability and protection of user privacy are considered prerequisites for building trust and adoption of AI systems in high stakes domains such as hiring, lending, and healthcare. We will first motivate the need for adopting a \"fairness, explainability, and privacy by design\" approach when developing AI/ML models and systems for different consumer and enterprise applications from the societal, regulatory, customer, end-user, and model developer perspectives. We will then focus on the application of responsible AI techniques in practice through industry case studies. We will discuss the sociotechnical dimensions and practical challenges, and conclude with the key takeaways and open challenges.\n* Krishnaram Kenthapadi, Nashlie Sephus,[Responsible AI in Industry: Practical Challenges and Lessons Learned], Invited talk at the[CVPR 2021 Workshop on Responsible Computer Vision] [[video]] [[slides]].Abstract\nHow do we develop machine learning models and systems taking fairness, accuracy, explainability, and transparency into account? How do we protect the privacy of users when building large-scale AI based systems? Model fairness and explainability and protection of user privacy are considered prerequisites for building trust and adoption of AI systems in high stakes domains such as hiring, lending, and healthcare. We will first motivate the need for adopting a \"fairness, explainability, and privacy by design\" approach when developing AI/ML models and systems for different consumer and enterprise applications from the societal, regulatory, customer, end-user, and model developer perspectives. We will then focus on the application of responsible AI techniques in practice through industry case studies. We will discuss the sociotechnical dimensions and practical challenges, and conclude with the key takeaways and open challenges.\n* Krishnaram Kenthapadi, Ben Packer, Mehrnoosh Sameki, Nashlie Sephus,**[Responsible AI in Industry: Practical Challenges and Lessons Learned] **, Tutorials at ACM Conference on Fairness, Accountability, and Transparency (FAccT 2021 [[Video Recording]]), AAAI Conference on Artificial Intelligence (AAAI 2021 [[Video Recording]]), The Web Conference (WWW 2021) [[slides]], and International Conference on Machine Learning (ICML 2021 [[slides]] [[Video Recording]]).Abstract\nArtificial Intelligence (AI) is increasingly playing an integral role in determining our day-to-day experiences. Increasingly, the applications of AI are no longer limited to search and recommendation systems, such as web search and movie and product recommendations, but AI is also being used in decisions and processes that are critical for individuals, businesses, and society. With web-based AI based solutions in areas such as hiring, lending, criminal justice, healthcare, and education, the resulting personal and professional implications of AI are far-reaching. With many factors playing a role in development and deployment of AI systems, they can exhibit different, and sometimes harmful, behaviors. For example, the training data often comes from society and real world, and thus it may reflect the society's biases and discrimination toward minorities and disadvantaged groups. For instance, minorities are known to face higher arrest rates for similar behaviors as the majority population, so building an AI system without compensating for this is likely to only exacerbate this prejudice. These concerns highlight the need for regulations, best practices, and practical tools to help data scientists and ML developers build AI systems that are secure, privacy-preserving, transparent, explainable, fair, and accountable --- to avoid unintended consequences and compliance challenges that can be harmful to individuals, businesses, and society.\nIn this tutorial, we will present an overview of responsible AI, highlighting model explainability, fairness, and privacy in AI, key regulations/laws, and techniques/tools for providing understanding around web-based AI/ML systems. Then, we will focus on the application of explainability, fairness assessment/unfairness mitigation, and privacy techniques in industry, wherein we present practical challenges/guidelines for using such techniques effectively and lessons learned from deploying models for several web-scale machine learning and data mining applications. We will present case studies across different companies, spanning application domains such as search and recommendation systems, hiring, sales, lending, and fraud detection. We will emphasize that topics related to responsible AI are socio-technical, that is, they are topics at the intersection of society and technology. The underlying challenges cannot be addressed by technologists alone; we need to work together with all key stakeholders --- such as customers of a technology, those impacted by a technology, and people with background in ethics and related disciplines --- and take their inputs into account while designing these systems. Finally, based on our experiences in industry, we will identify open problems and research directions for the data mining/machine learning community.\n* Krishnaram Kenthapadi,[Privacy in AI/ML Systems: Practical Challenges and Lessons Learned], Invited talk at the[EMNLP 2020 Workshop on Privacy in Natural Language Processing (PrivateNLP)] [[video]] [[slides]].Abstract\nHow do we protect the privacy of users when building large-scale AI based systems? How do we develop machine learning models and systems taking fairness, accuracy, explainability, and transparency into account? Model fairness and explainability and protection of user privacy are considered prerequisites for building trust and adoption of AI systems in high stakes domains. We will first motivate the need for adopting a “fairness, explainability, and privacy by design” approach when developing AI/ML models and systems for different consumer and enterprise applications from the societal, regulatory, customer, end-user, and model developer perspectives. We will then focus on the application of privacy-preserving AI techniques in practice through industry case studies. We will discuss the sociotechnical dimensions and practical challenges, and conclude with the key takeaways and open challenges.\n* Krishnaram Kenthapadi,[Fairness and Privacy in AI/ML Systems], Invited talk at LinkedIn - Microsoft Research - Indian Institute of Science Workshop on Fairness, Accountability, Transparency, and Ethics in Machine Learning 2020 [[slides]]Abstract\nHow do we protect privacy of users when building large-scale AI based systems? How do we develop machine learned models and systems taking fairness, accountability, and transparency into account? With the ongoing explosive growth of AI/ML models and systems, these are some of the ethical, legal, and technical challenges encountered by researchers and practitioners alike. In this talk, we will first motivate the need for adopting a \"fairness and privacy by design\" approach when developing AI/ML models and systems for different consumer and enterprise applications. We will then focus on the application of fairness-aware machine learning and privacy-preserving data mining techniques in practice, by presenting case studies spanning different LinkedIn applications (such as fairness-aware talent search ranking, privacy-preserving analytics, and LinkedIn Salary privacy &amp; security design), and conclude with the key takeaways and open challenges.\n* Vidya Ravipati, Erika Pelaez Coyotl, Ujjwal Ratan, Krishnaram Kenthapadi,[Fairness, Explainability, and Privacy in AI/ML Systems], NeurIPS 2020 Expo Talk [[slides and video]].Abstract\nHow do we develop machine learning models and systems taking fairness, accuracy, explainability, and transparency into account? How do we protect the privacy of users when building large-scale AI based systems? Model fairness and explainability and protection of user privacy are considered prerequisites for building trust and adoption of AI systems in high stakes domains such as lending and healthcare requiring reliability, safety, and fairness. We will first motivate the need for adopting a “fairness, explainability, and privacy by design” approach when developing AI/ML models and systems for different consumer and enterprise applications from the societal, regulatory, customer, end-user, and model developer perspectives. We will then focus on the application of fairness-aware ML, explainable AI, and privacy-preserving AI techniques in practice through industry case studies. We will discuss the sociotechnical dimensions and practical challenges, and conclude with the key takeaways and open challenges.\n* Krishnaram Kenthapadi, Fairness and Privacy in AI/ML Systems, Invited talk at[@Scale 2019] [[Video Recording]] and[Pinterest Distinguished Lecture], October 2019 [[slides]]Abstract\nHow do we protect privacy of users when building large-scale AI based systems? How do we develop machine learned models and systems taking fairness, accountability, and transparency into account? With the ongoing explosive growth of AI/ML models and systems, these are some of the ethical, legal, and technical challenges encountered by researchers and practitioners alike. In this talk, we will first motivate the need for adopting a \"fairness and privacy by design\" approach when developing AI/ML models and systems for different consumer and enterprise applications. We will then focus on the application of fairness-aware machine learning and privacy-preserving data mining techniques in practice, by presenting case studies spanning different LinkedIn applications (such as fairness-aware talent search ranking, privacy-preserving analytics, and LinkedIn Salary privacy &amp; security design), and conclude with the key takeaways and open challenges.\n* Sriram Vasudevan, Krishnaram Kenthapadi,[LiFT: A Scalable Framework for Measuring Fairness in ML Applications], ACM International Conference on Information and Knowledge Management (CIKM), 2020 [[arxiv]] [**Open Source Toolkit:[The LinkedIn Fairness Toolkit (LiFT)] **] [LinkedIn Engineering Blog Posts:[LiFT announcement];[Using LiFT in large-scale AI systems]] [**Press Coverage**:[Wall Street Journal];[VentureBeat];[Toolbox];[Software Development (SD) Times];[MarkTechPost]]Abstract\nMany internet applications are powered by machine learned models, which are usually trained on labeled datasets obtained through either implicit / explicit user feedback signals or human judgments. Since societal biases may be present in the generation of such datasets, it is possible for the trained models to be biased, thereby resulting in potential discrimination and harms for disadvantaged groups. Motivated by the need for understanding and addressing algorithmic bias in web-scale ML systems and the limitations of existing fairness toolkits, we present the LinkedIn Fairness Toolkit (LiFT), a framework for scalable computation of fairness metrics as part of large ML systems. We highlight the key requirements in deployed settings, and present the design of our fairness measurement system. We discuss the challenges encountered in incorporating fairness tools in practice and the lessons learned during deployment at LinkedIn. Finally, we provide open problems based on practical experience.\n* Cyrus DiCiccio, Sriram Vasudevan, Kinjal Basu, Krishnaram Kenthapadi, Deepak Agarwal,[Evaluating Fairness Using Permutation Tests], ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD), 2020 [[arxiv]] [[code]] [[slides]]Abstract\nMachine learning models are central to people's lives and impact society in ways as fundamental as determining how people access information. The gravity of these models imparts a responsibility to model developers to ensure that they are treating users in a fair and equitable manner. Before deploying a model into production, it is crucial to examine the extent to which its predictions demonstrate biases. This paper deals with the detection of bias exhibited by a machine learning model through statistical hypothesis testing. We propose a permutation testing methodology that performs a hypothesis test that a model is fair across two groups with respect to any given metric. There are increasingly many notions of fairness that can speak to different aspects of model fairness. Our aim is to provide a flexible framework that empowers practitioners to identify significant biases in any metric they wish to study. We provide a formal testing mechanism as well as extensive experiments to show how this method works in practice.\n* G Roshan Lal, Sahin Cem Geyik, Krishnaram Kenthapadi,[Fairness-Aware Online Personalization],[FAccTRec Workshop on Responsible Recommendation] at ACM Recommender Systems Conference (RecSys), 2020 [[arxiv]] [[code]]Abstract\nDecision making in crucial applications such as lending, hiring, and college admissions has witnessed increasing use of algorithmic models and techniques as a result of a confluence of factors such as ubiquitous connectivity, ability to collect, aggregate, and process large amounts of fine-grained data using cloud computing, and ease of access to applying sophisticated machine learning models. Quite often, such applications are powered by search and recommendation systems, which in turn make use of personalized ranking algorithms. At the same time, there is increasing awareness about the ethical and legal challenges posed by the use of such data-driven systems. Researchers and practitioners from different disciplines have recently highlighted the potential for such systems to discriminate against certain population groups, due to biases in the datasets utilized for learning their underlying recommendation models. We present a study of fairness in online personalization settings involving the ranking of individuals. Starting from a fair warm-start machine-learned model, we first demonstrate that online personalization can cause the model to learn to act in an unfair manner if the user is biased in his/her responses. For this purpose, we construct a stylized model for generating training data with potentially biased features as well as potentially biased labels and quantify the extent of bias that is learned by the model when the user responds in a biased manner as in many real-world scenarios. We then formulate the problem of learning personalized models under fairness constraints and present a regularization based approach for mitigating biases in machine learning. We demonstrate the efficacy of our approach through extensive simulations with different parameter settings.\n* Kathy Baxter, Yoav Schlesinger, Sarah Aerni, Lewis Baker, Julie Dawson, Krishnaram Kenthapadi, Isabel Kloumann, Hanna Wallach,[Bridging the gap from AI ethics research to practice], ACM Conference on Fairness, Accountability, and Transparency (FAccT) CRAFT Session, 2020Abstract\nThe study of fairness in machine learning applications has seen significant academic inquiry, research and publication in recent years. Concurrently, technology companies have begun to instantiate nascent program in AI ethics and product ethics more broadly. As a result of these efforts, AI ethics practitioners have piloted new processes to evaluate and ensure fairness in their machine learning applications. In this session, six industry practitioners, hailing from LinkedIn, Yoti, Microsoft, Pymetrics, Facebook, and Salesforce share insights from the work they have undertaken in the area of fairness, what has worked and what has not, lessons learned and best practices instituted as a result. Building on those insights, we discuss insights and brainstorm modalities through which to build upon the practitioners' work. Opportunities for further research or collaboration are identified, with the goal of developing a shared understanding of experiences and needs of AI ethics practitioners. Ultimately, the aim is to develop a playbook for more ethical and fair AI product development and deployment.\n* Krishnaram Kenthapadi, Stuart Ambler, Ahsan Chudhary, Mark Dietz, Sahin Cem Geyik, Krishnaram Kenthapadi, Ian Koeppe, Varun Mithal, Guillaume Saint-Jacques, Amir Sepehri, Thanh Tran, Sriram Vasudevan,[Fairness, Privacy, and Transparency by Design in AI/ML Systems], LinkedIn Engineering Blog Post, July 2019\n* Sahin Cem Geyik, Stuart Ambler, Krishnaram Kenthapadi,[Fairness-Aware Ranking in Search &amp; Recommendation Systems with Application to LinkedIn Talent Search], ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD), 2019 [[arxiv]]Abstract\nWe present a framework for quantifying and mitigating algorithmic bias in mechanisms designed for ranking individuals, typically used as part of web-scale search and recommendation systems. We first propose complementary measures to quantify bias with respect to protected attributes such as gender and age. We then present algorithms for computing fairness-aware re-ranking of results towards mitigating algorithmic bias. For a given search or recommendation task, our algorithms seek to achieve a desired distribution of top ranked results with respect to one or more protected attributes. We show that such a framework can be utilized to achieve fairness criteria such as*equality of opportunity*and*demographic parity*depending on the choice of the desired distribution. We evaluate the proposed algorithms via extensive simulations over different parameter choices, and study the effect of fairness-aware ranking on both bias and utility measures. We finally present the online A/B testing results from applying our framework towards representative ranking in LinkedIn Talent Search, and discuss the lessons learned in practice. Our approach resulted in tremendous improvement in the fairness metrics (nearly three fold increase in the number of search queries with representative results) without affecting the business metrics, which paved the way for deployment to 100% of*LinkedIn Recruiter*users worldwide. Ours is the first large-scale deployed framework for ensuring fairness in the hiring domain, with the potential positive impact for more than 630M LinkedIn members.\n* Krishna Gade, Sahin Cem Geyik, Krishnaram Kenthapadi, Varun Mithal, Ankur Taly,**[Explainable AI in Industry] **, Tutorials at ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining ([KDD 2019] [[slides]] [[proposal]]), ACM Conference on Fairness, Accountability, and Transparency ([FAccT 2020] [[slides]] [[proposal]] [[Video Recording]] ), AAAI Conference on Artificial Intelligence (Broader day-long tutorial from the following contributors: Freddy Lecue, Krishna Gade, Sahin Cem Geyik, Krishnaram Kenthapadi, Varun Mithal, Ankur Taly, Riccardo Guidotti, Pasquale Minervini;[AAAI 2020] [[slides]]), The Web Conference ([WWW 2020] [[slides]] [[proposal]]) [[Video Recording]] [[Amazon Science Article]]Abstract\nArtificial Intelligence is increasingly playing an integral role in determining our day-to-day experiences. Moreover, with proliferation of AI based solutions in areas such as hiring, lending, criminal justice, healthcare, and education, the resulting personal and professional implications of AI are far-reaching. The dominant role played by AI models in these domains has led to a growing concern regarding potential bias in these models, and a demand for model transparency and interpretability. In addition, model explainability is a prerequisite for building trust and adoption of AI systems in high stakes domains requiring reliability and safety such as healthcare and automated transportation, and critical industrial applications with significant economic implications such as predictive maintenance, exploration of natural resources, and climate change modeling.\nAs a consequence, AI researchers and practitioners have focused their attention on explainable AI to help them better trust and understand models at scale. The challenges for the research community include (i) defining model explainability, (ii) formulating explainability tasks for understanding model behavior and developing solutions for these tasks, and finally (iii) designing measures for evaluating the performance of models in explainability tasks.\nIn this tutorial, we will present an overview of model interpretability and explainability in AI, key regulations / laws, and techniques / tools for providing explainability as part of AI/ML systems. Then, we will focus on the application of explainability techniques in industry, wherein we present practical challenges / guidelines for effectively using explainability techniques and lessons learned from deploying explainable models for several web-scale machine learning and data mining applications. We will present case studies across different companies, spanning application domains such as hiring, sales, lending, and fraud detection. Finally, based on our experiences in industry, we will identify open problems and research directions for the data mining / machine learning community.\n* Sarah Bird, Ben Hutchinson, Krishnaram Kenthapadi, Emre Kiciman, Margaret Mitchell,**[Fairness-Aware Machine Learning: Practical Challenges and Lessons Learned] **, Tutorials at ACM International Conference on Web Search and Data Mining ([WSDM 2019] [[slides]] [[proposal]]), The Web Conference ([WWW 2019] [[slides]] [[proposal]]), and ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining ([KDD 2019] [[slides]] [[proposal]])Abstract\nResearchers and practitioners from different disciplines have highlighted the ethical and legal challenges posed by the use of machine learned models and data-driven systems, and the potential for such systems to discriminate against certain population groups, due to biases in algorithmic decision-making systems. This tutorial aims to present an overview of algorithmic bias / discrimination issues observed over the last few years and the lessons learned, key regulations and laws, and evolution of techniques for achieving fairness in machine learning systems. The tutorial will then focus on the application of fairness-aware machine learning techniques in practice, by presenting case studies from different technology companies. Based on our experiences in industry, we will identify open problems and research challenges for the data mining / machine learning community.\n* Krishnaram Kenthapadi, Ilya Mironov, Abhradeep Guha Thakurta,**[Privacy-preserving Data Mining in Industry] **, Tutorials at ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining ([KDD 2018] [[slides]]), ACM International Conference on Web Search and Data Mining ([WSDM 2019] [[slides]] [[proposal]]), and The Web Conference ([WWW 2019] [[slides]] [[proposal]])Abstract\nPreserving privacy of users is a key requirement of web-scale data mining applications and systems such as web search, recommender systems, crowdsourced platforms, and analytics applications, and has witnessed a renewed focus in light of recent data breaches and new regulations such as GDPR. In this tutorial, we will first present an overview of privacy breaches over the last two decades and the lessons learned, key regulations and laws, and evolution of privacy techniques leading to differential privacy definition / techniques. Then, we will focus on the application of privacy-preserving data mining techniques in practice, by presenting case studies such as Apple's differential privacy deployment for iOS / macOS, Google's RAPPOR, LinkedIn Salary, and Microsoft's differential privacy deployment for collecting Windows telemetry. We will conclude with open problems and challenges for the data mining / machine learning community, based on our experiences in industry.\n* Alexey Romanov, Maria De-Arteaga, Hanna Wallach, Jennifer Chayes, Christian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi, Anna Rumshisky, Adam Kalai,[What's in a name? Reducing bias in bios without access to protected attributes], Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 2019.**[Best Thematic Paper Award] **. [[arxiv]] [[ACL]] [[Alexey's talk video]] [[blog]]Abstract\nThere is a growing body of work that proposes methods for mitigating bias in machine learning systems. These methods typically rely on access to protected attributes such as race, gender, or age. However, this raises two significant challenges: (1) protected attributes may not be available or it may not be legal to use them, and (2) it is often desirable to simultaneously consider multiple protected attributes, as well as their intersections. In the context of mitigating bias in occupation classification, we propose a method for discouraging correlation between the predicted probability of an individual's true occupation and a word embedding of their name. This method leverages the societal biases that are encoded in word embeddings, eliminating the need for access to protected attributes. Crucially, it only requires access to individuals' names at training time and not at deployment time. We evaluate two variations of our proposed method using a large-scale dataset of online biographies. We find that both variations simultaneously reduce race and gender biases, with almost no reduction in the classifier's overall true positive rate.\n* Maria De-Arteaga, Alexey Romanov, Hanna Wallach, Jennifer Chayes, Christian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi, Adam Kalai,[Bias in Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting], ACM Conference on Fairness, Accountability, and Transparency (FAccT), 2019 [[arxiv]] [[code]] [[Maria's talk video]] [[Maria's talk slides]] [[blog]]Abstract\nWe present a large-scale study of gender bias in occupation classification, a task where the use of machine learning may lead to negative outcomes on peoples' lives. We analyze the potential allocation harms that can result from semantic representation bias. To do so, we study the impact on occupation classification of including explicit gender indicators -- such as first names and pronouns -- in different semantic representations of online biographies. Additionally, we quantify the bias that remains when these indicators are \"scrubbed,\" and describe proxy behavior that occurs in the absence of explicit gender indicators. As we demonstrate, differences in true positive rates between genders are correlated with existing gender imbalances in occupations, which may compound these imbalances.\n* Krishnaram Kenthapadi,[Fairness, Transparency, and Privacy in AI @ LinkedIn], Invited talk at QConSF 2018 [[slides]] [[video]][Related:[Fairness, Privacy, and Transparency by Design in AI/ML Systems], LinkedIn Engineering Blog Post, July 2019]Abstract\nHow do we protect privacy of users in large-scale systems? How do we ensure fairness and transparency when developing machine learned models? With the ongoing explosive growth of AI/ML models and systems, these are some of the ethical and legal challenges encountered by researchers and practitioners alike. In this talk, we will first present an overview of privacy breaches as well as algorithmic bias / discrimination issues observed in the Internet industry over the last few years and the lessons learned, key regulations and laws, and evolution of techniques for achieving privacy and fairness in data-driven systems. We will motivate the need for adopting a \"privacy and fairness by design\" approach when developing data-driven AI/ML models and systems for different consumer and enterprise applications. We will also focus on the application of privacy-preserving data mining and fairness-aware machine learning techniques in practice, by presenting case studies spanning different LinkedIn applications, and conclude with the key takeaways and open challenges.\n* Sahin Geyik, Krishnaram Kenthapadi,[Building Representative Talent Search at LinkedIn], LinkedIn Engineering Blog Post, October 2018 [**Press Coverage**:[MIT Technology Review (June 2021)];[Business Insider];[GeekWire];[Tech Crunch];[Fast Company];[CNBC]]\n* Krishnaram Kenthapadi, Thanh T. L. Tran,[PriPeARL: A Framework for Privacy-Preserving Analytics and Reporting at LinkedIn], ACM International Conference on Information and Knowledge Management (CIKM), 2018 [[arxiv]] [[slides]] [[LinkedIn Engineering Blog Post]]Abstract\nPreserving privacy of users is a key requirement of web-scale analytics and reporting applications, and has witnessed a renewed focus in light of recent data breaches and new regulations such as GDPR. We focus on the problem of computing robust, reliable analytics in a privacy-preserving manner, while satisfying product requirements. We present PriPeARL, a framework for privacy-preserving analytics and reporting, inspired by differential privacy. We describe the overall design and architecture, and the key modeling components, focusing on the unique challenges associated with privacy, coverage, utility, and consistency. We perform an experimental study in the context of ads analytics and reporting at LinkedIn, thereby demonstrating the tradeoffs between privacy and utility needs, and the applicability of privacy-preserving mechanisms to real-world data. We also highlight the lessons learned from the production deployment of our system at LinkedIn.\n* Rohan Ramanath, Hakan Inan, Gungor Polatkan, Bo Hu, Qi Guo, Cagri Ozcaglar, Xianren Wu, Krishnaram Kenthapadi, Sahin Cem Geyik,[Towards Deep and Representation Learning for Talent Search at LinkedIn], ACM International Conference on Information and Knowledge Management (CIKM), 2018 [[arxiv]]Abstract\nTalent search and recommendation systems at LinkedIn strive to match the potential candidates to the hiring needs of a recruiter or a hiring manager expressed in terms of a search query or a job posting. Recent work in this domain has mainly focused on linear models, which do not take complex relationships between features into account, as well as ensemble tree models, which introduce non-linearity but are still insufficient for exploring all the potential feature interactions, and strictly separate feature generation from modeling. In this paper, we present the results of our application of deep and representation learning models on LinkedIn Recruiter. Our key contributions include: (i) Learning semantic representations of sparse entities within the talent search domain, such as recruiter ids, candidate ids, and skill entity ids, for which we utilize neural network models that take advantage of LinkedIn Economic Graph, and (ii) Deep models for learning recruiter engagement and candidate response in talent search applications. We also explore learning to rank approaches applied to deep models, and show the benefits for the talent search use case. Finally, we present offline and online evaluation results for LinkedIn talent search and recommendation systems, and discuss potential challenges along the path to a fully deep model architecture. The challenges and approaches discussed generalize to any multi-faceted search engine.\n* Krishnaram Kenthapadi,[Privacy-preserving Analytics and Data Mining at LinkedIn: Practical Challenges &amp; Lessons Learned], Invited talks at the[\"Privacy in Graphs (PiG)\" workshop], co-organized by Lise Getoor at UC Santa Cruz, November 2018 and the \"Differential Privacy Deployed\" workshop, co-organized by Cynthia Dwork at Harvard, September 2018Abstract\nPreserving privacy of users is a key requirement of web-scale data mining, machine learning, and analytics applications, and has witnessed a renewed focus in light of recent data breaches and new regulations such as GDPR. In this talk, we will focus on the application of privacy-preserving data mining techniques in practice, by presenting two case studies at LinkedIn: differential privacy for LinkedIn analytics applications (https://arxiv.org/pdf/1809.07754), and LinkedIn Salary privacy design (https://arxiv.org/pdf/1705.06976), and reflect on the privacy challenges associated with applications enabled by the LinkedIn Economic Graph. The first part presents a framework to compute robust, privacy-preserving analytics, while the second part focuses on the privacy challenges/design for a large crowdsourced system (LinkedIn Salary). We will highlight the practical challenges/requirements, techniques, and lessons learned from deployment.\n* Stuart Ambler, Sahin Geyik, Krishnaram Kenthapadi, Fairness Aware Talent Search Ranking at LinkedIn, Microsoft's AI/ML conference (MLADS Spring 2018).**[Distinguished Contribution Award] **. [[Video of LinkedIn CEO, Jeff Weiner referring to our work &amp; this award].]Abstract\nRecently, policymakers, regulators, and advocates have raised awareness about the ethical, policy, and legal challenges posed by machine learning and data driven systems. In particular, they have expressed concerns about the potentially discriminatory impact of such systems, for example, due to inadvertent encoding of bias into automated decisions. In the context of LinkedIn Talent Search, our goal is to understand whether there is bias in our machine learning models, and then devise techniques to ensure fairness/representation in the ranked results. This work presents the first large-scale framework for ensuring fairness/representation in the hiring domain, with the potential positive impact for more than 500M LinkedIn members.\nWe propose complementary measures to quantify bias with respect to attributes such as gender and age, and perform a study of these measures in the context of LinkedIn Talent Search application. We then describe our approach for computing a fairness-aware reranking of results. We present the results from extensive evaluation of the effect of fairness-aware ranking on both bias and utility measures over a large LinkedIn dataset on talent search. Finally, we discuss fairness notions for broad classes of web applications, alternate approaches, and extensions.\n* Xi Chen, Yiqun Liu, Liang Zhang, Krishnaram Kenthapadi,[How LinkedIn Economic Graph Bonds Information and Product: Applications in LinkedIn Salary], ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD), 2018 [[arxiv]] [[LinkedIn Engineering Blog Post]]Abstract\nThe LinkedIn Salary product was launched in late 2016 with the goal of providing insights on compensation distribution to job seekers, so that they can make more informed decisions when discovering and assessing career opportunities. The compensation insights are provided based on data collected from LinkedIn members and aggregated in a privacy-preserving manner. Given the simultaneous desire for computing robust, reliable insights and for having insights to satisfy as many job seekers as possible, a key challenge is to reliably infer the insights at the company level when there is limited or no data at all. We propose a two-step framework that utilizes a novel, semantic representation of companies (Company2vec) and a Bayesian statistical model to address this problem. Our approach makes use of the rich information present in the LinkedIn Economic Graph, and in particular, uses the intuition that two companies are likely to be similar if employees are very likely to transition from one company to the other and vice versa. We compute embeddings for companies by analyzing the LinkedIn members' company transition data using machine learning algorithms, then compute pairwise similarities between companies based on these embeddings, and finally incorporate company similarities in the form of peer company groups as part of the proposed Bayesian statistical model to predict insights at the company level. We perform extensive validation using several different evaluation techniques, and show that we can significantly increase the coverage of insights while, in fact, even improving the quality of the obtained insights. For example, we were able to compute salary insights for 35 times as many title-region-company combinations in the U.S. as compared to previous work, corresponding to 4.9 times as many monthly active users. Finally, we highlight the lessons learned from deployment of our system.\n* Sahin Cem Geyik, Qi Guo, Bo Hu, Cagri Ozcaglar, Ketan Thakkar, Xianren Wu, Krishnaram Kenthapadi,[Talent Search and Recommendation Systems at LinkedIn: Practical Challenges and Lessons Learned], International ACM SIGIR Conference on Research &amp; Development in Information Retrieval (SIGIR), 2018 [[arxiv]] [[slides]] [[LinkedIn Engineering Blog Post]]Abstract\nLinkedIn Talent Solutions business contributes to around 65% of LinkedIn's annual revenue, and provides tools for job providers to reach out to potential candidates and for job seekers to find suitable career opportunities. LinkedIn's job ecosystem has been designed as a platform to connect job providers and job seekers, and to serve as a marketplace for efficient matching between potential candidates and job openings. A key mechanism to help achieve these goals is the LinkedIn Recruiter product, which enables recruiters to search for relevant candidates and obtain candidate recommendations for their job postings. In this work, we highlight a set of unique information retrieval, system, and modeling challenges associated with talent search and recommendation systems.\n* Krishnaram Kenthapadi, Stuart Ambler, Liang Zhang, Deepak Agarwal,[Bringing Salary Transparency to the World: Computing Robust Compensation Insights via LinkedIn Salary], ACM on Conference on Information and Knowledge Management (CIKM), 2017.**[Best Case Studies Paper Award] **. [[arxiv]] [[slides]] [[LinkedIn Engineering Blog Post]]Abstract\nThe recently launched LinkedIn Salary product has been designed with the goal of providing compensation insights to the world's professionals and thereby helping them optimize their earning potential. We describe the overall design and architecture of the statistical modeling system underlying this product. We focus on the unique data mining challenges while designing and implementing the system, and describe the modeling components such as Bayesian hierarchical smoothing that help to compute and present robust compensation insights to users. We report on extensive evaluation with nearly one year of de-identified compensation data collected from over one million LinkedIn users, thereby demonstrating the efficacy of the statistical models. We also highlight the lessons learned through the deployment of our system at LinkedIn.\n* Krishnaram Kenthapadi, Ahsan Chudhary, Stuart Ambler,[LinkedIn Salary: A System for Secure Collection and Presentation of Structured Compensation Insights to Job Seekers], IEEE Symposium on Privacy-Aware Computing (PAC), 2017 [[arxiv]] [[slides]] [[LinkedIn Engineering Blog Post]]Abstract\nOnline professional social networks such as LinkedIn have enhanced the ability of job seekers to discover and assess career opportunities, and the ability of job providers to discover and assess potential candidates. For most job seekers, salary (or broadly compensation) is a crucial consideration in choosing a new job. At the same time, job seekers face challenges in learning the compensation associated with different jobs, given the sensitive nature of compensation data and the dearth of reliable sources containing compensation data. Towards the goal of helping the world's professionals optimize their earning potential through salary transparency, we present LinkedIn Salary, a system for collecting compensation information from LinkedIn members and providing compensation insights to job seekers. We present the overall design and architecture, and describe the key components needed for the secure collection, de-identification, and processing of compensation data, focusing on the unique challenges associated with privacy and security. We perform an experimental study with more than one year of compensation submission history data collected from over 1.5 million LinkedIn members, thereby demonstrating the tradeoffs between privacy and modeling needs. We also highlight the lessons learned from the production deployment of this system at LinkedIn.\n* Krishnaram Kenthapadi, Benjamin Le, Ganesh Venkataraman,[Personalized Job Recommendation System at LinkedIn: Practical Challenges and Lessons Learned], ACM Conference on Recommender Systems (RecSys), 2017 [[PDF]] [[slides]]Abstract\nOnline professional social networks such as LinkedIn play a key role in helping job seekers find right career opportunities and job providers reach out to potential candidates. LinkedIn's job ecosystem has been designed to serve as a marketplace for efficient matching between potential candidates and job postings, and to provide tools to connect job seekers and job providers. LinkedIn's job recommendations product is a crucial mechanism to help achieve these goals, wherein personalized sets of recommended job postings are presented for members based on the structured, context data present in their profiles. We present how we formulated and addressed the unique information retrieval, system, and modeling challenges associated with personalized job recommendation, the overall system design and architecture, the challenges encountered in practice, and the lessons learned from the production deployment at LinkedIn.\n* Dhruv Arya, Ganesh Venkataraman, Aman Grover, Krishnaram Kenthapadi,[Candidate Selection for Large Scale Personalized Search and Recommender Systems], Tutorial at International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), 2017 [[PDF]] [[slides]] [[Github code assignments]]Abstract\nModern day social media search and recommender systems require complex query formulation that incorporates both user context and their explicit search queries. Users expect these systems to be fast and provide relevant results to their query and context. With millions of documents to choose from, these systems utilize a multi-pass scoring function to narrow the results and provide the most relevant ones to users. Candidate selection is required to sift through all the documents in the index and select a relevant few to be ranked by subsequent scoring functions. It becomes crucial to narrow down the document set while maintaining relevant ones in resulting set. In this tutorial we survey various candidate selection techniques and deep dive into case studies on a large scale social media platform. In the later half we provide hands-on tutorial where we explore building these candidate selection models on a real world dataset and see how to balance the tradeoff between relevance and latency.\n* Fedor Borisyuk, Liang Zhang, Krishnaram Kenthapadi,[LiJAR: A System for Job Application Redistribution towards Efficient Career Marketplace], ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2017 [[PDF]]Abstract\nOnline professional social networks such as LinkedIn serve as a marketplace, wherein job seekers can find right career opportunities and job providers can reach out to potential candidates. LinkedIn's job recommendations product is a key vehicle for efficient matching between potential candidates and job postings. However, we have observed in practice that a subset of job postings receive too many applications (due to several reasons such as the popularity of the company, nature of the job, etc.), while some other job postings receive too few applications. Both cases can result in job poster dissatisfaction and may lead to discontinuation of the associated job posting contracts. At the same time, if too many job seekers compete for the same job posting, each job seeker's chance of getting this job will be reduced. In the long term, this reduces the chance of users finding jobs that they really like on the site. Therefore, it becomes beneficial for the job recommendation system to consider values provided to both job seekers as well as job posters in the marketplace.\nIn this paper, we propose the job application redistribution problem, with the goal of ensuring that job postings do not receive too many or too few applications, while still providing job recommendations to users with the same level of relevance. We present a dynamic forecasting model to estimate the expected number of applications at the job expiration date, and algorithms to either promote or penalize jobs based on the output of the forecasting model. We also describe the system design and architecture for LiJAR, LinkedIn's Job Applications Forecasting and Redistribution system, which we have implemented and deployed in production. We perform extensive evaluation of LiJAR through both offline and online A/B testing experiments. Our production deployment of this system as part of LinkedIn's job recommendation engine has resulted in significant increase in the engagement of users for underserved jobs (6.5%) without affecting the user engagement in terms of the total number of job applications, thereby addressing the needs of job seekers as well as job providers simultaneously.\n* Jian Wang, Krishnaram Kenthapadi, Kaushik Rangadurai, David Hardtke,[Dionysius: A Framework for Modeling Hierarchical User Interactions in Recommender Systems], LinkedIn Technical Report, 2017Abstract\nWe address the following problem: How do we incorporate user item interaction signals as part of the relevance model in a large-scale personalized recommendation system such that, (1) the ability to interpret the model and explain recommendations is retained, and (2) the existing infrastructure designed for the (user profile) content-based model can be leveraged? We propose Dionysius, a hierarchical graphical model based framework and system for incorporating user interactions into recommender systems, with minimal change to the underlying infrastructure. We learn a hidden fields vector for each user by considering the hierarchy of interaction signals, and replace the user profile-based vector with this learned vector, thereby not expanding the feature space at all. Thus, our framework allows the use of existing recommendation infrastructure that supports content based features. We implemented and deployed this system as part of the recommendation platform at LinkedIn for more than one year. We validated the efficacy of our approach through extensive offline experiments with different model choices, as well as online A/B testing experiments. Our deployment of this system as part of the job recommendation engine resulted in significant improvement in the quality of retrieved results, thereby generating improved user experience and positive impact for millions of users.\n* Fedor Borisyuk, Krishnaram Kenthapadi, David Stein, Bo Zhao,[CaSMoS: A Framework for Learning Candidate Selection Models over Structured Queries and Documents], ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2016 [[PDF]]Abstract\nUser experience at social media and web platforms such as LinkedIn is heavily dependent on the performance and scalability of its products. Applications such as personalized search and recommendations require real-time scoring of millions of structured candidate documents associated with each query, with strict latency constraints. In such applications, the query incorporates the context of the user (in addition to search keywords if present), and hence can become very large, comprising of thousands of Boolean clauses over hundreds of document attributes. Consequently, candidate selection techniques need to be applied since it is infeasible to retrieve and score all matching documents from the underlying inverted index. We propose CaSMoS, a machine learned candidate selection framework that makes use of Weighted AND (WAND) query. Our framework is designed to prune irrelevant documents and retrieve documents that are likely to be part of the top-k results for the query. We apply a constrained feature selection algorithm to learn positive weights for feature combinations that are used as part of the weighted candidate selection query. We have implemented and deployed this system to be executed in real time using LinkedIn's Galene search platform. We perform extensive evaluation with different training data approaches and parameter settings, and investigate the scalability of the proposed candidate selection model. Our deployment of this system as part of LinkedIn's job recommendation engine has resulted in significant reduction in latency (up to 25%) without sacrificing the quality of the retrieved results, thereby paving the way for more sophisticated scoring models.\n**Document Understanding and Augmentation with Multimedia Content / Computational Education:**\n* Rakesh Agrawal, Sreenivas Gollapudi, Anitha Kannan, and Krishnaram Kenthapadi,[Similarity Search using Concept Graphs], ACM International Conference on Information and Knowledge Management (CIKM), 2014 [[PDF]]Abstract\nThe rapid proliferation of hand-held devices has led to the development of rich, interactive and immersive applications, such as e-readers for electronic books. These applications motivate retrieval systems that can implicitly satisfy any information need of the reader by exploiting the context of the user's interactions. Such retrieval systems differ from traditional search engines in that the queries constructed using the context are typically complex objects (including the document and its structure).\nIn this paper, we develop an efficient retrieval system, only assuming an oracle access to a traditional search engine that admits 'succinct' keyword queries for retrieving objects of a desired media type. As part of query generation, we first map the complex query object to a concept graph and then use the concepts along with their relationships in the graph to compute a small set of keyword queries to the search engine. Next, as part of the result generation, we aggregate the results of these queries to identify relevant web content of the desired type, thereby eliminating the need for explicitly computing similarity between the query object and all web content. We present a theoretical analysis of our approach and carry out a detailed empirical evaluation to show the practicality of the approach for the task of augmenting electronic documents with high quality videos from the web.\n* Rakesh Agrawal, Sreenivas Gollapudi, Anitha Kannan, and Krishnaram Kenthapadi,[Study Navigator: An Algorithmically Generated Aid for Learning from Electronic Textbooks], Journal of Educational Data Mining, vol. 6 (1), International Educational Data Mining Society, 2014.Abstract\nWe present*study navigator*, an algorithmically-generated aid for enhancing the experience of studying from electronic textbooks. The study navigator for a section of the book consists of helpful*concept references*for understanding this section. Each concept reference is a pair consisting of a concept phrase explained elsewhere and the link to the section in which it has been explained. We propose a novel reader model for textbooks and an algorithm for generating the study navigator based on this model. We also present an extension of the study navigator specialized to accommodate the information processing preference of the student. Specifically, this specialization allows a student to control the balance between references to sections that help refresh material already studied vs. sections that provide more advanced information. We also present two user studies that demonstrate the efficacy of the proposed system across textbooks on different subjects from different grades.\n* Marios Kokkodis, Anitha Kannan, and Krishnaram Kenthapadi,[Assigning Educational Videos at Appropriate Locations in Textbooks], International Conference on Educational Data Mining (EDM), 2014 [[Extended version]]Abstract\nThe emergence of tablet devices, cloud computing, and abundant online multimedia content presents new opportunities to transform traditional paper-based textbooks into tablet-based electronic textbooks. Towards this goal, techniques have been proposed to automatically augment textbook sections with relevant web content such as online educational videos. However, a highly relevant video can be created at a granularity that may not mimic the organization of the textbook. We focus on the video assignment problem: Given a candidate set of relevant educational videos for augmenting an electronic textbook, how do we assign the videos at appropriate locations in the textbook? We propose a rigorous formulation of the video assignment problem and present an algorithm for assigning each video to the optimum subset of logical units. Our experimental evaluation using a diverse collection of educational videos relevant to multiple chapters in a textbook demonstrates the efficacy of the proposed techniques for inferring the granularity at which a relevant video should be assigned.\n* Rakesh Agrawal, Maria Christoforaki, Sreenivas Gollapudi, Anitha Kannan, Krishnaram Kenthapadi, and Adith Swaminathan,[Mining Videos from the Web for Electronic Textbooks], International Conference on Formal Concept Analysis (ICFCA), 2014 [[Extended version]]Abstract\nWe propose a system for mining videos from the web for supplementing the content of electronic textbooks in order to enhance their utility. Textbooks are generally organized into sections such that each section explains very few concepts and every concept is primarily explained in one section. Building upon these principles from the education literature and drawing upon the theory of*Formal Concept Analysis*, we define the*focus*of a section in terms of a few*indicia*, which themselves are combinations of concept phrases uniquely present in the section. We identify videos relevant for a section by ensuring that at least one of the indicia for the section is present in the video and measuring the extent to which the video contains the concept phrases occurring in different indicia for the section. Our user study employing two corpora of textbooks on different subjects from two countries demonstrate that our system is able to find useful videos, relevant to individual sections.\n* Rakesh Agrawal, M. Hanif Jhaveri, and Krishnaram Kenthapadi,[Evaluating Educational Interventions at Scale], ACM Conference on Learning at Scale (L@S), 2014 [[PDF]]Abstract\nEducation and learning are currently undergoing transformative changes due to the emergence of tablet devices, cloud computing, and abundant online content. These trends present opportunities to transform traditional paper-based textbooks into tablet-based electronic textbooks, and to further enrich the educational experience by augmenting them with relevant supplementary materials. A natural question is whether this educational intervention, namely, enriching textbooks with relevant web articles, images and videos, is effective. It turns out that designing an experiment at scale for this purpose is nontrivial. We report on progress in designing and carrying out such an experiment.\n* Rakesh Agrawal, Sreenivas Gollapudi, Anitha Kannan, and Krishnaram Kenthapadi,[Studying from Electronic Textbooks], ACM International Conference on Information and Knowledge Management (CIKM), 2013 [[PDF]]Abstract\nWe present*study navigator*, an algorithmically-generated aid for enhancing the experience of studying from electronic textbooks. The study navigator for a section of the book consists of helpful concept references for understanding this section. Each*concept reference*is a pair consisting of a concept phrase explained elsewhere and the link to the section in which it has been explained. We propose a novel reader model for textbooks and an algorithm for generating the study navigator based on this model. We also present the results of an extensive user study that demonstrates the efficacy of the proposed system across textbooks on different subjects from different grades.\n* Rakesh Agrawal, Sunandan Chakraborty, Sreenivas Gollapudi, Anitha Kannan, and Krishnaram Kenthapadi,[Empowering Authors to Diagnose Comprehension Burden in Textbooks], ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2012 [[PDF]]Abstract\nGood textbooks are organized in a systematically progressive fashion so that students acquire new knowledge and learn new concepts based on known items of information. We provide a diagnostic tool for quantitatively assessing the comprehension burden that a textbook imposes on the reader due to non-sequential presentation of concepts. We present a formal definition of comprehension burden and propose an algorithmic approach for computing it. We apply the tool to a corpus of high school textbooks from India and empirically examine its effectiveness in helping authors identify sections of textbooks that can benefit from reorganizing the material presented.\n* Rakesh Agrawal, Sreenivas Gollapudi, Anitha Kannan, and Krishnaram Kenthapadi,[Electronic Textbooks and Data Mining (invited paper)], International Conference on Web-Age Information Management (WAIM), LNCS 7418, 2012Abstract\nEducation is known to be the key determinant of economic growth and prosperity [8,12]. While the issues in devising a high-quality educational system are multi-faceted and complex, textbooks are acknowledged to be the educational input most consistently associated with gains in student learning [11]. They are the primary conduits for delivering content knowledge to the students and the teachers base their lesson plans primarily on the material given in textbooks [7]. With the emergence of abundant online content, cloud computing, and electronic reading devices, textbooks are poised for transformative changes. Notwithstanding understandable misgivings (e.g. Gutenberg Elegies [6]), textbooks cannot escape what Walter Ong calls 'the technologizing of the word' [9]. The electronic format comes naturally to the current generation of 'digital natives' [10]. Inspired by the emergence of this new medium for \"printing\" and \"distributing\" textbooks, we present our early explorations into developing a data mining based approach for enhancing the quality of electronic textbooks. Speci�&#129;cally, we �&#129;rst describe a diagnostic tool for authors and educators to algorithmically identify de�&#129;ciencies in textbooks. We then discuss techniques for algorithmically augmenting different sections of a book with links to selective content mined from the Web. Our tool for diagnosing de�&#129;ciencies consists of two components. Abstracting from the education literature, we identify the following properties of good textbooks: (1) Focus : Each section explains few concepts, (2) Unity: For every concept, there is a unique section that best explains the concept, and (3) Sequentiality: Concepts are discussed in a sequential fashion so that a concept is explained prior to occurrences of this concept or any related concept. Further, the tie for precedence in presentation between two mutually related concepts is broken in favor of the more signi�&#129;cant of the two. The �&#129;rst component provides an assessment of the extent to which these properties are followed in a textbook and quanti�&#129;es the comprehension load that a textbook imposes on the reader due to non-sequential presentation of concepts [1,2]. The second component identi�&#129;es sections that are not written well and can bene�&#129;t from further exposition. We propose a probabilistic decision model for this purpose, which is based on the syntactic complexity of writing and the notion of the dispersion of key concepts mentioned in the section [4]. For augmenting a section of a textbook, we �&#129;rst identify the set of key concept phrases contained in a section. Using these phrases, we �&#129;nd web articles that represent the central concepts presented in the section and endow the section with links to them [5]. We also describe techniques for �&#129;nding images that are most relevant toa section of the textbook, while respecting the constraint that the same image is not repeated indifferent sections of the same chapter. We pose this problem of matching images to sections in a textbook chapter as an optimization problem and present an ef�&#129;cient algorithm for solving it [3].\n* Rakesh Agrawal, Sunandan Chakraborty, Sreenivas Gollapudi, Anitha Kannan, and Krishnaram Kenthapadi,[Quality of Textbooks: An Empirical Study], ACM Symposium on Computing for Development (ACM DEV), 2012 [[PDF]]Abstract\nTextbooks are the educational input most consistently associated with gains in student learning. Particularly in developing countries, textbooks are the primary conduits for delivering content knowledge to the students and the teachers base their lesson plans on the material given in textbooks. Abstracting from the education literature, we propose that well-written textbooks exhibit the following properties:\nFOCUS. Each section explains very few concepts.\nUNITY. For each concept, there is a unique section that best explains the concept.\nSEQUENTIALITY. Concepts are discussed in a sequential fashion: a concept is explained prior to occurrences of this concept or any related concept. Further, the tie for precedence in presentation between two mutually related concepts is broken in favor of the more significant of the two.\n* Rakesh Agrawal, Sreenivas Gollapudi, Anitha Kannan, and Krishnaram Kenthapadi,[Data Mining for Improving Textbooks (invited paper)], ACM SIGKDD Explorations Newsletter, December 2011 [[PDF]]Abstract\nWe present our early explorations into developing a data mining based approach for enhancing the quality of textbooks. We describe a diagnostic tool to algorithmically identify deficient sections in textbooks. We also discuss techniques for algorithmically augmenting textbook sections with links to selective content mined from the Web. Our evaluation, employing widely-used textbooks from India, indicates that developing technological approaches to help improve textbooks holds promise.\n* Rakesh Agrawal, Sreenivas Gollapudi, Anitha Kannan, and Krishnaram Kenthapadi,[Enriching Textbooks with Images], ACM International Conference on Information and Knowledge Management (CIKM), 2011 [[PDF]]Abstract\nTextbooks have a direct bearing on the quality of education imparted to the students. Therefore, it is of paramount importance that the educational content of textbooks should provide rich learning experience to the students. Recent studies on understanding learning behavior suggest that the incorporation of digital visual material can greatly enhance learning. However, textbooks used in many developing regions are largely text-oriented and lack good visual material. We propose techniques for finding images from the web that are most relevant for augmenting a section of the textbook, while respecting the constraint that the same image is not repeated in different sections of the same chapter. We devise a rigorous formulation of the image assignment problem and present a polynomial time algorithm for solving the problem optimally. We also present two image mining algorithms that utilize orthogonal signals and hence obtain different sets of relevant images. Finally, we provide an ensembling algorithm for combining the assignments. To empirically evaluate our techniques, we use a corpus of high school textbooks in use in India. Our user study utilizing the Amazon Mechanical Turk platform indicates that the proposed techniques are able to obtain images that can help increase the understanding of the textbook material.\n* Rakesh Agrawal, Sreenivas Gollapudi, Anitha Kannan, and Krishnaram Kenthapadi,[Enriching Education through Data Mining (invited paper)], International Conference on Pattern Recognition and Machine Intelligence (PReMI), LNCS 6744, 2011Abstract\nEducation is acknowledged to be the primary vehicle for improving the economic well-being of people [1,6]. Textbooks have a direct bearing on the quality of education imparted to the students as they are the primary conduits for delivering content knowledge [9]. They are also indispensable for fostering teacher learning and constitute a key component of the ongoing professional development of the teachers [5,8]. Many textbooks, particularly from emerging countries, lack clear and adequate coverage of important concepts [7]. In this talk, we present our early explorations into developing a data mining based approach for enhancing the quality of textbooks. We discuss techniques for algorithmically augmenting different sections of a book with links to selective content mined from the Web. For �&#129;nding authoritative articles, we �&#129;rst identify the set of key concept phrases contained in a section. Using these phrases, we �&#129;nd web (Wikipedia) articles that represent the central concepts presented in the section and augment the section with links to them [4]. We also describe a framework for �&#129;nding images that are most relevant to a section of the textbook, while respecting global relevancy to the entire chapter to which the section belongs. We pose this problem of matching images to sections in a textbook chapter as an optimization problem and present an e�&#402;cient algorithm for solving it [2].\n* Rakesh Agrawal, Sreenivas Gollapudi, Anitha Kannan, and Krishnaram Kenthapadi,[Identifying Enrichment Candidates in Textbooks], International World Wide Web Conference (WWW), 2011 [[PDF]]Abstract\nMany textbooks written in emerging countries lack clear and adequate coverage of important concepts. We propose a technological solution for algorithmically identifying those sections of a book that are not well written and could benefit from better exposition. We provide a decision model based on the syntactic complexity of writing and the dispersion of key concepts. The model parameters are learned using a tune set which is algorithmically generated using a versioned authoritative web resource as a proxy. We evaluate the proposed methodology over a corpus of Indian textbooks which demonstrates its effectiveness in identifying enrichment candidates.\n* Rakesh Agrawal, Sreenivas Gollapudi, Krishnaram Kenthapadi, Nitish Srivastava, and Raja Velu,[Enriching Textbooks Through Data Mining], ACM Symposium on Computing for Development (ACM DEV), 2010 [[PDF]]Abstract\nTextbooks play an important role in any educational system. Unfortunately, many textbooks produced in developing countries are not written well and they often lack adequate coverage of important concepts. We propose a technological solution to address this problem based on enriching textbooks with authoritative web content. We augment textbooks at the section level for key concepts discussed in the section. We use ideas from data mining for identifying the concepts that need augmentation as well as to determine the links to the authoritative content that should be used for augmentation. Our evaluation, employing textbooks from India, shows that we are able to enrich textbooks on different subjects and across different grades with high quality augmentations using automated techniques.\n* Rakesh Agrawal, Sreenivas Gollapudi, and Krishnaram Kenthapadi,[Computing for the Underserved (invited paper)], CCC Workshop on Computer Science and Global Development, Computing Community Consortium, August 2009Abstract\nWe make the case for focusing on economically disadvantaged and for developing technological solutions to address their needs. We then discuss compelling computing applications for the underserved: (a) viewing mobile phone as the first computing device (b) democratizing access to education.\n**Algorithmic Identification and Ranking of Twitter Groups:**\n* James Cook, Abhimanyu Das, Krishnaram Kenthapadi, and Nina Mishra,[Ranking Twitter Discussion Groups], ACM Conference on Online Social Networks (COSN), 2014 [[PDF]]Abstract\nA discussion group is a repeated, synchronized conversation organized around a specific topic. Groups are extremely valuable to the attendees, creating a sense of community among like-minded users. While groups may involve many users, there are many outside the group that would benefit from participation. However, finding the right group is not easy given their quantity and given topic overlap. We study the following problem: given a search query, find a good ranking of discussion groups. We describe a random walk model for how users select groups: starting with a group relevant to the query, a hypothetical user repeatedly selects an authoritative user in the group and then moves to a group according to what the authoritative user prefers. The stationary distribution of this walk yields a group ranking. We analyze this random walk model, demonstrating that it enjoys many natural properties of a desirable ranking algorithm. We study groups on Twitter where conversations can be organized via pre-designated hashtags. These groups are an emerging phenomenon and there are at least tens of thousands in existence today according to our calculations. Via an extensive collection of experiments on one year of tweets, we show that our model effectively ranks groups, outperforming several baseline solutions.\n* James Cook, Krishnaram Kenthapadi, and Nina Mishra,[Group Chats on Twitter], International World Wide Web Conference (WWW), 2013 [[PDF]]Abstract\nWe report on a new kind of group conversation on Twitter that we call a group chat. These chats are periodic, synchronized group conversations focused on specific topics and they exist at a massive scale. The groups and the members of these groups are not explicitly known. Rather, members agree on a hashtag and a meeting time (e.g., 3pm Pacific Time every Wednesday) to discuss a subject of interest. Topics of these chats are numerous and varied. Some are serious: for example, there are support groups for post-partum depression and borderline personality disorder. Others are about a passionate interest: topics include skiing, photography, movies, wine and foodie communities. We develop a definition of a group that is inspired by how sociologists define groups and present an algorithm for discovering groups. We prove that our algorithms find all groups under certain assumptions. While these groups are of course known to the people who participate in the discussions, what we do not believe is known is the number and variety of groups. We provide some insight into the nature of these groups based on two years of tweets.\n**Web Search and Related Topics:**\n* Marc Najork, Dennis Fetterly, Alan Halverson, Krishnaram Kenthapadi, and Sreenivas Gollapudi,[Of Hammers and Nails: An Empirical Comparison of Three Paradigms for Processing Large Graphs], ACM International Conference on Web Search and Data Mining (WSDM), 2012 [[PDF]]Abstract\nMany phenomena and artifacts such as road networks, social networks and the web can be modeled as large graphs and analyzed using graph algorithms. However, given the size of the underlying graphs, efficient implementation of basic operations such as connected component analysis, approximate shortest paths, and link-based ranking (e.g. PageRank) becomes challenging.\nThis paper presents an empirical study of computations on such large graphs in three well-studied platform models, viz., a relational model, a data-parallel model, and a special-purpose in-memory model. We choose a prototypical member of each platform model and analyze the computational efficiencies and requirements for five basic graph operations used in the analysis of real-world graphs viz., PageRank, SALSA, Strongly Connected Components (SCC), Weakly Connected Components (WCC), and Approximate Shortest Paths (ASP). Further, we characterize each platform in terms of these computations using model-specific implementations of these algorithms on a large web graph. Our experiments show that there is no single platform that performs best across different classes of operations on large graphs. While relational databases are powerful and flexible tools that support a wide variety of computations, there are computations that benefit from using special-purpose storage systems and others that can exploit data-parallel platforms.\n* Shuai Ding, Sreenivas Gollapudi, Samuel Ieong, Krishnaram Kenthapadi, and Alexandros Ntoulas,[Indexing Strategies for Graceful Degradation of Search Quality], International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), 2011 [[PDF]]Abstract\nLarge web search engines process billions of queries each day over tens of billions of documents with often very stringent requirements for a user�&#8364;&#8482;s search experience, in particular, low latency and highly relevant search results. Index generation and serving are key to satisfying both these requirements. For example, the load to search engines can vary drastically when popular events happen around the world. In the case when the load is exceeding what the search engine can serve, queries will get dropped. This results in an ungraceful degradation in search quality. Another example that could increase the query load and affect the user�&#8364;&#8482;s search experience are ambiguous queries which often result in the execution of multiple query alterations in the back end.\nIn this paper, we look into the problem of designing robust indexing strategies, i.e. strategies that allow for a graceful degradation of search quality in both the above scenarios. We study the problems of index generation and serving using the notions of document allocation, server selection, and document replication. We explore the space of efficient algorithms for these problems and empirically corroborate with existing theory that it is hard to optimally solve the allocation and selection problems without any replication. We propose a greedy replication algorithm and study its performance under different choices of allocation and selection. Further, we show that under random selection and allocation, our algorithm is optimal.\n* Aleksandra Korolova, Krishnaram Kenthapadi, Nina Mishra, and Alex Ntoulas,[Releasing Search Queries and Clicks Privately], International World Wide Web Conference (WWW), 2009 [[PDF]] (**[Best Paper Award Finalist]; Media coverage:[New Scientist] [[PDF]],[ACM TechNews],[TechFlash], and[other sources] **)Abstract\nThe question of how to publish an anonymized search log was brought to the forefront by a well-intentioned, but privacy-unaware AOL search log release. Since then a series of ad-hoc techniques have been proposed in the literature, though none are known to be provably private. In this paper, we take a major step towards a solution: we show how queries, clicks and their associated perturbed counts can be published in a manner that rigorously preserves privacy. Our algorithm is decidedly simple to state, but non-trivial to analyze. On the opposite side of privacy is the question of whether the data we can safely publish is of any use. Our findings offer a glimmer of hope: we demonstrate that a non-negligible fraction of queries and clicks can indeed be safely published via a collection of experiments on a real search log. In addition, we select an application, keyword generation, and show that the keyword suggestions generated from the perturbed data resemble those generated from the original data.\n* Rakesh Agrawal, Alan Halverson, Krishnaram Kenthapadi, Nina Mishra, and Panayiotis Tsaparas,[Generating Labels from Clicks], ACM International Conference on Web Search and Data Mining (WSDM), 2009 [[PDF]]Abstract\nThe ranking function used by search engines to order results is learned from labeled training data. Each training point is a (query, URL) pair that is labeled by a human judge who assigns a score of Perfect, Excellent, etc., depending on how well the URL matches the query. In this paper, we study whether clicks can be used to automatically generate good labels. Intuitively, documents that are clicked (resp., skipped) in aggregate can indicate relevance (resp., lack of relevance). We give a novel way of transforming clicks into weighted, directed graphs inspired by eye-tracking studies and then devise an objective function for finding cuts in these graphs that induce a good labeling. In its full generality, the problem is NP-hard, but we show that, in the case of two labels, an optimum labeling can be found in linear time. For the more general case, we propose heuristic solutions. Experiments on real click logs show that clickbased labels align with the opinion of a panel of judges, especially as the consensus of the panel grows stronger.\n**Large Graph/Data Algorithms:**\n* Marc Najork, Dennis Fetterly, Alan Halverson, Krishnaram Kenthapadi, and Sreenivas Gollapudi,[Of Hammers and Nails: An Empirical Comparison of Three Paradigms for Processing Large Graphs], ACM International Conference on Web Search and Data Mining (WSDM), 2012 [[PDF]]Abstract\nMany phenomena and artifacts such as road networks, social networks and the web can be modeled as large graphs and analyzed using graph algorithms. However, given the size of the underlying graphs, efficient implementation of basic operations such as connected component analysis, approximate shortest paths, and link-based ranking (e.g. PageRank) becomes challenging.\nThis paper presents an empirical study of computations on such large graphs in three well-studied platform models, viz., a relational model, a data-parallel model, and a special-purpose in-memory model. We choose a prototypical member of each platform model and analyze the computational efficiencies and requirements for five basic graph operations used in the analysis of real-world graphs viz., PageRank, SALSA, Strongly Connected Components (SCC), Weakly Connected Components (WCC), and Approximate Shortest Paths (ASP). Further, we characterize each platform in terms of these computations using model-specific implementations of these algorithms on a large web graph. Our experiments show that there is no single platform that performs best across different classes of operations on large graphs. While relational databases are powerful and flexible tools that support a wide variety of computations, there are computations that benefit from using special-purpose storage systems and others that can exploit data-parallel platforms.\n* Krishnaram Kenthapadi and Rina Panigrahy,[Balanced Allocation on Graphs], ACM-SIAM Symposium on Discrete Algorithms (SODA), 2006 [[arxiv]] (**Student Best Paper Award**)Abstract\nIt is well known that if $n$ balls are inserted into $n$ bins, with high probability, the bin with maximum load contains $(1+ o(1)) \\\\log n / \\\\log\\\\log n$ balls. Azar, Broder, Karlin, and Upfal [ABKU99] showed that instead of choosing one bin, if $d \\\\ge 2$ bins are chosen at random and the ball inserted into the least loaded of the $d$ bins, the maximum load reduces drastically to $\\\\log\\\\log n /\\\\log d + O(1)$. In this paper, we study the two choice balls and bins process when balls are not allowed to choose any two random bins, but only bins that are connected by an edge in an underlying graph. We show that for $n$ balls and $n$ bins, if the graph is almost regular with degree $n^\\\\epsilon$, where $\\\\epsilon$ is not too small, the previous bounds on the maximum load continue to hold. Precisely, the maximum load is $\\\\log \\\\log n + O(1/\\\\epsilon) + O(1)$. So even if the graph has degree $n^{\\\\Omega(1/\\\\log\\\\log n)}$, the maximum load is $O(\\\\log\\\\log n)$. For general $\\\\Delta$-regular graphs, we show that the maximum load is $\\\\log\\\\log n + O(\\\\frac{\\\\log n}{\\\\log (\\\\Delta/\\\\log^4 n)}) + O(1)$ and also provide an almost matching lower bound of $\\\\log \\\\log n + \\\\frac{\\\\log n}{\\\\log (\\\\Delta \\\\log n)}$. Further this does not hold for non-regular graphs even if the minimum degree is high.\nV{\\\\\"o}cking [Voc99] showed that the maximum bin size with $d$ choice load balancing can be further improved to $O(\\\\log\\\\log n /d)$ by breaking ties to the left. This requires $d$ random bin choices. We show that such bounds can be achieved by making only two random accesses and querying $d/2$ contiguous bins in each access. By grouping a sequence of $n$ bins into $2n/d$ groups, each of $d/2$ consecutive bins, if each ball chooses two groups at random and inserts the new ball into the least-loaded bin in the lesser loaded group, then the maximum load is $O(\\\\log\\\\log n/d)$ with high probability. Furthermore, it also turns out that this partitioning into groups of size $d/2$ is also essential in achieving this bound, that is, instead of choosing two random groups, if we simply choose two random sets of $d/2$ consecutive bins, then the maximum load jumps to $\\\\Omega(\\\\log\\\\log n /\\\\log d)$.\n* Krishnaram Kenthapadi and Gurmeet Singh Manku,[Decentralized Algorithms using both Local and Random Probes for P2P Load Balancing], ACM Symposium on Parallelism in Algorithms and Architectures (SPAA), 2005 [[PDF]]Abstract\nWe study randomized algorithms for placing a sequence of n nodes on a circle with unit perimeter. Nodes divide the circle into disjoint arcs. We desire that a newly-arrived node (which is oblivious of its index in the sequence) choose its position on the circle by learning the positions of as few existing nodes as possible. At the same time, we desire that that the variation in arc-lengths be small. To this end, we propose a new algorithm that works as follows: The kth node chooses r random points on the circle, inspects the sizes of v arcs in the vicinity of each random point, and places itself at the mid-point of the largest arc encountered. We show that for any combination of r and v satisfying $rv \\\\ge c \\\\log k$, where c is a small constant, the ratio of the largest to the smallest arc-length is at most eight w.h.p., for an arbitrarily long sequence of n nodes. This strategy of node placement underlies a novel decentralized load-balancing algorithm that we propose for Distributed Hash Tables (DHTs) in peer-to-peer environments.\nUnderlying the analysis of our algorithm is*Structured Coupon Collection*over n/b disjoint cliques with b nodes per clique, for any $n, b \\\\ge 1$. Nodes are initially uncovered. At each step, we choose d nodes independently and uniformly at random. If all the nodes in the corresponding cliques are covered, we do nothing. Otherwise, from among the chosen cliques with at least one uncovered node, we select one at random and cover an uncovered node within that clique. We show that as long as $bd \\\\ge c \\\\log n$, O(n) steps are sufficient to cover all nodes w.h.p. and each of the first $\\\\Omega(n)$ steps succeeds in covering a node w.h.p. These results are then utilized to analyze a stochastic process for growing binary trees that are highly balanced - the leaves of the tree belong to at most four different levels with high probability.\n**Privacy:**\n* Krishnaram Kenthapadi, Nina Mishra, and Kobbi Nissim,[Denials Leak Information: Simulatable Auditing], Journal of Computer and System Sciences, vol. 79 (8), pp. 1322-1340, Elsevier, December 2013Abstract\nImagine a data set consisting of private information about individuals. The online query auditing problem is: given a sequence of queries that have already been posed about the data, their corresponding answers and given a new query, deny the answer if privacy can be breached or give the true answer otherwise. We investigate the fundamental problem that query denials leak information. This problem was largely overlooked in previous work on auditing. Because of this oversight, some of the previously suggested auditors can be used by an attacker to compromise the privacy of a large fraction of the individuals in the data. To overcome this problem, we introduce a new model called simulatable auditing where query denials provably do not leak information. We present a simulatable auditing algorithm for max queries under the classical definition of privacy where a breach occurs if a sensitive value is fully compromised. Because of the known limitations of the classical definition of compromise, we describe a probabilistic notion of (partial) compromise, closely related to the notion of semantic security. We demonstrate that sum queries can be audited in a simulatable fashion under probabilistic compromise, making some distributional assumptions.\n* Krishnaram Kenthapadi, Aleksandra Korolova, Ilya Mironov, and Nina Mishra,[Privacy via the Johnson-Lindenstrauss Transform], Journal of Privacy and Confidentiality, vol. 5 (1), July 2013Abstract\nSuppose that party A collects private information about its users, where each user�&#8364;&#8482;s data is represented as a bit vector. Suppose that party B has a proprietary data mining algorithm that requires estimating the distance between users, such as clustering or nearest neighbors. We ask if it is possible for party A to publish some information about each user so that B can estimate the distance between users without being able to infer any private bit of a user. Our method involves projecting each user�&#8364;&#8482;s representation into a random, lower-dimensional space via a sparse Johnson-Lindenstrauss transform and then adding Gaussian noise to each entry of the lower-dimensional representation. We show that the method preserves differential privacy�&#8364;&#8221;where the more privacy is desired, the larger the variance of the Gaussian noise. Further, we show how to approximate the true distances between users via only the lower-dimensional, perturbed data. Finally, we consider other perturbation methods such as randomized response and draw comparisons to sketch-based methods. While the goal of releasing user-specific data to third parties is more broad than preserving distances, this work shows that distance computations with privacy is an achievable goal.\n* Gagan Aggarwal, Tomas Feder, Krishnaram Kenthapadi, Samir Khuller, Rina Panigrahy, Dilys Thomas, and An Zhu,[Achieving anonymity via clustering], ACM Transactions on Algorithms (TALG), vol. 6, no. 3, pp. 49:1-49:19, ACM, July 2010 [[PDF]]Abstract\nPublishing data for analysis from a table containing personal records, while maintaining individual privacy, is a problem of increasing importance today. The traditional approach of deidentifying records is to remove identifying fields such as social security number, name, etc. However, recent research has shown that a large fraction of the U.S. population can be identified using nonkey attributes (called quasi-identifiers) such as date of birth, gender, and zip code. The k-anonymity model protects privacy via requiring that nonkey attributes that leak information are suppressed or generalized so that, for every record in the modified table, there are at least k�&#710;&#8217;1 other records having exactly the same values for quasi-identifiers. We propose a new method for anonymizing data records, where quasi-identifiers of data records are first clustered and then cluster centers are published. To ensure privacy of the data records, we impose the constraint that each cluster must contain no fewer than a prespecified number of data records. This technique is more general since we have a much larger choice for cluster centers than k-anonymity. In many cases, it lets us release a lot more information without compromising privacy. We also provide constant factor approximation algorithms to come up with such a clustering. This is the first set of algorithms for the anonymization problem where the performance is independent of the anonymity parameter k. We further observe that a few outlier points can significantly increase the cost of anonymization. Hence, we extend our algorithms to allow an εfraction of points to remain unclustered, that is, deleted from the anonymized publication. Thus, by not releasing a small fraction of the database records, we can ensure that the data published for analysis has less distortion and hence is more useful. Our approximation algorithms for new clustering objectives are of independent interest and could be applicable in other clustering scenarios as well.\n* Aleksandra Korolova, Krishnaram Kenthapadi, Nina Mishra, and Alex Ntoulas,[Releasing Search Queries and Clicks Privately], International World Wide Web Conference (WWW), 2009 [[PDF]] (**[Best Paper Award Finalist]; Media coverage:[New Scientist] [[PDF]],[ACM TechNews],[TechFlash], and[other sources] **)Abstract\nThe question of how to publish an anonymized search log was brought to the forefront by a well-intentioned, but privacy-unaware AOL search log release. Since then a series of ad-hoc techniques have been proposed in the literature, though none are known to be provably private. In this paper, we take a major step towards a solution: we show how queries, clicks and their associated perturbed counts can be published in a manner that rigorously preserves privacy. Our algorithm is decidedly simple to state, but non-trivial to analyze. On the opposite side of privacy is the question of whether the data we can safely publish is of any use. Our findings offer a glimmer of hope: we demonstrate that a non-negligible fraction of queries and clicks can indeed be safely published via a collection of experiments on a real search log. In addition, we select an application, keyword generation, and show that the keyword suggestions generated from the perturbed data resemble those generated from the original data.\n* Shubha Nabar, Krishnaram Kenthapadi, Nina Mishra, and Rajeev Motwani,[A Survey of Query Auditing Techniques for Data Privacy], Chapter in*Privacy-Preserving Data Mining: Models and Algorithms*, Kluwer Academic Publishers, 2008Abstract\nThis chapter is a survey of query auditing techniques for detecting and preventing disclosures in a database containing private data. Informally, auditing is the process of examining past actions to check whether they were in conformance with official policies. In the context of database systems with specific data disclosure policies, auditing is the process of examining queries that were answered in the past to determine whether answers to these queries could have been used by an individual to ascertain confidential nformation forbidden by the disclosure policies. Techniques used for detecting disclosures could potentially also be used or extended to prevent disclosures, and so in addition to the retroactive auditing mentioned above, researchers have also studied an online variant of the auditing problem wherein the task of an online auditor is to deny queries that could potentially cause a breach of privacy.\n* Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor,[Our Data, Ourselves: Privacy Via Distributed Noise Generation], International Conference on the Theory and Applications of Cryptographic Techniques, Advances in Cryptology (EUROCRYPT), 2006Abstract\nIn this work we provide efficient distributed protocols for generating shares of random noise, secure against malicious participants. The purpose of the noise generation is to create a distributed implementation of the privacy-preserving statistical databases described in recent papers [14,4,13]. In these databases, privacy is obtained by perturbing the true answer to a database query by the addition of a small amount of Gaussian or exponentially distributed random noise. The computational power of even a simple form of these databases, when the query is just of the form $\\\\sum\\_i f(d\\_i)$, that is, the sum over all rows i in the database of a function f applied to the data in row i, has been demonstrated in [4]. A distributed implementation eliminates the need for a trusted database administrator.\nThe results for noise generation are of independent interest. The generation of Gaussian noise introduces a technique for distributing shares of many unbiased coins with fewer executions of verifiable secret sharing than would be needed using previous approaches (reduced by a factor of n). The generation of exponentially distributed noise uses two shallow circuits: one for generating many arbitrarily but identically biased coins at an amortized cost of two unbiased random bits apiece, independent of the bias, and the other to combine bits of appropriate biases to obtain an exponential distribution.\n* Shubha U. Nabar, Bhaskara Marthi, Krishnaram Kenthapadi, Nina Mishra, and Rajeev Motwani,[Towards Robustness in Query Auditing], International Conference on Very Large Data Bases (VLDB), 2006Abstract\nWe consider the online query auditing problem for a database containing private information about individuals: given a sequence of queries posed by an attacker, when should queries be denied to prevent privacy breaches. Historically, all research in auditing has focused on static datasets and known algorithms do not work in the presence of updates. We consider a new auditing problem where records can be inserted, deleted or modified and obtain algorithms for auditing sum queries, max queries and bags of max and min queries in this scenario under the classical notion of compromise. In [KMN05], a new probabilistic definition of compromise was proposed due to the limitations of classical compromise. Since this is a recent development, not much is known about auditing different kinds of queries under this definition. We obtain new algorithms for auditing max queries and bags of max and min queries under probabilistic compromise. We conclude with a study of a certain aspect of the utility delivered by an auditing scheme and obtain initial results for the utility of sum auditing under classical compromise.\n* Gagan Aggarwal, Tomas Feder, Krishnaram Kenthapadi, Samir Khuller, Rina Panigrahy, Dilys Thomas, and An Zhu,[Achieving Anonymity via Clustering], ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems (PODS), 2006 [[PDF]]Abstract\nPublishing data for analysis from a table containing personal records, while maintaining individual privacy, is a problem of increasing importance today. The traditional approach of de-identifying records is to remove identifying fields such as social security number, name etc. However, recent research has shown that a large fraction of the US population can be identified using non-key attributes (called quasi-identifiers) such as date of birth, gender, and zip code. The k-anonymity model protects privacy via requiring that non-key attributes that leak information are suppressed or generalized so that, for every record in the modified table, there are at least k�&#710;&#710;&#8217;&#8217;1 other records having exactly the same values for quasi-identifiers. We propose a new method for anonymizing data records, where quasi-identifiers of data records are first clustered and then cluster centers are published. To ensure privacy of the data records, we impose the constraint that each cluster must contain no fewer than a pre-specified number of data records. This technique is more general since we have a much larger choice for cluster centers than k-Anonymity. In many cases, it lets us release a lot more information without compromising privacy. We also provide constant factor approximation algorithms to come up with such a clustering. This is the first set of algorithms for the anonymization problem where the performance is independent of the anonymity parameter k. We further observe that a few outlier points can significantly increase the cost of anonymization. Hence, we extend our algorithms to allow an $\\\\epsilon$ fraction of points to remain unclustered, i.e., deleted from the anonymized publication. Thus, by not releasing a small fraction of the database records, we can ensure that the data published for analysis has less distortion and hence is more useful. Our approximation algorithms for new clustering objectives are of independent interest and could be applicable in other clustering scenarios as well.\n* Gagan Aggarwal, Mayank Bawa, Prasanna Ganesan, Hector Garcia-Molina, Krishnaram Kenthapadi, Rajeev Motwani, Utkarsh Srivastava, Dilys Thomas, and Ying Xu,[Two Can Keep a Secret: A Distributed Architecture for Secure Database Services], Conference on Innovative Data Systems Research (CIDR), 2005Abstract\nRecent trends towards database outsourcing, as well as concerns and laws governing data privacy, have led to great interest in enabling secure database services. Previous approaches to enabling such a service have been based on data encryption, causing a large overhead in query processing. We propose a new, distributed architecture that allows an organization to outsource its data management to two untrusted servers while preserving data privacy. We show how the presence of two servers enables efficient partitioning of data so that the contents at any one server are guaranteed not to breach data privacy. We show how to optimize and execute queries in this architecture, and discuss new challenges that emerge in designing the database schema.\n* Krishnaram Kenthapadi, Nina Mishra, and Kobbi Nissim,[Simulatable Auditing], ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems (PODS), 2005 [[PDF]]Abstract\nGiven a data set consisting of private information about individuals, we consider the*online query auditing problem*: given a sequence of queries that have already been posed about the data, their corresponding answers -- where each answer is either the true answer or \"denied\" (in the event that revealing the answer compromises privacy) -- and given a new query, deny the answer if privacy may be breached or give the true answer otherwise. A related problem is the offline auditing problem where one is given a sequence of queries and all of their true answers and the goal is to determine if a privacy breach has already occurred.\nWe uncover the fundamental issue that solutions to the offline auditing problem cannot be directly used to solve the online auditing problem since query denials may leak information. Consequently, we introduce a new model called*simulatable auditing*where query denials provably do not leak information. We demonstrate that max queries may be audited in this simulatable paradigm under the classical definition of privacy where a breach occurs if a sensitive value is fully compromised. We also introduce a probabilistic notion of (partial) compromise. Our privacy definition requires that the a-priori probability that a sensitive value lies within some small interval is not that different from the posterior probability (given the query answers). We demonstrate that sum queries can be audited in a simulatable fashion under this privacy definition.\n* Gagan Aggarwal, Tomas Feder, Krishnaram Kenthapadi, Rajeev Motwani, Rina Panigrahy, Dilys Thomas, and An Zhu,[Approximation Algorithms for k-Anonymity], Journal of Privacy Technology, 2005. Preliminary version:[Anonymizing Tables], International Conference on Database Theory (ICDT), 2005.Abstract\nWe consider the problem of releasing a table containing personal records, while ensuring individual privacy and maintaining data integrity to the extent possible. One of the techniques proposed in the literature is k-anonymization. A release is considered k-anonymous if the information corresponding to any individual in the release cannot be distinguished from that of at least k�&#710;&#8217;1 other individuals whose information also appears in the release. In order to achieve k-anonymization, some of the entries of the table are either suppressed or generalized (e.g. an Age value of 23 could be changed to the Age range 20-25). The goal is to lose as little information as possible while ensuring that the release is k-anonymous. This optimization problem is referred to as the k-Anonymity problem. We show that the k-Anonymity problem is NP-hard even when the attribute values are ternary and we are allowed only to suppress entries. On the positive side, we provide an O(k)-approximation algorithm for the problem. We also give improved positive results for the interesting cases with specific values of k �&#8364;&#8221; in particular, we give a 1.5-approximation algorithm for the special case of 2-Anonymity, and a 2-approximation algorithm for 3-Anonymity.\n* Gagan Aggarwal, Mayank Bawa, Prasanna Ganesan, Hector Garcia-Molina, Krishnaram Kenthapadi, Nina Mishra, Rajeev Motwani, Utkarsh Srivastava, Dilys Thomas, Jennifer Widom, and Ying Xu,[Vision Paper: Enabling Privacy for the Paranoids], International Conference on Very Large Data Bases (VLDB), 2004Abstract\nP3P is a set of standards that allow corporations to declare their privacy policies. Hippocratic Databases have been proposed to implement such policies within a corporation's datastore. From an end-user individual's point of view, both of these rest on an uncomfortable philosophy of trusting corporations to protect his/her privacy. Recent history chronicles several episodes when such trust has been willingly or accidentally violated by corporations facing bankruptcy courts, civil subpoenas or lucrative mergers. We contend that data management solutions for information privacy must restore controls in the individual's hands. We suggest that enabling such control will require a radical re-think on modeling, release, and management of personal data.\n**PhD Thesis:**\n* Krishnaram Kenthapadi,[Models and Algorithms for Data Privacy], Stanford University, September 2006Abstract\nOver the last twenty years, there has been a tremendous growth in the amount of private data collected about individuals. With the rapid growth in database, networking, and computing technologies, such data can be integrated and analyzed digitally. On the one hand, this has led to the development of data mining tools that aim to infer useful trends from this data. But, on the other hand, easy access to personal data poses a threat to individual privacy. In this thesis, we provide models and algorithms for protecting the privacy of individuals in such large data sets while still allowing users to mine useful trends and statistics.\nWe focus on the problem of statistical disclosure control - revealing aggregate statistics about a population while preserving the privacy of individuals. A statistical database can be viewed as a table containing personal records, where the rows correspond to individuals and the columns correspond to different attributes. For example, a medical database may contain attributes such as name, social security number, address, age, gender, ethnicity, and medical history for each patient. We would like the medical researchers to have some form of access to this database so as to learn trends such as correlation between age and heart disease, while maintaining individual privacy. There are broadly two frameworks for protecting privacy in statistical databases. In the interactive framework, the user (researcher) queries the database through a privacy mechanism, which may deny the query or alter the answer to the query in order to ensure privacy. In the non-interactive framework, the original database is first sanitized so as to preserve privacy and then the modified version is released. We study methods under both these frameworks as each method is useful in different contexts.\nThe first part of the thesis focuses on the interactive framework and provides models and algorithms for two methods used in this framework. We first consider the online query auditing problem: given a sequence of queries that have already been posed about the data, their corresponding answers and given a new query, deny the answer if privacy can be breached or give the true answer otherwise. We uncover the fundamental problem that query denials leak information. As this problem was overlooked in previous work, some of the previously suggested auditors can be used by an attacker to compromise the privacy of a large fraction of the individuals in the data. To overcome this problem, we introduce a new model called simulatable auditing where query denials provably do not leak information. We also describe a probabilistic notion of (partial) compromise, in order to overcome the known limitations of the existing privacy definition. We then present simulatable auditing algorithms under both these definitions. The second problem we consider is output perturbation, in which the database administrator computes exact answer to the query and then outputs a perturbed answer (by adding random noise) as the response to the query. Inspired by the desire to enable individuals to retain control over their information, we provide a fault-tolerant distributed implementation of output perturbation schemes, thereby eliminating the need for a trusted database administrator.\nThe second part of the thesis focuses on the non-interactive framework and considers two anonymization methods for publishing data for analysis from a table containing personal records. We present approximation algorithms for anonymizing databases using the k-Anonymity model. Then we propose a new method for anonymizing data records, where the data records are clustered and then cluster centers are published. To ensure privacy of the data records, we impose the constraint that each cluster must contain no fewer than a pre-specified number of data records. We provide approximation algorithms to come up with such a clustering.\n[Krishnaram Kenthapadi]",
    "length": 161564,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "AI Playground Quick Start Guide",
    "url": "https://uit.stanford.edu/aiplayground",
    "text": "AI Playground Quick Start Guide | University IT[Skip to main content] \n[![Stanford] University IT] Main Menu\nTopic Menu\n# AI Playground Quick Start Guide\nExplore and experiment with AI in a Stanford-hosted environment\n[Visit the AI Playground] \n* [About] \n* [Open the playground] \n* [Try out prompts] \n* [Find &amp; explore models] \n* [Assistants &amp; agents] \n* [Explore right-side panel] \n* [Share] \n* [Customize] \n* [New features] \n* [FAQs] \n![] \n## In the AI Playground, Now You Can…\nDecember 1, 2025\n[Learn more about In the AI Playground, Now You Can…] \n## About the AI Playground\nThe Stanford AI Playground is a user-friendly platform, built on open-source technologies, that allows you to safely try various AI models from vendors like OpenAI, Google, and Anthropic in one spot. The AI Playground is being managed by University IT (UIT) as a pilot for most people at Stanford. This includes all active faculty,staff, students, postdocs, visiting scholars, and affiliates.\n### AI Playground: An Introduction\nWith this short video, discover how exploring AI tools and technologiescan benefit you. Plus, find outhow the AI Playground works overall withshort demos of how to use prompts and understand replies.\n[![Embedded YouTube video]] \n### Playground safety\n![] \n**Do not use**[**high-risk data**] **in your attachments or prompts.**\nAnd remember, while large language models (LLMs) are advanced tools, they are not flawless and may create errors or hallucinations. Take caution before trusting or using results verbatim.\n[Responsible AI guidance »] \n## [1. Open the playground] \nDive in and get started with a visit to the AI Playground!\n**How to log into the Playground:**\n1. Visit[**aiplayground.stanford.edu**] \n2. Follow the steps to log in with Single Sign On (SSO).\\*\n\\* You might be taken to an Information Release settings page, especially if it's your first time visiting.\n1. Select your consent duration preference.\n2. Click**Accept**to keep going. (All data shown is kept within Stanford systems.)\n**Note:**The Information Release is part of the university's authentication system and is used solely for logging into the platform. This information will not leave university systems and is only shared at the time of logging in. Learn more in the[FAQ section] under \"Data privacy and security in the AI Playground.\"\n[![Information Release settings page]] \n## [2. Try out prompts] \n**How to enter a prompt:**\n1. To use the default settings with your prompts, navigateinto the empty field at the bottom of the welcome screen.\n2. Type a prompt into the field and press the**Return**or**Enter**key.\n3. Once you receive the result, continue to modify or refine your prompt.\n* [See guidance].\n[![Stanford AI Playground screenshot]] \n### Attach Files from the prompt bar\nSeveral file tools are available from the prompt bar in the Attach Files menu (paperclip icon):\n1. **Upload Image.**\n2. **Upload as Text:**Makes it easier to pull the content of uploaded documents directly into your conversations. Includes enhanced OCR support using Mistral’s OCR API, so that images and PDFs can be converted to text automatically.\n3. **Upload for File Search:**File Search stores the uploaded document in a way that the AI can search and fetch relevant pieces when needed, rather than reading it all at once. This is ideal for large files or when you need to have a lengthy conversation about a file.\nPlease note, the paperclip icon does not appear when using agents.\n[![Attachments instructions]] \n### Launch Tools from the prompt bar\nSpecialized tools are now built into the prompt bar.\nTo use, open the Tools menu on the prompt bar and select the tool.\n1. **File Search:**When selected, a \"File Search\" indicator will appear in the prompt bar to show it is active. With the Upload for File Search option (from the Attach Files menu), the File Search tool (from the Tools menu) enables semantic retrieval of data in files in an efficient way. File Search stores the uploaded document in a way that the AI can search and fetch relevant pieces when needed, rather than reading it all at once. This is ideal for large files or when you need to have a lengthy conversation about a file.\n2. **Web Search:**When selected, a \"Search\" indicator will appear in the prompt bar to show it is active. Once you enter your prompt, linked online sources will be included in the response to your prompt.\n3. **Artifacts:**When selected, an \"Artifacts\" indicator will appear in the prompt bar to show it is active. Optionally, choose either \"Include shadcn/ui components instructions\" or \"Custom Prompt Mode\" from the Artifact tool's flyout menu. For more on Artifacts, see the Artifacts section of this guide (section 9).\n*Tip: To add a shortcut for any of these tools to the prompt bar, click the pushpin icon next to tool name in the Tools menu.*\n![Prompt toolbar options] \n### Interact with a message in the conversation\nBeneath your prompts and the responses generated by the LLMs, you'llfind several more advanced features:\n1. **Read aloud:**Read outs the message via a computer-generated voice.\n2. **Copy to clipboard:**Copies the content of the selected message to your clipboard to be pasted into another window or program.\n3. **Edit:**Allows you to edit your prompts as well as the responses of the various models.\n* *Save &amp; Submit:*Saves your edit and resubmits the information to regerate the AI's response.\n* *Save:*Saves your edit without regenerating the response.\n* *Cancel:*Closes the edit window without saving changes.\n* **Fork:**Creates a new conversation that starts from the specific message selected. This can be useful for refocusing the conversation, creating branching separate scenarios, preserving context, and more.\n* **Feedback**: On any response, choose the thumbs up or thumbs down icon for specific feedback options. Select the best choice.\n* **Regenerate:**Forces the model to try to createa new response without any additional context.\n[![Read aloud; Edit; Copy to clipboard; Regenerate; Fork]] \n### Temporary Chat\nTemporary Chats are excluded from your personal search results, cannot be bookmarked, and are automatically deleted after 30 days. You can open the Temporary Chat option at the top right of the main/center panel of the AI Playground. Temporary Chat can be used to keep your chat history clean and focused, and for sensitive topics, quick experiments, or anything you don’t need to permanently save.\n## [3. Find and explore models] \nSelect your preferred model at the top of the page. You can also adjust and switch models in the middle of a conversation.\n* For example, you can start a conversation in OpenAI with the prompt \"Write an article about topic A\" and then switch to the DALL-E-3 agent to request an image to go with the article, then switch to Anthropic and request a list of headline options to go with the article.\n### Find model choices\n[![All LLM choices]] \nChoose between these available versions for each model.\n### Azure OpenAI (ChatGPT) models\n|Model|Best for|Strengths|Notes|Use if you need|\n**gpt-5**|Complex reasoning, document processing (text, images, PDFs), long form synthesis, and advanced coding|Strongest reasoning and instruction following capabilities, reliable with longer context, high quality writing, and code generation|Higher cost and much slower than other GPT models, still benefits from structured prompts and examples|Intricate work for technical, policy, research, data analysis, coding, mathematics, and executive summaries|\n**gpt-5-mini**|Quick responses and drafts, chat assistants, lightweight code, and email summarization|Quick with and good reasoning, better grounding than previous mini models|Trades depth for speed, not ideal for long discussions or large documents|Quick answers, note cleanup, triage, batch content generation, chatbots, simple agents, rapid prototyping under tight budgets|\n**gpt-4o-mini**|Speedy responses, lightweight content generation, casual coding support|Cost-effective, solid for general tasks|Think of it as a faster, trimmed-down gpt-4o with solid performance and less nuance|Speed and value over cutting edge intelligence|\n**gpt-4o**|Complex reasoning, image and PDF analysis, advanced code generation|Deep comprehension, contextual awareness, handles vision input natively|Fast and high-performance across creative and technical domains|Advanced problem-solving, file analysis, data visualization, coding, long essays|\n**gpt-4.1**|Complex reasoning, professional communications, data analysis, and code generation|Multimodal capabilities, natural conversation flow, coding and technical support|May occasionally generate incorrect or outdated information, needs clear prompts for image generation|A smart creative or professional collaborator, help explaining complex topics simply, fast and reliable assistance on code or data tasks|\n**o1**|Academic use, symbolic math, structured data analysis|Especially good at formal reasoning and tough equations|Slower than gpt-4o, but built for precision in logical tasks|Math derivations, proofs, or rigorous academic writing|\n**o3-mini**|Quick responses to technical questions|Optimized for STEM, excels at coding, math equations, and science questions|Great utility model for short form technical queries|Quick help with formulas, equations, code snippets|\n### Google models\n|Model|Best for|Strengths|Notes|Use if you need|\n**gemini-2.0-flash-lite-001**|Very quick responses, great at back and forths conversations|Speed focused, ideal for chatbots and mobile use|Sacrifices complexity and subtlety for responsiveness|Fast responses and do not require deep reasoning|\n**gemini-2.0-flash-001**|Quick language translation, friendly emails, and strong technical reasoning|Handles idioms, coding, high context tasks|Sweet spot for blend of speed + sophistication at lower price|Multilingual understanding and real-time support|\n**gemini-2.5-pro**|Complex reasoning, advanced coding, multimodal analysis|State-of-the-art performance, massive context window (1 million tokens), adaptive reasoning, rich multimodal input support|Heavy compute and latency due to deep thinking, still in beta, susceptible to hallucinations|Deep research and document analysis, complex web development, reviewing videos, reading charts|\n### Anthropic models\n|**Model**|**Best for**|**Strengths**|**Notes**|**Use if you need**|\n**claude-3-7-sonnet**|Code analysis, deep reasoning, and summarizing long text|Larger context windows allows model to see more data at once|Solid general-purpose model with strong alignment across categories|Help with legal documents, research papers, and deep Q&amp;A on longer texts|\n**claude-3-5-sonnet**|Creating large bodies of creative or academic writing|Nuanced, writer friendly model with higher context limit|Excellent for research papers, detailed reports, or complex narratives|Essays, storytelling, or reviewing large documents|\n**claude-3-haiku**|Fast processing of structured tasks and existing data sets|Efficient and great at for form fill tasks with clean outputs|Use it for quick instruction following or form based responses|Snappy summaries, quick data extraction, JSON handling|\nclaude-4-sonnet|Everyday coding tasks, writing and document review, reading text and images|Efficient and fast, large context support, strong reasoning|Can be compared to Gemini-2.5-pro, still prone to hallucinations|Coding assistance, analyzing screenshots, documents, and writing|\nclaude-4.5-sonet|Deep reasoning, long running tasks, and advanced coding workflows|Handling complex, multistep projects with consistency|Most accurate and detailed model from Anthropic|Software development, data analysis, research, and agent orchestration|\n### DeepSeek models\n#### DeepSeek Open Thinking feature\nThe DeepSeek models uniquely offer an \"Open Thinking\" feature, which allows you to see real-time the criteria and inputs that the model is considering as part of its response. This allows you to improve your prompts and better tailor follow-ups. To enable, go to Settings &gt; Chat &gt; Open Thinking Dropdown by Default.\n#### DeepSeek models\n|**Model**|**Best for**|**Strengths**|**Notes**|**Use if you need**|\n**deepseek-r1**|Coding, technical Q&amp;A, advanced math reasoning|Open model which gives you insight into its \"thinking\" process|If you want an open alternative to gpt-4, this is a good pick with verbose results|Help with engineering tasks, logic puzzles, or lots of information on a single topic|\n**DeepSeek-V3-0324**|Code generation, complex reasoning, academic-style Q&amp;A|Quality responses across logic heavy and technical domains, excels in benchmark style tasks|Stronger performance in math and step by step reasoning than earlier DeepSeek models|Reliable model for technical asks, code heavy workflows, or educational content generation|\n### Meta models\n|Model|Best for|Strengths|Notes|Use if you need|\n**Llama-3.2**|Adapting writing style and as well as multilingual tasks and translation|Open source and responds well to various types of input|Capable of great instructional clarity with fast responses|Assistance with writing tasks and multilingual support|\n**Llama-4**|Open source alternative to more expensive models, especially for writing and multilingual tasks|Strong performance in summarization, translation, and context sensitive outputs|More capable than Llama 3, with improvements in consistency and language understanding|Open source model for writing, creative tasks, and research support across languages|\n## [4. Compare models side-by-side] \nTo compare the result of two models simultaneously for the same prompt, use the model compare feature.\n**How to compare models side-by-side:**\n1. **Select one model**for your comparison in the top menu drop-down options.\n2. Click the**plus icon in the top menu**options.\n* You will notice the selected model is indicated in the prompt field.\n* **Select the second model**for your comparison in the top menu.\n* **Type and enter your prompt**into the prompt field.\n* You will see both results side-by-side.\n* Once you click out of the conversation, you will be able to see both responses by tabbing back and forth using the numbered tool below the conversation.\n## [5. Integrate Azure Assistants and agents] \nThe AI Playground also provides access to AI agents and an Azure Assistant, which although they work differently, both provide specific and enhanced functionality.\n***Note:**For direct API access to the models within the AI Playground, developers and tinkerers can use our*[*AI API Gateway service*] *(requires a valid PTA).*\n### Agents\nPreviously known as plugins, agents are similar to Azure Assistants in their ability to perform tasks like coding, searching documents, or generating images. However, agents offer greater customization and can integrate a larger variety of third party capabilities and model providers. Available agents include:\n* **Google Gemini 2.5 Flash Image**(aka nano-banana): This image generation tool produces images from text prompts. Especially useful for rapid prototyping, brainstorming, or when you need fast image generation without sacrificing overall quality.*(Not available to students, postdocs, or visiting scholars at this time.)*\n* **Imagen 3:**Generates exceptional photorealistic images based on text descriptions.*(Not available to students, postdocs, or visiting scholars at this time.)*\n* **Dall-E 3:**Turns your natural text prompts into AI generated images.*(Not available to students, postdocs, or visiting scholars at this time.)*\n* **Google:**An AI-assisted Google web search. You can use it paired with the GPT models to help conduct Google searches through the lens of an AI agent.\n* **Financial Info Navigator (FIN):**Helps you explore financial questions and transactions. It combines Fingate and the Administrative Guide (both updated weekly) with real-time, read-only access to select Oracle Financials data.\n* **DoResearch Policy guide:**Allows for AI assisted searching of Stanford's comprehensive collection of policies, guidelines, and general information related to the university's research activities.\n* **Admin GuideSearch:**Can be used to help answer simple questions about the Stanford Admin Guide through the lens of an AI model.\n* **Wolfram:**Provides computational intelligence for solving complexmathematical equations.\n* **Faculty Handbook Search:**Designed to help you find specific information related to Stanford University faculty policies and guidelines.\n* **AI Helper agent:**Provides quick answers about the AI Playground, showing you how to get more out of the platform. This agent can help identify areas where AI can help support your day-to-day tasks.![January 2026 Agents Update] \n### Azure Assistants\nAzure Assistants function similarly to traditional chatbots but offer enhanced capabilities. Powered by Azure OpenAI, these assistants do not yet support the full range of advanced features available through integration with external tools.\nAt this time, the only available Azure Assistant is:\n**Data/Code Analyst**\n* **Best for:**Processing files with diverse data and formatting likes spreadsheets and PDFs\n* **Strengths:**Excels at solving challenging code problems and provides more robust data analytics capabilities\n* **Notes:**Power by OpenAI’s code interpreter to help improve data analysis over conventional models\n* **Use if you need:**Assistance reviewing complex spreadsheets and PDFs with unstructured data\n[![assistants dropdown menu]] \n## [6. Explore the right-side panel] \nReview and manage both saved prompts, files, bookmarks, and more in the right sidebar or panel.\n1. Open or close the right-side panel using the**Open/Close Sidebar****button. The button shrinks down when the mouse is not hovering over it, so look for the small button in the middle of the right hand side of the screen.\n2. Once the panel is open, you can manage the following:\n* **Prompts:**Allows you to save prompts toreuse over and over.\n* **Memories:**Enables remembered context across conversations. Allows all available AI models to recall user preferences, important facts, and context from other conversations (such as details you’ve shared about your role, recurring patterns, or your writing style).\n* **Parameters:**Allows you to fine-tune the model's behavior for specific goals or contexts.\n* **Attach Files:**Allows you to manage files shared with and generated by the models of the AI Playground.\n* **Bookmarks:****Allows you to create and manage bookmarks, which are a new way to group your conversations with the AI models.\n* **Hide Panel:**Collapses right-side panel/sidebar.\n[![right sidebar]] \n### Adjust model parameters\nYou can also use the configuration options button to the right of the selected model to customize your settings.\nBefore entering your prompt, you can choose to modify the selected model's settings. This is optional, and most people can leverage the default options for the best experience.\nNote that the settings might vary slightly for each model type, but many models will have the below configuration options:\n* *Max context tokens -*Defines maximum input size. (1000 tokens is about 750 words)\n* *Max output tokens -*Defines maximum output size. (1000 tokens is about 750 words)\n* *Temperature -*Controls the “creativity\" or randomness of the content being generated.\n* *Top P -*Alternative to temperature, refines the number of possible responses based on context provided.\n* *Reasoning effort -*For some models, increasing reasoning effort can help increase quality of responses, by indicating that more computational effort is required. Increased reasoning effort might increase the amount of processing time, as well.### Use bookmarks to organize conversations\nUse the Bookmark tool to organize your conversations by similar topics, to find them easily in the future.\n**Step-by-step:**\n1. When viewing a conversation, click the**bookmark icon**in the top bar.\n2. Click**New Bookmark**.\n3. Fill in the Bookmark details and click**Save**.\n* To apply the bookmark to the current conversation (likely desired), check the box to “**Add to current conversation**.”\n* Now you can use the Bookmarks selector in the left-side panel to view only conversations with the applied bookmarks.\n* Manage, rename, and delete bookmarks in the right-side panel under the**Bookmarks**section.\n## [7. Learn to share] \nYou have the ability to share conversations you have had with the models.Your name and any messages you add to the conversation after creating the link stay private.\n**Share a link to a conversation:**\n1. Click**Share**.\n2. Click the**Create link**button to generate a shareable link.\n* **Note:**Be careful when using this feature. While you must log in to view the link, the conversation will become accessible to any authenticated Stanford users with the link.\n![share flyout] \n**Remove a link to a shared conversation:**\n1. Click on**your own name**in the bottom left corner.\n2. Click**Settings**.\n3. In the new pop up window, click**Data controls**.\n4. Next to Shared links, click on the**Manage**button.\n5. This will display all the chats with shared links, the date shared, and give you an option to delete the link.\n* **Note:**Deleting a shared link is a permanent action and cannot be undone. Resharing the conversation would include any new information inputor generated in the conversation since the original link was generated.\n![clear shared links] \n## [8. Customize your settings] \n#### Memories\nOne way to customize your experience is with the**Memories**smart system for maintaining context across conversations. This system is on by default, but can be toggled off from the Memories menu in the right-side panel. Memories enable all available AI models to recall user preferences, important facts, and context across your conversations.\n#### Settings\nYou can also use customize options in the**Settings**menu that impact your entire AI Playground experience.\n**To access the settings menu:**\n1. Click on**your own name**in the bottom left corner.\n2. Click**Settings**.\n![] \nGeneral settings\n* **Theme:**Allows you to change between Light and Dark mode.\n* **Language**\n* **Render user messages as markdown**\n* **Auto-Scroll to latest message on chat open:**When enabled, this will automatically move your view to the last message in the conversation.\n* **Hide right-most side panel:**When enabled, this will remove the pop up side panel menu.\n* **Archived chats:**Allow you to unarchive conversations or delete them from the system entirely.\nChat settings\n* **Message Font Size:**Adjust the size of fonts used in the AI Playground.\n* **Chat direction:**Change the direction of writing systems used in the AI Playground.\n* **Press Enter to send message:**When enabled,pressing the Enter key will send your message.\n* **Maximize chat space:**Increases the visible space used by conversations.\n* **Open Thinking Dropdowns by Default**\n* **Always show code when using code interpreter**\n* **Parsing LaTeX in messages:**When enabled, thinking dropdown menus are expanded automatically. Currently only works with DeepSeek models.\n* **Save drafts locally:**When enabled, texts and attachments you enter in the chat will be saved locally as drafts. Drafts are deleted once the message is sent.\n* **Scroll to the end button**\n* **Save badges state**\n* **Enable switching Endpoints mid-conversation**\n* **Use default fork option:**Defines what information is visible when forking conversations.\n* **Start fork from target message by default**\nBeta features\n* **Toggle Artifacts UI:**Show or hide the Artifacts panel to view, manage, or interact with generated outputs like files, code, and designs.\n* **Include shadcn/ui instructions:**Enable guidance and usage tips or examples for integrating or customizing shadcn/ui components directly in your workflow.\n* **Custom Prompt Mode:**Enable to craft and run highly tailored prompts with full control over structure, behavior, and formatting—ideal for advanced or repeatable tasks.\nCommands\nTurn on these command shortcuts to help streamline prompt configuration and boost productivity.\n* Toggle command**“@”**for switching endpoints, models, presets, etc.\n* Toggle command**“+”**for adding a multi-response setting\n* Toggle command**“/”**for selecting a prompt via keyboard\nData controls\n* **Import conversations from a JSON file:**Allows you to import conversations exported from other GPT chat applications.\n* **Shared links:**Allows you to view and delete allshared conversations under your account.\n* **Delete TTS cashe storage**\n* **Clear all chats:**Deletes all conversations from the left most side panel. (Does not delete archived conversations.)\nAccount settings\n* **Display username in messages:**When enabled, your name is shown next to your prompts in your conversations. When disabled,prompts you sendwill be labeled as \"You\" in conversations.\n* **Profile picture:**Allows you to upload a profile picture for yourself, which is shown in your conversations with the AI models. (Image must be under 2MB.)\n![settings] \n## [9. Code and preview generated items (Artifacts)] \nExplore the ability to render graphs, charts, webpages, and applications on screen using React, HTML5, three.js, WebGL, and more.\n**How to enable Artifacts UI:**\n1. To use, open the Tools menu on the prompt bar and select the tool.\n2. Click on Artifacts.\nThe button should light up in the prompt bar now to let you know it is enabled.\n**Note:**Artifacts work with each of the major models, but tends to work best with Anthropic and Azure OpenAI models. The artifacts feature does not work in conjunction with agents at this time.\n![Finding Artifacts in the Prompt Toolbar.] \n## [10. Try new features] \nThe AI Playground is continually evolving. Features are being updated and added regularly. ([View AI Playground release notes].)Here you will find details for using several recently released features.\n### AI Helper agent\nThis feature was enabled in September 2025; it can provide quick answers about the AI Playground, show you how to get more out of the platform, and help identify areas where AI can help support your day-to-day tasks.[Refer to this article for more details about the AI Helper agent].\n### Memories\nThe AI Playground now has a smart Memories system for context across conversations. This allows all available AI models to recall user preferences, important facts, and context from other conversations (such as details you’ve shared about your role, recurring patterns, or your writing style).[Refer to this article for more details about Memories] \n### Tools from the prompt bar\nThese three tools are now available from the new Tools menu built into the prompt bar:\n![Prompt toolbar options] \n* **File Search:**With this tool active, you can prompt for semantic retrieval of data in files you have uploaded via Attach Files.\n* **Web Search:**With this tool active, once you enter your prompt, linked online sources will be included in the response to your prompt.\n* **Artifacts:**With this tool active, you can use the improved Artifacts tool for generating code and corresponding prototypes using React, HTML5, three.js, WebGL, and more.\n[Refer to this article for more details about these Tools].\n### File Search improvements\nYou can now activate File Search via the prompt bar and keep it active in your chats.\nIn addition to the new File Search capabilities, the normal attachment feature has also been updated. Now you can**upload files as text**, making it easier to pull the content of uploaded documents directly into your conversations. This update also adds enhanced OCR support using Mistral’s OCR API, so that images and PDFs can be converted to text automatically.\n### Feedback on responses\nA new chat rating system allows users to provide feedback on AI responses. Your feedback is stored in the conversation metadata, so that conversation is better tailored to you. (Your feedback does not train or fine tune the models in the AI Playground, and ratings do not affect saved user memories.)[Refer to this article for more details about Feedback options].\n### Improved formatting with LaTeX\nIf you use LaTeX typesetting format for mathematical or technical prompts and responses, you’ll also notice improved formatting of responses that include combined LaTeX typesetting and standard typesetting together.\n### Add the AI Playground to your phone or computer as a Progressive Web App (PWA)\nYou can enjoy faster, easier access to the Stanford AI Playground by installing it as a Progressive Web App on your computer or mobile device. UIT has created instructions for Windows, Mac, and iOS.\nPlease review the[Install Stanford AI Playground Like an App] page for detailed instructions.\n## Resources and support\n### **\nFAQs\nVisit our FAQs for more information about using the AI Playground.\n[Go to FAQs »] \n### **\nAI Playground community\nShare and learn from the Stanford community on the AI Playground the[#ai-playground] Slack channel.\n[Join AI Playground conversation »] \n### **\nFeedback\nDo you have questions, suggestions, or thoughts to share about the AI Playground? Reach out and let us know what's on your mind.\n[Share feedback »] \n[![Stanford University]] \n* [Stanford Home] \n* [Maps & Directions] \n* [Search Stanford] \n* [Emergency Info] \n* [Terms of Use] \n* [Privacy] \n* [Copyright] \n* [Trademarks] \n* [Non-Discrimination] \n* [Accessibility] \n©CopyrightStanford University.Stanford,California94305.",
    "length": 29452,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Generative ML and CSAM: Implications and Mitigations",
    "url": "https://fsi.stanford.edu/publication/generative-ml-and-csam-implications-and-mitigations",
    "text": "Generative ML and CSAM: Implications and Mitigations | FSI\n[Stanford University] \n[![Home/]] \nMenuClose\nSearch\n# Generative ML and CSAM: Implications and Mitigations\n# Generative ML and CSAM: Implications and Mitigations\nA collaboration between the Stanford Internet Observatory and Thorn looks at the risks of sexual abuse material produced using machine learning image generators.\n* [David Thiel],\n* Melissa Stroebel,\n* Rebecca Portnoff\nJune 24, 2023\n[Download] \nA joint report between the Stanford Internet Observatory and Thorn examines implications of fully realistic child sexual abuse material (CSAM) produced by generative machine learning models. Advances in the open-source generative ML community have led to increasingly realistic adult content, to the point that content indistinguishable from actual photographs is likely to be common in the very near future. These same models and techniques have been also leveraged to produce CSAM. We examine what has enabled this state of affairs, the potential societal consequences of the proliferation of such content, and measures that can be taken to minimize harm from current and future visual generative ML models.\n[All FSI Publications]",
    "length": 1197,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "GenAI FAQs | University IT",
    "url": "https://uit.stanford.edu/ai/faqs",
    "text": "GenAI FAQs | University IT[Skip to main content] \n[![Stanford] University IT] Main Menu\nTopic Menu\n# GenAI FAQs\n* [Basics] \n* [Ethics, safety, and privacy] \nFor help with learning about GenAI, browse these frequently asked questions for basic information and details about ethics, safety, and privacy. Or, try exploring[FAQs more specific to the Stanford AI Playground].\n![] \n## Basics\nLet's start with the basics. What is AI?\nArtificial intelligence, or AI, involves using computers and software toperform tasks traditionallyrequiringhuman intelligence.\nWhat is machine learning?\nMachine learning, often abbreviated as ML, is a subset ofAI that focuses on enablingcomputers to extractinsights from data. This is doneby using algorithms to make informed decisions or predictions. ML differs fromtraditional programming, where computers perform tasks based on establishedinstructions.\nNeed more clarity? Let's say we want a computer to recognize images of dogs. Instead of giving the computer instructions on what a dog looks like, we would provide it with many images of dogs, enabling the algorithm to figure out the recurring patterns and features associated with a dog. Over time, the algorithm would become adept at identifying dogs.\nWhat is a large language model?\nA large language model, or LLM, uses mathematical principles to understand the relationship betweenwords and sentences. It can predict what words might come next in a sentence based on prior training data.\nWhat is generative AI? How is it different from regular AI?\nGenerative AI, or GenAI, refers to a subsetof AI that uses prompts to analyzedata, detect patterns, and thencreate new or original content,like text, images, or even computer code.\nWhat is prompt engineering, and how does it enable GenAI?\nA prompt is the starting point for generating AI responses. Simply put, a prompt is just a way of telling the AI what task you want it to perform, like asking it a question or giving it an instruction. The more preciseand specific you are with your prompt, the better the AI can understand what you need and give you the correctanswer or result. The term \"prompt engineering\" is about creating clear and useful instructions— or prompts —so the GenAIknows what youwant itto do.\nWhat are some of the applications for GenAI in day-to-day business operations?\nGenAIcan help us innovate by creating fresh content, discovering trends and extractinginsights from data, summarizinginformation efficiently, and automatingsolutions and processes.\nWhy is there suddenly so much hype around GenAI?\nGenAIattracted mainstream attention with the public releaseof ChatGPT in late 2022.\nWhat is ChatGPT?\nChatGPT is a conversation AI developed by a company called Open AI. It simulateshuman-like, text-based interactions. Noteworthy competitors to ChatGPT includeGoogle's Gemini, Microsoft's Bing Chat, Meta's Llama 2, and Amazon's Codewhisperer.\n![] \n## Ethics, safety, and privacy\nWhat are some of the ethical concerns surrounding GenAI?\nConcerns about GenAI revolvearound several key areas:\n* Accuracy: GenAI is known to hallucinate by providing inaccurate, misleading or incomplete information.\n* Bias: GenAI tools have the potential to perpetuate biases present in the data they are trained on,exacerbatingexisting inequities.\n* Digital divide: GenAI may widen the gap between those who have access to AI tools and resources and those who don't.\n* Ethics: GenAI-generated content can make it difficult to determine who created a piece of work.\nWhat are some privacy and security surrounding GenAI?\nWhen using AI, you shoulderr on the side of caution when it comes to what you input with a generative AI platform. Also, consider disabling options related to saved history to prevent the information from being logged or tracked for training model purposes.Avoid inputting any sensitive data, such as[Moderate or High Risk Data], whether using a personal or Stanford accountwith a third-party AI platform or tool that is not covered by a Stanford Business Associates Agreement. Review Stanford[approved services] by data risk classification.\nHave more questions?[Start a discussion with ISO].\nHow can I learn more about using AI responsibly?\nSecurity and privacy experts at Stanford, including the Information Security Office (ISO),offerconsiderations for GenAI with a new site:[Responsible AI at Stanford (responsibleai.stanford.edu)].\n### **\nAI Playground FAQs\nVisit our FAQs for more information about using the AI Playground.\n[Explore AI Playground FAQs »] \n### **\nGenAI at Stanford\nFor questions specific to Stanford services and efforts, consider exploring the Topics and Services List.\n[GenAI Topics and Services List »] \n[![Stanford University]] \n* [Stanford Home] \n* [Maps & Directions] \n* [Search Stanford] \n* [Emergency Info] \n* [Terms of Use] \n* [Privacy] \n* [Copyright] \n* [Trademarks] \n* [Non-Discrimination] \n* [Accessibility] \n©CopyrightStanford University.Stanford,California94305.",
    "length": 4958,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "",
    "url": "https://hai.stanford.edu/sites/default/files/2024-02/White-Paper-Rethinking-Privacy-AI-Era.pdf",
    "text": "Rethinking Privacy \nin the AI Era\nPolicy Provocations for a Data-Centric World \nWhite Paper\nFebruary 2024\nJennifer King\nCaroline Meinhardt\nWhite Paper\nRethinking Privacy \nin the AI Era\n2\nAuthors \nJennifer King is the Privacy and Data Policy Fellow at the Stanford University \nInstitute for Human-Centered Artificial Intelligence (HAI). An internationally \nrecognized expert in information privacy, her research examines the public’s \nunderstanding and expectations of online privacy as well as the policy implications \nof emerging technologies, including artificial intelligence. Her recent research \nexplores alternatives to notice and consent (with the World Economic Forum), the \nimpact of California’s new privacy laws, and manipulative design (dark patterns). \nShe also co-directs the Dark Patterns Tip Line repository at Stanford. Prior to joining \nHAI, she was the Director of Consumer Privacy at the Center for Internet and Society \nat Stanford Law School from 2018 to 2020. Dr. King completed her doctorate in \ninformation management and systems (information science) at the University of \nCalifornia, Berkeley School of Information. \nCaroline Meinhardt is the policy research manager at the Stanford Institute for \nHuman-Centered Artificial Intelligence (HAI), where she develops and oversees \npolicy research initiatives. She is passionate about harnessing AI governance \nresearch to inform policies that ensure the safe and responsible development of \nAI around the world—with a focus on research on the privacy implications of AI \ndevelopment, the implementation challenges of AI regulation, and the governance \nof large-scale AI models. Prior to joining HAI, Caroline worked as a China-focused \nconsultant and analyst, managing and delivering in-depth research and strategic \nadvice regarding China’s development and regulation of emerging technologies \nincluding AI. She holds a Master’s in International Policy from Stanford University, \nwhere her research focused on global governance solutions for AI, and a Bachelor’s \nin Chinese Studies from the University of Cambridge.\nAcknowledgments\nThe authors would like to thank Brenda Leong, Cobun Zweifel-Keegan, Justin West, \nKevin Klyman, and Daniel Zhang for their valuable feedback, Nicole Tong and Cole \nFord for research assistance, and Jeanina Casusi, Joe Hinman, Nancy King, Shana \nLynch, Carolyn Lehman, and Michi Turner for preparing the publication.\nDisclaimer\nThe Stanford Institute for Human-Centered Artificial Intelligence (HAI) is a nonpartisan research institute, \nrepresenting a range of voices. The views expressed in this White Paper reflect the views of the authors.\nWhite Paper\nRethinking Privacy \nin the AI Era\n3\nTable of Contents\nAuthors 2\nAcknowledgments 2\nTable of Contents 3\nExecutive Summary 4\nChapter 1: Introduction 5\nChapter 2: Data Protection and Privacy: \nKey Concepts and Regulatory Landscape 7\na. Fair Information Practice Principles: \nThe framework behind data protection and privacy 9\nb. General Data Protection Regulation: \nThe “global standard” for data protection 10\nc. U.S. State Privacy Laws: Filling the federal privacy vacuum 12\nd. Predictive AI vs. Generative AI: An inflection point \nfor data protection regulation 14\nChapter 3: Provocations and Predictions 17\na. Data is the foundation of AI systems, \nwhich will demand ever greater amounts of data 17\nb. AI systems pose unique risks to both individual and \nsocietal privacy that require new approaches to regulation 19\nc. Data protection principles in existing privacy laws \nwill have an implicit, but limited, impact on AI development 22\nd. The explicit algorithmic and AI-based provisions in \nexisting laws do not sufficiently address privacy risks 25\ne. Closing thoughts 29\nChapter 4: Suggestions for Mitigating the Privacy Harms of AI 31\nSuggestion 1: Denormalize data collection by default 33\n Suggestion 2: Focus on the AI data supply chain to \nimprove privacy and data protection 36\nSuggestion 3: Flip the script on the management of personal data 41\nChapter 5: Conclusion 45\nEndnotes 46\nWhite Paper\nRethinking Privacy \nin the AI Era\n4\nExecutive Summary\nIn this paper, we present a series of arguments and predictions about how existing and future privacy and data \nprotection regulation will impact the development and deployment of AI systems.\nData is the foundation of all AI systems. Going forward, AI development will continue to increase developers’ hunger \nfor training data, fueling an even greater race for data acquisition than we have already seen in past decades.\nLargely unrestrained data collection poses unique risks to privacy that extend beyond the individual level—they \naggregate to pose societal-level harms that cannot be addressed through the exercise of individual data rights \nalone.\nWhile existing and proposed privacy legislation, grounded in the globally accepted Fair Information Practices \n(FIPs), implicitly regulate AI development, they are not sufficient to address the data acquisition race as well as the \nresulting individual and systemic privacy harms. \nEven legislation that contains explicit provisions on algorithmic decision-making and other forms of AI does not \nprovide the data governance measures needed to meaningfully regulate the data used in AI systems.\nWe present three suggestions for how to mitigate the risks to data privacy posed by the development and \nadoption of AI:\n1. Denormalize data collection by default by shifting away from opt-out to opt-in data collection. \nData collectors must facilitate true data minimization through “privacy by default” strategies and adopt \ntechnical standards and infrastructure for meaningful consent mechanisms.\n2. Focus on the AI data supply chain to improve privacy and data protection. Ensuring dataset \ntransparency and accountability across the entire life cycle must be a focus of any regulatory system that \naddresses data privacy.\n3. Flip the script on the creation and management of personal data. Policymakers should support the \ndevelopment of new governance mechanisms and technical infrastructure (e.g., data intermediaries and \ndata permissioning infrastructure) to support and automate the exercise of individual data rights and \npreferences.\nWhite Paper\nRethinking Privacy \nin the AI Era\n5\nChapter 1: Introduction\nIn the opening months of 2024, artificial intelligence \n(AI) is squarely in the sights of regulators around the \nglobe. The European Union is set to finalize its AI Act \nlater this year. Other parts of the world, from the United \nKingdom to China, are also contemplating and, in some \ncases already implementing, wide-ranging AI regulation. \nIn the United States, a recent milestone Executive \nOrder on AI marked the clearest signal yet that the \nBiden administration is poised to take a comprehensive \napproach to AI governance.1\n With federal legislation to \nregulate AI yet to pass, a growing number of federal \nagencies and state legislators are clarifying how existing \nregulation relates to AI within their jurisdictional areas \nand proposing AI-specific regulation.2\nWhile much of the discussion in the AI regulatory \nspace has centered on developing new legislation to \ndirectly regulate AI, there has been comparatively little \ndiscourse on the laws and regulations that already \nimpact many forms of commercial AI. In this white \npaper, we focus on the intersection of AI regulation \nwith two specific areas: privacy and data protection \nlegislation. The connective tissue between privacy and \nAI is data: Nearly all forms of AI require large amounts \nof training data to develop classification or decisional \ncapabilities. Whether or not an AI system processes \nor renders decisions about individuals, if a system \nincludes personal information, particularly identifiable \npersonal information, as part of its training data, it is \nlikely to be subject—at least in part—to privacy and \ndata protection regulations. \nWe make a set of arguments and predictions about \nhow existing and future privacy and data protection \nregulations in the United States and the EU will impact \nthe development and deployment of AI systems. We \nstart with the fundamental assumption that AI systems \nrequire data—massive amounts of it—for training \npurposes. It is this need for data, as best evidenced by \ndata-hungry generative AI systems such as ChatGPT, \nthat we predict will fuel an even greater race for data \nacquisition than we’ve witnessed over the last decades \nof the “Big Data” era. This need will in turn impact both \nindividual and societal information privacy—not just \nthrough the demand for data, but also by the impacts \nthis need will have on specific issues such as consent, \nprovenance, and the entire data supply pipeline and \nlife cycle more generally.3\nWe move on to examining AI’s unique risks to \nconsumer and personal privacy, which—unlike many \ntechnology-fueled privacy harms that primarily impact \nindividuals—aggregate to pose societal-level risks \nthat existing regulatory privacy frameworks are not \ndesigned to address. We argue that existing governance \napproaches, which are based predominantly on the \nglobally accepted Fair Information Practices (FIPs), \nwill not be sufficient to address these systemic privacy \nrisks. Finally, we close with suggested solutions for \nmitigating these risks while also offering new directions \nfor regulation in this area.\nWhat’s at Stake: The Future of \nBoth Privacy and AI\nData is a key component for all AI systems—to date, \nthe most significant improvements in AI systems \nhave been tied to access to very large amounts of \ntraining data. This fact does not necessarily mean \nthat all advancements in AI will require massive \namounts of data; as we discuss later, some researchers \nare observing quality versus quantity trade-offs \nWhite Paper\nRethinking Privacy \nin the AI Era\n6\nthat indicate more may not reliably mean better. \nRegardless, we are presently at an inflection point \nwhere there is considerable pressure for companies \nto build massive training datasets to maintain their \ncompetitive advantage.\nA primary concern motivating this paper is that despite \nthe fact that existing and proposed privacy and data \nprotection laws on both sides of the Atlantic will have \nan impact on AI, they will not sufficiently regulate \nthe data sources that AI systems require in a way \nthat will substantively preserve, or even improve, our \ndata privacy. In this paper, we explore several related \nconcerns:\n1. The framework that underlies data protection laws \nhas weaknesses that will not give individuals the \ntools they need to preserve their data privacy as \nAI advances; \n2. It also fails to address societal-level privacy risks;\n3. Policymakers must expand the scope of how we \napproach privacy and data protection to address \nthese weaknesses and bolster data privacy in an \nincreasingly AI dominant world. \nWe start from the assumption that for most of us \nthe current state of our data privacy ranges from \nsuboptimal to dismal. In the United States, polls have \nshown that the public largely feels as if they have no \ncontrol over the data that is collected about them \nonline;4 that the benefits they receive in exchange for \ntheir data are not always worth the bargain of free \naccess; and that in most data relationships, consumers \nhave no ability to negotiate more favorable terms—\nand in many instances, believe they are locked in or \nhave few if any alternatives.5\nIn short, as we move toward a future in which AI \ndevelopment continues to increase demands for \ndata, data protection regulation that at best maintains \nthe status quo does not inspire confidence that the \ndata rights we have will preserve our data privacy \nas the technology advances. In fact, we believe \nthat continuing to build an AI ecosystem atop this \nfoundation will jeopardize what little data privacy we \nhave today. \nThis paper focuses on the core issues that we believe \nrequire the most attention to address this state of \naffairs. It does not claim to address or solve everything. \nBut we do believe that if these issues aren’t sufficiently \nacknowledged and addressed through regulation and \nenforcement, we leave ourselves open to a situation \nwhere privacy protection continues to deteriorate. \nThere are many worries attached to how our world \nwill change as it continues to embrace AI. Concerns \nrelated to bias and discrimination have already \ngenerated extensive debate and discussion, and we \nargue that a substantial loss of data privacy is another \nmajor risk that deserves our heightened concern.\nWhite Paper\nRethinking Privacy \nin the AI Era\n7\nChapter 2: Data Protection and Privacy: \nKey Concepts and Regulatory Landscape\nThe last two years have seen groundbreaking advances \nin AI, a period in which generative AI tools became \nwidely available, inspiring and alarming millions of \npeople around the world. Large language models \n(LLMs) such as GPT-4, PaLM, and Llama, as well as \nAI image generation systems such as Midjourney and \nDALL-E, have made a tremendous public splash, while \nmany other less headline-grabbing forms of AI also \ncontinued to advance at breakneck speed.\nWhile recognizing the recent dominance of LLMs in \npublic discourse, in this paper we consider the data \nprivacy and protection implications of a wider array \nof AI systems, defined more broadly as “engineered \nor machine-based system[s] that can, for a given set \nof objectives, generate outputs such as predictions, \nrecommendations, or decisions influencing real or \nvirtual environments.”6 For example, we consider a \nrange of predictive AI systems, such as those based \non machine learning, that analyze vast amounts of \ndata to make classifications and predictions, ranging \nfrom facial recognition systems to hiring algorithms, \ncriminal sentencing algorithms, behavioral advertising \nand profiling, and emotion recognition tools, to \nname a few. These systems operate with varying \nlevels of autonomy, with “automated decision\u0002making” referring to AI systems making decisions \n(such as awarding a loan or hiring a new employee) \nwithout any, or minimal, human involvement.7\nWhile generative AI systems also rely on predictive \nprocesses, those systems ultimately focus on creating \nnew content ranging from text to images, video, and \naudio as their output. \nIn response to these widely publicized developments, \nboth policymakers and the general public have \ncalled for regulating AI technologies. Since 2020, \ncountries around the world have begun passing \nAI-specific legislation.8 While the EU finalizes the \nparameters of its AI Act, the bloc’s attempt to provide \noverarching regulation of AI technologies, the United \nStates presently lacks a generalized approach to AI \nregulation, though multiple federal agencies have \nreleased policy statements asserting their authority \nover AI systems that produce outputs in violation \nof existing law, such as civil rights and consumer \nprotection statutes.9 Several U.S. states and \nmunicipalities have also tackled general consumer \nregulation of AI systems.10\nWhile some policymakers are keen to \ndemonstrate that they are assuaging \nthe public’s growing concerns \nabout the rapid development and \ndeployment of AI by introducing new \nlegislation, there is a growing debate \nover whether existing laws provide \nsufficient protection and oversight of \nAI systems. \nWhite Paper\nRethinking Privacy \nin the AI Era\n8\nWhile some policymakers are keen to demonstrate \nthat they are assuaging the public’s growing concerns \nabout the rapid development and deployment of AI \nby introducing new legislation, there is a growing \ndebate over whether existing laws provide sufficient \nprotection and oversight of AI systems. As we discuss \nin this white paper, privacy and data protection laws \nin the United States and the EU already do the work \nof regulating some—though not all—aspects of AI. \nWhether these existing laws, and proposed ones based \non these frameworks, are adequate to anticipate and \nrespond to emergent forms of AI while also addressing \nprivacy risks and harms is a question we will address \nlater in this paper. \nBefore we delve into the details of our arguments, we \nprovide a brief overview of the present state of data \nprotection and privacy regulations in the EU and the \nUnited States that impact AI systems, starting with the \nfoundational Fair Information Practices (FIPs). Those \nfamiliar with these regulations may wish to skip ahead \nto the next chapter.\nData Privacy and Data Protection \nData privacy and data protection are sometimes used interchangeably in casual conversation. While \nthese terms are related and have some overlap, they differ in significant ways. \nData privacy is primarily concerned with who has authorized access to collect, process, and \npotentially share one’s personal data, and the extent to which one can exercise control over that access, \nincluding by opting out of data collection. The term’s scope is fairly broad, as it pertains not just to \npersonal data but to any kind of data that, if accessed by others, would be seen as infringing on one’s \nright to a private life and personal autonomy. \nPrivacy is often described in terms of personal control over one’s information, though this conception \nhas been challenged by the increasing loss of control that many have over their data. But it is this \nnotion of personal control that underlies both existing privacy regulations and frameworks. What is \nconsidered “private” is also contextually contingent, in that data shared in one context may be viewed \nas appropriate by an individual or data subject (e.g., sharing one’s real time location data with a friend) \nbut not in another (e.g., a third party collecting one’s real time location data and using it for advertising \npurposes without explicit permission). The relational nature of data has also challenged the idea of \nprivacy as personal control, as data that is social in nature (e.g., shared social media posts) or data that \ncan reveal both biological ties and ethnic identities (e.g., genetic data) continue to grow. \nWhite Paper\nRethinking Privacy \nin the AI Era\n9\nData Privacy and Data Protection (cont’d)\nData protection refers to the act of safeguarding individuals’ personal information using a set of \nprocedural rights, which includes ensuring that data is processed fairly, for specified purposes, and \ncollected on the basis of one of six accepted bases for processing.11 Consent is the strictest basis and \nallows individuals to withdraw it after the fact. By contrast, legitimate interest provides the greatest \nlatitude—this legal ground for processing data allows processors to justify data processing on the basis \nof this data being needed to carry out tasks related to their business activity. Data processors must still \nrespect individuals’ fundamental data protection rights, such as providing notice when data is collected, \ngiving access to one’s collected information, providing the means to correct errors, delete, or transfer it \n(data portability) to other processors, and affording the right to object to the processing itself. But there \nis a bias toward accepting as a given the collectibility of some forms of personal data by default.\nThe EU formally distinguishes between personal privacy (i.e., respect for an individual’s private life) and \ndata protection, enshrining each in its European Charter of Fundamental Rights. Nevertheless, there \nare areas of overlap and the concepts complement each other. When data protection principles do not \napply because the collected information is not personal data (e.g., anonymized body scanner data), the \nfundamental right to privacy applies as the collection of bodily information affects a person’s individual \nautonomy. Conversely, data protection principles can ensure limits on personal data processing, even \nwhen such processing is not thought to infringe upon privacy.12\na. Fair Information Practice \nPrinciples: The framework \nbehind data protection and \nprivacy\nMost modern privacy legislation, at its core, is \nbased on the Fair Information Practices (FIPs), a \n50-plus-year-old set of principles that are accepted \naround the globe as the fundamental framework for \nproviding individuals with due process rights for their \npersonal data.13 Proposed as a U.S. federal code of fair \ninformation practices for automated personal data \nsystems in the early 1970s, the FIPs introduced five \nsafeguard requirements regarding personal privacy \nas a means of ensuring “informational due process.”14\nThey focus on the obligations of record-keeping \norganizations to allow individuals to know about, \nprevent alternative uses of, and correct information \ncollected about them.15 As policy expert Mark \nMacCarthy describes, “All these measures worked \ntogether as a coherent whole to enforce the rights \nof individuals to control the collection and use of \ninformation about themselves.”16\nRather than framing information privacy as a \nfundamental human right, as both the United Nations \nUniversal Declaration of Human Rights and the \nWhite Paper\nRethinking Privacy \nin the AI Era\n10\nEuropean Charter of Fundamental Rights do with a \nmore general conception of privacy, the FIPs outline \na set of rules and obligations between the individual \n(data subject) and the record-keeper (data processor).17\nThe FIPs were drafted around a core assumption that \nthe state has a legitimate need to collect data about \nits citizens for administrative and record-keeping \npurposes.18 This assumption—that data collection \nis necessary and appropriate for the workings of \nthe modern state but must be done fairly and with \nprocedural safeguards in place—was incorporated \ninto subsequent revisions of the FIPs, even as they \nwere increasingly applied to the private sector. \nThe most internationally influential version, developed \nby the Organisation for Economic Cooperation \nand Development (OECD) in 1980 and amended in \n2013, consolidates and expands the original FIPs \ninto eight principles covering collection limitation, \ndata quality, purpose specification, use limitation, \nsecurity safeguards, openness, individual participation, \nand accountability.19 The guidelines reflect a broad \ninternational consensus on how to approach \nprivacy protection that has translated into a policy \nconvergence around enshrining the FIPs as a core part \nof information privacy legislation around the world.20\nDespite having been conceived long before the \nemergence of the commercial internet, let alone \nsocial media platforms and generative AI tools, core \ncomponents of the FIPs, such as data minimization \nand purpose limitation21, directly impact today’s AI \nsystems by limiting how broadly companies can \nrepurpose data collected for one context or purpose to \ncreate or train new AI systems. The EU’s General Data \nProtection Regulation (GDPR), as well as California’s \nprivacy regulations and the proposed American Data \nPrivacy and Protection Act (ADPPA), relies heavily on \nthese principles. These regulations’ attempts to clarify \nthe application of the FIPs to privacy controls amid \nexponentially increasing volumes of online consumers \nand commercial data shed further light on the impact \nof privacy regulation on AI. \nb. General Data Protection \nRegulation: The “global \nstandard” for data protection\nPassed in 2016 and in effect as of 2018, the General \nData Protection Regulation is the EU’s attempt to \nboth update the 1995 Data Protection Directive and \nharmonize the previous patchwork of fragmented \nnational data privacy regimes across EU member \ncountries and to enable stronger enforcement of \nEuropeans’ data rights.22 At its core, the GDPR is \ncentered on personal data, which is defined as “any \ninformation relating to an identified or identifiable \nnatural person.”23 It grants individuals (“data subjects”) \nrights regarding the processing of their personal data, \nsuch as the right to be informed and a limited right to \nbe forgotten, and guides how businesses can process \npersonal information. It is arguably the most significant \ndata protection legislation in the world today, spurring \ncopycat legislation and impacting the framing of data \nprotection around the globe. As a result of the GDPR’s \ndirect applicability to AI and its dominance across \nThe FIPs were drafted around a \ncore assumption that the state has a \nlegitimate need to collect data about \nits citizens for administrative and \nrecord-keeping purposes.\nWhite Paper\nRethinking Privacy \nin the AI Era\n11\nthe globe, data protection and privacy concerns are \nlargely absent from the EU’s AI Act.\nThe GDPR contains several provisions that apply \nto AI systems, even though it does not specifically \ninclude the term “artificial intelligence.” Instead, \nArticle 22 provides protections to individuals against \ndecisions “based solely on automated processing” of \npersonal data without human intervention, also called \nautomated decision-making (ADM).24 It enshrines \nthe right of individuals not to be subject to ADM \nwhere these decisions could produce an adverse \nlegal or similarly significant effect on them. Given the \nwidespread use of ADM as it relates to health, loan \napprovals, job applications, law enforcement, and \nother fields, the article plays a crucial role in enforcing \na minimum degree of human involvement in such \ndecision-making processes.\nBeyond Article 22, the GDPR also puts in place several \nkey data protection principles that affect AI systems \n(see table). Most notably, the purpose limitation \nprinciple forbids the processing of personal data for \npurposes other than those specified at collection, and \nthe data minimization principle restricts the collection \nand retention of data to that which is absolutely \nnecessary. These principles, in theory, curb unfettered \npersonal data collection (or data mining) that is \ncommon for data-intensive AI applications. Despite \nthe commonly held assumption that more data always \nmakes for better AI, and that such constraints on data \ncollection and use will hamper progress in AI, there is \nCore Data Protection Principles\nData Protection \nPrinciple\nSummary of Relevance\nData \nMinimization\nDefined in Article 5 of the GDPR as ensuring that collected data is “adequate, relevant and limited to \nwhat is necessary in relation to the purposes for which they are processed.” This principle prescribes \nproportionality: Data processors should not collect as much data as possible, particularly out of the \ncontext provided for collection. The intent is to prevent data collectors from engaging in indiscriminate \ndata collection.\nPurpose \nLimitation\nDefined in Article 5 as data “collected for specified, explicit and legitimate purposes and not further \nprocessed in a manner that is incompatible with those purposes.” This principle emphasizes the \nimportance of context, restricting uses of data beyond the explicit purpose given at collection. If a data \nprocessor wishes to repurpose collected data, they need to seek consent for that new use. \nConsent Defined in Article 7 and Recital 32 as a key requirement for data processing. Consent must be “given by \na clear affirmative act establishing a freely given, specific, informed and unambiguous indication of the \ndata subject’s agreement to the processing of personal data relating to him or her, such as by a written \nstatement, including by electronic means, or an oral statement.” Notably, consent is required for all \nprocessing, including if data is collected for multiple purposes. Recital 42 describes the burden of proof \ndata processors must meet to prove data subject consent, noting that “[c]onsent should not be regarded \nas freely given if the data subject has no genuine or free choice or is unable to refuse or withdraw \nconsent without detriment.” \nWhite Paper\nRethinking Privacy \nin the AI Era\n12\nextensive research demonstrating that building ADM \nsystems within these constraints is feasible and even \ndesirable.25\nThe GDPR also enshrines transparency obligations \nin the form of rules about giving notice to individuals \nwhen their personal information is processed for the \npurpose of profiling or ADM.26 It further establishes \nrules granting individuals the right to access their own \ndata and ensure the accuracy of the data processing. \nFinally, it introduces Data Protection Impact \nAssessments (DPIA)—an accountability measure that \nrequires the collecting organization assess the potential \nrisks and harms of data processing activities (as they \npertain to the relevant organization but also potential \nsocietal-level harms) prior to conducting them.27\nc. U.S. State Privacy Laws: Filling \nthe federal privacy vacuum \nAs of 2024, the United States still lacks a federal \nomnibus consumer privacy law similar to the GDPR. \nThe closest it has come to passing consumer privacy \nregulation is the American Data Privacy and Protection \nAct (ADPPA), which was introduced in the House \nin 2022 but did not advance to a floor vote in that \nsession and has yet to be reintroduced.\n28 Similar to \nthe GDPR, the ADPPA would have imposed limits \non the “collection, use, and sharing of personal \ninformation, requiring that such a process be \n“necessary and proportionate.” It would acknowledge \nthe connection between information privacy and civil \nrights, strengthening relevant civil rights laws and \nessentially enacting the privacy section of the Biden \nadministration’s subsequent “Blueprint for an AI Bill of \nRights.”29 ADPPA was the result of lengthy bipartisan \nnegotiations and future privacy legislation is likely to \nhew closely to the original 2022 bill.\nIn the absence of consumer-specific federal \nlegislation, several sectoral laws have created a \npatchwork of privacy protections over the decades, \nsuch as the Family Educational Rights and Privacy \nAct (FERPA), the Children’s Online Privacy Protection \nAct (COPPA), the Health Insurance Portability and \nAccountability Act (HIPAA), and even the Video \nPrivacy Protection Act (VPPA), to name a few. In this \nsplintered landscape, U.S. states have been passing \ntheir own consumer privacy laws. As of 2023, 12 states \nhave passed consumer privacy regulations, though \nCalifornia’s Consumer Privacy Act (CCPA) remains the \nmost far-reaching.30 For that reason, we will focus on \nthe CCPA for discussion purposes.\nSometimes dubbed California’s version of the GDPR, \nthe CCPA—together with its 2022 update, the \nCalifornia Privacy Rights Act (CPRA)—is arguably the \nmost significant state-level effort so far to enact both \nstringent and broad consumer privacy protections.31\nWhile some scholars have argued that the CCPA \nconsciously creates a fundamentally different data \nprivacy regime for California than the GDPR, it \nnevertheless marks a landmark shift in the U.S. privacy \nregulation debate.32\n[The purpose limitation and data \nminimization] principles, in theory, \ncurb unfettered personal data \ncollection (or data mining) that \nis common for data-intensive \nAI applications.\nWhite Paper\nRethinking Privacy \nin the AI Era\n13\nThe initial version of the CCPA created rights of data \naccess, deletion, and portability, as well as a right to opt \nout of sales of personal data for two-year cycles, and \na purpose limitation provision. Businesses are obliged \nto provide notice of the types of data they collect, to \nobtain opt-in consent for data collection from children \nages 13 to 16, and to abide by purpose limitations when \ncollecting and using or reusing data, which must be \nconsistent with individuals’ general expectations and \nthe purpose specified upon collection. The subsequent \nCPRA, passed as a ballot proposition (Proposition 24), \namends the CCPA to add a data minimization prong as \nwell as a right to correct personal data, a right to opt \nout of processing categories of sensitive personal data, \nand—similar to the GDPR—a right to opt out of some \nforms of ADM (those with significant effects, such as \non housing and employment), which in draft regulations \nhas been interpreted by California’s privacy regulator \nto include AI systems.33 Businesses must conduct \nprivacy risk assessments and cybersecurity audits, offer \nalternatives for accessing services for those who opt \nout, and cannot discriminate against consumers for \nexercising these rights. \nA notable difference between California’s privacy \nregime and other states is that California remains the \nonly state to have created an enforcement agency \n(the California Privacy Protection Agency, or CPPA) \nwith rulemaking authority, rather than delegating this \nfunction to the state’s attorney general’s office, as many \nsuch laws do. In practice, this may mean that the CPPA \nhas more in-house expertise than most state attorneys \ngeneral and latitude to both engage in proactive \nenforcement via published guidance and tackle \ncomplex and emergent issues at the intersection of AI \nand personal data. \nBeyond the EU and United States: Data Protection in China \nIn 2021, China’s legislature followed the EU’s example by promulgating a comprehensive and stringent \ndata privacy law. Heavily inspired by the GDPR, China’s Personal Information Protection Law (PIPL) \nwas designed to give Chinese citizens control over their personal and sensitive data by delineating who \ncan access, process, and share their information.34 As such, it incorporates many elements of the FIPs, \nincluding data collection limitations, purpose specification requirements, and use limitations. \nDespite commonly being referred to as a privacy law, the PIPL never directly mentions privacy but instead \nfocuses on curbing the abuse and mishandling of personal information—theoretically by both corporate \nand state actors, though practically the state’s ability to surveil its citizens remains unchecked.35 Like \nthe GDPR and the CCPA, the law contains explicit provisions banning automated decision-making that \nenables differential treatment of consumers, including price discrimination. More broadly, it introduces \nlimits on what was largely unfettered data collection by data-hungry AI companies, requiring informed \nconsent for all kinds of data-processing activities and granting individuals key rights over their data, \nincluding the right to amend, delete, and request copies of information collected about them.\nSince the PIPL predominantly acts as a framework law that sets out broad principles and requirements, it \nwas followed by a string of more granular implementing regulations, which have been directly impacting \nWhite Paper\nRethinking Privacy \nin the AI Era\n14\nBeyond the EU and United States: Data Protection in China (cont’d) \nAI companies, particularly those with facial recognition products.36 However, the true impact of the PIPL on \nChina’s AI ecosystem remains hard to assess given the government’s tendency to use it as a political tool. For \nexample, in 2022 when China’s ride-hailing giant Didi was fined by the government following a comprehensive \ncybersecurity review, the regulatory decision cited the PIPL and Didi’s illegal collection of data, including facial \nrecognition data.37 However, the unprecedented size of the fine and opaque application of a variety of laws and \nregulations may point to the PIPL being used as a tool to control the country’s tech giants.38\nd. Predictive AI vs. Generative \nAI: An inflection point for data \nprotection regulation\nUntil generative AI systems broke through the \npublic and policymaker consciousness in late 2022, \ndiscussions about AI regulation were focused on \npredictive AI systems that use data to classify, sort, and \npredict outcomes. Within the scope of predictive AI, \nconcerns focused primarily on the outputs produced by \nthese systems, with less focus on the data used to train \nthem. Both policy discussions and proposed regulation \nfor AI were primarily concerned with algorithmic \naudits39 and impact assessments,40 transparency \nand explainability,41 and enforcing civil rights42 as a \nmeans of ensuring decisional outputs were fair and \nunbiased.43 To the extent that privacy played a role in \nthese discussions, concerns were typically related to \nthe growing awareness of our main argument in this \npaper—that existing privacy laws such as the GDPR \nwould impact aspects of AI development and that \npassing AI regulation without comprehensive privacy \nlegislation, as would currently be the case in the United \nStates, would be a job half-finished.44\nIt is not an overstatement to say that generative AI \nsubstantially shifted the terms of the debate. Awe \nover the capabilities of image generators such as \nDALL-E or Midjourney and LLMs such as ChatGPT \nsimultaneously raised questions about how these \nsystems were built and what data was used to power \nthem. As it became more widely understood that \ngenerative systems are built predominantly on data \nscraped from across the internet, concerns mounted \nabout exactly what data—and whose data—was \npowering these systems.45\nThese weren’t novel concerns. Facial recognition \nsoftware company Clearview AI had already raised \nthe ire of privacy and civil liberties advocates, as \nwell as European policymakers, for their aggressive \nacquisition of facial images to power their predictive \ncriminal suspect identification app. Clearview built \ntheir software by scraping image data from across \nthe internet, including from online services that \nexplicitly prohibit such scraping. But given Clearview’s \nniche product (available only to law enforcement \norganizations) and targeted impact (used to identify \ncriminal suspects), their data use wasn’t widely \ndiscussed, despite extensive reporting on the company \nby Kashmir Hill of The New York Times.\n46 Clearview \nhas virtually been shut out of the EU marketplace \nafter its data-gathering practices were found to be in \ngross violation of the GDPR. 47 In the United States, \na 2020 lawsuit by the American Civil Liberties Union \nWhite Paper\nRethinking Privacy \nin the AI Era\n15\nleveraging the state of Illinois’ Biometric Information \nPrivacy Act resulted in a settlement that prohibits \nthe company from making its products available to \nindividuals and companies across the country, as \nwell as also prohibiting use of its products by law \nenforcement agencies in Illinois.48\nMeanwhile, as generative AI systems gained greater \nexposure, privacy regulators around the world \nscrambled to understand the impacts of these systems \non the public and whether they violated existing laws.49\nThe G7 data protection authorities went so far as to \nissue a group statement summarizing their concerns—\nspecifically calling out the legal authority generative \nsystems may have for processing personal information, \nespecially related to children; the potential for \ngenerative systems to be used for attacks to extract \npersonal information; and the need to produce \ncompliance documentation about the life cycle of \nthe data used to develop and train their models. The \nstatement also called for “privacy by design,” the \npractice of taking privacy into account throughout all \nstages of system development, while reiterating the \nneed for developers to respect data protection rights \nand the data minimization principle.50\nThe Italian data protection authority went so far \nas to ban ChatGPT until OpenAI, its creator, put \nspecific practices in place (see below). The fact that \nmany generative systems are built at least in part on \nscraped data raises questions about whether and \nunder what contexts data-scraping practices can be \ncompliant with the GDPR, particularly when personally \nidentifiable data is scraped and included in training \ndata, even if that data is publicly available. In particular, \nit may place consent and legitimate interest at odds, as \ncompanies like Clearview argue (albeit unsuccessfully \nin this instance) that they do not need consent for \npublicly accessible data.51 Generative systems raise \nother crucial questions about training data, such as \nthe extent to which procedural data rights will apply \nto them, if individuals can request to delete their \ndata from training datasets or object to this form of \nprocessing, and whether any of this will depend on the \ncontext of use of the generative application in making \nthese determinations. \nItaly Scrutinizes ChatGPT’s Data Practices \nOn March 20, 2023, the Italian Data Protection Authority (the Garante) received a report that OpenAI—the \ncompany that developed GPT-4, the AI model which is the basis for ChatGPT— experienced a breach of \nuser data. The Garante swiftly launched an investigation that found OpenAI was collecting user-generated \ndata to train its AI model, including “users’ conversations and information on payments by subscribers to the \nservice.”56 It deemed the collection of this data to train ChatGPT’s language model unlawful under the GDPR.\nOn March 31, 2023, the Garante demanded that OpenAI block Italian users from having access to ChatGPT. \nIt further required OpenAI to disclose how it utilizes user data to train its AI model, to address concerns that \nChatGPT produced inaccurate information about individuals, and to create an age verification mechanism \nwithin a month—or risk being fined 20 million euros or 4% of the company’s annual turnover.57\nWhite Paper\nRethinking Privacy \nin the AI Era\n16\nItaly Scrutinizes ChatGPT’s Data Practices (cont’d) \nThroughout April, OpenAI implemented changes to meet the Garante’s demands, including a new \ninformation notice describing how personal data is used to train its AI model, as well as a new, ad-hoc \nform that allows users to opt out from having their data processed to train the algorithms. They also added \nan age verification system and gave users the ability to erase personal information they deem inaccurate. \nHowever, OpenAI stated that “it is technically impossible, as of now, to rectify inaccuracies.”58\nThe Garante accepted OpenAI’s changes and allowed Italians to access the chatbot again. Yet the \nregulator continued its investigations into the developer’s data practices, concluding on January 29, 2024, \nthat ChatGPT is in breach of the GDPR and giving OpenAI 30 days to respond with a defense against the \nalleged breaches.59\nIn the United States, discussions about the permission \nneeded for data used to build generative AI have \ntended to shift toward copyright given that, in the \nabsence of a federal consumer privacy law, copyright \nhas offered the clearest path for content creators \nto demand that companies remove their data from \ntraining datasets.52 This approach yields mixed \nresults, given the challenge of reverse engineering the \nexistence of a particular item of content in a system’s \ntraining data absent any transparency obligations \nby the companies to share how and with what they \ntrained their models. It is also a poor approach for \nresolving privacy issues other than those that may \nimplicate copyrightable content. \nIn July 2023, the Federal Trade Commission (FTC) \nissued a civil investigative demand to Open AI with \ndetailed requests concerning their training data.53 This \nhighly specific focus on obtaining information about a \ncompany’s training data is not without precedent; the \nFTC has settled multiple investigations with companies \nthat used AI in their product offerings, demanding that \nthe companies delete their model and the associated \ndata because the data used to train it was improperly \nacquired.54 Lina Khan, chair of the FTC, argued in a \nNew York Times op-ed that “exploitative collection or \nuse of personal data” falls within the agency’s authority \nto prohibit “unfair or deceptive trade practices.”55\nThese events demonstrate that both EU and U.S. \nregulators have some flexibility and regulatory tools \nat their disposal to adapt enforcement to changes \nin technology. Nonetheless, relying only on existing \nlegislation, especially in the United States, is akin \nto bringing a knife to a gunfight. While the GDPR \nis settled law, as of early 2024 the CCPA remains a \nwork in progress that is unlikely to be finalized until \nlater in the year. As we discuss in the next chapter, \nincorporating automated decision-making into \nthese regulations provides the necessary latitude for \nregulators to include AI in their oversight of algorithmic \nsystems, and to potentially broaden their scope to \nfocus on AI-specific issues, such as training data. \nWhite Paper\nRethinking Privacy \nin the AI Era\n17\nChapter 3: Provocations and Predictions\nIn this chapter, we present a set of four provocations \nand predictions that we believe highlight the key \nissues that must be confronted as we continue with \nregulating both privacy and AI. \nFirst, we predict that continued AI development will \ncontinue to increase developers’ hunger for data—\nthe foundation of AI systems. Second, we stress that \nthe privacy harms caused by largely unrestrained \ndata collection extend beyond the individual level \nto the group and societal levels and that these \nharms cannot be addressed through the exercise of \nindividual data rights alone. Third, we argue that while \nexisting and proposed privacy legislation based on \nthe FIPs will implicitly regulate AI development, they \nare not sufficient to address societal level privacy \nharms. Fourth, even legislation that contains explicit \nprovisions on algorithmic decision-making and other \nforms of AI is limited and does not provide the data \ngovernance measures needed to meaningfully regulate \nthe data used in AI systems. \na. Data is the foundation of AI \nsystems, which will demand ever \ngreater amounts of data \nThe era of “Big Data”—the exponentially increased \namount of data collected, created, and stored as the \ninternet expanded and people’s online activities grew \nto encompass virtually every aspect of their lives—\ncreated one of the preconditions for the explosive \ngrowth of AI. Companies now know more about our \npersonal lives than we ever thought they would: who \nwe are, what we like, where we go, what we do, whom \nwe do it with, and what we think and even feel. \nWe predict that the expansion of AI systems across the \nglobe will continue to increase the demand for data \namong developers. This growing demand will heighten \nthe pressure on the entire existing data ecosystem \nto increase the amount and types of data collected \nfrom consumers, as well as incentivize companies to \nviolate the principles of data minimization and purpose \nlimitation in its pursuit of ever more data. Both the \ntotality of data, and the surface areas by which data is \ngenerated and collected, such as embedded sensors \nin household objects, smart appliances, and biometric \ncameras in public spaces, will continue to expand. \nAI’s appetite for data currently knows few bounds. \nAccording to the Global Partnership of AI, “[b]uilding \nan AI system typically involves sourcing large amounts \nof data and creating datasets for training, testing \nand evaluation, and then deployment. This process is \niterative in the sense that it may require several rounds \nof training, testing and evaluation until the desired \noutcome is achieved and data plays an important role \nat each step.”60 None of the AI advances achieved over \nthe past decade would have happened without this \nbroad availability combined with the massively more \npowerful computers, processing capacity, and cloud \nstorage that developed at the same time. As Mark \nWe predict that the expansion of \nAI systems across the globe will \ncontinue to increase the demand for \ndata among developers. \nWhite Paper\nRethinking Privacy \nin the AI Era\n18\nMacCarthy describes, “artificial intelligence, machine \nlearning, cloud computing, big data analytics and the \nInternet of Things rest firmly on the ubiquity of data \ncollection, the collapse of data storage costs, and the \nastonishing power of new analytic techniques to derive \nnovel insights that can improve decision-making in all \nareas of economic, social and political life.”61\nCompanies have not been incentivized to curb their \ncollection of consumer data, in part due to competitive \npressures to maximize targeted, highly personalized \nservices, a task that requires data collection for \nanalytical purposes even if the initial purpose and value \nof collecting the data is speculative. As commercial \nsector AI development increased, so did companies’ \ndemands for data—the result, in part, of testing \nAI systems that generally show improvements in \nthe accuracy and validity of outputs when exposed \nto greater amounts of sufficiently representative \ntraining data.62 Predictive AI in particular demands \nlarge datasets in order to complete advanced pattern \nanalysis, where almost any variable could potentially \nhold the key to reliable correlations or associations \nbetween inputs and outputs. However, a growing \nbody of research is increasingly challenging the \nassumption that more data is better by showing that \nsimilar performance levels can be achieved using \ncomparatively less data overall when it is selected with \nmore intentionality and specificity.63\nWhile not all applications of AI require consumer data, \nthe largest technology companies which have been \nbuilding massive stores of consumer data for at least \nfifteen years and in some cases longer, have emerged \nwith a marketplace advantage in the development \nof AI in part because of their ready access to these \nimmense datasets. Newer AI developers like Anthropic \nor OpenAI have had to turn to other data sources to \nacquire the data to build and train their systems.64\nWhile most forms of predictive (machine learning–\nbased) AI are data-dependent for their development, \nit is the recent emergence of powerful generative AI \nsystems that best illustrates the magnitude of data \nrequired for model training. Generative systems such \nas LLMs (like GPT-4) and user-facing tools built on top \nof them (like ChatGPT), as well as image generation \nsystems like Stable Diffusion or Midjourney have \ndazzled the public with their practical as well as \nentertaining applications. At the same time, as we \ndiscussed in Chapter 2, their high visibility has raised \nquestions about how such systems operate, including \nwhat data they are trained on, and the potential privacy \nand other risks of interacting with these systems.65\nThere are presently no transparency mandates \nrequiring companies to detail where and how they \nacquire their training data outside of the EU AI Act, and \nthose requirements only apply to systems designated \nas high-risk.66 Many of the largest companies building \ngenerative AI systems have not been responsive to \npublic inquiries into where they source their data and \nwhat procedures they use to strip their training data of \npersonally identifiable information and other sensitive \nCompanies have not been \nincentivized to curb their collection \nof consumer data, in part due to \ncompetitive pressures to maximize \ntargeted, highly personalized \nservices.\nWhite Paper\nRethinking Privacy \nin the AI Era\n19\naspects.67 Of course, legal jurisdictions also matter; \nweb scraping that captures personal information that \nis legal in the United States may not be permissible \nunder the GDPR, and companies are increasingly \nforced to navigate territorial issues, both between \nthe United States and the EU and others following the \nGDPR model. \nb. AI systems pose unique risks \nto both individual and societal \nprivacy that require new \napproaches to regulation\nExisting and proposed privacy regulations are largely \na retrospective answer to the past twenty years of \ntechnological change and increasing threats to our \nindividual data privacy. However, the rise in the \nbreadth and amount of data collected from individuals \nacross all aspects of their online interactions, as well \nas new threats posed by AI systems, require that we \nthink prospectively and ensure that we have the tools \nin place to grapple with the changes ahead. Further, \nbeyond the documented harms to individuals, AI \nsystems also pose considerable societal privacy risks \nthat existing regulations are ill equipped to address.\nRisks and Harms to Individuals from \nAI Systems\nInformation privacy can be a difficult concept to specify \nas it is both multidimensional and highly contextual. Law \nprofessors Danielle Citron and Daniel Solove created a \ntaxonomy of information privacy harms, which include \nphysical, economic, reputational, emotional, and \nrelational harms to individuals.68 Citron and Solove also \ncall out discrimination and vulnerability-based harms—\nthose that can occur due to information asymmetries \nbetween individuals and data collectors. There are \nalso the harms that the FIPs were intended to address: \nharms to one’s autonomy, the inability to make informed \nchoices, the inability to correct data, and a general lack \nof control over how one’s information is gathered and \nused. All of these are relevant to AI based systems as \nthey were to the technological developments of the past \nthree decades of internet expansion.\nWhile these harms predate the application of AI to the \nconsumer sector, commercial AI systems will cause \nthem, exacerbate them, and even pose new ones. \nFor example, recent research based on Solove’s own \nprivacy taxonomy69 identified not only existing privacy \nrisks that AI exacerbates but also those the authors \nargue AI creates, such as new forms of identity-based \nrisks, data aggregation and inference risks, personality \nand emotional state inferences via applications of \nphrenology and physiognomy, exposure of previously \nunavailable or redacted sensitive personal information, \nmisidentification and defamation.70 Technical advances \nin AI are also creating new avenues for privacy harm, \nsuch as the harms caused by generative AI systems \ninferring personal information about individuals or \nproviding users with the ability to target individuals \nby creating content about them that is defamatory or \nimpersonates them. \nIn addition to traditional concerns about individual \nprivacy and personal data, these systems generate \npredictive or creative output that, through relational \ninferences, can even impact people whose data was \nnot included in the training datasets or who may never \nhave been users of the systems themselves. When \npersonal data is included in the training dataset, \nresearch has demonstrated that these systems can \nmemorize the data and then expose it to other users \nas part of the outputs.71 While most generative AI \nsystems advise that individuals not include personal \ndata in prompts or other inputs, many people still \ndo, and when users of these systems input personal \nWhite Paper\nRethinking Privacy \nin the AI Era\n20\ninformation, including confidential or legally protected \ndata, these systems may store this data for future uses, \nincluding model retraining, or share it with other users \nas part of the system outputs. \nThe evolution of risks to online information privacy \nover the past two decades is a history of ever\u0002increasing consumer surveillance and individual \nprofiling, primarily driven by the goal of targeting \nconsumers with advertisements and offers based on \ntheir behavior both on- and offline. As social media \nplatforms proliferated and grew, they too became \nan avenue for consumer surveillance, expanding the \nrealm of information that could be collected about \nconsumers across a growing set of contexts. Mobile \ndevices and apps, smart speakers, smart home \ndevices—each new technological development added \nanother layer of information that could be collected \nbeyond the initial ambit of online shopping.\nShoshana Zuboff terms the practice of extracting value \nfrom and about individuals surveillance capitalism, \nwhich “unilaterally claims human experience as free \nraw material for translation into behavioral data.”72\nToday it is exceedingly difficult, if not impossible, \nfor an individual using online or connected products \nor services to escape systematic digital surveillance \nacross most facets of their life. The collection of \npersonal data occurs not only in instances where \nindividuals make the choice to engage directly with an \napp or a service; in many cases, it also occurs silently \nby invisible third parties tracking individuals’ actions in \nbrowsers and mobile apps without giving affirmative \nnotification or securing their consent.\nThe focus on capturing consumer behavior and \nusing it for predictive purposes expanded with the \nproliferation of sources for data collection. Individual \nprofiling and inference-making became indispensable \nfor a broadening range of contexts beyond merely \nserving ads. Profiling for determining credit, insurance, \nemployment, housing, and medicine are but a few \nexamples. Over the past five years, emergent AI \nsystems have increasingly been deployed in these \ncontexts as well, as their predictive capabilities are \neven greater than previous big data applications due to \nthe computational capacities of AI. \nThe Future of Privacy Forum categorizes the harms to \nindividuals from automated systems into four areas: \nlosses of opportunity, liberty, economic losses, and \nsocial detriments.73 These can result in harms such as \ndiscrimination in housing, employment, education, and \nother areas; surveillance and incarceration; denials of \ncredit, differential pricing, and an overall narrowing \nof available choices; and harms to dignity due to bias \nor opportunity losses, as well as algorithm-based \nsocial sorting and filtering that can influence what or \nwhom you connect with in digitally mediated social \nenvironments. Profiling in particular increases the scope \nand scale of data collected about individuals and the \nrelated inference-building across a variety of contexts. \nThere is also a lack of transparency about how \nautomated systems function, making it challenging \nfor individuals to alter or limit their impact. AI \nThe evolution of risks to online \ninformation privacy over the past \ntwo decades is a history of ever\u0002increasing consumer surveillance \nand individual profiling.\nWhite Paper\nRethinking Privacy \nin the AI Era\n21\nsystems can automate many forms of decision\u0002making and classification, exacerbating the privacy \nrisks and harms already present in our “pre-AI” data \necosystem.74 The potential harms resulting from such \nprivacy infringements aren’t limited to the consumer \nmarketplace, where today companies can not \nonly tailor advertisements to you with fine-grained \nprecision, but in some cases also use the data they \nhave collected and inferred about you for manipulative \nor discriminatory commercial uses.75 The result is an \nexploitation of data that undermines societal norms \nand values by removing the structural and contextual \nbarriers that previously acted as safeguards against \nits widespread access.76 This is a means of collecting \ndata against which FIPs-based, individual due process \nrights offer little protection or recourse. Individuals \ncannot use the FIPs effectively to protect themselves \nfrom this form of data collection, especially when \nit happens without notice, through inferences, and \neven from sources we may be unaware of (including \nwhen data scrapers obtain data from services without \npermission, such as from social media or photo\u0002sharing sites).77\nAs nearly all facets of our lives are increasingly \nmediated through technology, the risks increase for AI \nsystems to perpetuate biases, stereotypes, and errors, \nmanipulate consumers, and enable discrimination, \nparticularly in the absence of regulation or \ntransparency measures designed to keep these harms \nin check. Already, the scope and scale of our “data \nrelationships”—with the companies that collect our \ndata directly (first party) and those that do so indirectly \n(third party)—are too numerous for individuals to \nmanage in any reasonable way, assuming we even \nknow who is collecting our data. Under existing or \nproposed privacy laws, the incentives for companies \nto collect as much data as they possibly can is unlikely \nto diminish. As generative AI systems continue to \nproliferate, many built with online data scraped from \nthe internet without consent, individuals stand little \nchance of addressing these privacy risks themselves \nthrough opt-out, correction, or deletion rights.\nSocietal Risks and Harms to Privacy from AI\nThe privacy risks and harms posed by AI systems are \nnot limited to individuals; they also threaten groups \nand society at large in ways that cannot be mitigated \nthrough the exercise of individual data rights. Returning \nto the Future of Privacy Forum’s taxonomy, the \nsocietal-level harms from automated systems based \non group membership include differential access \nto opportunities such as jobs, housing, education, \ncredit, goods and services; increased surveillance and \ndisproportionate incarceration of specific groups; and \nreinforcement of negative stereotypes and biases.78\nAI systems create the capacity for large-scale societal \nrisks precisely because they operate at scale, analyzing \ntremendous amounts of data and in turn making \nconnections and predictions previously not possible \nthrough other means. This capacity can result in \nclassifying and applying decisional outcomes to large \nswaths of the population based on group affiliation—\nthereby amplifying social biases for particular groups. \nHarms at the societal level can also pose threats \nto democracy, as well as impact the benefits that \nprivacy affords individuals, which in turn impact the \ndevelopment of autonomy necessary for cultural and \nsocietal flourishing.79\nFrom a privacy perspective, a specific concern is that \nprofiling at a societal level contributes to a widespread \nerosion of privacy norms and expectations. The \nexpectation that your data will be gathered at every \nturn, the powerlessness of being unable to do anything \nabout it, and the lack of transparency about how \none’s data is used or decisions are made about you \nall feed a growing sense of inevitability that data \nWhite Paper\nRethinking Privacy \nin the AI Era\n22\nprivacy has already been lost.80 This is not simply a \nreflection of changing norms about online sharing \nand publicness, as Mark Zuckerberg disingenuously \nargued in 2009 when forcing Facebook users’ data \nto be public by default—and setting the stage for \nthe Cambridge Analytica scandal.81 The growth of \ngenerative AI has drawn attention to the pervasiveness \nof data collection, and the sources of that data, as the \nconnection between scraped data and the ability of \ngenerative systems to create their wondrous outputs \nhas raised questions about exactly where the data is \ncoming from. The more we mine the public sphere \nfor data, the more we erode the sense that we should \nhave a right to exist in public, whether a digitally \nmediated space or a physical one, with any degree of \nprivacy or anonymity.\nThis shift toward using AI in contexts with civil rights \nimplications, such as hiring,82 criminal justice,83\nand policing84 have profound implications for both \nindividuals and society at large. Individuals interact with \nsystems they may not think of as highly technical (such \nas applying for a job), or to which they haven’t signed up \nas users, but within which AI calculations are applied to \nthem through inferences—to, say, predict their health \noutcomes, calculate their insurance rates, or determine \nwhether their employment application gets reviewed. \nAs these systems proliferate, they can amplify existing \nbiases and inequities. At their most extreme, they can be \nused by governments as tools of social control. \nc. Data protection principles in \nexisting privacy laws will have an \nimplicit, but limited, impact on \nAI development \nThe application of specific fair information practices \nin existing regulations, such as requirements for data \nminimization and purpose limitation, will impact \nAI development. The question we raise is whether \nthese principles are sufficient for tackling the privacy \nrisks and harms posed by AI. In the United States, \nlawmakers are increasingly arguing that passing \nfederal privacy legislation, similar to the GDPR, is a \nnecessary precondition to any regulation that explicitly \ntargets AI systems.85 Existing privacy and data \nprotection laws in both the EU and the United States \n(at the state level) will regulate AI systems that rely on \npersonal data for training purposes or that ingest it as \npart of the service they offer—but only up to a point. \nEven if the United States adopts a GDPR-esque law \nthat provides FIPs-based rights, this approach will not \nbe sufficient on its own to address the risks and harms \nwe discussed above. \nIndirect Regulation Through Data \nMinimization and Purpose Limitation\nBoth the CCPA and the GDPR, as well as other similar \nfederal agency and state-level regulations in the United \nStates and EU member state regulations, impact the \ndevelopment and deployment of AI-based systems by \nlimiting the personal data that companies can collect \nand use to train and retrain AI models ad infinitum.86\nSpecifically, the principles of data minimization and \npurpose limitation, if clearly delineated and enforced, \nshould limit how much personal information is \ncollected and how it can be used and reused for AI \nThe question we raise is whether \n[existing data protection] principles \nare sufficient for tackling the privacy \nrisks and harms posed by AI.\nWhite Paper\nRethinking Privacy \nin the AI Era\n23\nsystems. Companies need to justify how data collected \nfrom consumers in one context for a particular use \ncould be reused in an entirely different context or for a \nnew purpose. However, the degree of protection varies \nconsiderably between jurisdictions. With the GDPR as \ntheir foundation since 2018, EU member states have a \nstronger and broader set of enforcement powers than \ndo the minority of U.S. states that have passed data \nprivacy laws. \nIn response, researchers and industry practitioners \nhave already developed, tested, and deployed a \nwide array of techniques to meet data minimization \nand purpose limitation requirements—without \ncompromising performance.87 During the training \nphase of AI models, privacy-preserving methods \n(including federated learning) have been employed to \nminimize data.88 During the model inference phase, \nexperts point to the conversion of personal data into \nless “human readable” formats, the anonymization \nof queries, and data shuffling among other privacy\u0002preserving techniques.89 Still, more research into data \nminimization and purpose limitation compliance in AI \nsystems is greatly needed.90\nExisting privacy laws do address the use of data \ncollected or generated directly by an AI system (e.g., \na user’s prompts to a chatbot or other generative AI \nsystem, or data processed by a predictive AI system, \nsuch as a recruiting and hiring software). To the \nextent that these systems directly ingest or process \npersonal data, or make predictions or inferences \nabout individuals based on this collected data, \nprivacy regulations implicitly regulate their operation \nby requiring compliance with individuals’ rights to \naccess, correct, and delete personal data, to request \na copy of their data, or to opt out of future sales or \nsharing of their data. In many AI use cases, companies \nmust conduct the same privacy or data protection \nimpact and/or risk assessments that they would even \nif not utilizing AI in order to demonstrate they have \nadequately considered the risks to individuals by \ncollecting and using personal data as part of deploying \ntheir systems. \nLimitations of the FIPs-based Framework\nThe FIPs provide the substantive framework for \nexisting privacy and data protection laws around \nthe globe, based on principles that were developed \nover 50 years ago. Many policymakers view them \nas a model for future privacy legislation; even China \nhas adopted a version of the FIPs in its own privacy \nlegislation, largely viewed as modeled after the GDPR \n(see Chapter 2). However, both the FIPs and the laws \nbased upon them have their critics. Law professor \nWoodrow Hartzog in particular has criticized the FIPs \nas inadequate but invaluable, noting that in a modern \nsociety awash in data and data collection, “control \ndoes not scale.”91\nData Minimization and Purpose Limitation\nEnforcement of the data minimization and purpose \nlimitation principles should, in theory, translate to more \nconservative and thoughtful personal data collection. \nHowever, these approaches as practiced today fail \nto address many of the fundamental weaknesses \nof our current data ecosystem. For example, they \ndo not address the inequitable power dynamics of \na data ecosystem in which the data collectors and \nprocessors, most of which are powerful private tech \ncompanies, hold far more market power over personal \ndata collection than do individuals. Further, it may \nbe reasonably straightforward to hold a company to \naccount if its use of data doesn’t match the purpose \nit gave at collection. However, in the absence of an \nagreement as to what constitutes too much data, it will \nbe a challenge for regulators to operationalize whether \na company is sufficiently practicing data minimization \noutside of egregious violations. Today, the pursuit of \nWhite Paper\nRethinking Privacy \nin the AI Era\n24\nquality (i.e., data that is reliable, relevant, and collected \nethically) is still mostly overridden by a pursuit of \nquantity (i.e., collecting vast amounts of data cheaply \nand at scale, by any means necessary)—especially in \nmarkets that lack robust privacy legislation like the \nUnited States.92\nThe Limits of Privacy Self-Management\nA core weakness with the FIPs framework is that \nindividuals are assumed to have a level of control and \npower equal to that held by companies and institutions \ncollecting and processing their data.93 However, this is \nnot the case; often individuals cannot simply choose \nan alternate product or service with more privacy \nprotective data collection practices. Monopolistic \npractices in the tech sector, consumer lock-in, and a \ngeneral incentive for businesses to collect as much \ndata as possible undermine privacy as a competitive \nfactor except in a few cases. Privacy law expert Daniel \nSolove named the burden on individuals to manage \nand exercise their rights to curb data collection \n“privacy self-management.” As our use of digital \nproducts and services has increased, privacy self\u0002management has failed to give individuals the tools \nthey need if they want to prevent, or at least reduce, \nthe amount of data collected about them.94\nThus, while the FIPs are a necessary baseline to ensure \nthat individuals have due process rights with respect \nto their personal information, they fail to empower \nindividuals to have a meaningful impact on their \nprivacy in the age of AI . FIPs-based regulations may \nbe designed to constrain companies from collecting \nand processing data for AI systems, but they ultimately \ndon’t solve the core problem of how to prevent data \ncollection in the first place in a society where it is \ndifficult, if not impossible, for the majority of people to \navoid interacting with technology. \nData Collection by Default: Opt In or Opt Out?\nThe expansion of the FIPs from their original \napplication to governmental data collection in the \nearly 1970s to the private sector reinforced the \napproach of allowing data collection by default. \nThere are legitimate reasons to allow governments to \ncollect data in many circumstances without requiring \nindividuals to give their explicit consent: tax collection, \ncensus taking, and provisioning public benefits are \nbut a few examples. But applying this rationale to the \nprivate sector normalized the idea that individuals \nshould have to opt out, rather than choose to opt in. \nThe GDPR tries but does not fully resolve this dilemma. \nAs Mark MacCarthy notes, the “GDPR provides \nprocedural, not substantive protections. Its goal is not \nto limit any specific use of information but to ensure \nthat all uses are subject to certain fair procedures to \nensure the protection of the rights of data subjects.”95\nThe GDPR threads the needle between always \nrequiring consent and allowing collection without \nit by providing six bases for processing data; the \ntwo most salient for this discussion are consent\nand legitimate interest.\n96 No matter which basis is \nused, data processors must inform individuals about \nthe processing when collecting their data, and the \nprocessing must not “seriously impact” individuals \nrights and freedoms.97\nThe FIPs (...) fail to empower \nindividuals to have a meaningful \nimpact on their privacy in the age \nof AI.\nWhite Paper\nRethinking Privacy \nin the AI Era\n25\nArguably, asking for consent (opt in) is the most \nstraightforward basis for data processing, as \nindividuals maintain the right to withdraw it. When \nit comes to legitimate interest, the U.K. Information \nCommissioner’s Office describes it as the most \n“flexible” of the bases.98 The off﻿ice suggests t\nlegitimate interest may be appropriate when \nprocessing offers a clear benefit “to you (the \nprocessor) or others, if there is limited privacy impact \non the individual, the use matches an individual’s \nreasonable expectations, or the controller cannot \nor does not want to give the individual “full upfront \ncontrol (i.e., consent) or bother them with disruptive \nconsent requests when they are unlikely to object to \nthe processing.”99 Legitimate interest can allow for \nopt-out-based data collection, though controllers \nare cautioned that it cannot be used as a basis for all \ndata processing, and controllers must have a clear \njustification for using it for their particular context.\nLegitimate interest has been criticized for allowing \ndata collection practices that some argue violate the \nbasis and act as an opt-out rather than an opt-in basis. \nFor example, in March 2023, in response to a ruling \nby the European Data Protection Board that denied \nMeta’s use of its sites’ terms of service as a basis for \nusing behavioral targeting in advertising, Meta then \nswitched to the legitimate interest basis,100 which \nactivist group Noyb argued was a violation of users’ \nfundamental rights.101 The switch requires Facebook \nand Instagram users to submit an online form to \nregister their objection to the use of their behavioral \nproduct usage for targeting; unless they object, \nhowever, Meta will proceed with the targeting, placing \nthe burden on individual users to sort out the details.102 \nWhile the FIPs provide crucial procedural data \nprotection rights, they fundamentally do not curb \ndata collection by default. Instead, the focus is on \nrights one can exercise after data has already been \ncollected, leaving the burden of managing one’s \nprivacy on individuals who may have little time or \ninclination to actively participate in this work. They do \nnot provide a right of refusal, or a clear, convenient, \nnon-fatiguing means to interact with digital products \nor services without having to give up some personal \ninformation.103 While the EU’s 2002 ePrivacy Directive \n(discussed in Chapter 4) attempted to curb cookie \nsetting by default, and GDPR has positively impacted \nthe design and simplicity of cookie consents, they \nremain the hallmark of how not to implement opt in. As \nwe will argue later, there are better ways to implement \nan opt in approach.\nd. The explicit algorithmic and \nAI-based provisions in existing \nlaws do not sufficiently address \nprivacy risks\nIn addition to the implicit impacts of FIPs-based laws \ndiscussed above, both existing and proposed privacy \nregulations include specific provisions targeted at \nalgorithmic systems in such a way that will include AI. \nThese include provisions in the GDPR and U.S. state \nlaws, such as California’s, that address automated \ndecision-making and profiling and that require data \nprotection impact assessments to obligate companies \nto identify uses of data that pose risks to their \ncustomers.104\nThese provisions are intended to ensure that privacy \nand data protection regulations cover specific data\u0002intensive practices that implicate individual data \nprivacy. They use a risk-based framework to place \nobligations on data processors to incorporate risk \nmitigation into their data governance practices that \nincludes risks to their customers, not just to the \nWhite Paper\nRethinking Privacy \nin the AI Era\n26\nbusiness. These measures will have some impact \non AI systems as we discuss below. However, these \nexplicit regulations do not address the limitations of \nthe FIPs framework, nor do they sufficiently focus on \nbroader data governance measures needed to regulate \nthe data used for AI development. Addressing these \nchallenges will require additional policy measures, \nwhich we discuss in Chapter 4. \nAutomated Decision-Making and AI\nThe term automated decision-making (ADM) is not a \nrecent invention. Concerns with delegating decision\u0002making about individuals using personal information \nto automated systems dates back at least to the \norigination of the FIPs in the early 1970s,105 though \nthe FIPs themselves do not address the issue of \nautomation. The U.K. Information Commissioner’s \nOffice defines ADM as “the process of making a \ndecision by automated means without any human \ninvolvement. These decisions can be based on \nfactual data, as well as on digitally created profiles or \ninferred data.”106 The GDPR incorporates the concept \nin Article 22, noting that “[t]he data subject shall \nhave the right not to be subject to a decision based \nsolely on automated processing, including profiling, \nwhich produces legal effects concerning him or her \nor similarly significantly affects him or her.”107 The \ninclusion of the term “profiling”—defined as “any form \nof automated processing of personal data consisting \nof the use of personal data to evaluate certain personal \naspects relating to a natural person, in particular to \nanalyse or predict aspects concerning that natural \nperson’s performance at work, economic situation, \nhealth, personal preferences, interests, reliability, \nbehaviour, location or movements”—is a specific \ncall-out of data mining and prediction practices that \nimplicate privacy through their use of personal data \nand their focus on individuals.108\nADM can arguably be construed as including any \nform of AI that trains on or ingests personal data, or \nthat makes predictions or decisions about individuals, \nthough existing laws narrow the applicability to \nADM with significant or legal impacts to avoid the \noverinclusion of low to no privacy risk ADM (e.g., an \nalgorithm that takes in an address to find a nearby store \nbut does not store that location data, or a clothing \nsizing algorithm that asks the customer for their size in \nspecific brands of clothing to calculate a more accurate \nsizing estimate). The term itself is not specific to any \nparticular technology, describing a process that can be \naccomplished using rule-based algorithms as well as \nforms of AI, such as predictive AI.109\nThe GDPR provides data subjects the rights to contest \nand withdraw from automated processing, creating \nguardrails to prevent a Kafkaesque landscape of ADM \nthat cannot be contested.110 In this context, the GDPR \nexpressly connects automated processing to the \npractice of profiling as a threat to privacy. The scope \nof the GDPR’s automated decision-making provision \narguably impacts AI to the extent that a system \nrenders a decision, or makes a prediction, that results \nin significant impact on individuals’ lives and relies on \nprocessing personal data to do so. Regulations that \nThese explicit regulations do not \naddress the limitations of the FIPs \nframework, nor do they sufficiently \nfocus on broader data governance \nmeasures needed to regulate the \ndata used for AI development. \nWhite Paper\nRethinking Privacy \nin the AI Era\n27\nspecifically call out ADM follow the GDPR’s lead with \nfocusing on systems with “legal effects” or similar \nimpact, such as extending or denying credit, hiring, \nhousing eligibility, and so on. \nThe present scope of ADM regulations in the EU \nand the United States focuses on providing notice to \nconsumers that automated processing is occurring, \ngiving them opt-out rights in qualifying contexts \n(e.g., with significant impacts or legal effects) and \nrequiring ADM systems to provide information about \nthe “logic” of the system design: its purpose, how \nit renders decisions, the potential safeguards in \nplace, and clarifying the extent of human oversight \nover the system. For example, the crafters of the \nCCPA111 and the subsequent update to the law (the \nCalifornia Privacy Rights Act of 2020112) tasked the new \nCalifornia Privacy Protection Agency with regulating \naccess and opt-out rights for businesses’ use of \nADM that processes personal information (including \ntraining data113) or otherwise poses a risk to privacy.114\nColorado’s 2023 privacy law also includes ADM \nregulations, distinguishing between solely automated, \nhuman-reviewed, or human-involved automated \nprocessing, and sets obligations in accordance with \nthe level of human involvement.115 These types of \nmeasures closely follow the FIPs (i.e., notice, data \naccess, data correction) in providing procedural rights \nand protections. \nPrivacy and Data Protection Impact \nAssessments\nImpact assessments for privacy and data protection \nhave their roots in the growth of environmental \nprotection regulation that emerged in the 1960s.116 In \nthe privacy and data protection sectors, they are used \nto guide both public and private sector organizations \ntoward proactive risk assessment when planning a \nnew product or service that utilizes personal data. \nIn the United States, Section 208 of the e-Government \nAct of 2002 obligates federal agencies to conduct \nprivacy impact assessments (PIAs) when “developing \nor procuring information technology that collects, \nmaintains, or disseminates information that is in an \nidentifiable form.”117 The GDPR requires data protection \nimpact assessments (DPIAs) that are triggered \n“whenever processing is likely to result in a high risk \nto the rights and freedoms of individuals,”118 such as \nlarge-scale uses of sensitive data or public surveillance, \nsystematic individual profiling, and automated \ndecision-making without human involvement. Once the \nregulatory process is completed in 2024,119 the CCPA \nmay require DPIAs from companies whose processing \nof data poses a substantial risk to privacy, including \nselling or sharing personal information; processing \nsensitive personal information; using ADM in specific \nways (including decisions with “legal or similarly \nsignificant effects”); profiling employees, job applicants, \nstudents, and consumers in publicly accessible places; \nbehavioral advertising profiling; and processing the \npersonal information of children under 16.120\nPIAs and DPIAs are tools to prompt organizations to \nengage in sufficient planning and self-reflection to \nforesee potential risks and to integrate mitigations \ninto their design and planning processes (or in the \ncase of startups, to compel them to adopt such \nprocesses) by considering both the data types and the \nprocessing activities that pose high risks to individuals. \nAlgorithmic impact assessments have been proposed \nas a tool for the general oversight of AI systems, and \nthough they may make mention of privacy and data, \nthey are not focused exclusively on these topics.121\nHow Do These Explicit Provisions Fall \nShort?\nThe scope of ADM provisions in both the GDPR \nand the CCPA seek to strike a balance between \nWhite Paper\nRethinking Privacy \nin the AI Era\n28\npreventing runaway ADM scenarios versus casting \nsuch a broad net that every form of ADM requires the \napplication of the full set of notice and opt-out rights. \nTo be sure, an opt-out right is a potentially powerful \ndeterrent against the over-application of ADM. The \nprospect of creating an alternate non-ADM process \nfor businesses to comply with opt-out requests may \nkeep privacy and data protection lawyers up at night. \nConsider, for example, a large company that receives \nthousands of job applications per month—requiring a \nnon-automated process for rendering judgments for \napplicants could be daunting. \nHowever, the underlying logic of both opt-outs and \nnotice requirements doubles down on the privacy \nself-management approach, placing the burden on \nindividuals to understand what automated decision\u0002making is and why they may wish to opt out of it. \nThe notice and consent approach for privacy already \nplaces a significant burden on individuals not only \nto exercise these rights but also to comprehend why \none might want to do so.122 Given the complexity of \nunderstanding AI systems and how one’s data may \ninteract with them, this presents an even heavier \nlift. And, it’s unclear what it may accomplish for \nconsumers. \nThe requirement to label ADM systems for the public \nmay raise more questions for consumers than answers. \nParadoxically, it’s conceivable that people might \nelect to opt out of ADM systems in favor of a non\u0002automated process that could turn out to be even \nmore arbitrary or biased than an ADM-based one. In \nessence, this becomes a form of labeling not unlike \nthe practice of genetically modified ingredient (GMO) \nlabeling on foods; in the absence of clear scientific \nevidence determining whether the consumption of \nGMO-based foods is harmful, the decision to consume \nthem or not is punted to the consumer who may have \nlittle to no understanding of the issue, leading them \nto make uninformed choices that could help or harm \nthem. This is not to say that labeling or providing \nnotice of the use of ADM has no benefit; certainly, \nto the extent that there are individuals who want \nto exercise the right not to be subject to ADM, this \nopt-out right is crucial. But it becomes an exercise of \npersonal preference that has the potential to harm an \nindividual given their unique circumstances. And while \nexisting federal and state laws may prohibit AI systems \nthat produce discriminatory or biased outputs, this \napproach leaves a loophole in regard to systems that \nmay have negative implications for one’s data privacy. \nA framework that would place strict limits on the use \nof AI systems that had negative privacy impacts both \nfor individuals and at a societal level would provide a \nmore consistent approach. Finally, the ADM track may \nmiss most uses of generative AI systems to the extent \nthat they are not used for decision-making purposes, \nleaving open the question of whether they could be \nimplemented in a way that consumers may not know \nthat AI is being utilized but not subject to notice \nrequirements. \nThe underlying logic of both opt\u0002outs and notice requirements \ndoubles down on the privacy \nself-management approach, \nplacing the burden on individuals \nto understand what automated \ndecision-making is and why they \nmay wish to opt out of it. \nWhite Paper\nRethinking Privacy \nin the AI Era\n29\nWhile data protection impact assessments are a \nnecessary and useful regulatory tool for protecting \ndata privacy, they are not a steadfast guarantee \nagainst either government or private companies \nimplementing harmful technologies. For one, they \ndepend on a regulatory or institutional structure that \nhas sufficient authority to act when DPIAs or PIAs \nare done poorly or fail to anticipate risks.123 Without \nsuch structural support, they are little more than \na bureaucratic hurdle with no teeth. In the United \nStates, for example, several of the large technology \ncompanies developing AI systems have elected to \nblow past the advice of their own risk-adverse legal \nstaff and responsible innovation teams and market \nAI tools without fully understanding the risks to their \nusers, let alone the larger public.124 Another issue is that \nif there isn’t a standard by which impact assessments \ncan be assessed, businesses can turn the process into \nan exercise of grading their own homework by setting \ntheir own internal standards. \nWhile the GDPR’s DPIA requirements recognize the \nheightened risks from some forms of data processing, \nincluding from fully automated decision-making \ntechnologies, this approach assumes one is acting \non data that has already been collected. It is possible \nthat the process of anticipating as well as completing \na DPIA could dissuade or prevent an organization \nfrom electing to launch a product or service due to \nthe risks it identifies. But the fact that these tools \npresently do not direct organizations to engage in \nthese processes before product creation—perhaps \neven before collecting training data—means that \ntheir ability to surface risky data collection or data \nmanagement decisions, or to prevent such actions, \nis lessened. It is possible in light of the importance \nof training data on the outputs of AI systems that the \nentire data development pipeline should be subject \nto data or privacy impact assessments. Mehtab Khan \nand Alex Hanna review the stages of dataset creation \nand discuss some of the documentation interventions \nthat are increasingly being suggested to add greater \naccountability to the dataset development process (we \nwill discuss these in more depth in Chapter 4).125\nThe proposed California regulations do attempt to \ntackle some of these issues. For example, §7154 places \ndisclosure obligations on businesses that process \npersonal information to train ADM systems or AI, \nrequiring that they disclose to downstream users \nthe appropriate uses of the technology, as well as \nconduct their own risk assessment that addresses \n“any safeguards the business has implemented or will \nimplement to ensure that the automated decision\u0002making technology or artificial intelligence is used for \nappropriate purposes by other persons.”126 Further, \nfor businesses required to conduct a risk assessment \nas described above, §7155 prohibits businesses \nfrom processing personal information if the “risks to \nconsumers’ privacy outweigh the benefits resulting \nfrom processing to the consumer, the business, other \nstakeholders, and the public.”127 In order for our existing \nframeworks to fully grapple with AI-based privacy \nthreats regulators will need to keep refining and \nexpanding provisions like these, and more. \ne. Closing thoughts\nOverall, FIPs-based privacy and data protection laws \nhave not anticipated the growth of AI systems or \nvariants such as generative AI. Despite a decade\u0002plus exposure to the emergence and growth of big \ndata, these regulatory frameworks are not prepared \nto respond to and oversee the data-intensive aspects \nof AI systems. Ultimately, existing FIPs-based privacy \nregulations cannot sufficiently regulate the data \nthat feeds AI development in a way that sustains our \nWhite Paper\nRethinking Privacy \nin the AI Era\n30\nexisting state of privacy today or, better yet, improves \nit. A privacy and data protection framework that \nplaces the primary responsibility on individuals to \nmanage their data across hundreds, even thousands, \nof digital relationships and channels fundamentally \ndoes not scale, and thus will not succeed in protecting \nindividual privacy. Nor will it solve population \nor societal level risks and harms to privacy. As \nlegal theorist Salomé Viljoen notes, “responding \nadequately to the economic imperatives and social \neffects of data production will require moving past \nproposals for individualist data-subject rights and \ntoward theorizing the collective institutional forms\nrequired for responsible data governance.”128 These \nchallenges have become more visible following the \nexplosion of generative AI systems, built primarily \nfrom data scraped online. It will be very difficult, if \nnot impossible, for individuals to shoulder the burden \nof exercising their deletion and correction rights with \nthese massive and nontransparent systems, let alone \nto proactively prevent their data from inclusion. Privacy \nself-management approaches that force individuals to \nbear the burden of systemic privacy challenges will not \nsubstantively improve individual privacy. \nUnfortunately, passing more FIPs-based regulations \nwill not resolve individual privacy challenges or \nsystemic risks posed by AI systems. Even if the United \nStates were to pass the 2022 version of ADPPA, \nneither it nor the GDPR provides sufficient oversight \nof the data used to develop and train AI. While these \nlaws nibble at the edges, they do not confront the bias \ntoward collecting data first and asking questions later. \nThey do not adequately address consent. They do \nnot provide sufficient methods for people to engage \nwith technological systems without ubiquitous data \ncollection. They do not address societal-level privacy \nharms. And they do not provide a framework for \naddressing the privacy issues raised by AI training \ndata, whether they be from proprietary datasets, \nopen source or public datasets, or data scraped \nfrom the internet. The computer science maxim of \n“garbage in, garbage out” is as relevant as ever when \nit comes to building AI systems. Whether they are \ntrained on curated but biased datasets, or on data \nscraped from questionable parts of the internet, the \nimpact assessment process must direct attention to \nthe antecedents of AI products and not simply their \noutputs. \nIn the next chapter, we make three suggestions we \nbelieve should be adopted in order to address these \nissues. They alone may not be sufficient to address the \nissues we have raised above, but we believe they are \nan important starting point for moving forward in the \nright direction. \nWhite Paper\nRethinking Privacy \nin the AI Era\n31\nChapter 4: Suggestions for Mitigating \nthe Privacy Harms of AI\nThe adoption of AI can bring benefits across many \ndifferent societal contexts if—and only if—AI systems \nare designed to center human needs and values. \nBut AI systems’ requirements for data, combined \nwith applications that will generate or consume an \nextraordinary amount of personal information, raise \nseveral crucial questions: Is data privacy compatible \nwith the growth of AI? Can we have a widespread \nadoption of AI and still preserve our information \nprivacy, even at the minimum state it exists today? Can \nwe do better?\nThe rapid growth and adoption of AI raises legitimate \nconcerns about its possible risks to humanity. At the \nsame time that we are debating questions of whether \nand how we want to live in a world that utilizes \nAI, some are questioning whether governments \nshould adopt bright line rules that forbid particular \napplications of AI completely.129 We suggest that \nwhen evaluating these issues, policymakers must also \nconsider that a side effect of AI’s adoption could be a \nworld with substantially diminished data privacy for all \nof us unless we specifically take measures to protect it. \nThe suggestions we make below are motivated by this \nquestion: What will it take for both data privacy and AI \nto coexist? \nAs we note in the introduction, a significant \nassumption in the framing of our questions is that, \nespecially in the United States but also in the EU (and \nmany countries around the world), the present state \nof data privacy is suboptimal. Individual data rights \nare both necessary and insufficient for protecting \ndata privacy in a world with AI. Even in countries and \nstates with data rights in place, the burden continues \nto be on individuals to exercise their rights after data \ncollection rather than for their preferences to be \nrespected at or before the initiation of any collection. \nThis approach, as we have argued, also neglects \nsocietal risks and threatens our collective data privacy.\nWhile data privacy is the focus of this paper, it isn’t \nthe only lens or priority when considering how to \nregulate AI. For example, the extent to which others \nare able to gather data about you and potentially make \ninferences about you has direct implications for issues \nof bias and discrimination, whether those others are \nprivate companies or the government. The reality that \nnon-personal information as well as others’ personal \ninformation can also be used to make inferences about \nboth individuals and groups is yet another reason \npolicymaking on data privacy must move beyond \nindividual control to set clear rules on data collection \nand use more broadly.\nAnother key aspect to this debate is that possessing \ndata gives rise to considerable market power. In the AI \nland grab presently underway, the actors who already \npossess large datasets have a significant advantage \nover developers that do not have stores of data and \nmust gather, purchase, or license it.130 Additionally, \nWhat will it take for both data \nprivacy and AI to coexist? \nWhite Paper\nRethinking Privacy \nin the AI Era\n32\none must also have the resources to pay for curating \nand labeling data, to transform it into a quality \nresource for training AI systems. While advocating \nfor data quality is an important issue in this debate, \nas higher quality data can help address issues of bias \nand discrimination, we must acknowledge that it can \nalso be a source of power and advantage as large \nenterprises can expend the resources to improve their \ndata or license quality data from others. \nWe are also pushing back against a strain of \ntechnological determinism in these debates. Much \nlike the arguments that privacy is dead and we should \nall acquiesce to a total loss of control over our data \nin exchange for the bounty of free online services, \nin many of the discussions around AI today, and \ngenerative AI specifically, there are assumptions that \nthere are no limits on what data should be included \nin AI models, particularly foundation models. When \nthe scrapable data on the entire public and publicly \naccessible internet appears up for grabs, we can \nbe forgiven for assuming that this path is inevitable. \nThis line of thinking in particular appears driven by \ncomputer scientists and others in AI development who \nare focused on the lure of quantity over quality and \ndo not consider the sociotechnical context in which \ndata resides. As we point out in Chapter 3, some \nresearchers are already questioning whether bigger \nwill always be better, even with regards to foundation\nmodels, given the trade-offs between capabilities \nand output quality. A LLM that can make inferences \nand reason in a human-like way is only useful if \nthe model produces accurate and reliable outputs. \nOtherwise, the technology may be nothing more than \na “stochastic parrot,” mimicking human language \nwithout connection to meaning.131 \nFinally, it’s important to note that these are concerns \nthat span both commercial and governmental \ncontexts. The primary focus of this paper has been \ncommercial data collection. But governments can \n(and do) purchase data from the private sector and \ndirect governmental deployment of AI systems that \nare trained on or that process personal information, \nwhich raises concerning questions about the potential \nfor surveillance and the impact on civil liberties.132\nTo date, several of the public sector uses of AI in \nthe United States that have garnered concern have \nfocused on predictive tools for criminal sentencing,133\nor assignment of public benefits134 that have \nperpetuated biases or raised questions about fairness \noutcomes. Governments building AI systems using \nadministrative data, for example, pose risks that are \nout of the scope of this paper to explore in depth. But \none cannot regulate the commercial sector’s data \npractices and turn a blind eye to how governments \nmay adopt and use this technology, including when it \nis procured from the private sector—which, in turn, \nimplicates the sources of the data used to train such \nsystems. In other words, the line between the training \ndata used for private and public uses of AI can easily \nbecome blurred. Neither usage exists in a vacuum, \nwhich points to the need for data provenance as well \nas downstream data privacy impacts to be centered in \nthese debates.135\nWith these concerns in mind, we offer the following \nthree suggestions that we believe will aid in mitigating \nthe risks to data privacy posed by the adoption of AI. \nTo corrupt a famous quote: “It’s the data, stupid.”136\nAny problem-solving about the impacts of AI on \ndata privacy must look beyond individual data rights \nto include strategies that include the governance \nand management of data as a resource in a privacy\u0002respecting and preserving manner, as well as a focus \non societal impacts and human rights.\nWhite Paper\nRethinking Privacy \nin the AI Era\n33\nSuggestion 1: Denormalize data \ncollection by default\nShift away from opt-out to opt-in data \ncollection by facilitating true data minimization \nand adopting technological standards to \nsupport it.\nAs discussed earlier, the FIPs provide a crucial \nframework of rights for our collected data. But the \nprinciples of data minimization, purpose limitation, \nand even consent have been operationalized in ways \nthat normalize data collection by default in many \ncontexts. This normalization can be traced in part to \nthe FIPs’ original focus of providing due process rights \nfor government-based data collection rather than for \nthe commercial sector. The FIPs do not include a right \nto refuse data collection, for example. There is also an \nassumption of exclusivity and intentionality: that an \nindividual has a one-to-one, known relationship with a \ndata collector with whom one intends (or is required) \nto interact. The architects of the FIPs did not anticipate \nthe ubiquitous, always-on digital surveillance and \ndata collection enabled by digital networks and \nmobile devices that emerged in the 2000s. Nor did \nthey foresee that our data would be collected by third \nparties that have no direct relationship with us. These \nassumptions have led to practices that prioritize the \nfrictionless operation of the market over adherence to \nprinciples. \nThe one major example of an experiment with both \nadding friction and surfacing the principle of consent \ninto data collection has not gone well: browser cookie \nconsent dialogs. Cookie consents are a prime example \nof consent fatigue and how not to denormalize data \ncollection. European regulators put consent for data \ncollection by websites front and center with the \nadoption of the EU’s 2002 ePrivacy Directive, the key \nregulation governing browser cookie consents.137 This \napproach quickly backfired: Requiring individuals \nto accept or reject website cookies with every visit \ninserted too much friction, causing annoyance and \nconfusion for the public. Browser cookies are not only \nthe mechanism that allows websites to identify their \nvisitors, they also allow data collectors to engage in \ncross-site tracking and profiling. Even today, many \ninternet users struggle to understand what cookies \nare and how their collection may undermine their \nprivacy. The implementation of the ePrivacy Directive \ndemonstrated the problem with requiring individuals \nto manage consent on a continual, site-by-site basis \nin a manner that treats a wide spectrum of possible \nrisks with the same level of notice and choice: The \napproach does not scale in a world where consumers \nhave relationships with many different online \nproviders. Recent changes sparked by GDPR consent \nrequirements have improved the format of consent \nnotices (e.g., consumers now have a “reject all” \noption), but they still remain a source of friction and \nannoyance.\nIn contrast, the approach taken by the U.S. Federal \nTrade Commission, which endorsed a watered-down \nversion of the FIPs in 1998, is particularly weak.138\nCalled “notice and consent,” the FTC’s implementation \nallows companies to post a notice of their practices (a \nprivacy policy) and to assert that a consumer’s use of \nthe service, or perfunctory acceptance of a service’s \nterms and conditions, constitutes adequate consent \nfor whatever data collection practices the company \nengages in.139 Most famously, privacy policies don’t \nactually have to protect consumers’ privacy—they \nmust only state what a company will do with the data \nit collects, which can include selling it to or sharing \nit with whomever it wishes. The FTC has recently \nstepped up enforcement of unfair or excessively \nabusive privacy practices140 and is signaling an intent \nto revisit the notice and consent paradigm.141 But the \nagency’s rulemaking process is lengthy (on the order \nWhite Paper\nRethinking Privacy \nin the AI Era\n34\nof five years or longer), and adopting federal legislation \nthat resolves these issues would provide a quicker \nsolution. \nWhile the U.S. policy approach illustrates the outcomes \nwith industry self-regulation as the guiding standard, \nthe EU’s demonstrates what happens when tech \npolicy is made without regard to how people actually \nuse technology. Neither way is a win for data privacy. \nUnfortunately, we continue to be in a stalemate with \nregard to resolving data collection by default. The \nEU has delayed the renegotiation of the ePrivacy \nDirective,142 and neither the GDPR nor the ePrivacy \nDirective has curbed all third party data collection \npractices.143 Similarly, if ADPPA were enacted in the \nUnited States, it too would not curb data collection \nby default; the law would add affirmative consent \nobligations for collecting sensitive data but continues \nthe practice of placing the burden on consumers to \nopt out of data collection after the fact. There is some \nprogress on this front regarding children—the GDPR \nprohibits data collection for children under 16 without \nparental consent, and the Federal Trade Commission \nis proposing updates to the Children’s Online Privacy \nProtection Act (COPPA) rules144—but not for adults. \nThe challenge moving forward in a world with greater \ndemands for data is how to mitigate excess data \ncollection without adding too much friction with \nexcessive consent requests. Digital services need \nconsumer data to operate, and not all such requests are \nexcessive. Some demographic data will be required to \nassess whether AI systems are biased or discriminatory, \nthough this can be accomplished within the scope of \npurpose limitation rules.145 But in the absence of clear \nrules, the incentives are such that companies will try \nto maximize data collection, especially if they are \nconcerned their competitors will do so even if they do \nnot. One emergent example in the United States is the \ncollection of consumer mobile phone numbers, which, \ndue to portability regulations, have become a form of \npersistent identifier similar to social security numbers. \nMany online services are now requiring that a customer \nprovide a mobile phone number at sign-up even if \nphone numbers are not necessary for the provisioning \nof the service. This is a clear overreach and one that \nshould be addressed with both data minimization and \npurpose limitation rules. If we don’t address how data \nescapes into the larger data ecosystem at the source, \nwe will neither be able to gain a foothold on improving \nour data privacy, nor exercise adequate control over \nhow our data feeds AI systems. \nData Minimization by Default, Through \nDefaults\nWe need to operationalize the principle of data \nminimization to prevent the collection of excess \ndata. One approach is to apply the privacy by default \nstrategy in recent children’s privacy legislation to \nadults.146 Digital services would have to set all user \naccounts by default to the strongest privacy option; \nit would be up to users whether to change these \ndefaults. Regulators would need to provide guidance \nThe challenge moving forward in \na world with greater demands for \ndata is how to mitigate excess data \ncollection without adding too much \nfriction with excessive consent \nrequests.\nWhite Paper\nRethinking Privacy \nin the AI Era\n35\nto determine which data use practices companies \nwould be allowed to ask consumers to opt in, or set \nclear limits through duties of loyalty. For example, \ncompanies could be required to ask whether a \ncustomer’s data could be used for training purposes, \nand the specificity by which one could be asked (e.g., \nfor any training purpose, or for a specific product?). \nThe opt-in mechanisms themselves would require \nstrict oversight. For example, the CCPA requires that \ncompanies not use dark patterns when consumers \nexercise their right to opt out of the sale of their \npersonal information, preventing companies from \nmanipulating or coercing consumers when making \ntheir choice.147 \nOne example of this approach is demonstrated by \nApple’s rollout of app tracking transparency (ATT) in \niOS 14.5.148 Users are asked when they f﻿irst open \napp whether they wish to allow an app to track their \nactivity across other apps and websites. The setting \nprohibits apps from using third party tracking methods \nunless the user approves it on a per-app basis. ATT \nwas available in a previous release149 as a setting that \nusers had to proactively find and switch on, but the \niOS 14.5 update put the option directly in front of \nusers. It is reported to have significant uptake, with \none source reporting the industry-wide average at \n70% of users opting out of tracking, and even higher \nrates in the United States and in Europe.150 It remains \none of the best examples to date of how consumers \nwill choose pro-privacy defaults when asked simply, \nclearly, and at a sensible decision point in their task \nflow without adding excessive friction. \nAutomate Consumer Preferences\nThe ATT example above succeeds in part because the \ndelegation of these choices is managed by iOS. We \nneed to expand this approach by making it possible \nfor consumers to delegate their data preferences \nand access permissions to software-based agents \nbeyond a single operating system or browser. In order \nto do so we must create the technical standards and \ninfrastructure that allow consumers to engage with \ndigital products and services, at least on a limited \nbasis, without giving up their data or relying on a \nconsent paradigm that forces them to make constant \none-off, case-by-case decisions. There is ample \nresearch from the field of human-computer interaction \nthat illustrates why such approaches are ineffective \nand burdensome.151\nCalifornia has opened the door to this approach with \nits explicit acceptance of a browser opt-out signal, \nGlobal Privacy Control (GPC), that can function as \nan automatic opt-out for the CCPA’s “do not sell my \npersonal information” provision. GPC is an example \nof the direction we must move in if we wish to curb \ndata collection by default: delegating and enforcing \ndata collection preferences to software rather than to \nindividuals on a constant basis. Transitioning to GPC \nor any other software-based solution cannot occur \nuntil one is adopted as an official W3C standard and \nlaws enforce its acceptance.152 Even so, GPC is but one \nexample of the direction we must take; we are overdue \nfor a re-architecting of the personal data ecosystem, \nwhich we discuss below.\nWhile California’s (and Colorado’s) laws include \nuniversal opt-outs for the sale of personal information \nvia an automated browser signal153 like Global Privacy \nControl,154 opt-outs are limited to the use of data for \ntargeted advertising purposes, and/or information that \nis “sold” by the collector as defined by the statute. The \nopt-outs are not required to be enabled by default, \nthey only apply to internet browsers, and not all \nbrowsers support them.155 The CCPA, in particular, \ngives companies a means to not adopt an opt-out \nsignal if they offer multiple means for consumers to use \nWhite Paper\nRethinking Privacy \nin the AI Era\n36\nnon-automated opt-outs, but this approach places the \nburden on consumers to exercise these rights, which \nfew are likely to do given the burden of doing so.\nToday, the incentives are such that when there are \nno restraints placed on data collection and use, \ncompanies will collect as much data as they possibly \ncan using any means possible. This is the lesson \nfrom the U.S. experiment in self-regulation of the \ndata economy, and it is a lesson in failure from the \nperspective of privacy. Without regulation there is \nno reason for these incentives to change, but even \nthe GDPR demonstrates that data protection isn’t \nnecessarily enough as long as there continue to be \nweaknesses in the framework. In order for the data \nminimization and purpose limitation aspects of the \nFIPs to be fully effective, the consent piece of the \nframework must also function to give individuals \nmeaningful, not de minimis, consent. If we fail to \naddress the weaknesses in the consent model, and \ndo not provide a right to refuse or mitigate data \ncollection, particularly by third party collectors, we \nrisk losing even more ground on privacy and any \nsemblance of control over our personal data in the face \nof rising demand from AI. \nSome will respond to this argument that adopting \nthese measures will throw a wrench in the data \neconomy and stall, if not destroy, progress in \ndeveloping AI. We disagree. Pushing back against \nthe status quo of ubiquitous data collection will not \ncripple data intensive industries. It might slow the \nrate of some progress in AI, though this may be a \nfeature, not a bug. Many have raised objections about \nthe rate at which AI is presently developing, with \nconsiderable concerns about safety. Though data is \nnot a tangible resource and may not have the same \nshort-term material impacts on the environment when \nexploited,156 it does impact both humans and human \nrights. The utilization of data can beget real world \nharms. To date, personal data has been treated like \nan inexhaustible resource to be harvested at will and \nexploited as desired. As more features of human life \nare mediated through technology and more people \nhave access to the internet, we have seen data’s \nimpacts on society increase significantly over the \npast two decades. Online data collection and use is \nno longer limited to influencing purchasing behavior; \ndata use by governments and private companies can \nimpact not only civil rights, but also the functioning of \ndemocratic governments. As both technology use and \nAI continue to grow and spread, the need to address \nthese fundamental flaws in the FIPs framework only \nbecome more urgent. \nSuggestion 2: Focus on the AI \ndata supply chain to improve \nprivacy and data protection\nSummary: Regulating the AI data supply chain \nmust be a focus of any regulatory system that \naddresses data privacy.\nThe emergence of AI, particularly generative AI, \npresents a test for the FIPs-based privacy and data \nprotection frameworks, especially the GDPR. While \nexisting regulations may provide some oversight of the \ndata collected and processed directly by an AI system, \nthere is less clarity regarding oversight of the training \ndata pipeline. For example, there are presently no \ntransparency or documentation requirements in the \nEU157 or the United States for companies to report the \nprovenance of their training data, to document how \nit conforms with the principles of data minimization \nand purpose specification, or to otherwise address \nquality issues that could lead to downstream negative \nprivacy outcomes such as a leakage of personally \nidentifiable information in a system’s outputs.158\nWhite Paper\nRethinking Privacy \nin the AI Era\n37\nFurthermore, existing regulations do not address how \nindividuals can learn whether their data is included in a \ncompany’s training datasets, what to do if that data is \ninadvertently revealed by an AI system in its outputs, \nor whether individuals can request deletion of it and \nthen verify its removal. With generative AI systems, \nthere is the additional complexity of untangling \nwhether an output of personal identifiable information \nwas hallucinated or based on inferences made by the \nmodel without the actual data in the training dataset.159\nUnderstanding the data development pipeline \nbecomes even more complex when companies source \nAI “models-as-a-service” from other businesses. In \nsuch instances, the relationship between the consumer \nand both the training data and model is a step \nremoved. But as the U.S. Federal Trade Commission \nrecently warned, that arm’s-length relationship does \nnot provide exemption from liability with regard to AI \ncompanies’ privacy commitments to the consumers \nwhose data power the models, and potentially to the \nbusiness utilizing the model(s) as well.160 Not only do \nprivacy implications arise from using models trained \non data of questionable or unknown provenance but \nalso with the collection of data from consumers using \na system operating on the underlying model. For \nexample, will that data be ingested by the frontline \nsystem to be used for training or fine-tuning purposes \nof the primary model? If the answer is yes, how will \nconsumers be properly informed in order to assert \ntheir due process rights?\nThese issues raise the need for a data governance \nframework that is aligned with data privacy concerns \nthat goes beyond the individual rights provided by \nthe FIPs. While individuals and their personal data \nare most certainly implicated in these governance \nquestions, they also raise societal level concerns that \naren’t captured by considering these activities solely \nfrom an individual rights perspective. \nWhy Training Data Matters\nTraining data is AI’s oxygen; an AI system cannot \nexist without it. Data quality and training set size \nare important inputs that impact the outputs of an \nAI system. Defining data quality can be a slippery \nconcept, but it generally refers to the accuracy \nand relevance of the data used to train the system \ngiven the system’s goals.161 “Dirty” data can include \nmislabeled data, biased data, error-ridden data, and \ndata that is inadequate for representing the problem in \nquestion, to name a few.162\nEstimating the desired size of a training dataset isn’t \nan exact science; while there is evidence that larger \ndatasets appear to improve system capabilities,163\nlarger doesn’t definitionally mean better. For example, \nan LLM such as ChatGPT has apparently benefitted \nfrom its massive dataset in terms of the system’s \nquality of response that mimics human reasoning, \nbut certainly not in terms of the accuracy of its \nresponses.164 In contrast, a relatively small synthetic \ndataset such as TinyStories using words and structure \nappropriate for young children has been successfully \nWhile existing regulations may \nprovide some oversight of the data \ncollected and processed directly \nby an AI system, there is less clarity \nregarding oversight of the training \ndata pipeline. \nWhite Paper\nRethinking Privacy \nin the AI Era\n38\nused to train small language models in coherent \nEnglish.165 The creators argue that there are distinct \nadvantages (namely interpretability and coherence of \noutput) of a simpler approach. Similarly, UC Berkeley’s \nKoala chatbot research prototype, built atop Meta’s \nLLaMA model, uses a curated dataset that the \ncreators claim offers results that are competitive with \nboth Stanford’s Alpaca and ChatGPT.166 The authors \nargue that their findings “suggest [...] that models \nthat are small enough to be run locally can capture \nmuch of the performance of their larger cousins if \ntrained on carefully sourced data. This might imply, for \nexample, that the community should put more effort \ninto curating high-quality datasets, as this might do \nmore to enable safer, more factual, and more capable \nmodels than simply increasing the size of existing \nsystems.”\nQuality and size do not necessarily have a direct \nrelationship—meaning a large dataset does not \nguarantee either higher or lower output quality. \nHowever, given the time and expense required to \ncreate a high-quality dataset, it is likely that generative \nsystems in particular that use large datasets have \nmore quality issues.167 For very large models such \nas LLMs, the size of the training dataset has been \ndecisively traded off for quality. Broadly scraping the \ninternet will yield content that is severely biased, toxic, \ninaccurate, spam-laden, or includes “by-catch” such \nas personal data.168 Going big on data yields benefits \nin some contexts, but scraping the world’s data should \nnot be thought of as the inevitable (and only) path for \ncreating advanced AI systems.\nPrivacy Issues with the Dataset Pipeline\nKhan and Hanna, in their paper decomposing the \ncomponents of dataset development, helpfully \nidentify two types of actors—data subjects and model \nsubjects—in the dataset development pipeline.169 Data \nsubjects are the individuals whose data was collected \nand included in the training data; model subjects are \nthose subject to the decisions of the downstream \nmodel.170 Thus far, regulators have been focused \nmore on the privacy issues raised for model subjects \nthan data subjects, though the release of generative \nAI systems to the public surfaced the issue of what \ndata protections applied to data subjects. While the \nbulk of the attention placed on AI systems has been \nwith system outputs, system inputs also matter. The \ninclusion of personal or identifiable data in training \ndatasets not only makes it possible that a model may \nmemorize and output that data, it also raises the issue \nof consent. Do data subjects know that their data \nwas included in a system’s training data?171 Were they \nasked for consent before being included? What rights \ndo they have to request exclusion or deletion from \nthese datasets? Can individuals have their personal \ndata deleted from a model? And how do individuals \neven proceed in discovering whether their data was \nincluded? \nTraining datasets present new challenges for \nregulators. The assumption in the FIPs-based rights \nframework is that individuals are engaged in a first\u0002party data relationship where the individual knows \nwho the data collector is and who may have consented \n(albeit perhaps under less than ideal circumstances, \nsuch as through notice and consent) to having their \ndata collected. This assumption is one reason FIPs\u0002based privacy and data protection regulations have \nbeen inadequate in fighting data collection by third \nparties such as advertising networks or data brokers. \nData that is scraped from unknown online sources, \npurchased or traded from data brokers, or reused \nby a data processor that had consent for the original \npurpose for which it was collected but not the \nsubsequent reuse, might all be components of training \ndatasets. While the GDPR may provide EU data \nWhite Paper\nRethinking Privacy \nin the AI Era\n39\nprotection regulators with the means to curb some of \nthese uses of data, presently in the United States these \nuses are broadly unregulated, and it is unlikely that an \nADPPA-esque law would prevent all of them. Even so, \nwithout more specific data minimization and purpose \nlimitation rules, or limits targeted at data gathered or \nreused for training purposes, individuals will continue \nto bear the burden of identifying where their data \nmay be ingested, submitting deletion requests, and \notherwise shoulder the bulk of the labor required to \nmaintain their privacy. \nArguably there are potential harms and risks for data \nsubjects raised by training datasets. The paradigm of \ndata collection by default means that today individuals \nare having their personal information pulled into \nmodel development without their explicit knowledge. \nThere are also complex technical issues around the \nquestion of whether one can request the deletion of \none’s data from a trained model without requiring the \ndestruction of the model itself. To date, the FTC has \nrequired both data and model deletion in a few cases \nwhere they argued that a company used customer \ndata without consent.172 Research by our Stanford \ncolleague Professor James Zou demonstrates that \n“approximate data deletion” is a potential method \nfor deleting data from models without requiring \nretraining, though whether this approach has practical \napplications is an open question.173 Companies will be \nincentivized to argue that data deletion from trained \nmodels cannot be accomplished to avoid the cost of \ncomplying. Further, without transparency requirements \nfor documenting training data, including inputs for \nretraining, being able to determine whether one’s \npersonal data was used to train (or retrain) a model will \nbe a challenge. \nTransparency and Governance for the Data \nDevelopment Pipeline\nThe gaps in FIPs-based frameworks with respect to \nthe privacy issues stemming from the creation and use \nof training data must be filled by a strategy that looks \nbeyond individual rights to address them. Policymakers \nmust establish new regulations or standards across \nthe data development supply chain that create and \nmandate dataset documentation and transparency \nprocesses. These should include documenting the \nprovenance or source of any data used for training and, \nif the data is related to an individual, include whether \nit was obtained with consent.174 This approach could \nincentivize the creation of new technical standards for \ndata subjects to uniquely identify their own data and \ndesignate its appropriate uses, a strategy that could \nalso have copyright management implications as well. \nFocus on the data supply chain\nThe principles of data minimization and purpose \nlimitation should be part of any strategy to address \ndataset development. However, data minimization in \nparticular is a concept that may prove challenging for \nregulators to enforce beyond more egregious abuses \nunless it is given more specificity. Similarly, developers \nshould be able to demonstrate that their uses of data \nPolicymakers must establish new \nregulations or standards across \nthe data development supply chain \nthat create and mandate dataset \ndocumentation and transparency \nprocesses. \nWhite Paper\nRethinking Privacy \nin the AI Era\n40\nfor training purposes are consistent with what was \ndisclosed and consented to at the point of collection, \nwithout the use of obfuscatory tactics such as broad, \nindeterminate disclosures (i.e., “We use your personal \ninformation to improve our products and services”). \nBut these two principles are not enough. A supply \nchain approach for AI data governance would bring \ngreater transparency to the entire life cycle of dataset \ndevelopment and foreground the need for responsible \ndata management from creation to deletion (including \nhighlighting the need for deletion when appropriate).175\nTo be clear, we are not arguing for data governance \nfor manufacturing supply chains, but rather to treat \nthe process for building datasets used to train AI \nas a supply chain. However, transparency is not the \nonly goal; attention to quality will both improve the \nperformance of AI systems and resolve some of the \nissues with bias and poor generalizability, which will in \nturn increase trustworthiness.176\nThere is an emergent field of dataset documentation \npractices that can aid in determining data provenance, \ntracking consent, and providing greater transparency \nabout the source of the data used to train AI models.177\nBut it is important to stress that these processes \nare still immature and policymakers should not \nmandate specific approaches at this stage without \nmore research and experimentation, such as through \nregulatory sandboxes, to understand which forms \nof documentation clearly address privacy risks and \nharms. Should algorithmic impact assessments gain \nsteam as a governance tool, they could require a form \nof dataset assessment as well to fully understand how \ndataset creation and management practices influence \nalgorithmic development. It is also possible that data \nprotection impact assessments are broadened in \nscope to include the impacts of datasets themselves, \nand that particular categories or types of data could \ntrigger higher risk classifications and obligations. As \none such example, California is considering requiring \nrisk assessments for certain high-risk category \nautomated systems when processing training data that \nincludes consumers’ personal information.178\nHowever, critical questions remain: What forms of \ndocumentation will be the most effective in achieving \nthe goals of protecting both individual and societal \ndata privacy? Are there other portions of the dataset \nsupply chain, such as data labeling, that deserve more \nscrutiny? And, importantly, under what circumstances \nwill these forms of documentation identify systems \nthat should not be built or deployed? Unfortunately, \nwe are not yet at a stage where the answer to this last \nquestion is clearly apparent. It is certainly true that \nthere is value in creating meaningful data compliance \nmeasures by requiring dataset creators to adopt \ncontrols and responsible practices for collecting \nand managing data to at a minimum provide greater \ntransparency into the creation process. This is an \narea where regulatory sandboxing would be an \nappropriate approach to help determine the types \nof documentation and processes that would provide \nregulators and researchers with greater transparency \nwithout creating an onerous burden on businesses. \nIncentivize responsible practices\nAny approach to resolving the privacy issues raised by \ntraining data must address existing incentives in the \ndata marketplace to obtain data cheaply or unethically. \nWhile mandating compliance requirements can \nshift both business practice and culture, doing so is \nunlikely to address the incentives that drive a race to \nthe bottom with anti-privacy data practices. In short, \nit’s hard to compete with free and unregulated data, \nparticularly when competition exists across legal \njurisdictions. \nWhite Paper\nRethinking Privacy \nin the AI Era\n41\nShifting the paradigm of data collection by default to \none where data collection requires permission and \npotentially negotiation is the prohibitive strategy; \nthe incentive-based strategy would, ideally, make \nethically sourced data cheaper, less risky, and of higher \nquality than either scraped data or data obtained \nfrom the third party data collection ecosystem. That \nis why adopting a purely regulatory approach may \nhave limited effectiveness. We need public and \nprivate investment in ethically sourced datasets, not \nonly to disincentivize scraping but also to open the \ndata ecosystem to a broader set of actors than the \nlargest tech companies who have a near monopoly \non consumer data. We also need investment in the \ntechnical underpinnings of the data infrastructure \nthat could support ethical data sourcing. This \nmeans creating new open standards for personal \ndata management, as well as new legal vehicles for \nmanaging, pooling, and licensing data. Synthetic \ndata, too, is an option for some contexts, as is using \ntechnical privacy measures, such as differential privacy \nor homomorphic encryption, to share or access data in \nnon-identifiable or secure ways. \nFinally, we need to make public data more accessible \nfor research and industry uses, as we and our Stanford \ncolleagues argue would come from creating a National \nAI Research Resource.179 Of the many ways this could \nbe accomplished, one idea is to support the creation \nof publicly available datasets that contend with issues \nof not only privacy and consent but ideally intellectual \nproperty as well. Data is as valuable a resource for \neconomic development and human flourishing as our \nnatural resources; accordingly, public investment in \ndata resources can bring social benefits as well as \ncreate the conditions for ethical data use. \nSuggestion 3: Flip the script on \nthe management of personal \ndata\nSummary: Support the development of data \nintermediaries as a way to both support and \nautomate the exercise of individual data rights \nand preferences, as well as collective privacy.\nNearly 30 years have elapsed since the creation of \nthe commercial internet, and yet the fundamentals \nof online data exchange remain largely unchanged, \nspecifically the unbalanced flow of data from \nconsumers to companies. New vectors for data \ncollection (e.g., third party cookies, smartphone APIs) \nhave emerged since the late 1990s, but the basic \nparadigm of paying for access to online products and \nservices with our data has only become more ossified. \nOur rapid transition from AI winter to the current, \nurgent AI gold rush has largely been enabled by the \nmassive quantity of data held by private companies \nand across the publicly accessible internet. Without \nthe access to data that researchers—and more \nimpactfully private companies—have had over the \npast decade, the trajectory of AI development that we \nare witnessing would arguably be far less rapid and \npronounced. \nWe need public and private \ninvestment in ethically sourced \ndatasets, not only to disincentivize \nscraping but also to open the data \necosystem to a broader set of actors.\nWhite Paper\nRethinking Privacy \nin the AI Era\n42\nAnd therein lies the conundrum in our current AI \nrace: AI is progressing at an enormous clip, in large \npart thanks to the availability and quantity of the \ndata we have all generated. But we, individually \nand collectively, were not asked if we wished to \ncontribute our data to this experiment, one that has \nalready caused and will likely continue to cause \nsignificant harm. And even if those concerns prove \nto be overblown, whether any of us, or the public at \nlarge, materially or personally benefits from AI—as \ncompared to those who are developing it and stand to \nprofit handsomely from its growth—is undetermined. \nFree AI services may be of little value if you’ve lost \nyour livelihood to AI automation. As we have argued at \nStanford HAI, the private, for-profit development of AI \nis far more likely to result in applications that benefit \nthe developers rather than society at large.180 Without \nsufficient public investment in basic AI research and \ndevelopment, we are far less likely to see public, \nnonprofit uses of AI that materially benefit society. \nOver the past decade, the trading of one’s data for \naccess to free online services has drawn increased \nscrutiny and critique from the public.181 The explosion \nof generative AI over the past year ratcheted up \nthose concerns once it became clear that the massive \ndatasets required by generative systems were built on \ndata that in some cases was obtained from any source \npossible. The public is increasingly aware—and often \ndispleased—with the hidden cost of free products: \npaying with your data, a trade-off that isn’t always \nin one’s best interest. In particular, there is growing \nconcern that one’s data isn’t just used as payment \nbut also to target and manipulate one’s behavior and \nchoices. \nIncreasingly, there are calls for data equity in the form \nof compensation for the use of personal data. Former \npresidential candidate Andrew Yang even made data \nequity a platform in his 2020 campaign.182 Startups \nare forming to capitalize on this trend by making \nthe exchange of data for access to online resources \nboth explicit and direct by attempting to provide \nusers with direct compensation for their data use. \nHowever, this approach is typically focused on selling \nindividuals’ data, which may yield a short-term gain but \nultimately lead to alienation and further, irrevocable \nloss of control over data.183 Instead of focusing on \nproperty rights, the focus of these efforts should be \non developing a permissioning or licensing regime for \nindividuals’ data through regulated data intermediaries, \nallowing them to reap any direct benefits while \nmaintaining control over its use.184\nWe argue above that data collection by default is one \nof the key issues we must address if we are to have \na fighting chance at improving our data privacy as \nAI development fuels the demand for personal data. \nBelow, we elaborate on the two components that \ncan aid with that goal: facilitating the creation of data \nintermediaries and building the technical architecture \nto support personal data licensing. These solutions are \nnot quick, short-term fixes to the problems we discuss \nabove, but instead take a longer view of the structural \nchanges we need to make to our digital ecosystem as \nwe face a world with ever larger appetites for data. \nData Intermediaries & Data Permissioning \nInfrastructure\nData intermediaries are a core component of “flipping \nthe script” on data collection to move away from a \ndigital landscape where data is acquired by digital \nservices to one where consumers decide the terms \nby which they will allow companies to use their data. \nThe goal of a data intermediary is to facilitate and \nmanage “data relations between data rights holders \n(such as people or businesses), depending on the \nparties’ relationships, intentions and resources. They \nWhite Paper\nRethinking Privacy \nin the AI Era\n43\ndo so by encapsulating, communicating and enacting \nthe shared interests of the relevant parties and \nsafeguarding their interests. At their most basic level \nthey facilitate the exchange of information; at their \nmost sophisticated they can assume decisionmaking, \nincluding on behalf of people.”185 They can take many \nforms: data trusts, collaboratives, cooperatives, \ncommons, stewards; for-profit, nonprofit, and publicly \nowned. \nA key goal for the consumer data space is to create \nan intermediary that can be entrusted to share data \naccording to the individual’s preferences, potentially \neven negotiating the terms, and to do so by delegating \none’s preferences to a software system that handles \ntransactions without constant micromanaging and \nintervention by the individual. Instead of one’s data \nbeing collected by hundreds or even thousands of \ndistinct entities, a data intermediary could manage \nthese sharing relationships and ensure that one’s data \nis being used according to negotiated terms. Instead \nof a data ecosystem where one’s data is spread hither \nand yon, and one has zero control over where it goes \nor how it is used, the data intermediary model centers \nthe individual and their preferences for data sharing, \nensuring that they maintain control and benefits flow \ndirectly to them. \nWhile we are focused primarily on the consumer \ndata space here, data intermediaries could manage \npersonal data across multiple contexts: healthcare, \ngenetics, education, research, and so on. They can also \noffer the power and leverage that come from collective \nbargaining; individuals today are disempowered \nin their data relationships with companies. Data \nintermediaries can provide the strength in numbers for \nindividuals to negotiate in their best interests. In order \nto do so, intermediaries would need to be regulated \nand hold fiduciary responsibilities for their users’ \ndata. Accordingly, they may be better positioned to \nact in consumers’ best interests than placing similar \nobligations on technology platforms for whom the \ncustomers’ best interests will always be in tension with \nthe company’s.\nHowever, without a technical layer to facilitate \nintermediary relationships, this vision will, at best, \ngrow very slowly, and certainly not fast enough to \nstem the ever-increasing tide of data collection. This \nis not an insignificant challenge—as evidenced by \ndata portability mandates. While data portability \nrequirements have been on the books since the \nadoption of the GDPR, the vision of being able to \ntake one’s social media data specifically and move \nit to a competing service remains largely unfulfilled \nbecause it is a complex technical problem: taking \nproprietary data that is potentially co-managed (e.g., \nthreaded posts) and sorting out who is the primary \n“owner” and how to make such data interoperable \nbetween services. The intermediary concept takes \nthe challenging goal of data portability and turns it on \nits head, focusing on the individual first as the atomic \nunit rather than with disparate platforms’ proprietary \nsystems.\nThe case of social media interoperability may in fact be \na far more difficult scenario than attempting to create \na standard for the exchange of a core set of personal \ndata. Sir Tim Berners-Lee has been working on this \nData intermediaries can provide the \nstrength in numbers for individuals to \nnegotiate in their best interests.\nWhite Paper\nRethinking Privacy \nin the AI Era\n44\nprecise problem for several years; his company, Inrupt, \nis creating an open-source standard for information \n“pods” (called Solid186) to create the infrastructure \nto overcome this challenge.187 Presently there is \nmore focus on building data intermediaries between \nbusinesses than for the consumer marketplace. But \nuntil we see policymakers advocate for or incentivize \nconsumer-centered solutions, we are left with the \nmore piecemeal approaches built on exercising data \nrights. Earlier in this paper, we discussed the Global \nPrivacy Control proposed standard and the need for \npolicymakers to mandate its adoption by browser \ncreators. Another example of using existing data \nrights to push for collective action is by Consumer \nReports, which created the app Permission Slip to use \nCalifornia’s CCPA Do Not Sell and Delete My Data \nrights, which includes a provision to allow individuals \nto designate an agent to exercise these rights. Rather \nthan having to identify and contact companies one \nby one, the app facilitates making these requests for \nconsumers from a single location. These examples \nshow the initial promise of automating this type of data \ncontrol, but the possible solutions could be much more \nrobust and advanced than these initial strategies.\nShifting the data ecosystem away from data collection \nby default will require more than policy change. It will \nrequire new legal entities for data governance, such as \ndata intermediaries, with clearly defined duties of care \nsuch that we do not inadvertently create a new class of \ndata brokers (or we create them with tight regulations \nand high ethical standards). It will require technical \ninfrastructure that can enable (and incentivize) ethical, \nhuman-centered data exchanges that respect user \nconsent and usage preferences. It may also require \nreopening the digital rights management debates of \nthe 2000s to allow individuals to protect both their \ndata privacy as well as intellectual property rights \nwith content they share online. But, as we discussed \nearlier, it will also require an investment in public data \nresources as well, so the value of large datasets does \nnot rest solely in the hands of private actors. \nWhite Paper\nRethinking Privacy \nin the AI Era\n45\nChapter 5: Conclusion\nAs we rapidly transition from an internet era into an \nAI era, not surprisingly, regulation continues to lag \nbehind advances in technology. The privacy and data \nprotection laws already in place or in development \ntoday do and will continue to impact the growth and use \nof AI systems, through both implicit privacy protections \nand explicit measures addressing automated decision\u0002making. But they are a reaction to the business models \nof the past, not the future. They were developed in \nresponse to the privacy threats that emerged over the \nlast 20 years of the commercial internet, before the \nwidespread deployment of powerful AI technologies.\nToday, the data protection and due process rights \nthat existing regulations offer are jeopardized by \nemergent technologies such as generative AI. Data \ncollection practices, such as data scraping or third \nparty data collection, conflict with many regulatory \nassumptions pertaining to how companies collect data \nfrom and about individuals. In addition to increasing \nthe demand for data, AI systems also create new \navenues for privacy harm through novel approaches \nto data collection and output. Existing regulations and \nframeworks also do not consider the ways in which \nprivacy risks and harms from data can be relational \nand social in nature. Data can implicate individual \nprivacy through inferences made through others’ \ndata, and through known or inferred relationships. \nAnd widespread data collection and surveillance can \npresent societal level risks and harms that individually \nfocused regulations simply cannot address. \nOne thing we do know with certainty is that AI needs \ndata to advance. Without data, AI advancement \ngrinds to a halt. As AI innovation continues apace, \nit is therefore crucial that we get the data piece of \nAI development “right” by centering individual and \ncollective privacy harms. Our focus throughout this \npaper has been to highlight the need to focus on \nthe data life cycle that feeds AI development as a \nmechanism for transparency and accountability—while \naddressing the data privacy issues raised by AI that \ncannot be resolved through the exercise of individual \ndata rights. \nOur goal is to provide policymakers and other \nstakeholders with sufficient background information \nto understand why existing privacy regulations and \nframeworks do not fully address these problems, and \nto offer suggestions for short and longer term actions \nto take to protect and preserve privacy while also \nensuring greater transparency and accountability in the \nAI data development life cycle. There are many other \noptions available to address these issues; this work will \ncertainly not be the final word on them. But we hope \nthat we have provided sufficient rationale as to why \nthey must be addressed if we are to have both privacy \nand AI moving forward. \nWe agree with scholars who have argued that nothing \nabout AI development is inevitable.188 We also don’t \nthink that privacy is dead or that a lack of protection \nof individual and collective data rights is a foregone \nconclusion.189 While data can be stored indefinitely, it \nisn’t necessarily permanent, and it certainly ages, often \npoorly, with time. The infrastructures that we build \nto support it aren’t immutable, and aren’t necessarily \nresilient in the fact of change or catastrophe. Even the \nfoundations upon which many are presently building \nfoundation models are mutable. We are the creators of \ntechnology, and we can shape it to support and reflect \nhuman values, rather than to undo them. In sum: the \nfuture of AI isn’t yet written; we can choose how we \nwant it to unfold. \nWhite Paper\nRethinking Privacy \nin the AI Era\n46\nEndnotes\n1 Rishi Bommasani, Christie M. Lawrence, Lindsey A. Gailmard, Caroline Meinhardt et al., “Decoding the White House AI Executive Order’s Achievements,” \nStanford Institute for Human-Centered Artificial Intelligence, November 2, 2023, https://hai.stanford.edu/news/decoding-white-house-ai-executive-orders\u0002achievements.\n2 See Lina M. Khan, “Joint Statement on Enforcement Efforts Against Discrimination and Bias in Automated Systems,” Federal Trade Commission, April 25, \n2023, https://www.ftc.gov/legal-library/browse/cases-proceedings/public-statements/joint-statement-enforcement-efforts-against-discrimination-bias\u0002automated-systems; California Legislature, An act to amend, repeal, and add Section 35 of the Code of Civil Procedure, and to amend, add, and repeal Section \n20010 of the Elections Code, relating to elections, California Assembly Bill No. 730, October 3, 2019, https://leginfo.legislature.ca.gov/faces/billTextClient.\nxhtml?bill_id=201920200AB730, Chapter 493; New York City Council, A Local Law to amend the administrative code of the city of New York, in relation \nto automated employment decision tools, New York City Council Int. No. 1894-A, December 11, 2021, https://legistar.council.nyc.gov/LegislationDetail.\naspx?ID=4344524&GUID=B051915D-A9AC-451E-81F8-6596032FA3F9&Options=Advanced&Search, Subchapter 25.\n3 When we use the term ‘provenance,’ we refer to the general sourcing of data. This is a different use than that of groups such as Content Credentials (https://\ncontentcredentials.org/), which aim to ensure trustworthy data creation. It is possible that efforts like these could eventually encompass a broader concept of \nprovenance to include metadata that will allow for tracking the source of created data for supply purposes.\n4 Colleen McClain, Michelle Faverio, Monica Anderson, and Eugenie Park. “How Americans View Data Privacy,” Pew Research Center, October 18, 2023. https://\nwww.pewresearch.org/internet/2023/10/18/how-americans-view-data-privacy/. \n5 Jennifer King, “Privacy, Disclosure, and Social Exchange Theory,” University of California Berkeley, Spring 2018, https://escholarship.org/uc/item/5hw5w5c1. \n6 National Institute of Standards and Technology, “Artificial Intelligence Risk Management Framework (AI RMF 1.0),” January 26, 2023, https://nvlpubs.nist.gov/\nnistpubs/ai/NIST.AI.100-1.pdf.\n7 Information Commissioner’s Office, “What is automated individual decision-making and profiling?” https://ico.org.uk/for-organisations/uk-gdpr-guidance-and\u0002resources/individual-rights/automated-decision-making-and-profiling/what-is-automated-individual-decision-making-and-profiling/.\n8 Nestor Maslej, Loredana Fattorini, Erik Brynjolfsson, John Etchemendy et al., “The AI Index 2023 Annual Report,” Stanford Institute for Human-Centered Artificial \nIntelligence, April 2023, https://aiindex.stanford.edu/report/, Chapter 6.\n9 Khan, “Joint Statement on Enforcement Efforts Against Discrimination and Bias in Automated Systems.”\n10 Katrina Zhu, “The State of State AI Laws: 2023,” Electronic Privacy Information Center, August 3, 2023, https://epic.org/the-state-of-state-ai-laws-2023/.\n11 Raphael Gellert and Serge Gutwirth, “The legal construction of privacy and data protection,” Computer Law & Security Review 29(5), October 2013, https://doi.\norg/10.1016/j.clsr.2013.07.005, 522-530. \n12 Gellert et al., “The legal construction of privacy and data protection.”\n13 For a general background, see Robert Gellman, “Fair Information Practices: A Basic History – Version 2.22,” SSRN, April 2022, https://doi.org/10.2139/\nssrn.2415020. \n14 Chris Jay Hoofnagle, “The Origin of Fair Information Practices: Archive of the Meetings of the Secretary’s Advisory Committee on Automated Personal Data \nSystems (SACAPDS),” SSRN, July 15, 2014, http://dx.doi.org/10.2139/ssrn.2466418.\n15 U.S. Secretary’s Advisory Committee on Automated Personal Data Systems, “Records, Computers and the Rights of Citizens, Chapter IV: Recommended \nSafeguards for Administrative Personal Data Systems,” June 30, 1973, https://aspe.hhs.gov/reports/records-computers-rights-citizens; See also Hoofnagle, \n“The Origin of Fair Information Practices.” The original FIPs are: A) There must be no personal-data record-keeping systems whose very existence is secret; \nB) There must be a way for an individual to find out what information about him is in a record and how it is used; C) There must be a way for an individual to \nprevent information about him obtained for one purpose from being used or made available for other purposes without his consent; D) There must be a way for \nan individual to correct or amend a record of identifiable information about himself; E) Any organization creating, maintaining, using, or disseminating records of \nidentifiable personal data must assure the reliability of the data for their intended use and must take reasonable precautions to prevent misuse of the data.\n16 Mark MacCarthy, Regulating Digital Industries: How Public Oversight Can Encourage Competition, Protect Privacy, and Ensure Free Speech, Brookings \nInstitution Press, 2023.\n17 For an extended discussion on the origin of the FIPs, see Gellman, “Fair Information Practices.” \n18 Private actors were largely ignored in the development of the FIPs, in part because in the early 1970s, it was presumed that no private actor could ever collect as \nmuch data as a government entity.\n19 Organisation for Economic Co-operation and Development, “Recommendation of the Council concerning Guidelines Governing the Protection of Privacy and \nTransborder Flows of Personal Data,” September 1980, https://legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0188. \n20 Gellman, “Fair Information Practices,” p.24.\n21 The 2013 OECD Principles include both purpose specification and use limitation principles, while the GDPR uses the term purpose limitation. While we \nacknowledge that there are subtleties between the two terms, this paper will use the term purpose limitation as specified in the GDPR, see European Parliament \nand European Council, Regulation 2016/679 of the European Parliament and of the Council [General Data Protection Regulation, hereinafter GDPR], April 27, 2016, \nhttps://eur-lex.europa.eu/eli/reg/2016/679/oj, Article 5(1)(b).\n22 Chris Jay Hoofnagle, Bart van der Sloot, and Frederik Zuiderveen Borgesius, “The European Union general data protection regulation: what it is and what it \nmeans,” Information & Communications Technology Law 28(1), February 2019, https://www.tandfonline.com/doi/full/10.1080/13600834.2019.1573501, 65-98. \nWhite Paper\nRethinking Privacy \nin the AI Era\n47\n23 European Parliament and European Council, GDPR, Article 4(1): “‘personal data’ means any information relating to an identified or identifiable natural person \n(‘data subject’); an identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an \nidentification number, location data, an online identifier or to one or more factors specific to the physical, physiological, genetic, mental, economic, cultural or \nsocial identity of that natural person.”\n24 European Parliament and European Council, GDPR, Article 22.\n25 Jennifer King, Caroline Meinhardt, Abel Ribbink, Pete Warden et al., “Response to the Request for Comments on Trade Regulation Rule on Commercial \nSurveillance and Data Security,” Stanford Institute for Human-Centered Artificial Intelligence, November 21, 2022, https://hai.stanford.edu/sites/default/\nfiles/2022-12/HAI%20-%20FTC%20ANPR%20Comments.pdf.\n26 Sebastião Barros and Gabriela Zanfir-Fortuna, “FPF Report: Automated Decision-Making Under the GDPR - A Comprehensive Case-Law Analysis,” Future of \nPrivacy Forum, May 17, 2022, https://fpf.org/blog/fpf-report-automated-decision-making-under-the-gdpr-a-comprehensive-case-law-analysis/.\n27 Information Commissioner’s Office, “Data protection impact assessments,” May 19, 2023, https://ico.org.uk/for-organisations/uk-gdpr-guidance-and\u0002resources/accountability-and-governance/guide-to-accountability-and-governance/accountability-and-governance/data-protection-impact-assessments/.\n28 117th U.S. Congress, American Data Privacy and Protection Act, H.R.8152, June 21, 2022, https://www.congress.gov/bill/117th-congress/house-bill/8152.\n29 Cameron F. Kerry, “How privacy legislation can help address AI,” Brookings, July 7, 2023, https://www.brookings.edu/articles/how-privacy-legislation-can-help\u0002address-ai/; The White House, Blueprint for an AI Bill of Rights, October 2022, https://www.whitehouse.gov/ostp/ai-bill-of-rights/.\n30 Andrew Folks, “US State Privacy Legislation Tracker,” International Association of Privacy Professionals, December 8, 2023, https://iapp.org/resources/article/\nus-state-privacy-legislation-tracker/#enacted-laws. \n31 George P. Slefo, “Marketers and Tech Companies Confront California’s Version of GDPR,” AdAge, June 29, 2018, https://adage.com/article/digital/california\u0002passed-version-gdpr/314079.\n32 Anupam Chander, Margot E. Kaminski, and William McGeveran, “Catalyzing Privacy Law,” Minnesota Law Review 105, 2021, https://scholarship.law.umn.edu/\nmlr/3305, 1733.\n33 California Privacy Protection Agency, “A New Landmark for Consumer Control Over Their Personal Information: CPPA Proposes Regulatory Framework for \nAutomated Decisionmaking Technology,” November 27, 2023, https://cppa.ca.gov/announcements/2023/20231127.html.\n34 Rogier Creemers and Graham Webster, “Translation: Personal Information Protection Law of the People’s Republic of China – Effective Nov. 1, 2021,” Stanford \nDigiChina Project, August 20, 2021, https://digichina.stanford.edu/work/translation-personal-information-protection-law-of-the-peoples-republic-of-china\u0002effective-nov-1-2021/.\n35 Alexa Lee, Mingli Shi, Qiheng Chen, Jamie P. Horsley et al., “Seven Major Changes in China’s Finalized Personal Information Protection Law,” Stanford DigiChina \nProject, September 15, 2021, https://digichina.stanford.edu/work/seven-major-changes-in-chinas-finalized-personal-information-protection-law/.\n36 In its August 2021 IPO filing, AI giant SenseTime listed “complex and evolving laws, regulations and governmental policies regarding privacy and data \nprotection” as a risk factor, citing the PIPL and Data Security Law, which were about to take effect. See Jane Li, “China’s new data laws are a risk factor in a facial\u0002recognition giant’s IPO filing,” Quartz, August 30, 2021, https://qz.com/2053040/chinas-new-data-laws-are-a-risk-factor-in-sensetimes-ipo-filing.\n37 Graham Webster, “Translation: Chinese Authorities Announce $1.2B Fine in DiDi Case, Describe ‘Despicable’ Data Abuses,” Stanford DigiChina Project, July 21, \n2022, https://digichina.stanford.edu/work/translation-chinese-authorities-announce-2b-fine-in-didi-case-describe-despicable-data-abuses/.\n38 Mingli Shi, Jamie P. Horsley, and Xiaomeng Lu, “Forum: Unpacking the DiDi Decision,” Stanford DigiChina Project, July 22, 2022, https://digichina.stanford.edu/\nwork/forum-unpacking-the-didi-decision/.\n39 Inioluwa Deborah Raji, Peggy Xu, Colleen Honigsberg, and Daniel Ho, “Outsider Oversight: Designing a Third Party Audit Ecosystem for AI Governance,” \nProceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society, July 2022, https://dl.acm.org/doi/abs/10.1145/3514094.3534181, 557–571.\n40 Emanuel Moss, Elizabeth Anne Watkins, Ranjit Singh, Madeleine Clare Elish et al., “Assembling Accountability: Algorithmic Impact Assessment for the Public \nInterest,” Data & Society, June 29, 2021, https://datasociety.net/library/assembling-accountability-algorithmic-impact-assessment-for-the-public-interest/. \n41 Brent Mittelstadt, “Interpretability and Transparency in Artificial Intelligence,” In The Oxford Handbook of Digital Ethics, edited by Carissa Véliz, Oxford University \nPress, October 20, 2021, https://doi.org/10.1093/oxfordhb/9780198857815.013.20.\n42 Samantha Lai and Brooke Tanner, “Examining the intersection of data privacy and civil rights,” Brookings, July 18, 2022, https://www.brookings.edu/articles/\nexamining-the-intersection-of-data-privacy-and-civil-rights/. \n43 118th U.S. Congress, Algorithmic Accountability Act of 2023, H.R. 5628, September 21, 2023, https://www.congress.gov/bill/117th-congress/house-bill/6580/\ntext. See also 117th U.S. Congress, American Data Privacy and Protection Act, Section 207(c).\n44 Gopal Ratnam, “Data privacy law seen as needed precursor to AI regulation,” Roll Call, September 26, 2023, https://rollcall.com/2023/09/26/data-privacy-law\u0002seen-as-needed-precursor-to-ai-regulation/.\n45 Lauren Leffer, “Your Personal Information Is Probably Being Used to Train Generative AI Models,” Scientific American; Sara Morrison, “The Tricky Truth About \nHow Generative AI Uses Your Data,” Vox, July 27, 2023, https://www.vox.com/technology/2023/7/27/23808499/ai-openai-google-meta-data-privacy-nope. \n46 Kashmir Hill, Your Face Belongs To Us: A Secretive Startup’s Quest To End Privacy As We Know It, Random House, September 2023.\n47 Natasha Lomas, “Clearview fined again in France for failing to comply with privacy orders,” TechCrunch, May 10, 2023, https://techcrunch.com/2023/05/10/\nclearview-ai-another-cnil-gspr-fine/. \n48 Illinois General Assembly, Biometric Information Privacy Act, 740 ILCS 14/, October 3, 2008, https://www.ilga.gov/legislation/ilcs/ilcs3.\nasp?ActID=3004&ChapterID=57.\nWhite Paper\nRethinking Privacy \nin the AI Era\n48\n49 Gabriela Zanfir-Fortuna, “How Data Protection Authorities are De Facto Regulating Generative AI,” Future of Privacy Forum, September 12, 2023, https://fpf.\norg/blog/how-data-protection-authorities-are-de-facto-regulating-generative-ai/.\n50 Office of the Privacy Commissioner of Canada, “Statement on Generative AI,” June 21, 2023, https://www.priv.gc.ca/en/opc-news/speeches/2023/\ns-d_20230621_g7/. \n51 ACLU v. Clearview AI, May 11, 2022, https://www.aclu.org/cases/aclu-v-clearview-ai. \n52 Pamela Samuelson, “Generative AI meets copyright,” Science 381, July 14, 2023, 10.1126/science.adi0656, 158-161.\n53 The FTC requested information on how training data was obtained, what categories of content were included in the corpus, who was responsible for reviewing \nthe data, how reinforcement learning was used for training, the extent to which models produced information about individuals (including the accuracy of personal \ninformation), the extent to which fine-tuning was used to address these issues, and how personal information is ingested by the system. See Center for AI and \nDigital Policy, “In the Matter of Open AI (Federal Trade Commission 2023),” https://www.caidp.org/cases/openai/.\n54 Tonya Riley, “The FTC’s biggest AI enforcement tool? Forcing companies to delete their algorithms,” Cyberscoop, July 5, 2023, https://cyberscoop.com/ftc\u0002algorithm-disgorgement-ai-regulation/.\n55 Lina M. Khan, “Lina Khan: We Must Regulate A.I. Here’s How,” The New York Times, May 3, 2023, https://www.nytimes.com/2023/05/03/opinion/ai-lina-khan\u0002ftc-technology.html.\n56 Luca Bertuzzi, “Italian data protection authority bans ChatGPT citing privacy violations,” Euractiv, March 31, 2023, https://www.euractiv.com/section/artificial\u0002intelligence/news/italian-data-protection-authority-bans-chatgpt-citing-privacy-violations/.\n57 Garante per la protezione dei dati personali [hereafter Garante], “Artificial intelligence: stop to ChatGPT by the Italian SA, Personal data is collected \nunlawfully, no age verification system is in place for children,” March 31, 2023, https://www.garanteprivacy.it/web/guest/home/docweb/-/docweb-display/\ndocweb/9870847#english.\n58 Garante, “Artificial intelligence: stop to ChatGPT by the Italian SA.”\n59 Garante, “ChatGPT: Italian DPA notifies breaches of privacy law to OpenAI,” January 29, 2024, https://garanteprivacy.it/home/docweb/-/docweb-display/\ndocweb/9978020#english.\n60 Digital Curation Centre, Trilateral Research, and School of Informatics, The University of Edinburgh, “The Role of Data In AI: Report for the Data Governance \nWorking Group of the Global Partnership of AI,” November 2020, https://gpai.ai/projects/data-governance/role-of-data-in-ai.pdf.\n61 Mark MacCarthy, “In Defense of Big Data Analytics,” In The Cambridge Handbook of Consumer Privacy, edited by Evan Selinger, Jules Polonetsky, and Omer \nTene, Cambridge University Press, 2018.\n62 King et al., “Response to the Request for Comments on Trade Regulation Rule on Commercial Surveillance and Data Security.”\n63 See Asia Biega and Michele Finck, “Reviving Purpose Limitation and Data Minimisation in Data-Driven Systems,” Technology and Regulation, 2021, https://doi.\norg/10.26116/techreg.2021.004, 44–61; Hongyi Wen, Longqi Yang, Michael Sobolev, and Deborah Estrin, “Exploring Recommendations Under User-Controlled Data \nFiltering,” Proceedings of the 12th ACM Conference on Recommender Systems, September 2018, https://doi.org/10.1145/3240323.3240399, 72-76.\n64 According to Open AI’s technical report, GPT-4 was pre-trained “using both publicly available data (such as internet data) and data licensed from third-party \nproviders.” According to Anthropic’s model card, Claude models “are trained on a proprietary mix of publicly available information from the Internet, datasets that \nwe license from third party businesses, and data that our users affirmatively share or that crowd workers provide.” See OpenAI, “GPT-4 Technical Report,” arXiv \npreprint, March 15, 2023, https://arxiv.org/pdf/2303.08774.pdf; Anthropic, “Model Card and Evaluations for Claude Models,” July 8, 2023, https://paperswithcode.\ncom/paper/model-card-and-evaluations-for-claude-models.\n65 Kristen E. Busch, “Generative Artificial Intelligence and Data Privacy: A Primer,” Congressional Research Service, May 23, 2023, https://crsreports.congress.gov/\nproduct/pdf/R/R47569.\n66 The draft EU AI Act includes in Annex IV (d): “where relevant, the data requirements in terms of datasheets describing the training methodologies and \ntechniques and the training data sets used, including a general description of these data sets, information about their provenance, scope and main\n67 See Rishi Bommasani, Kevin Klyman, Shayne Longpre, Sayash Kapoor et al., “The Foundation Model Transparency Index,” arXiv preprint, October 19, 2023, \nhttps://arxiv.org/abs/2310.12941; Cat Zakrzewski, “FTC investigates OpenAI over data leak and ChatGPT’s inaccuracy,” The Washington Post, July 13, 2023, https://\nwww.washingtonpost.com/technology/2023/07/13/ftc-openai-chatgpt-sam-altman-lina-khan/.\n68 Danielle Keats Citron and Daniel J. Solove, “Privacy Harms,” Boston University Law Review 102, 2022, https://scholarship.law.gwu.edu/faculty_\npublications/1534/, 793.\n69 Daniel J. Solove, “A Taxonomy of Privacy,” University of Pennsylvania Law Review 154, 2006, https://scholarship.law.upenn.edu/penn_law_review/vol154/iss3/1, \n477. \n70 Existing harms include: surveillance, secondary uses of data, loss of control over data, data security risks, increased identification, and accessibility that can \ncause intrusion. Hao-Ping Lee, Yu-Ju Yang, Thomas Serban von Davier, Jodi Forlizzi et al., “Deepfakes, Phrenology, Surveillance, and More! A Taxonomy of AI \nPrivacy Risks,” arXiv preprint, October 11, 2023, https://arxiv.org/abs/2310.07879.\n71 See Will Knight, “AI Chatbots Can Guess Your Personal Information From What You Type,” Wired, October 2023, https://www.wired.com/story/ai-chatbots-can\u0002guess-your-personal-information/; Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski et al., “Extracting Training Data from Large Language Models,” \n30th USENIX Security Symposium, June 15, 2021, https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting, 2633-2650; Lauren Leffer, \n“Your Personal Information Is Probably Being Used to Train Generative AI Models,” Scientific American, October 19, 2023, https://www-scientificamerican-com.\nstanford.idm.oclc.org/article/your-personal-information-is-probably-being-used-to-train-generative-ai-models/; Robin Staab, Mark Vero, Mislav Balunović, \nand Martin Vechev, “Beyond Memorization: Violating Privacy Via Inference with Large Language Models,” arXiv preprint, October 11, 2023, https://arxiv.org/\nabs/2310.07298.\n72 Shoshana Zuboff, The Age of Surveillance Capitalism: The Fight For A Human Future At The New Frontier of Power, Profile Books, 2018.\nWhite Paper\nRethinking Privacy \nin the AI Era\n49\n73 Future of Privacy Forum, “Unfairness by Algorithm: Distilling the Harms of Automated Decision-Making,” December 11, 2017, https://fpf.org/blog/unfairness-by\u0002algorithm-distilling-the-harms-of-automated-decision-making/. \n74 See Cameron F. Kerry, “Protecting privacy in an AI-driven world,” Brookings, February 10, 2020, https://www.brookings.edu/articles/protecting-privacy-in-an\u0002ai-driven-world/; Office of the Victoria Information Commissioner, “Artificial Intelligence and Privacy - Issues and Challenges,” August 2018, https://ovic.vic.gov.au/\nprivacy/resources-for-organisations/artificial-intelligence-and-privacy-issues-and-challenges/. \n75 M.R. Leiser and Cristiana Santos, “Dark Patterns, Enforcement, and the emerging Digital Design Acquis: Manipulation beneath the Interface,” SSRN, April 27, \n2023, https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4431048.\n76 See Harry Surden, “Structural Rights in Privacy,” SMU Law Review 60, 2007, https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1004675, 1605-1629; \nWoodrow Hartzog and Frederic D. Stutzman, “The Case for Online Obscurity,” California Law Review 101, 2013, https://ssrn.com/abstract=1597745, 1.\n77 Kate Crawford and Jason Schultz, “Big Data and Due Process: Toward a Framework to Redress Predictive Privacy Harms,” Boston College Law Review 55, 2014, \nhttps://ssrn.com/abstract=2325784, 93.\n78 Future of Privacy Forum, “Unfairness by Algorithm.” \n79 Citron et al., “Privacy Harms.”\n80 Nora A. Draper and Joseph Turow, “The corporate cultivation of digital resignation,” New Media & Society 21(8), March 8, 2019, https://journals.sagepub.com/\ndoi/full/10.1177/1461444819833331, 1824-1839.\n81 Bobbie Johnson, “Privacy no longer a social norm, says Facebook founder,” The Guardian, January 10, 2010, https://www.theguardian.com/technology/2010/\njan/11/facebook-privacy. \n82 Jeffrey Dastin, “Insight - Amazon scraps secret AI recruiting tool that showed bias against women,” Reuters, October 10, 2018, https://www.reuters.com/article/\nus-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G/.\n83 Andrew Lee Park, “Injustice Ex Machina: Predictive Algorithms in Criminal Sentencing,” UCLA Law Review 19, February 19, 2019, https://www.uclalawreview.\norg/injustice-ex-machina-predictive-algorithms-in-criminal-sentencing.\n84 Will Douglas Heaven, “Predictive policing algorithms are racist. They need to be dismantled,” MIT Technology Review, July 17, 2020, https://www.\ntechnologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice.\n85 Ratnam, “Data privacy law seen as needed precursor to AI regulation.”\n86 Without clear operational criteria for purpose limitation and data minimization, interpreting these principles in practice will be dependent upon how a \ncompany’s practices are stated in their privacy policy. If a company’s statements are ambiguous (as many are today) regarding how data might be repurposed, \ncompanies will have too much latitude to interpret these principles as they wish.\n87 See King et al., “Response to the Request for Comments on Trade Regulation Rule on Commercial Surveillance and Data Security”; Divya Shanmugam, Samira \nShabanian, Fernando Diaz, Michèle Finck et al., “Learning to Limit Data Collection via Scaling Laws: A Computational Interpretation for the Legal Principle of Data \nMinimization,” Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, June 21, 2022, https://doi.org/10.48550/arXiv.2107.08096, \n839-849.\n88 Reuben Binns and Valeria Gallo, “Data Minimisation and Privacy-Preserving Techniques in AI Systems,” Information Commissioner’s Office, August 21, 2021, \nhttps://ico.org.uk/about-the-ico/media-centre/ai-blog-data-minimisation-and-privacy-preserving-techniques-in-ai-systems/.\n89 Biega et al., “Reviving Purpose Limitation and Data Minimisation in Data-Driven Systems.”\n90 Shanmugam et al., “Learning to Limit Data Collection via Scaling Laws.”\n91 Woodrow Hartzog, “The Inadequate, Invaluable Fair Information Practices,” Maryland Law Review 76, 2017, https://www.marylandlawreview.org/volume-76-\nissue-4-symposium/the-inadequate-invaluable-fair-information-practices, 952. \n92 King et al., “Response to the Request for Comments on Trade Regulation Rule on Commercial Surveillance and Data Security.”\n93 King, “Privacy, Disclosure, and Social Exchange Theory.” \n94 Daniel J. Solove, “Introduction: Privacy Self-Management and the Consent Dilemma,” Harvard Law Review 126, May 2013, https://harvardlawreview.org/print/\nvol-126/introduction-privacy-self-management-and-the-consent-dilemma/, 1880.\n95 MacCarthy, Regulating Digital Industries.\n96 European Parliament and European Council, GDPR, Article 6 (a-f).\n97 European Commission, “What does ‘grounds of legitimate interest’ mean?” https://commission.europa.eu/law/law-topic/data-protection/reform/rules\u0002business-and-organisations/legal-grounds-processing-data/grounds-processing/what-does-grounds-legitimate-interest-mean_en. \n98 Since Brexit, the U.K. has observed its own (albeit nearly identical) version of the GDPR. See Information Commissioner’s Office, “The UK GDPR,” https://ico.org.\nuk/for-organisations/data-protection-and-the-eu/data-protection-and-the-eu-in-detail/the-uk-gdpr/.\n99 Information Commissioner’s Office, “When can we rely on legitimate interests?” https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/lawful\u0002basis/legitimate-interests/when-can-we-rely-on-legitimate-interests/.\n100 Sam Schechner and Jeff Horwitz, “Meta to Let Users Opt Out of Some Targeted Ads, but Only in Europe,” The Wall Street Journal, March 30, 2023, https://\nwww.wsj.com/articles/meta-to-let-users-opt-out-of-some-targeted-ads-but-only-in-europe-44b20b6d.\n101 Noyb, “Meta (Facebook, Instagram) switching to “Legitimate Interest” for Ads,” March 30, 2023, https://noyb.eu/en/meta-facebook-instagram-switching\u0002legitimate-interest-ads.\nWhite Paper\nRethinking Privacy \nin the AI Era\n50\n102 Meta Newsroom, “Facebook and Instagram to Offer Subscription for No Ads in Europe,” October 30, 2023, https://about.fb.com/news/2023/10/facebook\u0002and-instagram-to-offer-subscription-for-no-ads-in-europe.\n103 The EU’s e-Privacy Directive, which is responsible for the deluge of cookie banners, is one exception. The enactment of the GDPR helped nudge the consent \ndialog for cookie banners toward a more simplified “reject all” choice, should individuals wish to use it, rather than just a notification that cookies were being \ncollected, which does not provide the user with any actual choices. It remains to be seen whether the renegotiation of the e-Privacy Directive will incorporate a \nmethod like Global Privacy Control to make cookie acceptance or rejection more streamlined, and whether an infrastructure can be adopted to preserve one’s \nchoices across multiple modalities, not just browsers (and potentially, not just cookies).\n104 Colorado presently has the most detailed regulations on the books regarding automated decision-making. For a current listing of U.S. state privacy laws, see \nFolks, “US State Privacy Legislation Tracker.”\n105 Organisation for Economic Cooperation and Development, “10 Policy Issues In Data Protection and Privacy: Concepts and Perspectives,” Proceedings of the \nOECD Seminar 24th-26th June 1974, June 1974, https://glgonzalezfuster.files.wordpress.com/2022/06/policy-issues-in-data-protection-and-privacy-concepts\u0002and-perspectives-proceeding-of-th-eoecd-seminar-24th-26th-june-1974.pdf, 73. \n106 Information Commissioner’s Office, “What is automated individual decision-making and profiling?” These documents in turn reference the Article 29 Working \nParty’s explainer on profiling and ADM, see European Commission, Guidelines on Automated individual decision-making and Profiling for the purposes of \nRegulation 2016/679 (wp251rev.01), Article 29 Data Protection Working Party, August 22, 2018, https://ec.europa.eu/newsroom/article29/items/612053/en. \n107 European Parliament and European Council, GDPR, Article 22, Sec.1. Profiling and automated processing is defined in Recital 71: “The data subject should \nhave the right not to be subject to a decision, which may include a measure, evaluating personal aspects relating to him or her which is based solely on automated \nprocessing and which produces legal effects concerning him or her or similarly significantly affects him or her, such as automatic refusal of an online credit \napplication or e-recruiting practices without any human intervention. Such processing includes ‘profiling’ that consists of any form of automated processing of \npersonal data evaluating the personal aspects relating to a natural person, in particular to analyse or predict aspects concerning the data subject’s performance at \nwork, economic situation, health, personal preferences or interests, reliability or behaviour, location or movements, where it produces legal effects concerning him \nor her or similarly significantly affects him or her.” \n108 European Parliament and European Council, GDPR, Article 4 (4).\n109 Angelina Wang, Sayash Kapoor, Solon Barocas, and Arvind Narayanan, “Against Predictive Optimization: On the Legitimacy of Decision-Making Algorithms \nthat Optimize Predictive Accuracy,” Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, June 2023, https://doi.\norg/10.1145/3593013.3594030, 626.\n110 While many suggest George Orwell’s 1984 as an example of such a society, Terry Gilliam’s 1985 film “Brazil” is another excellent example: “Brazil (1985 film),” \nWikipedia, February 1, 2024, https://en.wikipedia.org/wiki/Brazil_(1985_film). \n111 The CCPA was initially introduced as a ballot initiative directly to the voters, not through the legislative process. See Nicholas Confessore, “The Unlikely Activists \nthat Took On Silicon Valley–And Won,” The New York Times Magazine, August 14, 2018, https://www.nytimes.com/2018/08/14/magazine/facebook-google\u0002privacy-data.html.\n112 Natasha Singer, “Group Behind California Privacy Law Aims to Strengthen It,” The New York Times, September 24, 2019, https://www.nytimes.\ncom/2019/09/24/technology/group-behind-california-privacy-law-aims-to-strengthen-it.html.\n113 California Privacy Protection Agency, Addendum to Draft Automated Decisionmaking Technology Regulations: New Rules Subcommittee Format Proposal, \nDecember 2023, https://cppa.ca.gov/meetings/materials/20231208_item2_addendum.pdf. \n114 California Privacy Protection Agency, “A New Landmark for Consumer Control Over Their Personal Information.”\n115 Colorado Department of Law, Colorado Privacy Act Rules, 4 CCR 904-3, March 15, 2023, https://coag.gov/app/uploads/2023/03/FINAL-CLEAN-2023.03.15-\nOfficial-CPA-Rules.pdf.\n116 Rabel J. Burdge, “A brief history and major trends in the field of impact assessment,” Impact Assessment 9(4), 1991, https://www.tandfonline.com/doi/abs/10.108\n0/07349165.1991.9726070, 93-104. \n117 107th U.S. Congress, e-Government Act of 2002, H.R. 2458, December 17, 2002, https://www.congress.gov/107/plaws/publ347/PLAW-107publ347.pdf. \n118 European Commission, “When is a Data Protection Impact Assessment (DPIA) required?” https://commission.europa.eu/law/law-topic/data-protection/\nreform/rules-business-and-organisations/obligations/when-data-protection-impact-assessment-dpia-required_en.\n119 The California Privacy Protection Agency is in its rulemaking phase for these regulations as of early 2024. The regulations may not be finalized until July 2024.\n120 See Theodore P. Augustinos, “Emerging Requirements for Data Protection Impact Assessments,” Locke Lord, Spring 2022, https://www.lockelord.com/\nnewsandevents/publications/2022/05/emerging-requirements; California Privacy Protection Agency, New Rules Subcommittee Revised Draft Risk Assessment \nRegulations, December 2023, https://cppa.ca.gov/meetings/materials/20231208_item2_draft_clean.pdf.\n121 For a general example, see 118th U.S. Congress, Algorithmic Accountability Act of 2023. For an overview of algorithmic impact assessments, see Moss et al., \n“Assembling Accountability.”\n122 Anne Josephine Flanagan, Jen King, and Sheila Warren, “Redesigning Data Privacy: Reimagining Notice & Consent for human technology interaction,” World \nEconomic Forum, July 30, 2020, https://www.weforum.org/publications/redesigning-data-privacy-reimagining-notice-consent-for-humantechnology-interaction/.\n123 Marci Meingast, Jennifer King, and Deirdre K. Mulligan, “Embedded RFID and Everyday Things: A Case Study of the Security and Privacy Risks of the US \ne-Passport,” 2007 IEEE International Conference on RFID, April 2007, https://ieeexplore.ieee.org/abstract/document/4143504, 7-14.\n124 Karen Weise, Cade Metz, Nico Grant, and Mike Isaac, “Inside the A.I. Arms Race That Changed Silicon Valley Forever,” The New York Times, December 5, 2023, \nhttps://www.nytimes.com/2023/12/05/technology/ai-chatgpt-google-meta.html. \n125 Mehtab Khan and Alex Hanna, “The Subjects and Stages of AI Dataset Development: A Framework for Dataset Accountability,” Ohio State Technology Law \nJournal 19, 2023, https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4217148.\nWhite Paper\nRethinking Privacy \nin the AI Era\n51\n126 California Privacy Protection Agency, New Rules.\n127 California Privacy Protection Agency, New Rules.\n128 Salomé Viljoen, “A Relational Theory of Data Governance,” Yale Law Journal 131(2), November 2021, https://www.yalelawjournal.org/feature/a-relational\u0002theory-of-data-governance.\n129 See European Digital Rights, “Civil society calls for AI red lines in the European Union’s Artificial Intelligence proposal,” January 12, 2021, https://edri.org/our\u0002work/civil-society-call-for-ai-red-lines-in-the-european-unions-artificial-intelligence-proposal/; Fanny Hidvegi, Daniel Leufer, and Estelle Massé, “The EU should \nregulate AI on the basis of rights, not risks,” Access Now, February 17, 2021, https://www.accessnow.org/eu-regulation-ai-risk-based-approach/.\n130 Amba Kak and Sarah Myers West, “AI Now 2023 Landscape: Confronting Tech Power”, AI Now\n131 Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell, “On the Dangers of Stochastic Parrots: Can Language Models Be Too \nBig?,” Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, March 2021, https://dl.acm.org/doi/abs/10.1145/3442188.3445922, \n610–623.\n132 Dahlia Peterson and Samantha Hoffman, “Geopolitical implications of AI and digital surveillance adoption,” Brookings, June 2022, https://www.brookings.edu/\narticles/geopolitical-implications-of-ai-and-digital-surveillance-adoption/. \n133 Karen Hao, “AI is sending people to jail—and getting it wrong,” MIT Technology Review, January 21, 2019, https://www.technologyreview.com/2019/01/21/\n137783/algorithms-criminal-justice-ai/. \n134 Ivey Dyson, “How AI Threatens Civil Rights and Economic Opportunities,” Brennan Center for Justice, November 16, 2023, https://www.brennancenter.org/\nour-work/analysis-opinion/how-ai-threatens-civil-rights-and-economic-opportunities.\n135 The 2023 Biden Executive Order on AI calls out this tension by mandating that privacy and security standards be incorporated into the software development \nlife cycle, and that the Office of Management and Budget must identify the commercial data agencies may have procured that contains personal identifiable \ninformation and provide guidance as to how to mitigate privacy risks in its use. See White House, Executive Order on the Safe, Secure, and Trustworthy \nDevelopment and use of Artificial Intelligence, October 30, 2023, https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive\u0002order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/. For example, §8(b)(i)(D) (“incorporation of safety, privacy, and \nsecurity standards into the software-development lifecycle for protection of personally identifiable information”) and §9(a)(i) (“evaluate and take steps to identify \ncommercially available information (CAI) procured by agencies, particularly CAI that contains personally identifiable information and including CAI procured \nfrom data brokers and CAI procured and processed indirectly through vendors, in appropriate agency inventory and reporting processes (other than when it is \nused for the purposes of national security); (ii)evaluate, in consultation with the Federal Privacy Council and the Interagency Council on Statistical Policy, agency \nstandards and procedures associated with the collection, processing, maintenance, use, sharing, dissemination, and disposition of CAI that contains personally \nidentifiable information (other than when it is used for the purposes of national security) to inform potential guidance to agencies on ways to mitigate privacy and \nconfidentiality risks from agencies’ activities related to CAI”).\n136 “It’s the economy, stupid,” Wikipedia, November 16, 2023, https://en.wikipedia.org/wiki/It%27s_the_economy,_stupid.\n137 European Parliament and European Council, Directive 2009/136/EC of the European Parliament and of the Council [ePrivacy Directive], November 25, 2009, \nhttps://edps.europa.eu/data-protection/our-work/subjects/eprivacy-directive_en. \n138 Federal Trade Commission, “Privacy Online: Fair Information Practices in the Electronic Marketplace - A Report to Congress,” May 2000, https://www.ftc.gov/\nsites/default/files/documents/reports/privacy-online-fair-information-practices-electronic-marketplace-federal-trade-commission-report/privacy2000text.pdf. \n139 King et al., “Online privacy notices don’t work.”\n140 For example, see Federal Trade Commission, “Rite Aid Banned from Using AI Facial Recognition After FTC Says Retailer Deployed Technology without \nReasonable Safeguards,” December 19, 2023, https://www.ftc.gov/news-events/news/press-releases/2023/12/rite-aid-banned-using-ai-facial-recognition-after\u0002ftc-says-retailer-deployed-technology-without.\n141 Federal Trade Commission, Trade Regulation Rule on Commercial Surveillance and Data Security, August 22, 2022, https://www.federalregister.gov/\ndocuments/2022/08/22/2022-17752/trade-regulation-rule-on-commercial-surveillance-and-data-security.\n142 European Parliament and European Commission, Proposal for a Regulation of the European Parliament and of the Council concerning the respect for private \nlife and the protection of personal data in electronic communications and repealing Directive 2002/58/EC [Regulation on Privacy and Electronic Communications], \nJanuary 10, 2017, https://www.europarl.europa.eu/legislative-train/theme-a-europe-fit-for-the-digital-age/file-jd-e-privacy-reform.\n143 European Commission, “Can data received from a third party be used for marketing?” https://commission.europa.eu/law/law-topic/data-protection/reform/\nrules-business-and-organisations/legal-grounds-processing-data/can-data-received-third-party-be-used-marketing_en. \n144 Federal Trade Commission, “FTC Proposes Strengthening Children’s Privacy Rule to Further Limit Companies’ Ability to Monetize Children’s Data,” December \n20, 2023, https://www.ftc.gov/news-events/news/press-releases/2023/12/ftc-proposes-strengthening-childrens-privacy-rule-further-limit-companies-ability\u0002monetize-childrens.\n145 Jennifer King, Daniel Ho, Arushi Gupta, Victor Wu et al., “The Privacy-Bias Tradeoff: Data Minimization and Racial Disparity Assessments in U.S. Government,” \nProceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, June 2023, https://dl.acm.org/doi/abs/10.1145/3593013.3594015, \n492–505.\n146 Jennifer King, “A Bill Designed to Protect Kids Could Change the Internet for the Better,” Tech Policy Press, September 15, 2022, https://www.techpolicy.\npress/a-bill-designed-to-protect-kids-could-change-the-internet-for-the-better/. \n147 Thomson Reuters Westlaw, “Barclays Official California Code of Regulations,” https://govt.westlaw.com/calregs/Document/\nIA39673F0D44D11ED8CB7A9089AB7E22E?h%2520Result&list=REGULATION_PUBLICVIEW&rank=8&t_T2=7004&t_S1=CA+ADC+s, 11 CA ADC § 7004(b-c). \n148 Apple Support, “If an app asks to track your activity,” https://support.apple.com/en-us/HT212025. \n149 Singular, “Limit Ad Tracking,” https://www.singular.net/glossary/limit-ad-tracking/. \nWhite Paper\nRethinking Privacy \nin the AI Era\n52\n150 Tiahn Wetzler, “ATT two years on: Opt-in rates keep climbing,” Adjust, July 5, 2023, https://www.adjust.com/blog/app-tracking-transparency-opt-in-rates/. \n151 King, “Online privacy notices don’t work.”\n152 Nick Doty, “It’s Time to Standardize the Global Privacy Control,” Center for Democracy and Technology, December 13, 2023, https://cdt.org/insights/its-time\u0002to-standardize-the-global-privacy-control/.\n153 See Colorado Department of Law, Colorado Privacy Act Rules, Rule 5.06(A)(1) (“The Universal Opt-Out Mechanism may communicate a Consumer’s opt\u0002out choice by sending an opt-out signal.”); California Consumer Privacy Act, §1798.185(a)(19)(A) (“Issuing regulations to define the requirements and technical \nspecifications for an opt-out preference signal sent by a platform, technology, or mechanism, to indicate a consumer’s intent to opt out of the sale or sharing of the \nconsumer’s personal information and to limit the use or disclosure of the consumer’s sensitive personal information.”)\n154 Global Privacy Control, https://globalprivacycontrol.org/. \n155 Katherine Chaves and Jamie Nafziger, “Universal Opt-Out/Global Privacy Control: Preparing for the New Online World,” JD Supra, December 7, 2022, https://\nwww.jdsupra.com/legalnews/universal-opt-out-global-privacy-7739024/.\n156 This is ignoring the material side effects of data processing and storage on the environment, through energy use, materials for computer hardware, etc. Data \ncollection and processing has material impacts, even if difficult to measure.\n157 Article 10 of the AI Act (not final at publication time) is a data governance provision focused on training, validation, and testing data sets for high risk systems \nand may address some quality issues, but not specifically the privacy based concerns we raise here. See Council of the European Union, Artificial Intelligence Act.\n158 Milad Nasr, Nicholas Carlini, Jon Hayase, Matthew Jagielski et al., “Extracting Training Data from ChatGPT,” November 28, 2023, https://not-just\u0002memorization.github.io/extracting-training-data-from-chatgpt.html.\n159 Drew Breunig, “Considering AI Privacy Scenarios,” dbreunig.com, May 15, 2023, https://www.dbreunig.com/2023/05/15/ai-privacy-scenarios.html.\n160 Federal Trade Commission, “AI Companies: Uphold Your Privacy and Confidentiality Commitments,” January 9, 2024, https://www.ftc.gov/policy/advocacy\u0002research/tech-at-ftc/2024/01/ai-companies-uphold-your-privacy-confidentiality-commitments.\n161 Digital Curation Centre et al., “The Role of Data In AI.”\n162 Nithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong et al., “Everyone wants to do the model work, not the data work,” Proceedings of the 2021 \nCHI Conference on Human Factors in Computing Systems, May 2021, https://dl.acm.org/doi/abs/10.1145/3411764.3445518.\n163 Alon Halevy, Peter Norvig, and Fernando Pereira, “The Unreasonable Effectiveness of Data,” IEEE Intelligent Systems 24(2), March 2009, https://ieeexplore.\nieee.org/document/4804817, 8-12.\n164 Craig S. Smith, “Hallucinations Could Blunt ChatGPT’s Success: OpenAI says the problem’s solvable, Yann LeCun says we’ll see,” IEEE Spectrum, March 13, \n2023, https://spectrum.ieee.org/ai-hallucination. \n165 Ronan Eldan and Yuanzhi Li, “TinyStories: How Small Can Language Models Be and Still Speak Coherent English?” arXiv preprint, May 12, 2023, https://arxiv.\norg/abs/2305.07759. The dataset is available on Hugging Face. See Hugging Face, TinyStories, https://huggingface.co/datasets/roneneldan/TinyStories. \n166 Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, et al., “Koala: A Dialogue Model for Academic Research,” Berkeley Artificial Intelligence Research, April \n3, 2023, https://bair.berkeley.edu/blog/2023/04/03/koala/.\n167 Sambasivan et al., “Everyone wants to do the model work, not the data work.”\n168 Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe, “Multimodal datasets: misogyny, pornography, and malignant stereotypes,” arXiv preprint, \nOctober 5, 2021, https://arxiv.org/abs/2110.01963.\n169 Khan et al., “The Subjects and Stages of AI Dataset Development.”\n170 Khan et al., “The Subjects and Stages of AI Dataset Development.”\n171 Chloe Xiang, “AI Is Probably Using Your Images and It’s Not Easy to Opt Out,” Vice, September 26, 2022, https://www.vice.com/en/article/3ad58k/ai-is\u0002probably-using-your-images-and-its-not-easy-to-opt-out. \n172 See Federal Trade Commission, “Rite Aid Banned from Using AI Facial Recognition After FTC Says Retailer Deployed Technology without Reasonable \nSafeguards,” December 19, 2023, https://www.ftc.gov/news-events/news/press-releases/2023/12/rite-aid-banned-using-ai-facial-recognition-after-ftc-says\u0002retailer-deployed-technology-without; Federal Trade Commission, “FTC and DOJ Charge Amazon with Violating Children’s Privacy Law by Keeping Kids’ Alexa \nVoice Recordings Forever and Undermining Parents’ Deletion Requests,” May 31, 2023, https://www.ftc.gov/news-events/news/press-releases/2023/05/\nftc-doj-charge-amazon-violating-childrens-privacy-law-keeping-kids-alexa-voice-recordings-forever; Federal Trade Commission, “FTC Takes Action Against \nCompany Formerly Known as Weight Watchers for Illegally Collecting Kids’ Sensitive Health Data,” March 4, 2022, https://www.ftc.gov/news-events/news/\npress-releases/2022/03/ftc-takes-action-against-company-formerly-known-weight-watchers-illegally-collecting-kids-sensitive; Federal Trade Commission, “FTC \nFinalizes Settlement with Photo App Developer Related to Misuse of Facial Recognition Technology,” May 7, 2021, https://www.ftc.gov/news-events/news/press\u0002releases/2021/05/ftc-finalizes-settlement-photo-app-developer-related-misuse-facial-recognition-technology.\n173 Zachary Izzo, Mary Anne Smart, Kamalika Chaudhuri, and James Zou, “Approximate Data Deletion from Machine Learning Models,” Proceedings of The 24th \nInternational Conference on Artificial Intelligence and Statistics, March 2021, https://proceedings.mlr.press/v130/izzo21a.html, 2008-2016.\n174 Ben Hutchinson, Andrew Smart, Alex Hanna, Emily Denton et al., “Towards Accountability for Machine Learning Datasets: Practices from Software Engineering \nand Infrastructure,” Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, March 2021, https://doi.org/10.1145/3442188.3445918, \n560–575. \n175 See generally Kenny Peng, Arunesh Mathur, and Arvind Narayanan, “Mitigating Dataset Harms Requires Stewardship: Lessons from 1000 Papers,” arXiv \npreprint, August 6, 2021, https://arxiv.org/abs/2108.02922; Amandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, Emily Denton, et al., “Data and its (dis)\ncontents: A survey of dataset development and use in machine learning research,” Patterns 2(11), November 12, 2021, https://doi.org/10.1016/j.patter.2021.100336.\nWhite Paper\nRethinking Privacy \nin the AI Era\n53\n176 Weixin Liang, Girmaw Abebe Tadesse, Daniel Ho, L. Fei-Fei et al., “Advances, challenges and opportunities in creating data for trustworthy AI,” Nature Machine \nIntelligence 4, August 17, 2022, https://doi.org/10.1038/s42256-022-00516-1, 669–677. \n177 See Alex LaCasse, “Proposed data provenance standards aim to enhance trustworthiness of AI training data,” International Association of Privacy Professionals, \nJanuary 17, 2024, https://iapp.org/news/a/leading-corporations-proposed-data-provenance-standards-aims-to-enhance-quality-of-ai-training-data/; Timnit \nGebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan et al., “Datasheets for datasets,” Communications of the ACM 64(12), https://dl.acm.org/\ndoi/fullHtml/10.1145/3458723, 86-92; Emily M. Bender and Batya Friedman, “Data Statements for Natural Language Processing: Toward Mitigating System Bias \nand Enabling Better Science,” Transactions of the Association for Computational Linguistics 6, December 1, 2018, https://direct.mit.edu/tacl/article/doi/10.1162/\ntacl_a_00041/43452/Data-Statements-for-Natural-Language-Processing, 587–604; The Data Nutrition Project, https://datanutrition.org/; Data Provenance \nExplorer, https://www.dataprovenance.org/. \n178 California Privacy Protection Agency, New Rules.\n179 Daniel E. Ho, Jennifer King, Russell C. Wald, and Christopher Wan, “Building a National AI Research Resource,” Stanford Institute for Human-Centered Artificial \nIntelligence, October 2021, https://hai.stanford.edu/white-paper-building-national-ai-research-resource.\n180 Ho et al., “Building a National AI Research Resource.” \n181 Zuboff, The Age of Surveillance Capitalism.\n182 Hannah Klein, “ Andrew Yang Wants You to Own and Sell Your Data,” Slate, June 23, 2020, https://slate.com/technology/2020/06/yang-launches-data\u0002dividend-project.html.\n183 Cameron F. Kerry and John B. Morris, “Why data ownership is the wrong approach to protecting privacy,” Brookings, June 26, 2019, https://www.brookings.\nedu/articles/why-data-ownership-is-the-wrong-approach-to-protecting-privacy/. \n184 Pamela Samuelson, “Privacy As Intellectual Property?” Stanford Law Review 52, 1999, https://heinonline.org/HOL/P?h=hein.journals/stflr52&i=1145, 1125.\n185 World Economic Forum, “Advancing Digital Agency: The Power of Data Intermediaries,” February 15, 2022, https://www.weforum.org/publications/advancing\u0002digital-agency-the-power-of-data-intermediaries/. See also M. Micheli, E. Farrell, B. Carballa Smichowski, M. Posada Sanchez et al., “Mapping the landscape of \ndata intermediaries: Emerging models for more inclusive data governance,” Publications Office of the European Union, August 24, 2023, https://publications.jrc.\nec.europa.eu/repository/handle/JRC133988.\n186 Solid, https://solidproject.org/. \n187 Steve Lohr, “He Created the Web. Now He’s Out to Remake the Digital World,” The New York Times, January 10, 2021, https://www.nytimes.com/2021/01/10/\ntechnology/tim-berners-lee-privacy-internet.html. \n188 Kak et al., “AI Now 2023 Landscape: Confronting Tech Power.”\n189 Evan Selinger and Woodrow Hartzog, “Stop Saying Privacy is Dead,” Medium, October 18, 2018, https://medium.com/@evanselinger/stop-saying-privacy-is\u0002dead-513dda573071.",
    "length": 197401,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Stanford CRFM",
    "url": "https://crfm.stanford.edu/2023/12/01/ai-act-compromise.html",
    "text": "Stanford CRFM\n[![]] \n## Towards compromise: A concrete two-tier proposal for foundation models in the EU AI Act\n#### **Authors:**[Rishi Bommasani] and[Tatsunori Hashimoto] and[Daniel E. Ho] and[Marietje Schaake] and[Percy Liang] \nThe European Union is in the final days of negotiating the AI Act, which would be the world’s first comprehensive legislation on artificial intelligence. Negotiations have[stalled] as the path forward on foundation models has become murky. The Parliament argues that[requirements are necessary], the Commission has proposed a[two-tiered approach], and the Council remains divided. Namely, the Spanish presidency is[pushing] for a two-tiered approach, while the trio of France, Germany, and Italy advocate for “[mandatory self-regulation] ” with minimal requirements.\nBeyond the negotiators, we highlight positions across stakeholders:\n* **Industry.**Foundation model developer Mistral AI has loudly[raised concerns] about the compliance burden of the AI Act. Other foundation model developers as part of DigitalEurope have[argued] against a two-tiered regime. In contrast, the broad ecosystem of European small and medium enterprises covered by the European DIGITAL SME Alliance have[advocated] for requirements on foundation models.\n* **Academia.**Computer scientist Yoshua Bengio has made clear that requirements are essential,[saying] “exemption [of foundation models] …goes completely against the European values of protecting the public and even the spirit of doing the AI Act in the first place”. Legal scholar Philipp Hacker has[raised] similar points in his previous[three-layer proposal] for foundation models as has[philosopher Luciano Floridi].\n* **Civil society.**Across the ideological spectrum, groups like the[Ada Lovelace Institute], the[AI Now Institute],[Open Future], the[Future of Life Institute], and the[European creative community] all have firmly argued for[requirements on foundation models]. Several groups favor a[tiered approach]. Former Estonian president Kersti Kaljulaid[writes] that a “tiered approach …allows for targeting so that competitors to major AI companies can emerge without onerous restrictions”.\n**Our position.**A comprehensive AI regulatory scheme cannot ignore foundation models: foundation models are at the epicenter of AI, the very basis for public awareness of AI, and are pivotal to the future of AI. Given the significance of foundation models, we argue that pervasive opacity compromises accountability for foundation models. Foundation models and the surrounding ecosystem are insufficiently transparent, with[recent evidence] showing transparency is deteriorating further.\nWithout sufficient transparency, the EU cannot implement meaningful accountability mechanisms as we cannot govern what we cannot see.\nPlacing all burdens on downstream application developers is*myopic*: foundation models play an outsized role in the AI value chain and their providers often wield asymmetric bargaining power to shape contracts. With that said, requirements for foundation model providers should provide value while not imposing unnecessary burden.\nTherefore, we put forth an exact proposal for how foundation models should be governed under the AI Act. In line with Spain, we designate two[tiers]: in the generic tier, foundation models are subject to disclosure requirements that improve transparency at minimal marginal compliance cost for companies. And in the high-impact tier, which triggers once foundation models show significant impact in society, more strenuous requirements are imposed. Our proposal interpolates between the positions of different EU negotiators to work towards compromise.\nTherefore, the proposal represents a potential common ground between the negotiators: it is not our view on what constitutes ideal regulation absent the current political realities for the AI Act negotiation.\n# The generic tier for foundation models\nFor all foundation models,[1] we recommend transparency-centric requirements for five reasons.\n1. **Demonstrated consensus.**In spite of some disagreement, we see transparency as the overarching focus for mandatory requirements across the[Parliament position],[Spanish proposal], and the[Italian, German, and French proposal].\n2. **Current opacity.**The[Foundation Model Transparency Index] [confirms] that the foundation model ecosystem suffers from[pervasive opacity].\n3. **Historic opacity.**The history of digital technologies is riddled with harms borne from opacity ([ghost work],[dark patterns],[scandals]). Years after those materialized, the EU has taken action via the Digital Services Act and the establishment of the[DSA Transparency Database].\n4. **Product safety.**The EU AI Act’s[legislative purpose and legal basis] is in product safety regulation under the EU’s[new legislative framework]; transparency aligns directly with these objectives.\n5. **Downstream coordination.**Foundation models are general-purpose technologies that power myriad downstream applications across market sectors; the EU recognizes that transparency on these critical intermediary technologies is essential for compliance, safety, and innovation along the value chain.\nWe recommend the following requirements.[2] The requirements we propose are proportional disclosure requirements for any commercial model provider or well-resourced entity. We additionally view exemptions for very low-resource entities (e.g. students, hobbyists, non-commercial academic groups) as important to ensuring compliance burdens remain proportionate with impact. We do not discuss those exemptions in detail here as this proposal is aimed at entities with substantial societal impact.\n1. **Registration.**Foundation models when placed on the market, put into service or made available in the Union shall be registered in a public EU database alongside high-risk AI systems.\n* Source: This requirement slightly expands the registration requirement under[Amendment 115 in Recital 69 of the Parliament position].\n* **Transparency Report.**Foundation model providers shall disclose the following Transparency Report[3] on their public website, along all distribution channels for the foundation model, and in the EU database:\n1. **Name.**Trade name and any additional unambiguous reference allowing the identification of the foundation model.\n* Source: This requirement is identical to the requirement under[Amendment 771 item 4 in Annex VIII –Section C of the Parliament position].\n* Note: In particular, this should be understood to require unambiguous versioning of a foundation model. For example, if OpenAI’s GPT-4 model is updated, the specific version should be identifiable.\n* **Data sources.**Description of the data sources used in the development of the foundation model.\n* Source: This requirement is identical to the requirement under[Amendment 771 item 5 in Annex VIII –Section C of the Parliament position].\n* Note: The AI Office and standards-setting bodies should be instructed to create templates and standards for the granularity of data sources required.\n* **Data summary.**Summary of the data, including the size of data, used in the development of the foundation model.\n* Source: This requirement modifies the requirement under[Amendment 399 item 4c in Article 28b of the Parliament position] to remove complications related to copyright.\n* Note: The AI Office and standards-setting bodies should incentivize the creation of tooling to produce summaries for large-scale datasets.\n* **Compute.**The amount of computing power, hardware, and time used in the development of the foundation model.\n* Source: This requirement clarifies the requirement under[Amendment 771 item 7 in Annex VIII –Section C of the Parliament position].\n* Note: The amount of computing power should be reported in FLOPs. The amount of hardware should be reported in relevant hardware units (e.g. number of NVIDIA A100-80GB GPUs). Standards-setting bodies should be required to establish standards for how training time should be measured.\n* **Environment.**The energy used and emissions emitted in the development of the foundation model OR the owner/provider and location of hardware.\n* Source: This requirement clarifies and expands the requirement under[Amendment 771 item 7 in Annex VIII –Section C of the Parliament position].\n* Note: To directly measure energy and emissions will likely require information about the specific data center. Therefore, if providers are not aware of this information (e.g. it is not provided by cloud providers), to facilitate compliance, they can instead disclose who operates the hardware in which location. From this information, along with the amount of hardware used and the duration, reasonable estimates can be computed for energy and emissions.\n* **Model.**The size, input modality and output modality of the foundation model.\n* Source: This requirement clarifies the misplaced requirement about model size under[Amendment 771 item 7 in Annex VIII –Section C of the Parliament position].\n* Note: In the event that either the input or output modality includes natural language, consideration should be made of whether disclosure should further specify which of the EU official languages are supported.\n* **Model properties.**A description of the significant capabilities and limitations of the foundation model.\n* Source: This requirement, along with the item below, is identical to the requirement under[Amendment 771 item 6 in Annex VIII –Section C of the Parliament position].\n* Note: The AI Office and standards-setting bodies should be instructed to create templates for disclosing this information.\n* **Risks and mitigations.**The reasonably foreseeable risks and mitigations implemented for each of those risks. For any unmitigated risks, an explanation on the reason why they are not mitigated.\n* Source: This requirement, along with item above, is identical to the requirement under[Amendment 771 item 6 in Annex VIII –Section C of the Parliament position], with the noted adjustment.\n* Note: Relative to the Parliament position, this phrasing clarifies the correspondence between risks and associated mitigations. It also weakens “cannot be mitigated” to “are not mitigated”.\n* **Evaluations.**Description of the model’s performance, including on public benchmarks and state of the art industry benchmarks.\n* Source: This requirement modifies the requirement under[Amendment 771 item 8 in Annex VIII –Section C of the Parliament position], replacing the “or” with an “and”.\n* Note: The AI Office and standards-setting bodies, in coordination with concurrent international efforts (e.g. the UK and US AI Institutes), should actively track and incentivize evaluation development, determining minimal standards.\n* **Distribution channels.**The distribution channels by which the foundation model is or has been knowingly placed on the market, put into service or made available in the Union.\n* Source: Indicator 68 in the[Foundation Model Transparency Index].\n* Note: It is possible that the below disclosures may depend on the distribution channel. If so, the foundation model provider should make this clear.\n* **Member states.**The Member States in which the foundation model is or has been placed on the market, put into service, or made available in the Union.\n* Source: This requirement is identical to the requirement under[Amendment 771 item 9 in Annex VIII –Section C of the Parliament position].\n* **Legal documents.**Links to any licenses, terms-of-service, or acceptable use policies that apply to the foundation model.\n* Source: Indicators 71 and 72 in the[Foundation Model Transparency Index].\n* **Uses.**The intended, permitted, restricted, and prohibited uses of the foundation model.\n* Source: Indicator 74 in the[Foundation Model Transparency Index].\n* **Users.**The permitted and prohibited users for the foundation model.\n* Source: Indicator 73 in the[Foundation Model Transparency Index].\n* In the event that the foundation model is distributed via a distribution channel with a user-facing interface:\n1. **User interaction.**The user should be aware they are interacting with an AI system.\n* Source: This requirement is identical to the requirement under[Amendment 484 in Article 52 of the Parliament position].\n* Note: Standards-setting bodies should set standards on the form and persistence of this disclosure to ensure user awareness.\n* **Machine-generated content.**The user should be aware of what content, if any, is machine-generated.\n* Source: This requirement clarifies the requirement about machine-generated content under[Amendment 101 in recital 60g of the Parliament position].\n* Note: Standards-setting bodies should set standards on the form and persistence of this disclosure to ensure user awareness.\n**Compliance costs.**Critical to the recent discourse around the AI Act has been the compliance burden for foundation model developers. The costs of compliances will depend on the developer, their specific operating environment, and several other factors. In short, we encourage developers arguing that compliance is costly to be precise: what are the specific line items that are especially onerous?\nAs a third party, we provide our assessment of compliance costs. We make these judgments from our significant expertise on foundation models, even if lacking certain practical context. These costs are primarily aimed at the technical costs of compliance: what work is required to acquire the relevant information to be disclosed? We do not account for the costs of having legal personnel. In turn, these cost assessments may neglect factors that arise in practice for some developers, and therefore may be inaccurate. Nonetheless, we provide them to help ground the discourse on compliance burden from perspectives beyond model developers, whose views on cost cannot be decoupled from their self-interest to minimize regulatory burden.\nWe argue the aforementioned requirements impose fairly minimal*marginal compliance cost*: the cost to a foundation model provider, over the set of basic practices most are already doing for their own internal purposes or for their clients, is small in our judgment. To make this argument crisp, we informally “price” each requirement for its marginal compliance cost. However, we cannot provide a fully precise compliance cost assessment, because the costs of compliance will depend on the legal interpretation: how implementing acts and subsequent standard-setting clarify the minimal expectations for compliance will substantially shape compliance costs.\n1. **Registration.**The[costs of registration] are largely determined by the complexity of the registration form and any processing time/bureaucracy. We recommend a digitized process using a structured form with no subsequent approvals to keep costs very low.\n2. **Transparency Report.**\n1. **Name.**Minimal cost. Note: some foundation model providers do not rigorously version models at present.\n2. **Data sources.**Some cost. To develop the foundation model, developers must select data sources and should already record this info. We acknowledge disclosure of this information may require legal review given the potential exposure to liability.\n3. **Data summary.**Some cost. We expect the cost of summary will go down considerably by the time the AI Act goes into force due to the development of more robust data summarization tooling.\n4. **Compute.**Minimal cost. Compute information is likely tracked by model providers to make core business decisions (e.g. to price the costs of computation).\n5. **Environment.**Minimal cost. Model providers know their compute providers, though there could be challenges for hardware location in some cases.\n6. **Model.**Minimal cost. Very basic information that all developers know.\n7. **Model properties.**Minimal cost. Very basic information that all developers know and should already report for sensible downstream use.\n8. **Risks and mitigations.**Some cost. Developers should report this information to coordinate with downstream actors to ensure risks are mitigated across the value chain.\n9. **Evaluations.**Minimal cost. We expect the cost of evaluations will go down considerably by the time the AI Act goes into force due to the development of third-party solutions for low-cost evaluations. In particular, we emphasize that the model developers already conduct evaluations internally to make business decisions and evaluations are expected by downstream actors to sensibly choose between models.\n10. **Distribution channels.**Minimal cost. Very basic information.\n11. **Member states.**Minimal cost. Very basic information.\n12. **Legal documents.**Minimal cost. Very basic information.\n13. **Uses.**Minimal cost. Very basic information.\n14. **Users.**Minimal cost. Very basic information.\n15. In the event that the foundation model is distributed via a distribution channel with a user-facing interface:\n1. **User interaction.**Minimal cost. The exact cost may depend upon the precise standards for this disclosure.\n2. **Machine-generated content.**Minimal cost. The exact cost may depend upon the precise standards for this disclosure.# The high-impact tier for the most regulated models\nIn addition to the requirements for the generic tier, foundation models that demonstrate significant societal impact[4] warrant greater scrutiny and should meet higher standards.\nWe recommend the following requirements:\n1. **Expanded Transparency Report.**Providers of high-impact foundation models shall disclose the following additional information in Transparency Reports on their public website, along all distribution channels for the foundation model, and in the EU database:\n1. **Model behavior policy.**The model behaviors that are permitted, restricted, and prohibited.\n* Source: Indicator 78 in the[Foundation Model Transparency Index].\n* **Risk evaluations**. Description of the model’s performance on a set of risk categories determined by the AI Office.\n* Source: Indicators 52 and 54 in the[Foundation Model Transparency Index].\n* Note: The AI Office should be instructed to create templates for disclosing this information, including the specification of risk categories and acceptable evaluations for each category.\n* **Risk management.**Risk management and governance policies, including for example accountability and governance processes to identify, assess, prevent, and address risks, where feasible throughout the AI lifecycle.\n* Source: Item 5 in the[G7 Hiroshima Process International Code of Conduct for Organizations Developing Advanced AI Systems].\n* **Data governance.**Providers of high-impact foundation models shall process and incorporate only datasets that are subject to appropriate data governance measures for foundation models, in particular measures to examine the suitability of the data sources and possible biases and appropriate mitigation.\n* Source: This requirement is identical to the requirement under[Amendment 399 item 2b in Article 28b of the Parliament position].\n* Note: The AI Office and standards-setting bodies should be instructed to provide public guidance on how datasets can be identified as appropriately governed.\n* **Energy efficiency.**Providers of high-impact foundation models shall design and develop the foundation model, making use of applicable standards to reduce energy use, resource use and waste, as well as to increase energy efficiency, and the overall efficiency of the system. This shall be without prejudice to relevant existing Union and national law and this obligation shall not apply before the standards referred to in Article 40 are published. They shall be designed with capabilities enabling the measurement and logging of the consumption of energy and resources, and, where technically feasible, other environmental impacts the deployment and use of the systems may have over their entire lifecycle.\n* Source: This requirement is identical to the requirement under[Amendment 399 item 2d in Article 28b of the Parliament position].\n* Note: The AI Office and standards-setting bodies should be instructed to provide public guidance on how models can be trained with greater energy efficiency.\n* **Cybersecurity**. Providers of high-impact foundation models shall implement operational security measures for information security and appropriate cyber/physical access controls to secure model weights, algorithms, servers, and datasets. Operational security measures include monitoring dependencies on external code to protect against cybersecurity vulnerabilities.\n* Source: Item 6 in the[G7 Hiroshima Process International Code of Conduct for Organizations Developing Advanced AI Systems].\n* **Internal red-teaming.**Providers of high-impact foundation models shall adversarially evaluate models and report results of the foundation model’s performance in relevant red-team testing to the AI Office.\n* Source: Item 3 in the[G7 Hiroshima Process International Code of Conduct for Organizations Developing Advanced AI Systems] and Section 4.2.i.c of the[US Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence] \n* **External auditing.**Providers of high-impact foundation models shall provide sufficient model access, as determined by the AI Office, to approved independent third-party auditors.\n* Source: The[Spanish position on the tiered approach].\n* Note: The AI Office should create and maintain a certification process for determining if auditors are sufficiently independent and qualified to audit high-impact foundation models. The Office should consult with both the foundation model providers and third-party auditors in determining the appropriate level of model access required on a case-by-case basis.\n* **Law-abiding generated content.**Providers of high-impact foundation models shall train, and where applicable, design and develop the foundation model in such a way as to ensure adequate safeguards against the generation of content in breach of Union law in line with the generally acknowledged state of the art, and without prejudice to fundamental rights, including the freedom of expression.\n* Source: This requirement is identical to the requirement under[Amendment 399 item 3b in Article 28b of the Parliament position].\n* **Adverse event reporting.**Report adverse events or other harms to a centralized government reporting platform and provide affordances for third-parties or users to similarly report such events.\n* Source: Item 2 in the[G7 Hiroshima Process International Code of Conduct for Organizations Developing Advanced AI Systems].\n* Note: The AI Office should create and maintain an adverse event reporting database, exercising periodic judgment to determine if incidents can be disclosed or summarized publicly.\n**Relationship with scientific literature on foundation models and AI.**The transparency requirements proposed draw inspiration from several resources across the scientific literature, but most directly three in particular:[model cards],[ecosystem cards], and the[Foundation Model Transparency Index]. These three works are natural sources of inspiration: model cards as defined by[Mitchell et al. (2018)] [5] pioneered the literature on transparency for machine learning models, ecosystem cards refined the approach to the setting of foundation models, and the Transparency Index characterized the current market state as of October 2023. The requirements beyond transparency draw inspiration from the scientific literature on[data governance],[energy efficiency],[evaluations],[model access],[auditing], and[adverse event reporting].\n**Relationship with concurrent governance approaches to foundation models.**The requirements stated draw direct inspiration from the[Parliament position], the[G7’s voluntary code of conduct], the[US Executive Order], and scientific research. For the generic tier, the transparency requirements are the transparency requirements from the Parliament position, with slight modifications to improve clarity and precision. For the high-impact tier, the requirements adjust the Parliament position, replacing some of the more substantive requirements in the position for other high-value matters that increase overall understanding of risk: internal red-teaming, third-party auditing and adverse event reporting. In particular, the focus on the three matters can be directly traced to the US Executive order (internal red-teaming) and[Guha et al] (third-party auditing and adverse event reporting).\n**Compliance costs.**At present, we cannot confidently price the compliance costs for this tier. However, we highlight that at present high-impact foundation models are themselves quite costly to build and deploy at scale. Consequently, we expect that the compliance burdens for this tier are likely to be of marginal cost relative to the costs of building/maintaining high-impact foundation models.\n# How to tier\nAs we discuss in our[previous post], many approaches can be considered for designing tiers. Our fundamental beliefs are that (i) the core basis for governments to apply scrutiny is impact and (ii) the immense uncertainty for foundation models points to legislative caution.\nTo that end, we recommend the following as the approach to tiers:\n1. The legislative text should indicate that the threshold for the high-impact tier should be based on measures of demonstrated societal/systemic impact.\n2. The legislative text should defer the determination of the threshold to the AI Office, which should be empowered to consult external experts and stakeholders. We note that deferring to standards-setting bodies should proceed with caution, given the standards-setting process may be largely driven by a small cadre of industry actors.\n3. The legislative test should empower the AI Office to rapidly change thresholds over time.\n4. The EU should ensure the AI Office is sufficiently well-resourced to conduct this work, including ensuring sufficient technical expertise.\n5. The legislative text may acknowledge other non-impact measures as alternatives in the event no satisfactory impact quantity can be effectively measured by the time the EU AI Act would go into force around 2025-2026.\nAt present, we describe some concrete quantities that are surrogates for impact:\n1. The number of customers or entities paying for the foundation model\n2. The number of downstream AI applications\n3. The number of downstream high-risk AI systems\n4. The aggregate number of users across all downstream AI systems\n5. The aggregate number of users across all downstream high-risk AI systems\n6. The number of high-risk areas (e.g. categories under Annex III) covered by downstream high-risk AI systems.\n7. The number of queries to the foundation model.\nOf these, 1 should be directly known by all foundation model providers. 3, 5, and 6 could be tracked via linking the registration requirements for foundation models and high-risk AI systems: every high-risk AI system provider would have to declare which, if any, foundation models their high-risk AI system depends upon. The remainder would either require coordination between foundation model providers and distribution channels, or more active market surveillance (e.g. akin to the[UK CMA’s efforts]) by bodies like the EU AI Office.\nWe acknowledge that some of these quantities are more difficult to track for open foundation models at present, but we believe societal infrastructure can correct for this. In particular, if downstream developers are required to declare dependencies on (all) foundation models, this would enable the foundation model providers, the EU government, and the public to easily track their downstream impact. As an instructive example, consider scientific papers. Scientific papers are released openly: the author of a scientific paper would find it very difficult, if not impossible, to directly track the use and uptake of their work. However, scientific papers declare (via citation) which papers they depend upon, allowing for centralized tracking (e.g. by Google Scholar) to publicly record the downstream impact (measured in citations) for all papers.\nFinally, we do not make a precise judgment of what current level of impact would make sense for the high-impact tier. In particular, we note that the[current opacity] on the impact of different foundation model providers makes it difficult to be precise. With that said, we remind the EU of the DSA: grounding out foundation models to the way they shape the lives of the EU citizenry, and the scale/nature of the impact on the EU citizens, is precisely how tiers should be drawn.\n# Conclusion\nWe provide a concrete proposal to ground the discourse in the AI Act negotiations. Too often, AI Act discourse devolves into speculation on phantom societal risks and phantom compliance costs. At this critical juncture, there is no time to waste: we need careful cost-benefit analyses. Finalizing the AI Act will require thoughtful political negotiation, weighing the interest of different stakeholders. We are hopeful the EU will achieve political compromise on the AI Act, setting a powerful precedent for the world on how to govern AI.\n# Authors\n[**Rishi Bommasani**] is the Society Lead at the[Stanford Center for Research on Foundation Models] (CRFM). He co-led the[report] that first introduced and defined foundation models: his research addresses the societal impact of foundation models spanning[evaluations],[supply chain monitoring],[transparency],[tiers],[open models],[policy], and[the EU AI Act].\n[**Tatsunori Hashimoto**] is an Assistant Professor of Computer Science at Stanford University.\n[**Daniel E. Ho**] is the Director of the[Stanford Regulation, Evaluation, and Governance Lab] (RegLab), Senior Fellow at the[Stanford Institute for Human-Centered Artificial Intelligence], and the William Benjamin Scott and Luna M. Scott Professor of Law and a Professor of Political Science at Stanford University. He serves on the[US’s National Artificial Intelligence Advisory Commission].\n[**Marietje Schaake**] is the International Policy Director at the[Stanford Cyber Policy Center] and the International Policy Fellow at the[Stanford Institute for Human-Centered Artificial Intelligence]. She served as a Member of European Parliament from 2009 to 2019, where she focused on trade, foreign affairs, and technology policies. She serves on the[UN’s AI Advisory Body].\n[**Percy Liang**] is the Director of the[Stanford Center for Research on Foundation Models] (CRFM), Senior Fellow at the[Stanford Institute for Human-Centered Artificial Intelligence], and an Associate Professor of Computer Science at Stanford University. He co-led the[report] that first introduced and defined foundation models.\n# Acknowledgements\nWe thank Arvind Narayanan, Daniel Zhang, Russell Wald, and Sayash Kapoor for their comments on this piece as well as Ashwin Ramaswami, Aviv Ovadya, Christie Lawrence, Connor Dunlop, Helen Toner, Florence G’Sell, Irene Solaiman, Judy Shen, Kevin Klyman, Markus Anderjlung, Neel Guha, Owen Larter, Peter Cihon, Peter Henderson, Risto Uuk, Rob Reich, Sanna Ali, Shayne Longpre, Steven Cao, Yacine Jernite, and Yo Shavit for discussions on this matter.\n# Citation\n```\n`@misc{bommasani2023eu-compromise, author ={Rishi Bommasani and Tatsunori Hashimoto and Daniel E. Ho and Marietje Schaake and Percy Liang}, title ={Towards compromise: A concrete two-tier proposal for foundation models in the EU AI Act}, url ={https://crfm.stanford.edu/2023/12/01/ai-act-compromise.html}, year ={2023}}`\n```\n## Footnotes\n1. **Definitions of and updates to foundation models.**The AI Office should be instructed to clarify and publish the criteria for determining (i) if a model is a foundation model and (ii) if a model derived from a foundation model is still a foundation model.[&#8617;] \n2. **Omitted details.**We note that we deliberately omit certain mechanical details to emphasize the substance. For example, Amendment 771 of the Parliament position requires “Name, address and contact details of the provider”, which we will also recommend but elide for simplicity.[&#8617;] \n3. **Strengthening the G7’s transparency reports.**The G7 Code of Conduct indicates that transparency reports should be prepared.[&#8617;] \n4. **Relevance under alternative tiering approaches.**In the event that greater scrutiny is placed on foundation models for a different reason than demonstrated impact, these requirements should be revisited. With that said, the principles of meaningful forms of additional scrutiny may generalize: these requirements are likely appropriate for several alternative two-tier schemes.[&#8617;] \n5. **Model cards is a vacuous concept.**We note that model cards today are used more loosely to describe any form of documentation; many model cards today do not include all the fields in the original proposal of Mitchell et al.[&#8617;]",
    "length": 32920,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "",
    "url": "https://hai.stanford.edu/sites/default/files/2023-12/Governing-Open-Foundation-Models.pdf",
    "text": "Key Takeaways\nOpen foundation models, \nmeaning models with \nwidely available weights, \nprovide significant benefits \nby combatting market \nconcentration, catalyzing \ninnovation, and improving \ntransparency.\nSome policy proposals have \nfocused on restricting open \nfoundation models. The critical \nquestion is the marginal risk\nof open foundation models \nrelative to (a) closed models or \n(b) pre-existing technologies, \nbut current evidence of this \nmarginal risk remains quite \nlimited.\nSome interventions are better \ntargeted at choke points \ndownstream of the foundation \nmodel layer.\nSeveral current policy \nproposals (e.g., liability for \ndownstream harm, licensing) \nare likely to disproportionately \ndamage open foundation \nmodel developers.\nPolicymakers should explicitly \nconsider potential unintended \nconsequences of AI regulation \non the vibrant innovation \necosystem around open \nfoundation models.\nIssue Brief\nHAI Policy & Society\nDecember 2023\nConsiderations for \nGoverning Open \nFoundation Models\nRishi Bommasani, Sayash Kapoor, Kevin Klyman, Shayne Longpre, \nAshwin Ramaswami, Daniel Zhang, Marietje Schaake, \nDaniel E. Ho, Arvind Narayanan, Percy Liang\nIntroduction\nFOUNDATION MODELS (E.G., GPT-4, LLAMA 2) ARE AT THE EPICENTER OF \nAI, driving technological innovation and billions in investment. This paradigm \nshift has sparked widespread demands for regulation. Animated by factors \nas diverse as declining transparency and unsafe labor practices, limited \nprotections for copyright and creative work, as well as market concentration\nand productivity gains, many have called for policymakers to take action. \nCentral to the debate about how to regulate foundation models is the process \nby which foundation models are released. Some foundation models like Google \nDeepMind’s Flamingo are fully closed, meaning they are available only to the model \ndeveloper; others, such as OpenAI’s GPT-4, are limited access, available to the public \nbut only as a black box; and still others, such as Meta’s Llama 2, are more open, with \nwidely available model weights enabling downstream modification and scrutiny. As \nof August 2023, the U.K.’s Competition and Markets Authority documents the most \ncommon release approach for publicly-disclosed models is open release based on \ndata from Stanford’s Ecosystem Graphs. Developers like Meta, Stability AI, Hugging \nFace, Mistral, Together AI, and EleutherAI frequently release models openly.\n1\n2\nIssue Brief \nConsiderations for Governing \nOpen Foundation Models\nOpen foundation models provide \nsignificant benefits to society \nby promoting competition, \naccelerating innovation, \nand distributing power.\nGovernments around the world are issuing policy \nrelated to foundation models. As part of these efforts, \nopen foundation models have garnered significant \nattention: The recent U.S. Executive Order on Safe, \nSecure, and Trustworthy Artificial Intelligence tasks \nthe National Telecommunications and Information \nAdministration with preparing a report on open \nfoundation models for the president. In the EU, open \nfoundation models trained with fewer than 1025\nfloating point operations (a measure of the amount \nof compute expended) appear to be exempted under \nthe recently negotiated AI Act. The U.K.’s AI Safety \nInstitute will “consider open-source systems as well as \nthose deployed with various forms of access controls” \nas part of its initial priorities. Beyond governments, \nthe Partnership on AI has introduced guidelines\nfor the safe deployment of foundation models, \nrecommending against open release for the most \ncapable foundation models.\nPolicy on foundation models should support the \nopen foundation model ecosystem, while providing \nresources to monitor risks and create safeguards to \naddress harms. Open foundation models provide \nsignificant benefits to society by promoting \ncompetition, accelerating innovation, and distributing \npower. For example, small businesses hoping to build \ngenerative AI applications could choose among a \nvariety of open foundation models that offer different \ncapabilities and are often less expensive than closed \nalternatives. Further, open models are marked by \ngreater transparency and, thereby, accountability. \nWhen a model is released with its training data, \nindependent third parties can better assess the \nmodel’s capabilities and risks. \nHowever, an emerging concern is whether open \nfoundation models pose distinct risks to society. Unlike \nclosed foundation model developers, open developers \nhave limited ability to restrict the use of their models \nby malicious actors that can easily remove safety \nguardrails. Recent studies claim that open foundation \nmodels are more likely to generate disinformation, \ncyberweapons, bioweapons, and spear-phishing emails. \nCorrectly characterizing these distinct risks requires \ncentering the marginal risk: To what extent do open \nfoundation models increase risk relative to (a) closed \nfoundation models or (b) pre-existing technologies like \nsearch engines? We find that for many dimensions, \nthe existing evidence about the marginal risk of open \nfoundation models remains quite limited. In some \ninstances, such as the case of AI-generated child \nsexual abuse material (CSAM) and nonconsensual \nintimate imagery (NCII), harms stemming from open \nfoundation models have been better documented. For \nthese demonstrated harms, proposals to restrict the \nrelease of foundation models via licensing of compute\u0002intensive models are mismatched, because the text-to\u0002image models used to cause these harms require vastly \nlower amounts of resources to train.\nMore broadly, several regulatory approaches under \nconsideration are likely to have a disproportionate \nimpact on open foundation models and their \n3\nIssue Brief \nConsiderations for Governing \nOpen Foundation Models\ndevelopers, without meaningfully reducing risk. Even \nthough these approaches do not differentiate between \nopen and closed foundation model developers, they \nyield asymmetric compliance burdens. For example, \nlegislation that holds developers liable for content \ngenerated using their models or their derivatives \nwould harm open developers as users can modify their \nmodels to generate illicit content. Policymakers should \nexercise caution to avoid unintended consequences \nand ensure adequate consultation with open \nfoundation model developers before taking action. \nBackground\nCentral to understanding foundation models is the \ntopic of release: To what extent and through what \nmechanisms are foundation models made available to \nentities beyond the foundation model developer? The \nlandscape of release is multidimensional: Different assets \n(e.g., training data, code, model weights) can be released \nto chosen entities or to the public at large. Developers \nhave many intermediary options between the fully \nclosed setting (nothing is released to anyone) and the \nfully open setting (every asset is released to everyone).\nThe release of foundation models is a gradient: \nModels can be fully closed (not available to anyone \noutside the developer organization, like Google \nDeepMind’s Flamingo); hosted access (available via \na web interface, like Inflection’s Pi); cloud-based \naccess (available via an API, like OpenAI’s GPT-4); \ncloud-based fine-tuning access (both the model and \nthe ability to fine-tune it are available via an API, like \nOpenAI’s GPT-3.5); widely available weights (like \nStability AI’s Stable Diffusion and Meta’s Llama 2); and \navailable with the weights, code, and data—either with \nuse restrictions, like BigScience’s BLOOM, or without, \nlike EleutherAI’s GPT-NeoX. \nWe use the notion of open foundation models: \nmodels released with widely available weights, which \ncorresponds to the three rightmost categories in \nthe figure (modified with permission from Solaiman \n(2023)). This aligns with the distinction drawn in the \nDevelopers have many \nintermediary options between the \nfully closed setting (nothing \nis released to anyone) and the \nfully open setting (every asset \nis released to everyone).\nLevel of \nAccess\nFully closed Hosted \naccess\nAPI access \nto model\nAPI access \nto fine tuning\nWeights \navailable\nWeights, data, \nand code \navailable with \nuse restrictions\nWeights, data, \nand code \navailable without \nuse restrictions\nFlamingo \n(Google)\nPi \n(As of 2023; \nInflection)\nGPT-4 \n(As of 2023; \nOpenAI)\nGPT-3.5 \n(OpenAI)\nLlama 2 \n(Meta)\nBLOOM \n(BigScience)\nGPT-NeoX\n(EleutherAI)\nExample\nFoundation models with widely available weights\n4\nIssue Brief \nConsiderations for Governing \nOpen Foundation Models\nU.S. Executive Order on Safe, Secure, and Trustworthy \nArtificial Intelligence. Many of the concerns \nsurrounding open foundation models arise from the \nfact that once model weights are released, developers \nrelinquish control over their downstream use. Even if \ndevelopers impose restrictions on downstream use \nand who can download the model, such restrictions \ncan be ignored by downstream users, especially \nmalicious users. In contrast, in the face of malicious \nuse or other important risks, closed foundation model \ndevelopers can restrict access to the models (i.e., \nreduce access by shifting to a more restrictive point \non the gradient of model release). We stress, however, \nthat this categorical distinction may oversimplify the \ngradient of model release: Closed models may also \nbe susceptible to malicious use, given that current \nsafeguards are not robust enough to withstand \nadversarial attacks. \nOpen foundation models are reminiscent of, but not \nthe same as, open-source software: Machine learning \nmodels are built using datasets as well as code, \nmaking them fundamentally different from many kinds \nof software. The standard definition of open-source \nsoftware prohibits restrictions on specific users or use \ncases, while open foundation models often include \nthese restrictions; Meta restricts the use of its Llama 2 \nmodel by entities with more than 700 million monthly \nactive users, and other organizations use Open & \nResponsible AI licenses with use restrictions.\nNevertheless, the history of open-source software \nprovides insight on how to govern open foundation \nmodels. Open-source software validates the \ntremendous societal benefits of open technologies: \nThe European Commision reports that an investment \nof “around €1 billion in [open-source software] … \nresulted in an impact on the European economy of \nbetween €65 and €95 billion.” Open-source software \nClosed models may also be \nsusceptible to malicious use, \ngiven that current safeguards \nare not robust enough to \nwithstand adversarial attacks.\npowers critical infrastructure: The U.S. Digital Services \nPlaybook encourages U.S. government digital services \nto “default to open” due to the benefits of reusability, \nrobustness, transparency, and collaboration. The \nDepartment of Defense’s website on open-source \nsoftware states, “Continuous and broad peer\u0002review, enabled by publicly available source code, \nimproves software reliability and security through the \nidentification and elimination of defects that might \notherwise go unrecognized by the core development \nteam.” There is no empirical evidence that open-source \nsoftware is more vulnerable or insecure than closed\u0002source software.\nBenefits of open \nfoundation models\nWe highlight three fundamental societal objectives\nwhere open foundation models provide clear benefits \nby distributing power, catalyzing innovation, and \nensuring transparency.\n5\nIssue Brief \nConsiderations for Governing \nOpen Foundation Models\nDistributing power. Foundation models are emerging \ntechnologies: Given their influence, these models \ncreate new forms of socioeconomic power, which \ndemands an assessment of how that power is \ndistributed. In the words of MIT economists Daron \nAcemoglu and Simon Johnson: “The consequences of \nany technology depend on who gets to make pivotal \ndecisions about how the technology develops. This \nis doubly true for AI, because these new tools can be \ndeveloped for many different types of activities, with \nthe potential to spread rapidly in every sector of the \neconomy and in every aspect of our lives.” Closed \nmodel developers exert greater power in defining \nand restricting use cases they deem unacceptable, \nwhereas downstream consumers of foundation models \ncan better make these decisions for themselves with \nopen models. Further, closed model developers may \nmore directly shape downstream markets through \nvertical integration, potentially leading to problematic \nmonocultures where many downstream products/\nservices depend on the same foundation model. \nOverall, closed foundation models may contribute to \nmore concentrated power in the hands of developers, \nwhich we should scrutinize given the well-established \nrisks of market concentration for digital technologies.\nCatalyzing innovation. Foundation models are general\u0002purpose technologies that can produce sharp increases \nin innovation. Notably, foundation models bolster \neconomic and scientific productivity, with Bloomberg \nIntelligence projecting that generative AI will become \na $1.3 trillion market by 2032. Open foundation \nmodels are necessary for several forms of research \n(e.g., interpretability work, public development of \nwatermark techniques, forms of security research, and \nmodel training and inference efficiency techniques). \nOverall, open foundation models are generally more \ncustomizable and provide deeper access, which are key \ningredients for greater innovation.\nEnsuring transparency. Digital technologies such as \nfoundation models are plagued by opacity, from dark \npatterns on social media to ghost work as invisible \nlabor. Adequate transparency from foundation model \ndevelopers is instrumental for many objectives: civil \nsociety, governments, industry, and academia have all \ncalled for transparency. UN Secretary-General António \nGuterres proposed to “make transparency, fairness \nand accountability the core of AI governance … [and] \nconsider the adoption of a declaration on data rights \nthat enshrines transparency.” The 2023 Foundation \nModel Transparency Index demonstrates that major \nopen foundation model developers are consistently \nand considerably more transparent on average than \ntheir closed counterparts. Open foundation model \ndevelopers score on average 20 percentage points \nhigher on the index, outperforming closed developers \nin terms of transparency with respect to each part of \nthe supply chain. Such transparency may be important \nto avoid reproducing the harms facilitated by opaque \ndigital technologies in the past, but the current lack \nof transparency about downstream impacts on the \neconomy and society remains a concern.\nRisks of open \nfoundation models\nIn spite of the significant benefits of open foundation \nmodels, current policy attention on the risks of open \nfoundation models is largely motivated by their potential \nfor malicious use. Here, we consider a range of misuse \nthreat vectors to better characterize the state of \nevidence today for each. Rigorous evidence of marginal \nrisk remains quite limited. This does not mean that open \nfoundation models pose no risk along these vectors \nbut, instead, that more rigorous analysis will be required \nto ground policy interventions. In particular, critical \n6\nIssue Brief \nConsiderations for Governing \nOpen Foundation Models\nresearch is needed on marginal risk: To what extent do \nopen foundation models increase risk either relative \nto closed foundation models or relative to pre-existing \ntechnologies (e.g., search engines)?\nDisinformation. Foundation models may reduce the \ncost of generating persuasive disinformation. While \nclosed foundation model providers may be better \npositioned to reject requests to generate disinformation, \nthe ambiguity of what constitutes disinformation calls \ninto question the technical feasibility of such refusals. \nMore fundamentally, the key bottleneck for effective \ninfluence operations is not disinformation generation \nbut disinformation dissemination: Online platforms that \ncontrol the reach of content are better targets for policy \nintervention. To date, we are unaware of empirical \nevidence that open foundation models increase societal \nsusceptibility to disinformation campaigns. \nBiorisk. Several studies have claimed that open \nfoundation models can instruct users on how to \nconstruct bioweapons. But the evidence behind \nthese studies remains weak. In particular, studies \nindicating that today’s language models provide \n“dangerous” information related to bioweapons \ndo not acknowledge that the same information is \navailable via Wikipedia and the National Academies \nof Sciences, Engineering, and Medicine. In addition, \neven if foundation models can be used to provide \nsensitive information, pathogens would still need to \nbe developed in labs and deployed in the real world. \nEach of these steps requires significant expertise, \nequipment, and real-world lab experience. As with \nmany other threat vectors, the best policy choke \npoints may hence lie downstream. For example, the \nU.S. AI Executive Order aims to strengthen customer \nscreening for purchasers of biological sequences. \nStill, there are efforts underway to measure the \nmarginal risk of textual foundation models relative to \ninformation on the internet that will provide useful \nevidence for future policy proposals.\nCybersecurity. Though open code models could \nimprove the speed and quality of offensive \ncyberattacks, it appears that cyber defenses will also \nimprove. For example, Google recently demonstrated \nthat code models vastly improve the detection of \nvulnerabilities in open-source software. As with \nprevious automated vulnerability-detection tools, \nwidespread access to open models for defenders, \nsupplemented by investment in tools for finding security \nvulnerabilities by companies and governments, could \nstrengthen cybersecurity.\nSpear-phishing scams. Foundation models are capable\nof generating high-quality spear-phishing emails. Both \nopen and closed models could be used to produce \nspear-phishing emails, because the key factor that \nmakes spear-phishing emails dangerous is the malware \nthat accompanies the email; the text itself is usually \nCritical research is needed on \nmarginal risk: To what extent do \nopen foundation models increase \nrisk either relative to closed \nfoundation models or relative \nto pre-existing technologies \n(e.g., search engines)?\n7\nIssue Brief \nConsiderations for Governing \nOpen Foundation Models\nbenign. As with disinformation, the key bottleneck \nfor spear-phishing is not always the text of emails but \ndownstream safeguards: Modern operating systems\nand browsers implement several layers of protection \nagainst such malware. Moreover, as a result of existing \nprotections, phishing emails might not reach the \nrecipient in the first place.\nVoice-cloning scams. The last few months have seen a \nnumber of examples of real-world voice-cloning scams \nwhere malicious users impersonate a person’s friends \nor family and successfully get them to transfer money. \nThese impersonations rely on AI tools that can clone \nsomeone’s voice based on a few seconds of audio—for \ninstance, from their social media account. As of now, it \nis unclear if voice-cloning scams are more effective or \nscalable compared to traditional scams, especially since \ntens of thousands of traditional scams are already being \nreported to the FTC each year. Though it is yet to be \ndetermined if closed model developers can successfully \nprevent such scams, they do offer a measure of \ndeterrence by, for instance, requiring users to sign up \nusing credit cards and being able to trace any audio \nback to the specific user who created it. \nNonconsensual intimate imagery (NCII) and child \nsexual abuse materials (CSAM). Open text-to-image \nmodels appear to present unique risks related to NCII \nand CSAM as they substantially lower the barrier to \ngenerating such content. Safeguards for closed models \nare relatively more effective in this area, and monitoring \nclosed models can deter the generating of such \nimagery, especially of real people. We have already \nseen open text-to-image models being used for creating \nnonconsensual deepfakes and CSAM. There remains \nan open question about whether policy interventions \nare more effective with downstream platforms, such \nas CivitAI and social media platforms. Organizations \nthat are tasked with combating NCII and CSAM such \nas the National Center for Missing & Exploited Children \nmay benefit from additional resources and support to \naddress AI-generated CSAM.\nGovernance\nWith policy efforts across the United States, China, \nEuropean Union, U.K., and G7 focusing on foundation \nmodels, we now consider how these efforts affect \nopen foundation models. \nAlthough many policy proposals and regulations \ndo not mention open foundation models by name, \nthey may have uneven distributive impact on open \nfoundation models. Specifically, they may impose \ngreater compliance burdens on open foundation \nmodel developers than their closed counterparts, even \nwhen open developers are more likely to be resource\u0002poor compared to the largest AI companies, which are \ndisproportionately closed developers. In particular, \ndownstream use compliance may be challenging for \nopen foundation model developers, since they exert far \nless control over downstream use. We illustrate these \nAlthough many policy proposals \nand regulations do not mention \nopen foundation models by name, \nthey may have uneven distributive \nimpact on open foundation models.\n8\nIssue Brief \nConsiderations for Governing \nOpen Foundation Models\ntensions of how proposals may disproportionately \ndamage open foundation models. \nLiability for downstream use. Since the distinction \nbetween open and closed foundation models is \npredicated on release, policies governing the usage \nof foundation models are likely to have differential \nimpacts. Therefore, liability for harms arising from \ndownstream usage could chill the open foundation \nmodel ecosystem by exposing open foundation model \ndevelopers to severe liability risk. For example, the U.S. \nAI Act, introduced by Senators Richard Blumenthal \nand Josh Hawley, suggests potential liability for \ndevelopers. In contrast, because closed foundation \nmodel developers exercise greater control over \ndownstream use, some developers already provide \nliability protections for copyright to downstream users \nof their models.\nContent provenance for downstream use. Akin to \nliability, if foundation model developers are required to \nensure content provenance for downstream use, then \nthese requirements may be technically infeasible for \nopen foundation models. Given that the most salient \napplications of foundation models are generative AI, \nthere is pervasive emphasis on content provenance \ntechniques like watermarking to detect machine\u0002generated content: The U.S. Executive Order, White \nHouse Voluntary Commitments, Canadian Voluntary \nCode of Conduct, Chinese generative AI regulations, \nand G7 Voluntary Code of Conduct all highlight \ncontent provenance. However, today’s watermarking \nmethods for language models do not persist if models \nare modified (e.g., fine-tuned) and require that users of \na model follow certain protocols for the watermarking \nguarantee to hold. Fundamentally, open foundation \nmodel developers do not control how their models \nare modified or used to generate content. By contrast, \nefforts to track the provenance of trustworthy content \nmay be more fruitful as such initiatives rely only on the \nparticipation of good-faith actors. \nLiability for open data. While foundation models \ncan be released openly without the release of the \nunderlying data used to build the model, some \ndevelopers choose to release both the model weights \nand the training data. Of the 10 major foundation \ndevelopers assessed by the 2023 Foundation Model \nTransparency Index, the two developers that released \ndata openly also released their foundation models \nopenly. In addition, several other open foundation \nmodel developers, such as EleutherAI, the Allen \nInstitute for Artificial Intelligence, and Together AI, \ntend to release data openly. However, open release \nof data exposes these entities to greater liability risk \nas exemplified by lawsuits against Stability AI on \nthe basis of its use of LAION datasets that allegedly \nincluded plaintiffs’ work. While the legality of training \nfoundation models on copyrighted data remains unclear \nacross many jurisdictions, the status quo poses perverse \nincentives. Namely, model developers that transparently \ndisclose and openly provide data are subject to greater \nrisk than developers that obfuscate the data they use, \neven if the underlying facts are identical. In light of this \nperverse incentive, mandated disclosure of training data \nmay be beneficial in some cases.\nConclusion\nGovernments around the world are crafting different \npolicies on foundation models: The design and \nimplementation of these policies should consider both \nopen and closed foundation model developers. In \nparticular, open foundation models provide significant \nsocietal benefits in terms of the distribution of power, \ninnovation, and transparency. While open foundation \n9\nIssue Brief \nConsiderations for Governing \nOpen Foundation Models\nmodels are conjectured to contribute to malicious \nuses of AI, the weakness of evidence is striking. More \nresearch is necessary to assess the marginal risk of \nopen foundation models. \nPolicymakers should also consider the potential for AI \nregulation to have unintended consequences on the \nvibrant innovation ecosystem around open foundation \nmodels. When regulations directly address open \nfoundation models, the precise definition used to \nidentify these models and developers should be duly \nconsidered. Hinging regulation exclusively on open \nweights may not be appropriate given the gradient of \nrelease. Hostile actors, for instance, could leverage \nopen data and source code—without model weights—\nto retrain models and generate comparable harms. \nAnd even when regulations do not directly address \nopen foundation models, they may have an adverse \nimpact: Liability for downstream harms and strict \ncontent provenance requirements may suppress the \nopen foundation model ecosystem. Consequently, if \npolicymakers are to implement such interventions, \ndirect consultation with the open foundation model \ncommunity should take place, with due consideration \ngiven to their interests.\n10\nStanford HAI: 353 Jane Stanford Way, Stanford CA 94305-5008 \nT 650.725.4537 F 650.123.4567 E HAI-Policy@stanford.edu hai.stanford.edu \nStanford University’s Institute on Human-Centered Artificial Intelligence \n(HAI) applies rigorous analysis and research to pressing policy questions \non artificial intelligence. A pillar of HAI is to inform policymakers, industry \nleaders, and civil society by disseminating scholarship to a wide audience. \nHAI is a nonpartisan research institute, representing a range of voices. \nThe views expressed in this policy brief reflect the views of the authors. \nFor further information, please contact HAI-Policy@stanford.edu. \nDaniel Zhang is the senior manager for policy initiatives \nat the Stanford Institute for Human-Centered Artificial \nIntelligence (HAI).\nSayash Kapoor is a researcher at the Princeton Center \nfor Information Technology Policy (CITP) and a PhD \ncandidate in computer science at Princeton University.\nKevin Klyman is a researcher at Stanford CRFM \nand an MA candidate in international policy at \nStanford University.\nShayne Longpre is a PhD candidate at the MIT \nMedia Lab.\nAshwin Ramaswami is a JD candidate at the \nGeorgetown University Law Center.\nRishi Bommasani is the society lead at the Stanford \nCenter for Research on Foundation Models (CRFM); \na graduate student fellow at the Stanford Regulation, \nEvaluation, and Governance Lab (RegLab); and a PhD \ncandidate in computer science at Stanford University.\nMarietje Schaake is the international policy director at \nthe Stanford Cyber Policy Center and an international \npolicy fellow at Stanford HAI.\nDaniel E. Ho is the William Benjamin Scott and Luna M. \nScott Professor of Law, professor of political science, \nprofessor of computer science (by courtesy), senior fellow \nat HAI and the Institute for Economic Policy Research, \nand director of the RegLab at Stanford University.\nArvind Narayanan is the director of Princeton CITP \nand a professor of computer science at Princeton \nUniversity.\nPercy Liang is the director of Stanford CRFM, senior \nfellow at HAI, and an associate professor of computer \nscience at Stanford University.",
    "length": 28738,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "",
    "url": "https://hai.stanford.edu/sites/default/files/2021-10/HAI_NRCR_2021_0.pdf",
    "text": "Building a \nNational \nAI Research \nResource: \nA Blueprint for \nthe National \nResearch Cloud\nDaniel E. Ho\nJennifer King\nRussell C. Wald\nChristopher Wan\nWHITE PAPER\nOCTOBER 2021\nPrincipal Authors \nDaniel E. Ho, J.D., Ph.D., is the William Benjamin Scott and Luna M. Scott Professor of Law, \nProfessor of Political Science, and Senior Fellow at the Stanford Institute for Economic Policy \nResearch at Stanford University. He directs the Regulation, Evaluation, and Governance Lab \n(RegLab) at Stanford, and is a Faculty Fellow at the Center for Advanced Study in the Behavioral \nSciences and Associate Director of the Stanford Institute for Human-Centered Artificial \nIntelligence (HAI). He received his J.D. from Yale Law School and Ph.D. from Harvard University \nand clerked for Judge Stephen F. Williams on the U.S. Court of Appeals for the District of \nColumbia Circuit. \nJennifer King, Ph.D., is the Privacy and Data Policy Fellow at the Stanford HAI. Dr. King \ncompleted her doctorate in Information Management and Systems (information science) at \nthe University of California, Berkeley School of Information. Prior to joining HAI, Dr. King was the \nDirector of Consumer Privacy at the Center for Internet and Society at Stanford Law School from \n2018 to 2020.\nRussell C. Wald is the Director of Policy for the Stanford HAI, leading the team that advances \nHAI’s engagement with governments and civil society organizations. Since 2013, Wald has held \nvarious government affair roles representing Stanford University. He is a Term Member with the \nCouncil on Foreign Relations, Visiting Fellow with the National Security Institute at George Mason \nUniversity, and a Partner with the Truman National Security Project. Wald is a graduate of UCLA. \nChristopher Wan is a JD/MBA candidate at Stanford University and was Teaching Assistant for \nthe Stanford Policy Practicum: Creating a National Research Cloud. He also serves as a Research \nAssistant for the Stanford HAI and as an investor at Bessemer Venture Partners. He received his \nB.S. in Computer Science from Yale University and worked as a software engineer at Facebook \nand as a venture investor at In-Q-Tel and Tusk Ventures. \nThe Stanford Institute for Human-Centered Artificial Intelligence\nCordura Hall, 210 Panama Street, Stanford, CA 94305-4101\nOctober 2021, V1.0\n2\nContributors \nMany dedicated individuals contributed to this White Paper. To acknowledge these contributions, \nwe list here the contributors for each chapter and section. \nExecutive Summary and Introduction \nDaniel E. Ho, Tina Huang, Jennifer King, Marisa Lowe, Diego Núñez, Russell Wald, Christopher Wan, \nDaniel Zhang \n \nThe Theory for a National Research Cloud \nNathan Calvin, Shushman Choudhury, Tina Huang, Daniel E. Ho, Kanishka Narayan, Diego Núñez, \nFrieda Rong, Russell Wald, Christopher Wan \nEligibility, Allocation, and Infrastructure for Computing\nDaniel E. Ho, Krithika Iyer, Tyler Robbins, Jasmine Shao, Russell Wald, Daniel Zhang \nSecuring Data Access\nNathan Calvin, Shushman Choudhury, Daniel E. Ho, Ananya Karthik, Jennifer King, Christopher Wan \nOrganizational Design \nSabina Beleuz, Drew Edwards, Daniel E. Ho, Jennifer King, Christopher Wan \nData Privacy Compliance \nSimran Arora, Neel Guha, Jennifer King, Sahaana Suri, Sadiki Wiltshire, Christopher Wan \nTechnical Privacy and Virtual Data Safe Rooms\nNeel Guha, Jennifer King, Christopher Wan \nSafeguards for Ethical Research \nDaniel E. Ho, Jennifer King, Diego Núñez, Russell Wald, Daniel Zhang \nManaging Cybersecurity Risks \nNeel Guha, Diego Núñez, Frieda Rong, Russell Wald \nIntellectual Property \nSabina Beleuz, Daniel E. Ho, Ananya Karthik, Diego Núñez, Christopher Wan \nCase Studies \nDaniel E. Ho, Krithika Iyer, Jennifer King, Marisa Lowe, Kanishka Narayan, Tyler Robbins \nWe also would like to thank Jeanina Casusi, Celia Clark, Shana Lynch, Kaci Peel, Stacy Peña, \nMike Sellitto, Eun Sze, and Michi Turner for their help in preparing this White Paper. \n3\nExternal Participants \nErik Brynjolfsson \nStanford University \n \nIsabella Chu \nPopulation Health \nSciences \nStanford University \n \nJack Clark \nAnthropic \nJohn Etchemendy \nStanford University \n \nFei-Fei Li \nStanford University \nMarc Groman \nGroman Consulting \nGroup LLC \n \nEric Horvitz \nMicrosoft \nSara Jordan \nThe Future of \nPrivacy Forum \nVince Kellen \nU.C. San Diego, \nCloudBank \n \nEd Lazowska \nUniversity of \nWashington \nNaomi Lefkovitz \nNational Institute of \nStandards and \nTechnology (NIST) \nBrenda Leong \nThe Future of \nPrivacy Forum \nAmy O’Hara \nGeorgetown Federal \nStatistical Research \nData Center \nGeorgetown University \n \nWade Shen \nActuate Innovation \n \nSuzanne Talon \nCompute Canada \n \nLee Tiedrich \nCovington & Burling LLP \n \nEvan White \nCalifornia Policy Lab \nU.C. Berkeley \nIn our process, we also engaged many civil society leaders and advocates who have expressed many \nperspectives about building a National Research Cloud. We are grateful for their shared thoughts and \nhelping us shape a better White Paper and have incorporated their feedback where possible. \nGuest Lecturers and Interviewees \nWe relied on extraordinary outside expert reviewers for feedback and guidance. We are grateful to \nJack Clark, Leisel Bogan, John Etchemendy, Mark Krass, Marietje Schaake, and Christine Tsang for \ntheir thoughtful review of the full White Paper, and thank Isabella Chu, Kathleen Creel, Luciana Herman, \nSara Jordan, Vince Kellan, Brenda Leong, Ruth Marinshaw, Amy O’Hara, and Lisa Ouellette for their \nsubject expertise on specific chapters. \nReviewers \n4\nTaka Ariga \nGovernment Accountability \nOffice \nKathy Baxter \nSalesforce \nLeisel Bogan \nBelfer Center \nHarvard University \nJeffrey Brown \nIBM \nMiles Brundage \nOpen AI \nL. Jean Camp \nUniversity of Indiana \nat Bloomington \nDakota Cary \nCenter for Security and \nEmerging Technology\nGeorgetown University \nShikai Chern \nVeritas Technologies \nIsabella Chu \nPopulation Health Sciences \nStanford University \nJack Clark \nAnthropic \nMeaghan English \nPatrick J. McGovern \nFoundation \nCyrus Hodes \nThe Future Society \nSara Jordan \nThe Future of Privacy Forum \nVince Kellen \nU.C. San Diego, CloudBank \nMichael Kratsios \nScale AI \nSamantha Lai \nBrookings Institution \nBrenda Leong \nThe Future of Privacy Forum \nRuth Marinshaw \nStanford Research \nComputing Center \nJoshua Meltzer \nBrookings Institution \nSam Mulopulous \nU.S. Senate \nDewey Murdick \nCenter for Security and \nEmerging Technology \nGeorgetown University \nHodan Omaar \nCenter for Data Innovation \nCalton Pu \nGeorgia Tech \nAsad Ramzanali \nU.S. House of \nRepresentatives \nDavid Robinson \nUpturn \nSaiph Savage \nNortheastern University \nMichael Sellitto \nStanford HAI \nIshan Sharma \nFederation of \nAmerican Scientists \nJohn Smith \nIBM \nBrittany Smith \nData and Society \nVictor Storchan \nJP Morgan Chase \nKeith Strier \nAI Compute Task \nForce Organisation \nfor Economic \nCo-operation and \nDevelopment (OECD) \nLee Tiedrich \nCovington & Burling LLP \n \nEvan White \nCalifornia Policy Lab \nU.C. Berkeley \nOn August 5, 2021, the co-authors hosted a feedback session to hear from a variety of stakeholders \nin academia, civil society, government, and industry. We are thankful for the time and helpful \nadvice participants offered. Workshop attendee affiliations are listed for identification purposes \nonly. Individuals from Microsoft and AI Now also attended the workshop but did not want to be \npersonally identified. \nWorkshop Participants \n5\nDisclosures\nStanford University actively engaged with Congress and lobbied for the National Artificial Intelligence \nResearch Resource Task Force Act. Co-author Russell Wald built a coalition of academic, civil society, \nand industry stakeholders and lobbied Congress to pass the National Artificial Intelligence Research \nResource Task Force Act. \nHAI Co-Director Fei-Fei Li, who served as a guest lecturer in the class, was an early supporter of a \ntask force to study the National Research Cloud. Dr. Li has been appointed to serve as a member of \nthe National Artificial Intelligence Research Resource (NAIRR) Task Force. \nCo-author Daniel Ho directs the Stanford RegLab, which has received compute support from HAI’s \ncloud credit program (AWS and GCP); Microsoft’s AI for Earth Azure compute credit grant program; \nand Google’s Cloud credit grant for COVID-19 research. \nCo-author Jennifer King received unrestricted gift funding for research from Mozilla, Facebook, \nand Accenture in her previous role at the Center for Internet and Society. \nThe Stanford Institute for Human-Centered Artificial Intelligence (HAI) receives financial and cloud \ncomputing support from A121 Labs, Amazon Web Services, Google, IBM, Microsoft, and OpenAI. \nAbout HAI\nAbout the SLS Policy Lab\nStanford University’s Institute for Human-Centered Artificial Intelligence (HAI) applies rigorous \nanalysis and research to pressing policy questions on artificial intelligence. A pillar of HAI is to inform \npolicymakers, industry leaders, and civil society by disseminating scholarship to a wide audience. \nHAI is a nonpartisan research institute, representing a range of voices. The views expressed in this \nWhite Paper reflect the views of the authors. \nThe Policy Lab at Stanford Law School offers students an immersive experience in finding solutions \nto some of the world’s most pressing issues under the direction of Stanford faculty and researchers. \nDirected by former SLS Dean Paul Brest, the Policy Lab reflects the school’s belief that systematic \nexamination of societal problems, informed by rigorous research, can generate solutions to society’s \nmost challenging public problems.\nAcademic Independence\nThis White Paper was developed independently by the research team. While we solicited feedback from a \nwide range of stakeholders, no HAI donors, corporations, or other stakeholders had any involvement with \nthe research and production of this White Paper. Per HAI policy, “Donors cannot dictate research topics \npursued by HAI researchers” nor “control permission to publish research results.” For more information, \nplease see HAI’s policy: https://hai.stanford.edu/about/fundraising-policy. \n6\nBuilding a National AI Research Resource: \nA Blueprint for the National Research Cloud\n7\nTable of Contents\nEXECUTIVE SUMMARY: Creating a National Research Cloud 9\nINTRODUCTION 15\nCHAPTER 1: A Theory for a National Research Cloud 17\nCHAPTER 2: Eligibility, Allocation, and Infrastructure for Computing 22\nCHAPTER 3: Securing Data Access 35\nCHAPTER 4: Organizational Design 48\nCHAPTER 5: Data Privacy Compliance 53\nCHAPTER 6: Technical Privacy and Virtual Data Safe Rooms 61\nCHAPTER 7: Safeguards for Ethical Research 66\nCHAPTER 8: Managing Cybersecurity Risks 70\nCHAPTER 9: Intellectual Property 76\nGLOSSARY OF ACRONYMS 82\nAPPENDIX 84\nENDNOTES 90\nBuilding a National AI Research Resource: \nA Blueprint for the National Research Cloud\n8\nCOMPUTE MODELS\nNSF CloudBank 27\nNSF XSEDE 29\nFugaku 32\nCompute Canada 34\nDATA MODELS\nColeridge Initiative 42\nStanford Population Health Sciences 43\nThe Evidence Act 46\nORGANIZATIONAL MODELS\nScience and Technology Policy Institute 50\nAlberta Data Partnerships 51\nOTHER MODELS\nAdministrative Data Research UK 58\nCalifornia Policy Lab 64\nCase Studies\nBuilding a National AI Research Resource: \nA Blueprint for the National Research Cloud\n9\nArtificial intelligence (AI) appears poised to transform the economy across sectors ranging from healthcare and finance, \nto retail and education. What some have coined the “Fourth Industrial Revolution”1 is driven by three key trends: greater \navailability of data, increases in computing power, and improvements to algorithm design. First, increasingly large amounts \nof data have fueled the ability for computers to learn, such as by training an algorithmic language model on all of Wikipedia.2\nSecond, better computational capacity (often termed “compute”) and compute capability have enabled researchers to build \nmodels that were unimaginable merely 10 years ago, sometimes spanning billions of parameters (an exponential increase \nin scope from previous models).3 Third, basic innovations in algorithms are helping scientists to drive forward AI, such as the \nreinforcement learning techniques that enabled a computer to defeat the world champion in the board game Go.4\nHistorically, partnerships between government(s), universities, and industries have anchored the U.S. innovation \necosystem. The federal government played a critical role in subsidizing basic research, enabling universities to undertake \nhigh-risk research that can take decades to commercialize. This approach catalyzed radar technology, the internet, and \nGPS devices. As the economists Ben Jones and Larry Summers put it, “[e]ven under very conservative assumptions, it is \ndifficult to find an average return below $4 per $1 spent” on innovation, and the social returns might be closer to $20 for \nevery dollar spent.5 Industry in turn, scales and commercializes applications. \nCHALLENGES TO THE AI INNOVATION ECOSYSTEM\nYet this innovation ecosystem faces serious potential challenges. Computing power has become critical for the \nadvancement of AI, but the high cost of compute has placed cutting edge AI research in a position accessible only to key \nindustry players and a handful of elite universities.6 Access to data—the raw ingredients used to train most AI models—is \nincreasingly limited to the private sector and large platforms7, since government data sources remain largely inaccessible \nto the AI research community.8 As the National Security Commission on AI (NSCAI) has determined, “[t]he consolidation \nof the AI industry threatens U.S. technological competitiveness.”9\nFour interrelated challenges illustrate this finding: First, we are seeing a significant brain drain of researchers \ndeparting universities.10 In 2011, AI Ph.D.s were roughly equally likely to go into industry vs. academia.11 Ten years later, \ntwo- thirds of AI Ph.D.s go to industry, and less than one quarter go into academia.12 Second, these trends indicate that \nmany university researchers struggle to engage in cutting-edge science, draining the field of the diverse set of research \nvoices that it needs. Third, the fundamental research that would guarantee the United States stays at the helm of AI \ninnovation is being crowded out. By one estimate, 82 percent of algorithms used today originated from federally funded \nnonprofits and universities, but “U.S. leadership has faded in recent decades.”13 Fourth, government agencies have faced \nchallenges in building compute infrastructure,14 and there are societal benefits to reducing the cost of core governance \nfunctions and improving government’s internal capacity to develop, test, and hold AI systems accountable.15 In short, \na growing imbalance in AI innovation tilts towards industry, leaving academic and non-commercial research behind. \nGiven the longstanding role of academic and non-commercial research in innovation, this shift has substantial negative \nconsequences for the American research ecosystem. \nExecutive Summary: \nCreating a National Research Cloud\nBuilding a National AI Research Resource: \nA Blueprint for the National Research Cloud\n10\nTHE NATIONAL AI RESEARCH \nRESOURCE TASK FORCE ACT\nResponding to these challenges, Congress enacted \nthe National AI Research Resource Task Force Act as \npart of the National Defense Authorization Act (NDAA) \nin January 2021.16 The Act forms part of the National \nArtificial Intelligence Initiative, which identifies further \nsteps to increase research investments, set technical \nstandards, and build a stronger AI workforce. The Act \ncreated a Task Force—the composition of which was \nannounced on June 10, 202117—to study and plan for \nthe implementation of a “National Artificial Intelligence \nResearch Resource,” (NAIRR) namely “a system that \nprovides researchers and students across scientific \nfields and disciplines with access to compute resources, co-located with publicly available, artificial intelligence-ready \ngovernment and non-government data sets.”18 This research resource has also been referred to as the National Research \nCloud (NRC) and was strongly endorsed by the NSCAI, which wrote that the NRC “will strengthen the foundation of \nAmerican AI innovation by supporting more equitable growth of the field, expanding AI expertise across the country, and \napplying AI to a broader range of fields.”19\nWhile other initiatives have sought to improve access to compute or data in isolation,20 the NRC will generate distinct \npositive externalities by integrating compute and data, the two bottlenecks for high-quality AI research. Specifically, the \nNRC will provide affordable access to high-end computational resources, large-scale government datasets in a secure \ncloud environment, and the necessary expertise to benefit from this resource through a close partnership between \nacademia, government, and industry. By expanding access to these critical resources in AI research, the NRC will support \nbasic scientific AI research, the democratization of AI innovation, and the promotion of U.S. leadership in AI. \nTHEMES\nStanford Law School’s Policy Lab program convened a multidisciplinary research team of graduate students, staff, \nand faculty drawn from Stanford’s business, law, and engineering schools to study the feasibility of, and considerations for \ndesigning the NRC. Over the past six months, this group studied existing models for compute resources and government \ndata, interviewed a wide range of government, computer science, and policy experts, and examined the technical, business, \nlegal, and policy requirements. This White Paper was commissioned by Stanford’s Institute for Human-Centered Artificial \nIntelligence (HAI), which originated the proposal for the NRC in partnership with 21 other research universities.21\nThroughout our research, we observed three primary themes that cut across all areas of our investigation. We have \nintegrated these themes into each section of our White Paper and drawn on them to explain our findings. \n• Complementarity between compute and data. As we evaluated the existing computing and data-sharing ecosystems, \none of the systemic challenges we observed was a decoupling of compute resources from data infrastructures. \nThe NRC directs more resources \ntoward AI development in the public \ninterest and helps ensure long-term \nleadership by the United States in the \nfield by supporting the kind of pure, \nbasic research that the private sector \ncannot undertake alone.\nBuilding a National AI Research Resource: \nA Blueprint for the National Research Cloud\n11\nHigh-performance computing can be useless \nwithout data; and a major impediment to data \nsharing, particularly for high-value government \ndata, lies in requirements for a secure, privacy\u0002protecting computing environment. \n• Rebalancing AI research toward long-term, \nacademic, and non-commercial research. Presently, \nAI innovation is disproportionately dependent on \nthe private sector. Public investment in basic AI \ninfrastructure can both support innovation in the \npublic interest and complement private innovation \nefforts. The NRC directs more resources toward \nAI development in the public interest and helps \nensure long-term leadership by the United States in the field by supporting the kind of pure, basic research that the \nprivate sector cannot undertake alone.\n• Coordinating short-term and long-term approaches to creating the NRC. Our research considers many near-term \npathways for standing up a working version of the NRC by spelling out how to work within existing constraints. We \nalso identify the structural, legal, and policy challenges to be addressed in the long term for executing the full vision \nof the NRC. \nWe summarize our main recommendations here.\nCOMPUTE MODEL\n• The “Make or Buy” Decision. The main policy choice will be whether to build public computing infrastructure or \npurchase services from existing commercial cloud providers. \n° It is well established that, based solely on hardware costs, it is more cost-effective to own infrastructure when \ncomputing demand is close to continuous.22 The government also has experience building high-performance \ncomputing clusters, typically built by contractors and operated by national laboratories.23 The National Science \nFoundation (NSF) has also supported many supercomputing initiatives at academic institutions.24\n° The main countervailing concerns are that existing commercial cloud providers have software stacks and usability \nthat AI researchers have widely adopted and may consider to be a more user-friendly platform. Commercial cloud \nproviders offer a way to expand capacity expeditiously, although scale and availability will still be constrained by \nthe availability of current graphics processing unit (GPU) computing resources. \n° We recommend a dual investment strategy: \n■ First, the compute model of the NRC can be quickly launched by subsidizing and negotiating cloud \ncomputing for AI researchers with existing vendors, expanding on existing initiatives like the NSF’s \nCloudBank project.25\n■ Second, the NRC should invest in a pilot for public infrastructure to assess the ability to provide similar \nresources in the long run. Such publicly owned infrastructure would still be built under contract or \nOne of the systemic challenges \n[to basic AI research is] a decoupling \nof compute resources from data \ninfrastructures . . . [A] secure, \nprivacy-protecting computing \nenvironment [will be critical].\nBuilding a National AI Research Resource: \nA Blueprint for the National Research Cloud\n12\ngrant, but could be operated much like national laboratories (e.g., Sandia National Laboratories, \nOak Ridge National Laboratory) that own sophisticated supercomputing facilities or academic \nsupercomputing facilities. \n• Researcher Eligibility. While some have argued the NRC should be open for commercial access, for the purposes of \nthis White Paper, we adhered to the spirit of the legislation forming the NAIRR Task Force and only reviewed the use \nof an NRC for academic and non-profit AI research. We recommend that the NRC eligibility start with academics who \nhold “Principal Investigator” (PI) status (i.e., most faculty) at U.S. colleges and universities, as well as to “Affiliated \nGovernment Agencies” willing to contribute previously unreleased, high-value datasets to the NRC in return for \nsubsidized compute resources. PI status should be interpreted expansively to encompass all fields of AI application. \nStudents working with PIs should presumptively gain access to the NRC. Scaling the NRC to meet the demand of all \nstudents in the United States may be challenging, but we also recommend the creation of educational programs as \npart of the new resource to help train the next generation of AI researchers. \n• Mechanism. In order to keep the award processing costs down, we recommend a base-level of compute access to \nmeet the majority of researcher computing needs. Base-level access avoids high overhead for grant administration \nand may meet the compute demands for the supermajority of researchers. For researchers with exceptional needs, \nwe recommend a streamlined grant process for additional compute access. \nDATA ACCESS MODEL\n• Focus on Government Data. We focus our recommendations for data provision/access to government data \nbecause: (1) there are already a wide range of platforms for sharing private data,26 and (2) distribution by the NRC \nof private datasets would raise a tangle of thorny IP issues. We recommend that researchers be allowed to compute \non any datasets they themselves contribute, provided they certify they have the rights to that data, and the use of \nsuch data is for academic research purposes.\n• Tiered Access. We recommend a tiered access model: by default, researchers will gain access to government data \nthat is already public; researchers can then apply through a streamlined process to gain access at higher security \nlevels on a project-specific basis. It will be critical for the NRC to ultimately displace the current fragmented, agency\u0002by-agency relational approach. By providing secure virtual environments and harmonizing security standards (e.g., \nFederal Risk and Authorization Management Program (FedRAMP)27), the NRC can collaborate with proposals for a \nNational Secure Data Service28 to provide a model for accelerating AI research, while protecting data privacy and \nprioritizing data security.\n• Agency Incentives. To incentivize federal agencies to share data with the NRC and improve the state of public \nsector technology, we recommend the NRC permit federal agency staff to use the NRC’s compute resources. In \nkeeping with the practices of existing data-sharing programs, such as the Coleridge Initiative,29 we also recommend \nthat the NRC provide training and support to work with agencies to modernize and harmonize their data standards.\n• Strategic Investment for Data Sources. In the short term, we recommend that the NRC focus its efforts on making \navailable non-sensitive, low- to moderate-risk government datasets, rather than sensitive government data (e.g., \ndata about individuals) or data from the private sector, due to data privacy and intellectual property concerns. \nResearchers can still use NRC compute resources on private data, but should rely on existing mechanisms to acquire \ndata for their own private buckets on the NRC. For example, images taken from Earth observation satellites, such as \nBuilding a National AI Research Resource: \nA Blueprint for the National Research Cloud\n13\nLandsat imagery, provide a promising low-risk, high-reward government dataset, as making such satellite imagery \nfreely available to researchers has generated an estimated $3-4 billion in annual economic benefits, particularly \nwhen combined with high-performance computing.30 Agencies such as the National Oceanic and Atmospheric \nAdministration, the U.S. Geological Survey, the Census Bureau, the Administrative Office of the U.S. Courts, and \nthe Bureau of Labor Statistics, for instance, also have rich datasets that can more readily be deployed. In the long \nrun, access to high-risk datasets, such as those owned by the Internal Revenue Service (IRS) and the Department of \nVeterans Affairs (VA), will depend on the tiered access model. \nORGANIZATIONAL FORM\nWhere to institutionally locate the NRC poses a tradeoff between ease of coordination to obtain compute and ease \nof data access. For instance, locating the NRC within a single agency would make coordination with compute providers \neasier, but would make data access across agencies more difficult, absent further statutory authority. Many efforts to \nmake data access to government data easier, most notably the Foundations for Evidence-Based Policymaking Act of \n2018, have proven to be among the most daunting challenges of government modernization.31 Building on those insights, \nwe ultimately recommend that the NRC be instituted as a Federally Funded Research and Development Center (FFRDC) \nin the short run, and a public-private partnership (PPP) in the long run.\n• FFRDC. FFRDCs at Affiliated Government Agencies would reduce the significant costs of securing data from those \nhost agencies. This approach will also cohere with the greater reliance on commercial cloud credits in the short run, \nmaking compute and data coordination less central. In the long run, however, streamlined coordination between \ndata and compute may be more difficult with FFRDCs hosted at specific agencies when (1) the NRC moves away \nfrom commercial cloud credits and towards its own high-performance computing cluster, and (2) a greater number \nof inter-agency datasets become available. \n• PPP. In the long run, we recommend the creation of a PPP model, governed by officers from Affiliated Government \nAgencies, academic researchers, and representatives from the technology sector, which can house both compute \nand data resources. \nADDITIONAL CONSIDERATIONS\n• Data Privacy. As an initial matter, an NRC where sensitive or individually identifiable administrative data from \nmultiple agencies are used to build and train AI models will face challenges from the Privacy Act of 1974.32 The Act is \nintended to put a check on interagency data sharing and disclosure of sensitive data without consent. \n° In order to avoid conflicts with non-consensual interagency data sharing, we recommend that the NRC should \nnot be instituted as its own federal agency, nor should federal agency staff be allowed access to interagency \ndata. \n° To avoid conflicts with the Act’s “no disclosure without consent” requirement, any data released to the NRC \nmust not be individually identifiable. Despite these constraints, the majority of AI research will likely fall under \nthe Act’s statistical research exception, contingent on proposals aligning with an agency’s core purpose. \n° Given concerns about the potential privacy risks, federal agencies may desire to share data, contingent on \nthe use of technical privacy measures (e.g., differential privacy). While useful in many instances, technical \nBuilding a National AI Research Resource: \nA Blueprint for the National Research Cloud\n14\napproaches are no panacea and should not \nsubstitute for data access policies. \n° The NRC should explore the design of virtual \n“data safe rooms” that enable researchers to \naccess data in a secure, monitored, and cloud\u0002based environment.\n° Additional legislative interventions could \nalso facilitate data sharing with the NRC (e.g., \nrequiring IT modernization to include data \nsharing plans with the NRC).\n• Ethics. Rapid innovation in AI research raises a \nhost of potential ethical challenges. Given the \nscope of the NRC, it will be infeasible to review \nevery single research proposal for potential ethical \nviolations, particularly since ethical standards are still in flux. The NRC should adopt a twofold approach. \n° First, for default PI access to base-level data and compute, the NRC should establish an ex-post review process \nfor allegations of ethical research violations. Access may be revoked when research is shown to manifestly and \nseriously violate ethical standards. We emphasize that the high standard for a violation should be informed \nby the academic speech implications and potential political consequences of government involvement in \nadministering the NRC and determining academic research directions. \n° Second, for applications requesting access to restricted datasets or resources beyond default compute, which \nwill necessarily undergo some review, researchers should be required to provide an ethics impact statement. \nOne of the advantages of beginning with PIs is that university faculty are accountable under existing IRBs for \nhuman subjects research, as well as to the tenets of peer review. \n° We urge non-NRC parties (e.g., universities) to explore a range of measures to address ethical concerns in AI \ncompute (e.g., an ethics review process33 or embedding ethicists in projects34).\n• Security. We recommend that the NRC take the lead in setting security classifications and protocols, in part to \ncounteract a balkanized security system across federal agencies that would stymie the ability to host datasets. The \nNRC should use dedicated security staff to work with Affiliated Government Agencies and university representatives \nto harmonize and modernize agency security standards. \n• Intellectual Property (IP). While the evidence on optimal IP incentives for innovation is mixed, we recommend \nthat the NRC adopt the same approach to allocating patent rights, copyrights, and data rights to NRC users \nthat apply to federal funding agreements. The NRC should additionally consider conditions for requiring NRC \nresearchers to disclose or share their research outputs under an open-access license.\n• Human Resources. Given its ambition, significant human resources – from systems engineers to data officers, and \nfrom grants administrators to privacy, ethics, and cybersecurity staff – will be necessary to make the NRC a success.\nGiven its ambition, significant \nhuman resources – from systems \nengineers to data officers, and \nfrom grants administrators to \nprivacy, ethics, and cybersecurity \nstaff – will be necessary to make \nthe NRC a success.\nBuilding a National AI Research Resource: \nA Blueprint for the National Research Cloud\n15\nIntroduction\nIn March 2020, Stanford’s Institute for Human-Centered Artificial Intelligence (HAI) published an open-letter, \nco-signed by Presidents and Provosts of 22 top universities in the country, to the President of the United States and \nCongress urging adoption of a National Research Cloud (NRC).1 The NRC proposal aims to close a significant gap in \naccess to computing and data that, proponents argue, has distorted the long-term trajectory of Artificial Intelligence (AI) \nresearch.2 Without access to such critical resources, AI research may be dominated by short-term commercial interests \nand undermine the historical innovation ecosystem where basic, fundamental, and non-commercial research have laid \nthe foundations for applications that may be decades away, not yet marketable, or promote the public interest. \nIn January 2021, the U.S. Congress enacted the National Artificial Intelligence Research Resource Task Force Act \n(NAIRR), constituting a Task Force to consider the design of the NRC.3\n The Task Force was announced in June of this year \nand includes one of the original proponents of the NRC and Co-Director of HAI (Fei-Fei Li).4\nThis White Paper is the culmination of a two-quarter, independent policy practicum at Stanford Law School’s Policy \nLab program, co-taught by three of us (Ho, King, Wald) and a teaching assistant (Wan), which brought together law, \nbusiness, and engineering students to contemplate key design dimensions of the NRC. We interviewed and convened \na wide range of stakeholders, including privacy attorneys, cloud computing technologists, government data experts, \ncybersecurity professionals, potential users, and public interest groups. Students researched governing legal provisions, \npolicy options, and avenues for the institutional design of the NRC. The practicum team worked independently to shape \nits recommendations.\nThe proposal for an NRC is an ambitious one, and this White Paper covers a lot of ground. We begin with the \nfundamental question—why build the NRC (Chapter One)?—and spell out what we view as a cogent theory of impact. \nWe then cover who should have access to the NRC (Chapter Two), what comprises the NRC (Chapter Two), how access to \nrestricted data may (or may not) be granted (Chapter Three), and where the NRC should be located (Chapter Four). We \nspend extensive time on the data access portion (Chapters Three, Five, and Six), due to the complexities of government \ndata sharing under the Privacy Act of 1974.5\n As we note in those chapters, the data portion of the NRC is complementary \nto longstanding efforts to enable greater research access to administrative data under, for instance, the Foundations for \nEvidence-Based Policymaking Act of 20186 and the National Secure Data Service Act proposal.7 Such sharing must be \ncarried out securely and in a privacy-protecting fashion. We also consider questions of ethical standards (Chapter Seven), \ncybersecurity (Chapter Eight), and intellectual property (Chapter Nine) that inform the design of the NRC. \nWe recognize the complexity of the enterprise and that there are many questions not answered herein. The \ncontemplated scale of the NRC may be to AI what the Human Genome Project was to genomics (or what particle \naccelerators were to physics): public investment for ambitious, non-commercial fundamental scientific research to \nensure the long-term flourishing of a critical area of innovation for the United States. There are many areas where we \nwish we had had the opportunity to engage in more extensive research. We hope this White Paper, nonetheless, will \nprovide a useful contribution for the NAIRR Task Force, Congress, the Administration, and all those interested in the AI \ninnovation ecosystem. \nBuilding a National AI Research Resource: \nA Blueprint for the National Research Cloud\n16\nWe owe gratitude to many people who contributed time, feedback, and insights. Most importantly, we thank the \nextraordinary students who shaped this White Paper: Simran Arora, Sabina Beleuz, Nathan Calvin, Shushman Choudhury, \nDrew Edwards, Neel Guha, Krithika Iyer, Ananya Karthik, Kanishka Narayan, Tyler Robbins, Frieda Rong, Jasmine Shao, \nand Sadiki Wiltshire. We benefited from too many individuals to name, but special thanks go to Taka Ariga, Kathy Baxter, \nMiles Brundage, Jean Camp, Shikai Chern, Bella Chu, Jack Clark, Kathleen Creel, John Etchemendy, Deep Ganguli, Eric \nHorvitz, Sara Jordan, Vince Kellen, Mark Krass, Sebastien Krier, Ed Lazowska, Brenda Leong, Fei-Fei Li, Ruth Marinshaw, \nMichelle Mello, Amy O’Hara, Hodan Omaar, Saiph Savage, Marietje Schaake, Mike Sellitto, Wade Shen, Keith Strier, \nSuzanne Talon, Lee Tiedrich, Christine Tsang, and Evan White for helpful insights and feedback. HAI staff and research \nassistants who were essential in helping us during the final stages of editing and compiling the White Paper include Tina \nHuang, Marisa Lowe, Diego Núñez, and Daniel Zhang. \nAs we spell out in this White Paper, the NRC is an idea worth taking seriously. It is worth being clear, however, what \nit would and would not solve. The NRC would enable much greater access to—and in that sense, democratize—forms \nof AI and AI research that have increased in computational demands; but it would not categorically prevent or shift \nthe centralization of power within the tech industry. The NRC would shift the attention of current AI efforts into more \npublic and socially driven dimensions by providing access to previously restricted government datasets, addressing \nlongstanding efforts to improve access to high-value public sector data; but it would not create a system to prevent all \nunethical uses of AI. The NRC would facilitate audits of large-scale models, datasets, and AI systems for privacy violations \nand bias; but it would not be tantamount to a regulatory requirement for fairness assessments and accountability. It \nis neither a tool of antitrust nor a certification body for ethical algorithms, which are areas worth taking seriously in \nindependent policy proposals.8\n These broader considerations, however, do play into key areas of design, and have very \nmuch informed our recommendations below on the design of the NRC. \nWhile it alone cannot solve all that ails AI, the NRC promises to take a major affirmative step forward.\nA Blueprint for the National Research Cloud 17\nCHAPTER 1\nThis chapter articulates a theory of impact for the NRC. In conventional policy \nanalytic terms,1 what problem (or market failure) does the NRC address? From \none perspective, AI innovation is vibrant in the United States, with major advances \noccurring in language, vision, and structured data and applications developing across \nall sectors. Yet from another perspective, current commercialization of past innovation \nmasks systematic underinvestment in basic, non-commercial AI research that could \nensure the long-term health of technological innovation in this country. \nChapter 1: \nThe Theory for a \nNational Research Cloud\nKEY \nTAKEAWAYS\n The federal government \nwill play a central role in \nshaping, coordinating, and \nenabling the development \nof AI.\n AI research and \ndevelopment is \nincreasingly dependent \non access to large-scale \ncompute and data, causing \nmigration of AI talent from \nthe academic to private \nsector and limiting the \nrange of voices able to \ncontribute to AI research.\n Noncommercial and basic \nAI research is critical to the \nlong-term health of the \ninnovation ecosystem. \n An NRC that provides data \nand compute access will \nhelp to promote the long\u0002term, national health of the \nAI ecosystem and mitigate \nthe risks of widening \ninequalities in the nation’s \nAI landscape.\nCurrent commercialization of past innovation masks \nsystematic underinvestment in basic, non-commercial \nAI research that could ensure the long-term health of \ntechnological innovation in this country. \nOur case for the NRC is grounded in both efficiency and distributive rationales. \nFirst, the NRC may yield positive externalities, particularly over time, by supporting \ninvestments in basic research that may be commercialized decades later. Second, it \nmay help to level the playing field by broadening researcher access to both compute \nand data, ensuring that AI research is feasible for not just the most elite academic \ninstitutions or large technology firms. Given the scale of economic transformation AI \nis posited to initiate over the next few decades, the stakes are potentially significant. \nWhile the largest private interests like platform technology companies and certain elite \nacademic institutions continue to design, develop, and deploy AI systems that can be \nreadily commercialized, a different story is playing out for the public sector and the vast \nmajority of academic institutions, which lack access to core inputs of AI research. The \nrising costs associated with carrying out research and development are exacerbating the \ndisconnect between current winners and losers in the AI space.\nThis chapter proceeds in three parts. First, we survey the current landscape of \nAI research. Second, we articulate shifting trends in AI research and the academic\u0002industry balance. Third, we spell out the risks of federal inaction and the benefits to an \ninvestment strategy that couples data and compute resources. \nA Blueprint for the National Research Cloud 18\nCHAPTER 1\nTHE AI RESEARCH LANDSCAPE\nThe field of AI research, as we consider it in this White \nPaper, is broadly construed. It includes not only academics who \nidentify themselves as researchers in artificial intelligence or \nmachine learning, but the broader community of researchers \nwho use applied AI in their work, as well as those who \nexamine its impacts on society and the environment. \nMany believe, consistent with the legislation calling for \nthe NAIRR Task Force, that AI will have dramatic impact on \nsociety. Nine of the world’s ten current largest companies \nby market capitalization are technology companies that \nplace AI at the core of their business models.2 Recent \nfigures from the AI Index demonstrate the growing amount \nof investment AI companies have drawn. The most recent \n2021 iteration of the Index details how global private \ninvestment in AI has grown by 40 percent since 2019 \nto a total of $67.9 billion, with the United States alone \naccounting for over $23.6 billion.3 While multiple private \nsector predictions of the economic impact of AI emphasize \nthe potential for AI to drive significant economic growth \nthrough a strong increase in labor productivity, others \nworry about the pace of structural change in the labor \nmarket and economic dislocation for workers automated \nout of their jobs or impacted by the gig economy.4\nSuch impacts are expected across domains. AI holds \nsubstantial promise to transform healthcare and scientific \nresearch: AI-related progress in the field of protein folding \nis poised to dramatically expedite vaccine development \nand pharmaceutical drug development.5 The integration \nof AI-related systems into agriculture may improve \ncrop yields through targeted use of pesticides and soil \nmonitoring.6\n And national security experts have identified \nAI as a key driver of novel defense capabilities,7 including \ncyberwarfare and intelligence collection. \nMany countries have recognized the significance \nof AI as a driver of progress in economic, scientific, and \nnational security, releasing national plans coordinating \ninvestment for continued progress in AI.8 China’s national \nplan announced billions of dollars in funding aimed at \nmaking China the international leader in AI by 2030.9 The \nJapanese government partnered with Fujitsu to build \nthe world’s fastest supercomputer (Fugaku).10 Compute \nCanada has similarly provided research computing access \nto academics across Canada. The UK’s national high end \ncomputing resource, HECToR, launched in 2007 at a cost \nof $118 million, was used by nearly 2,500 researchers from \nover 250 separate organizations who produced over 800 \nacademic publications.11\nThe U.S. government initially presented a more \ndecentralized approach, providing support for AI \ndevelopment through National Science Foundation \ngrants and defense spending, but refrained from releasing \na unified national plan to coordinate resources across \ngovernment, private industry, and universities.12 The \ncreation of a National AI Initiative Office,13 the updating \nof the National Strategic Computing Initiative,14 and the \nrelease of the National Security Commission on Artificial \nIntelligence’s (NSCAI) final report15 introduced a more \ncomprehensive and coordinated approach. Within the \nUnited States, the closest model to the NRC may be the \nCOVID-19 HPC consortium which quickly provisioned \ncompute of 50K GPUs and 6.8 million cores for close to \n100 projects across 43 academic, industry, and federal \ngovernment consortium members united by the common \ngoal of combating the COVID-19 pandemic.16\nHistorically, partnerships between government, \nuniversities, and industry have anchored the U.S. innovation \necosystem. The federal government played critical roles \nin subsidizing basic research, enabling universities to \nundertake high-risk research that can take decades to \ncommercialize. This approach catalyzed radar technology,17\nthe internet, 18 and GPS devices.19 This history informed the \nNSCAI’s recommendation for substantial new investments in \nAI R&D by establishing a national AI research infrastructure \nthat democratizes access to the resources which fuel AI. \nMany policymakers believe that substantial investment \nwill be needed over the next several years to support these \nefforts, with returns on such investments could potentially \ntransforming America’s economy, society, and national \nsecurity.20 \nTo be sure, some may challenge the theory of impact. \nFirst, some studies dispute the premise that AI will be \neconomically transformative. Some economists argue that \nA Blueprint for the National Research Cloud 19\nCHAPTER 1\nmany of the optimistic assessments fail to consider how \nconstrained the uptake of AI innovation may be due to AI’s \ninability to change essential, yet hard-to-improve tasks.21\nOthers similarly critique the evidence for a fourth industrial \nrevolution.22 Second, some suggest that the provisioning \nof the NRC may strengthen the position of large platform \ntechnology companies (which of course provokes debates \nover antitrust in the technology sector23), as the NRC may \nbe hard to launch without some involvement of hardware \nor cloud providers in the procurement process. Third, some \nwould argue that the NRC would generate large negative \nexternalities in the form of energy footprints. For instance, \none study found that the amount of energy needed to train \nGPT-3, a leading natural language processing (NLP) model, \nrequired the greenhouse emissions equivalent of 552.1 \ntons of carbon dioxide,24 approximately thirty-five times \nthe yearly emissions of an average American.25 Expanding \naccess to compute without appropriate controls may \ncontribute to wasteful computing.26 Finally, some critics \nargue that any advances in AI are inherently too risky for \nfurther investment,27 given widely documented risks of \nbias,28 unintended consequences,29 and harm.30\nWe are cognizant of these critiques and take them \nseriously. This White Paper proceeds on the operative \npremise animating the NRC legislation: that it will be \nimportant for the country to maintain leadership in AI— \nincluding rigorous interrogation of its uses, limits, and \npromises—and that this requires supporting access to \ncompute and data. Public investment in AI research for \nnoncommercial purposes may help to address some of \nthe issues of social harm we see presently in commercial \ncontexts31, as well as contribute to shifting the broader \nfocus of the field toward technology developed in the \npublic interest by the public sector and civil society, \nincluding academia. The preceding considerations, \nhowever, have shaped our views in key respects, such as \nthe sequential investment strategy, given the uncertainty \nof AI’s potential; the serious consideration of publicly \nowned infrastructure, the provisions for ethical review \nof compute and data access, and, most importantly, \nthe enablement of independent academic inquiry into \nthe potential harms of AI systems. The NRC is not an \nendorsement of blind and naïve AI adoption across the \nboard; it is a mechanism to ensure that a greater range of \nvoices will have access to the basic elements of AI research. \nSHIF TING SOURCES OF \nAI RESEARCH\nWe now articulate how and why AI research has \nmigrated away from basic, long-term research into \ncommercial, short-term applications. \nFirst, many current advances fueled by large-scale \nmodels are costly to train, relative to the size of typical \nacademic budgets. For example, the estimated cost \nof training Alphabet subsidiary DeepMind’s AlphaGo \nZero algorithm, capable of beating the human world \nchampion of the game Go, was more than $25 million.32\nFor reference, the total annual 2019 budget for Carnegie \nMellon University’s Robotics Institute, one of the premier \nacademic research institutions in the nation, was $90 \nmillion.33 A white paper from the Bipartisan Policy Center34\nand the Center for a New American Security noted that the \nFY2020 budget for non-defense AI R&D announced by the \nWhite House was $973 million. In contrast, the combined \nspending on R&D in 2018 by five of the major technology \nplatform companies was $80 billion. In sum, research \nuniversities cannot keep pace with private sector resources \nfor compute. This is not to say that large-scale compute is \nnecessary for all academic AI research, or that academic \nresearch is in competition with industry research, but it \ndoes illustrate why certain sectors of AI research are no \nlonger accessible to the academic researcher. \nThe NRC is not an endorsement \nof blind and naïve AI adoption \nacross the board; it is a mechanism \nto ensure that a greater range of \nvoices will have access to the basic \nelements of AI research. \nA Blueprint for the National Research Cloud 20\nCHAPTER 1\nSecond, the academic-industry divide masks \nsignificant disparities between academic institutions. \nUsing the QS World University Rankings since 2012, \nFortune 500 technology companies and the top 50 \nuniversities have published five times more papers \nannually per AI conference than universities ranked \nbetween 200 and 500.35 Private firms also collaborate \nsix times more with top 50 universities than with those \nranked between 301 and 500.36 This internal compute \ndivide across universities poses significant challenges \nfor who is at the table. \nThird, basic AI research has lost human capital.37\nWhen this is combined with decreased access to \ncompute and data in the academy, the prospect of \nconducting basic research at universities becomes \nless attractive. Top talent in AI now commands private \nsector salaries far in excess of academic salaries.38 The \ndeparture of AI faculty from American universities has \nled to what some analysts have dubbed the AI Brain \nDrain: while AI Ph.D.s in 2011 were roughly equally \nlikely to go into industry as opposed to academia, two\u0002thirds of AI Ph.D.s now go to industry and less than a \nquarter go into academia.39 One study suggests that \nthe departure of AI faculty also has a negative effect on \nstartup formation by students.40\nFourth, as large-scale AI research migrates to industry, \nthe focus of research inevitably shifts. While academic \nresearchers in AI may lack access to the volume of data \nneeded to train AI models,41 large-platform companies \nhave access to vast datasets, including those about, or \ncreated by their customers. This data divide in turn distorts \nAI research toward applications that are focused on private \nprofit, rather than public benefit.42 Put more colorfully by \nJeff Hammerbacher, “The best minds of my generation are \nthinking about how to make people click ads.” 43 The NRC \ncan play a key role in unlocking access to public sector \ndata, which may help to reorient the focus of AI research \naway from private sector datasets.44\nThe hollowing out of academic AI capacity can be \nseen in OpenAI’s analysis of the relationship between \ncompute and 15 relatively well-known “breakthroughs” \nin AI between 2012 and 2018.45 Although the analysis was \nmeant to emphasize the role of computing power, it also \nillustrates an emerging gap between private sector and \nacademic contributions over time. Of the 15 developments \nexamined, 11 were achieved by private companies, while \nonly four came from academic institutions. Furthermore, \nthis imbalance increases over time: though private sector \nresearch has continued accelerating since 2012, academic \noutput has stagnated. The last of the major compute\u0002intensive breakthroughs in OpenAI’s analysis stemming \nfrom academia was Oxford’s 2014 release of its VGG image \nrecognition program; NYU’s work on Convolutional Neural \nNetworks dates back to 2013. From 2015 to 2018, all eight \nbreakthroughs included in OpenAI’s analysis came out \nof private companies. All of this together leads observers \nto argue that academic researchers are increasingly \nunable to compete at the frontier of AI research.46 While \nacademic researchers have continued to make important \ncontributions in AI, these are increasingly restricted to \nless compute-intensive problems. With fewer compute\u0002intensive academic breakthroughs, AI innovations have \nfocused on private interests (e.g., online advertising) as \nopposed to long-term, non-commercial benefits. To be \nsure, the private sector has, of course, been central to AI \nresearch, but the concern is about the long-term balance \nof the AI innovation ecosystem. \nWhile AI Ph.D.s in 2011 were \nroughly equally likely to go into \nindustry as opposed to academia, \ntwo-thirds of AI Ph.D.s now go to \nindustry and less than a quarter \ngo into academia.\nA Blueprint for the National Research Cloud 21\nCHAPTER 1\nSCOPING FEDERAL INTERVENTION \nIN DATA AND COMPUTE\nHow can we achieve a more balanced approach \ntowards research and development? We first consider the \nrisks of federal inaction and discuss some of the unique \nadvantages of addressing data and compute together. \nRisks of Federal Inaction\nThe risks of federal inaction are twofold. First, basic \nAI research that has to date paved the way for advances \nin AI and machine learning will slow. According to a recent \nstudy, approximately 82 percent of the algorithms used \ntoday originated from nonprofit groups and universities \nsupported by government spending.47 Even when industry \nresearch is successful, it is typically product-focused \nor incremental, harder to reproduce, and may not be \npublished or open-sourced. An interesting case lies in \nrecent breakthroughs in protein folding. In late 2020, \nthe Alphabet subsidiary DeepMind announced that it \nhad developed a program called AlphaFold, an AI-driven \nsystem capable of accurately predicting the structure of \na vast number of proteins, using only the sequence of \nnucleotides contained in its DNA. Whether out of concern \nfor the privatization or to accelerate adoption of related \nsystems, a consortium of academics, led by scientists at \nthe University of Washington, developed an open source \ncompetitor called, RoseTTaFold.48 DeepMind did make \nAlphaFold available to a broad audience, but the concerns \nillustrate the risks of science posed by exclusively private \nAI research, reminiscent of the race to sequence the \nhuman genome, where public investment into the Human \nGenome Project preempted concerns about a private firm \npatenting of the human genome.49\nSecond, federal inaction could widen significant \ninequalities in the AI landscape. Without increased access \nto computing, education, and training, large parts of the \neconomy may be unable to adapt—whether that be in \nfinancial services, healthcare, education, or government. \nDiversifying the range of AI research may also promote \nprogress and productivity. One study suggests that the \ndiversity of AI research trajectories—that is, the specific \nquestions, topics, and problems researchers choose to \ninvestigate—has become more constrained in recent \nyears and that private sector AI research is less diverse \nthan academic research.50 Smaller academic groups \nwith lower private sector collaboration appear to bolster \nthe diversity of AI research.51 From the standpoint of \nunderdeveloped avenues of research, such as ethics and \naccountability in AI, increasing the range of research topics \nand methods in the field raises the likelihood of finding \nbreakthroughs that make additional progress possible in \nthe long term possible.52 Recent evidence suggests that \njust five metro areas in the U.S. shared 90 percent of the \ngrowth in innovation sector jobs, between 2005 and 2017.53\nAccording to Stanford economist Erik Brynjolfsson, the \nlikely impact of geographic concentration is that: “there \nare a whole lot of people—hundreds of millions in the U.S. \nand billions worldwide—who could be innovating and who \nare not because they do not have access to basic computer \nscience skills, or infrastructure, or capital, or even culture \nand incentives to do so.”54 AI technologies can be hard to \ndiagnose and interpret and be prone to substantial bias.55\nBroadening the set of voices that can interrogate such \nsystems will be critical to an inclusive and equitable future. \nIn sum, federal investment in public AI infrastructure \nmay promote a more equitable distribution of \nparticipation in and gains to AI innovation broadly, bolster \nU.S. competitiveness, and support fundamental research \ninto noncommercial and public sector applications.\nA Blueprint for the National Research Cloud 22\nCHAPTER 2\nThis chapter discusses eligibility, resource allocation, and computing \ninfrastructure for the NRC: who should get access to what and how? \nFirst, when determining who should get access, it is critical to bear in mind the \nbroad goals of the NRC. As discussed in Chapter One, there is a large resource gap \nin academia as compared to private industry. In the interest of supporting basic \nresearch and democratizing the field, this section will focus on identifying a target \ngroup for eligibility. As we articulate below, we refrain from considering expansion \nto a broader set of commercial, non-academic parties because of the NRC’s focus \non long-term, fundamental scientific research. One of the narrowest approaches \nwould be a specialty faculty model which would target researchers engaged in core \nAI work. But, the difficulties with defining AI and the rapidly expanding domains in \nwhich AI is being applied make this model too constrained to realize the full impact \nof the NRC. Instead we recommend tracking the most common criterion for federal \nresearch funding and advocate that eligibility hinge on “Principal Investigator” (PI) \nstatus at U.S. universities.1\n One of the tradeoffs is that PIs may be less diverse than a \nbroader segment of researchers,2 so a longer-term expansion could consider moving \nbeyond this group. While the NRC aims to train the next generation of AI researchers, \nwe caution that an immediate expansion to all graduate and undergraduate students \nwould pose considerable challenges in scaling. Therefore, we recommend that \nstudents primarily gain access by participation in faculty sponsored AI research, \ninstead of blanket student access, and that they gain training through the creation of \neducational programs. \nSecond, we discuss three models for allocating computing credit: development \nof a new grant process, delegating block compute grants to universities for internal \nallocation among faculty, or universal access. Each of these models trades off the ease \nof administration against tailoring for specific NRC goals. We recommend an approach \nused by other national research clouds: namely a hybrid approach of universal default \naccess for the majority of researchers, with a grant process for excess computing \nbeyond the default allocation. Such an approach would keep administrative costs low \nfor the vast majority of researchers, while enabling tailoring through a competitive \ngrant process for the highest-need users. \nChapter 2: \nEligibility, Allocation, \nand Infrastructure \nfor Computing\nKEY \nTAKEAWAYS\n Researcher eligibility \nfor NRC access should \nbegin with “Principal \nInvestigator” status at U.S. \nuniversities.\n The NRC should adopt \na hybrid approach of \nuniversal default access for \nthe majority of researchers \nand a grant process when \nrequests for compute or \ndata exceed base levels.\n The NRC should adopt a \ndual investment strategy \nby developing programs \nfor expanding access to \nexisting cloud services \nand piloting the ability to \nprovide publicly owned \nresources. \nA Blueprint for the National Research Cloud 23\nCHAPTER 2\nThird, we consider the “make-or-buy” decision for \nthe NRC. One option would be for the NRC to provide \nresearch grants for the use of commercial cloud services \nthat many researchers already rely on (the “buy” decision). \nAlternatively, the NRC could create and provision access \nto a publicly high-performance computing cluster (the \n“make” decision). It is well-established that, based \nsolely on hardware costs, it is more cost-effective to \nown infrastructure when computing demand is close to \ncontinuous. On the other hand, existing commercial cloud \nproviders have developed highly usable software stacks \nthat AI researchers have widely adopted. Commercial \ncloud providers offer a way to quickly expand capacity. \nWe hence recommend a dual investment strategy to (a) \nquickly launch the NRC by subsidizing and negotiating \ncloud computing for AI researchers with existing vendors, \nexpanding on existing initiatives like the National Science \nFoundation’s CloudBank project; and (b) invest in a pilot \nfor public infrastructure to assess the ability to provide \nsimilar resources in the long run. Such publicly owned \ninfrastructure would likely be built under contract or grant, \nbut could be operated much like national laboratories \nthat own sophisticated supercomputing facilities, as is the \ncase for other national research resources (e.g., Compute \nCanada, Japan’s Fugaku).\nOur recommendations are informed by a series of \ncase studies that are presented throughout this chapter, \nas well as through the remainder of the White Paper. \nTable 1 summarizes how different existing models \ncompare on the three key design decisions. At the outset, \nwe note that few existing initiatives have attempted to \nprovide compute power at the scale of the NRC. At the \nsame time, we view the NRC as complementary to more \ntraditional areas of scientific computing.3\nELIGIBILITY ALLOCATION OWNERSHIP\nExisting \nProgram\nPI \nonly\nAny \nFaculty Students\nExisting \nGrant \nProcess\nUniversity \nAllocation\nNew \nProcess\nDefault \nAccess \nw/Tiers\nPrivate Public\nCloudBank X X X X\nStanford \nHAI-AWS \nCloud Program\nX X X\nStanford \nSherlock Cluster X X X\nGoogle Colab X X X X\nCompute \nCanada X X X\nFugaku X X X\nXSEDE X X X X\nDOE INCITE X X X\nTable 1: Key design differences between computing case studies. “Other faculty” indicates an eligibility set for faculty other than PI status \n(e.g., requiring Stanford affiliation for the Sherlock cluster) and “new process” is used to indicate the creation of any other process other than \nthose currently listed (e.g., Fugaku is currently soliciting proposals with research facilities). \nA Blueprint for the National Research Cloud 24\nCHAPTER 2\nEligibility\nThe first task is identifying which researchers should \nbe eligible for the NRC. Chapter One discussed the need \nto support AI innovation in universities. Therefore, this \nsection will scope eligibility within academia by analyzing \nthe access-resource trade-offs in alignment with the NRC \ngoals.\nAt the outset, we note that we do not analyze eligibility \nin depth beyond academic researchers. The legislation \nconstituting the NRC task force specifically contemplates \n“access to computing resources for researchers across the \ncountry.”4 The NRC is defined as “a system that provides \nresearchers and students across scientific fields and \ndisciplines with access to compute resources.”5 The most \nnatural interpretation of this language suggests a core \nfocus on scientific and academic research.6\nIntroducing commercial access to the NRC, particularly \nfor under-resourced firms such as small businesses \nand startups may very well benefit the U.S. innovation \necosystem. But the challenges of incorporating commercial \naccess to the NRC are enormously complex. First, including \nsoftware developers at start-up companies as “researchers” \nwithin the meaning of the NDAA would raise a wide \nrange of boundary questions that the NRC may be poorly \nequipped to adjudicate. According to the Small Business \nAdministration (SBA), there are over 31 million small \nbusinesses in the United States.7\n Over 627,000 businesses \nopen each year.8 Should all such businesses be eligible to \ncompute on the NRC? How would one avoid gaming (e.g., \nstrategic subsidiaries/spinoffs) eligibility? And, how would \nthis advance the scientific mission of the NRC? Second, \nwhile potentially valuable, it is less clear how the inclusion \nof start-ups and small businesses meets the theory of \nimpact of the NRC. As currently construed, the concern \nanimating the NRC lies in the importance of long-term, \nnon-commercial fundamental research that can ensure AI \nleadership for decades to come. Commercialization is not \nthe element of the AI innovation ecosystem that faces the \nstructural challenges articulated in Chapter One. Finally, \nscaling the NRC to allow meaningful commercial access \nwould pose serious practical challenges. Because the \nTask Force must also consider the feasibility of the NRC, \nwe have not considered in depth here, a conception that \nwould extend the term “researcher” to encompass large \nportions of the commercial private sector. Expansion to \nnon-academic non-profit organizations may be a more \nreasonable consideration, as the objective of some entities \n(e.g., not-for-profit investigative journalism, civil society \norganizations) may be closer to the core of the NRC’s \nmission of empowering long-term beneficial research that \ncannot currently occur.9\n In the long term, the NRC should \nconsider the tradeoffs to such an expansion. \nEven if the NRC adopts a broader computing model \ndown the road, we believe that focusing on academic \nresearchers is an important starting point, as it illuminates \nsome of the main operational considerations for NRC \naccess. \nSPECIALT Y FACULT Y MODEL \nOne of the narrowest approaches to NRC eligibility \nwould be to restrict it narrowly to faculty engaged in AI \nresearch. Under this approach, policymakers would direct \ncomputing resources exclusively toward faculty working \non identifiable AI projects, which often need large amounts \nof compute power. A benefit of this approach is that \nresearchers’ familiarity with the infrastructure would likely \nmean that fewer funds would be devoted to cloud service \ntraining for novice users. \nYet the set of self-identified core AI faculty remains \nrelatively small and concentrated in a smaller number \nof universities, which are already more likely to gain \naccess to large-scale computing. Limiting access to \ncore AI faculty would hence undermine the mission of \ndemocratizing AI research. In addition, the application of \nAI is expanding rapidly across domains. Interdisciplinary \nresearch deploying AI in new domains will be just as vital \nfor maintaining American leadership in AI, as well as for \nanimating basic research questions. Restricting eligibility \nto core AI faculty (however defined) could jeopardize the \nability of researchers from all academic disciplines (e.g., in \nthe physical sciences, social sciences, and humanities) to \ncontribute to realizing AI’s full potential.\nA Blueprint for the National Research Cloud 25\nCHAPTER 2\nGENERAL FACULTY MODEL\nA more natural starting point for NRC eligibility is with \nPrincipal Investigators (PIs) at U.S. colleges and universities, \nthe most commonly deployed criterion for federal grants. \nRequirements for PI status are set by individual universities \nand include a broad range of researchers certified by their \nuniversity as qualified to lead large research projects.10\nWhile PI certification may vary from institution to institution, \nan important baseline criterion of PI status is that the \nresearcher is subject to their institution’s training and \ncertification processes, which in turn clarify a researcher’s \nresponsibilities regarding the management and execution \nof their research proposals. Existing programs for allocating \ncomputing power typically set eligibility based on PI \nstatus as it ensures the researcher has the infrastructure \nto carry out a large-scale research project. CloudBank, an \nNSF program that distributes funds for commercial cloud \ncomputing resources, awards grants to PIs, who may \ndistribute funds to other co-researchers and students on \nthe project.11 Compute Canada allows all faculty granted PI \nstatus by their university to automatically receive a pre-set \namount of computing credits and apply for further credit \nas needed. The PI may then sponsor others to access the \ncredit.12\nWe recognize that PI status does not include all \nuniversity-affiliated researchers. In 2013, of the over \n200,000 self-identified academic researchers, a little \nunder 60,000 were employed in a role other than full-time \nfaculty, a position that may not be eligible for PI status.13\nFrom 1973 to 2013, the percentage of full-time faculty \namong engineering doctorate holders decreased by 2 \npercent, while the percentage of “other” academic jobs \n(including research associates) increased by 12 percent14\nBut, the reliance on PI status would not prevent PIs from \nallocating access to non-PI status researchers on a project, \nand administrative ability weighs strongly in favor of \nconsistency with current grant eligibility criteria. \nSTUDENTS \nShould graduate and undergraduate students be \nable to access the NRC? One of the principal challenges \nhere lies in scale and administrability. One estimate \nis that there are nearly 20 million college students \nin the U.S.15 Second, PI-oriented eligibility does not \npreclude university students from accessing resources \nto undertake AI research under the direction of PIs. The \nCompute Canada model, for example, restricts eligibility \nto faculty, but allows faculty to sponsor collaborators, \nincluding any student researcher. An access model for the \nNRC that allows PIs to sponsor students provides further \nresearch and training opportunities for students. Third, a \nnumber of existing cloud services already provide limited \naccess to computing credits for educational purposes. \nGoogle Colaboratory, for instance, provides free, but not \nreliably guaranteed, access to cloud services.16 Amazon \nWeb Services provides up to $35 of AWS credits for free \nto all university faculty and students. Despite existing \nresources, students may need more resources. The \nGoogle subsidiary and online community, Kaggle, for \nexample, provides 30 hours of GPU access per week for \nfree and found that 15 percent of users exceeded the \nlimit.17\nWhile the exact scope of student computing power \nneeds is unclear, we recommend funding an educational \nresource, once researcher needs and resource limitations \nare gauged. Currently, the NSF’s CloudBank is piloting a \nCommunity & Education Resource to earmark a small set \nof credits for educational purposes.18 This resource allows \na university professor to request a small number of credits \nfor student coursework or small-scale research. \nRegardless of which eligibility model the NRC adopts, \nthere will also be a significant need for support staff, \ntraining documentation, and educational materials so \nthat researchers can effectively make use of the compute \nand data resources (see Appendix D). The reason why \nsome students and researchers may not take advantage \nof all available cloud credits could, for instance, stem from \nthe difficulty in using cloud platforms. If the NRC serves \nacademics from a range of disciplines, this question of \nhuman capital will be especially relevant to serve different \nmodels of research. A robust training program for users of \nthe NRC will ensure ease of use and encourage appropriate \nutilization of the cloud. \nA Blueprint for the National Research Cloud 26\nCHAPTER 2\nResource Allocation \nModels\nWe now consider three resource allocation models: (1) a \nnew grant process; (2) block grant allocation to universities; \nand (3) universal—but potentially tiered—access. \nNRC GRANT PROCESS\nEstablishing a new grant process for compute access \nwould have one main advantage. The program could \nbe built specifically for the purpose of AI research, with \nreviewers who are familiar with AI concepts, practices, and \ntrends. Such a process might therefore enable improved \nallocation decisions and provide the NRC with greater \ncontrol over its investments. \nThat said, establishing a peer-review process for \nall applications would be resource-intensive, requiring \nthe establishment of a grant administration program \nakin to those at the National Science Foundation (NSF) \nor the National Institutes of Health (NIH). For instance, \nto implement peer review required for the merit review \nprocess, the NSF annually needs a community large \nenough to conduct nearly 240,000 reviews per year.19 Since \nthe contemplated reach is broad, we are mindful of adding \na significant service burden for faculty conversant in AI \nfor every application for compute access. Peer review for \ncompute access would require significant overhead and \ndelays in compute allocation. \nUNIVERSITY ACCESS\nTo reduce administrative costs, an alternative scheme \nwould be to allocate credits to universities based on the \nnumber of eligible researchers. The NRC could allocate \nresources to universities as block grants, and in turn, rely \non the university to distribute computing access. (For \nexample, the NRC could purchase significant amounts of \ncompute from cloud providers, create virtual credits that \nare convertible into appropriate cloud resources, and \ndelegate allocation to universities.) This approach would \nhave the advantage of tapping into the universities’ local \nexpertise for reviewing and distributing resources. It would, \nhowever, lead to a highly decentralized process, providing \nlittle oversight to understand the distribution of usage, and \ngive the NRC little control over resource allocation. While \nwe do not recommend this route as the principal allocation \nscheme, we do believe that some allocation to university\u0002based IT support teams may be warranted to support \nresearchers in using the NRC. XSEDE’s “Campus Champions” \nprogram, for instance, provides university employees access \nto the system to support the computational transition.20\nUNIVERSAL ACCESS\nThe last potential model would provision universal \naccess to base-level compute to all eligible PIs. The closest \nmodel is Compute Canada’s national research cloud, \nwhich provides base-level compute access to all faculty \nin Canada. This would significantly reduce administrative \noverhead, both for an institution running the review \nprocess, and academics seeking NRC access. The primary \ndownside is that base-level compute may be insufficient \nfor specialized needs.\nWe recommend combining a universal baseline model \nwith a grant process for compute needs beyond base-level \naccess. The reduced complexity in administering a universal \nbaseline access compute model makes it an attractive \noption for the NRC in allocating compute resources, \nespecially with respect to the NRC’s goal of opening access \nto compute resources.21 XSEDE, for instance, uses a similar \nmodel of streamlined “Startup Allocations” (issued for \none-year terms, typically within two weeks of application) \nand “Research Allocations” for more significant compute \nrequests. Compute Canada provides access to 15 percent \nof PIs to increased compute capacity based on a merit \ncompetition. A critical question will, of course, be the level \nof baseline computing which will determine overall costs, \nphysical space requirements, and the like. To benchmark \nthis, we recommend an in-depth study of the anticipated \ncomputing needs, based on existing academic computing \ncenters.22\nThe grant process for additional compute could \ntake multiple forms; for example, while one could allow \nindividual PIs to apply directly to the NRC for excess \nA Blueprint for the National Research Cloud 27\nCHAPTER 2\ncompute, the NRC could also allocate “blocks” of resources \nat the university level and allow universities to oversee \ntheir administration. In any case, due to the size of such \nrequests, grant reviews should be conducted on a merit \nbasis and administered by a combination of NRC staff and \nan external advisory board of university faculty. In 2021, \nCompute Canada, for instance, completed its review of \n650 research submissions in about 5 months, with only 80 \nvolunteer reviewers from Canadian academic institutions \nto assess the scientific merit of the proposal.23 In order \nto avoid conflicts of interest, we strongly recommend \nagainst the participation of any faculty or private sector \nadvisors who have conflicts of interest with any vendors \nthat provide services to the NRC. Ideally, proposal review \nshould be independent, blinded, and based on scientific \nmerit to the extent possible.\nKEY TAKEAWAYS\n Built into existing \ngrant process: \nResearchers eligible \nfor certain existing \nNSF grants can \nsimply request \naccess to CloudBank \nthrough the same \ngrant application.\n Single point of entry \nfor compute access:\nThe CloudBank \nportal provides a \nsingle point of entry \nfor researchers to \naccess funds to \nuse on whichever \ncommercial cloud \nprovider they prefer.\n Cost reduction: No \noverhead costs are \nassociated with using \nCloudBank.\n Student access:\nLimited funds are set \naside for grants to \nstudents and classes.\nCASE STUDY: CLOUDBANK\nIn 2018, the National Science Foundation’s (NSF) Directorate for Computer and \nInformation Science and Engineering (CISE) created the Cloud Access Solicitation \nto provide funding for AI-related research endeavors.24 A platform initially created to \nmeet the needs of the NSF funding recipients to access public clouds, CloudBank, \nis an interesting case study for exploring resource allocation models. Accessible \nthrough a portal, CloudBank aids researchers in using cloud resources fully by \nfacilitating the process of “managing costs, translating and upgrading computing \nenvironments to the cloud, and learning about cloud-based technologies.”25\nCloudBank is a collaboration project established via an NSF Cooperative \nAgreement with the San Diego Supercomputer Center (SDSC) and the Information \nTechnology Services Division at UC San Diego, the University of Washington \neScience Institute, and UC Berkeley’s Division of Data Science and Information.26\nEach of these institutions handles an area, according to its comparative advantage.27\nFor example, SDSC is responsible for building the online portal, and UC San Diego is \nin charge of managing the accounts of the users.28\nCloudBank also aims to reduce the cost of cloud computing: it uses both \nthe ongoing discounts with cloud providers from the University of California and \nthe discounts that come with bulk cloud purchase from the cloud procurement \nconsulting firm, Strategic Blue, which regularly partners with the likes of AWS, \nMicrosoft, and Google.29 Furthermore, there is no overhead cost associated with \nthe cloud allocations through CloudBank, since the terms of the NSF cooperative \nagreement prohibit indirect costs.30 With these cost-saving mechanisms, researchers \ncan afford more computing capacities from a variety of major cloud vendors. \nBy requesting the use of CloudBank during their application to the selected \nNSF projects,31 researchers can gain access not only to various advanced hardware \nresources, but also to a variety of services to make the process more supported and \nmonitored.32 CloudBank also gives research community members access to its \neducation and training information.33 \nA Blueprint for the National Research Cloud 28\nCHAPTER 2\nComputing \nInfrastructure \nCloud computing environments connect local \ncomputing devices such as desktop computers to large, \ntypically geographically distributed servers containing \nphysical hardware. This hardware, in turn, is responsible for \nstoring data and performing computation over computer \nnetworks—all of which is mediated through a collection \nof software services. This model centralizes the usual \noperational management for those using the network and \nprovides adjustable units of computation and data storage \nto allow for fluctuations in demand. Users interact with the \ncloud by launching virtual connections to the server—cloud \ninstances—and running containerized processes remotely. \nThese operations are managed by the cloud and available \nfor monitoring through dashboards. Cloud computing \nmay be serviced through on-premises clusters, via external \nvendors, or some combination thereof, and accessed over \nnetworks with varying security and connectivity, from \ninternet-accessible to air-gapped regions.\nThe infrastructure to the NRC could be developed \nwith two general approaches: (1) the NRC could use \ncommercial cloud platforms as its infrastructure backbone; \nor (2) the federal government could engage a contractor \nto build a high-performance computing (HPC) public \nfacility specifically for the NRC. This section addresses \nsome advantages and disadvantages of both. (We provide \nan estimated cost comparison of these two approaches \nin Appendix A.) The two approaches discussed here are \nnot mutually exclusive, and we ultimately recommend \na hybrid investment strategy. In the short run, the NRC \nshould scale up cloud credit programs (similar to NSF’s \nCloudBank program) to provide both streamlined base\u0002level access and merit review for applications going beyond \nbase-level access. In the long run, the NRC should invest \nin a pilot to develop public computing infrastructure. Even \nwith public infrastructure, it will be critical to meet “burst \ndemand” (to expand resources when compute demand \npeaks). The success of the initial investments should guide \nthe prospective model as to whether to rely on publicly \nor privately owned infrastructure in the longer term. We \nnote that in order to scale successfully to either resource \nwill require building institutional capacity at academic \ninstitutions. \nCOMMERCIAL CLOUD\nThe greatest advantage of using commercial cloud \nservices for the NRC is that significant infrastructure \nalready exists.34 Under this model, the NRC would simply \nsubsidize credits for using commercial cloud services \n(similar to NSF’s CloudBank program). Thus, rather \nthan spending years building new computing resources, \npolicymakers could launch the NRC soon after they \ndetermine the program’s administrative details. (We note, \nhowever, that there may still be significant GPU shortages \nin the short run; with the contemplated scale of the NRC, \nsignificant infrastructure would need to be built.) Since \nmany researchers already use commercial cloud services \nfor their AI research, the transition into the NRC program \ncould be relatively seamless. Furthermore, commercial \ncloud platforms offer the NRC greater flexibility to change \nthe size and scope of the program. Commercial cloud \nplatforms charge for the amount of compute actually \nused.35 Thus, the size of the NRC could expand or retract \nin line with shifting demand. In contrast, a dedicated HPC \nsystem would have a set amount of hardware that costs \nthe same, no matter how effectively it’s being used.\nWorking directly with commercial cloud providers also \noffers several advantages for the NRC. The commercial \ncloud services market is highly competitive and features \nnumerous providers capable of meeting the NRC’s needs. \nThe NRC would have the option of using one provider or \nmultiple providers. If opting to use just one provider, the \ngovernment’s bargaining power may be at its strongest \nin helping to drive down prices for the NRC. Alternatively, \nusing multiple providers gives the NRC greater flexibility in \navailable services and hardware. Either way, policymakers \nwould have the opportunity to negotiate contracts and \nprices with commercial cloud providers every few years, \nwhich will be critical to cost containment.36 The NRC would \nalso not be locked into using the same provider or set of \nproviders for the duration of the program. Rather, NRC \nstaff could reevaluate which commercial cloud provider’s \ninfrastructure would best meet the NRC’s needs at the \nstart of each new contract.\nA Blueprint for the National Research Cloud 29\nCHAPTER 2\nKEY TAKEAWAYS\n Federally-funded \ninfrastructure: \nXSEDE is an NSF\u0002funded initiative \nthat integrates and \ncoordinates shared \nsupercomputing and \ndata analysis resources \nwith researchers.\n Tiered access to \ncompute: For baseline \naccess to compute, \nXSEDE leverages a \nfast, low-hurdle review \nprocess. For access \nbeyond the baseline, \nXSEDE has its own \nresource allocations \ncommittee that \nreviews applications \nevery quarter.\n “Campus Champions \nProgram:” XSEDE \npartners with \nemployees and \naffiliates at colleges, \nuniversities, and \nresearch institutions \nto help researchers \nget access to compute \nresources.\n Collaboration: XSEDE \ncollaborates with \nthe private sector in \nacquiring, operating, \nand managing \ncompute resources.\nCASE STUDY: XSEDE\nThe Extreme Science and Engineering Discovery \nEnvironment (XSEDE) is an NSF-funded organization that \nintegrates and coordinates the sharing of advanced digital \nservices such as supercomputers and high-end visualization \nand data analysis resources.37 XSEDE is a collaborative \npartnership of 19 institutions, or “Service Providers,” many \nof which are nonprofits or supercomputing centers at \nuniversities and provide computing facilities for XSEDE \nresearchers.38 XSEDE supports work from a wide variety \nof fields, including the physical sciences, life sciences, \nengineering, social sciences, the humanities, and the \narts.39 XSEDE allocations are available to any researcher \nor educator at a U.S. academic, nonprofit research, or \neducational institution, not including students.40 However, \nresearchers can share their allocations by establishing user \naccounts with other collaborators, including students.41\nResearchers have two different paths to requesting \nallocations: Startup Allocation and Research Allocation. \nStartup Allocations apportion XSEDE resources for small\u0002scale computational activities.42 Startup Allocations are one \nof the fastest ways to gain access to and start using XSEDE \nresources, as requests are typically reviewed and awarded \nwithin two weeks.43 Startup Allocation requests also \nrequire minimal documentation: the project’s abstract and \nthe researchers’ curriculum vitae (CV).44 Startup Allocations \ntypically last for one year, but requests supported by merit\u0002reviewed grants can ask for allocations that last up to three \nyears. Researchers can also submit renewal requests, if \ntheir work needs ongoing low-level resources.45\nFor research needs that go beyond the computational \nlimits under a Startup Allocation, researchers must \nsubmit a Research Allocation request.46 XSEDE strongly \nencourages its users to request a Startup Allocation, prior \nto requesting a Research Allocation, in order to obtain \nbenchmark results and more accurately document their \nresearch needs in the Research Allocation.47 Research \nAllocation requests must include a host of documents, \nsuch as a resource-use plan, a progress report, code \nperformance calculations, CVs, and references.48\nRequests are accepted and reviewed quarterly by the \nXSEDE Resource Allocations Committee (XRAC), which \nassesses the proposals’ appropriateness of methodology, \nappropriateness of research plan, efficient use of \nresources, and intellectual merit.49\nXSEDE abides by a “one-project rule,” whereby each \nresearcher only has one XSEDE allocation for their research \nactivities.50 For instance, if a researcher has several grants \nthat require computational support, those lines of work \nshould be combined into a \nsingle allocation request. This \nminimizes the effort required \nby the researcher to submit \nrequests and reduces the \noverhead in reviewing those \nrequests.\nXSEDE also uses \na “Campus Champion \nProgram” to streamline \naccess to resources.51 The \nCampus Champion Program \nis a group of over 700 \nCampus Champions, who \nare employees or affiliates \nat over 300 U.S. colleges, \nuniversities, and research\u0002focused institutions.52\nThese Campus Champions \nfacilitate and support use of \nXSEDE-allocated resources \nby researchers, educators, \nand students on their \ncampuses. For instance, the \nCampus Champions host \nawareness sessions and \ntraining workshops for their \ninstitutions’ researchers while \nalso capturing information \non problems and challenges \nthat need to be addressed by \nXSEDE resource owners.53\nFinally, XSEDE \nwelcomes collaboration \nopportunities with other \nmembers of the research \nand scientific community.54\nFor example, XSEDE assists \nother organizations in \nacquiring and operating \ncomputing resources and \nhelps to allocate and manage \naccess to those resources. \nRecently, XSEDE worked \nwith academics and private \nindustry to form the COVID-19 \nHigh Performance Computing Consortium, which \nprovides researchers with powerful computing resources \nto better understand COVID-19 and develop treatments \nto address infections.55 \nA Blueprint for the National Research Cloud 30\nCHAPTER 2\nCommercial cloud platforms also provide other \nadvantages to the NRC. The labor of managing, \nmaintaining, and upgrading the hardware behind the \nNRC would be handled by private parties that already \nhave expertise in running cloud services at scale and \nhave invested billions of dollars into doing it. This \narrangement allows researchers access to a greater \nvariety of hardware that is constantly being expanded \nand upgraded.56 With a strong economic incentive to keep \nimproving cloud offerings, commercial cloud services \noffer an assortment of instance types—i.e., the various \npermutations and combinations of GPU/CPU, memory, \nstorage, and networking specifications that constitute a \ncompute instance—with different hardware at a range of \nprice points. Thus, researchers would have the flexibility \nto choose both what hardware would best fit the needs of \ntheir projects and how to best allocate their limited cloud \ncredits. Researchers could also have access to cutting-edge \ntechnology specially designed for AI research, such as \nchips optimized for training and inference, developed and \nexclusively used by commercial cloud providers.\nUsing commercial cloud services for the NRC comes \nwith significant tradeoffs, however. While the initial costs \nof subsidizing cloud credits might be less than building \npublic infrastructure, many studies show that relying on \ncommercial cloud services would likely be much more \nexpensive in the long term.57 For example, a study of \nPurdue University’s Community Cluster Program shows \nthat the amortized cost of its on-prem cluster over five \nyears is 2.73 times cheaper than using AWS, 3.24 times \ncheaper than using Azure, and 5.54 times cheaper than \nusing Google Cloud.58 A similar study at Indiana University \nestimates that the total investment into its locally owned \nsupercomputer, Big Red II, is about $10.1 million, while \nthe total cost of a 3-year reservation on AWS about $24.9 \nmillion.59 Cost comparisons in other studies are even more \ndramatic. For instance, a study of the Advanced Research \nComputing clusters at Virginia Tech shows that the 5-year \ncost for its on-premises cloud is about $15.5 million, while \nthe 5-year cost for reserved AWS instances using the same \nworkloads would be about $136.3 million.60\nWhat explains these cost disparities? Estimates \ncomparing commercial cloud services to a dedicated HPC \ncluster show that commercial cloud services are more \nexpensive per compute cycle.61 At least in part, this is due \nto the fact that commercial services are optimized for \ncommercial applications. Compute Canada, for example, \nfound that building their own infrastructure was cheaper \nthan using commercial services, because they did not \nhave the same core use needs as commercial customers, a \ntradeoff that gained their system more computing power \nat the expense of availability.62 Although the analysis was \npublished in 2016, Compute Canada’s own benchmarking \nof costs concluded:\nCurrently, it is far more cost effective for the \nCompute Canada federation to procure and \noperate in-house cyberinfrastructure than to \noutsource to commercial cloud providers. . . \nCloud-based costs ranged from 4x to 10x more \nthan the cost of owning and operating our own \nclusters. Some components were dramatically \nmore expensive, notably persistent storage which \nwas 40x the cost of Compute Canada’s storage.63\nUltimately, the cost difference between commercial \ncloud services and HPC systems depends on how often \nand how efficiently the HPC system is used. We provide a \ncost calculation that updates Compute Canada’s below, \narriving at cost differentials of comparable magnitude. \nCommercial cloud instances with comparable hardware \nunder constant usage, even with substantial discounts, \nWhile the initial costs of subsidizing \ncloud credits might be less than \nbuilding public infrastructure, \nmany studies show that relying on \ncommercial cloud services would \nlikely be much more expensive in the \nlong term.\nA Blueprint for the National Research Cloud 31\nCHAPTER 2\nwould be significantly more expensive over time for \nthe NRC than a dedicated HPC system. Bringing the \ncost of commercial cloud services under that of an HPC \nsystem would require policymakers to either negotiate \nexceptionally high discounts with commercial cloud \nproviders or make major sacrifices in hardware speed or \noverall scale of the NRC. A similar cost calculation is also \nwhat led Stanford University to simultaneously invest \nin both on-premises hardware and a commercial cloud\u0002based solution for its Population Health Sciences initiative \n(see box case study in Chapter Three). The most common \npractice across NSF centers, such as the XSEDE initiative \n(see box case study below), is also to build infrastructure \ninstead of relying on commercial cloud credits, due to \nthese cost considerations. \nFinally, relying on the commercial cloud may raise \nquestions about industry consolidation. There are two \nmain answers to this question. One is that building a \ndedicated, publicly owned HPC clusters would require \npurchasing sophisticated hardware from existing industry \nplayers, which also exist in concentrated industries. In \nother words, it is difficult to imagine no involvement \nof private industry under either option. Another is that \na major constraint lies in time: a fully mature, public \ninfrastructure NRC could not be stood up overnight. \nMoreover, a publicly owned cloud would still likely require \na major technology company to build the infrastructure \nunder contract, as is the case for National Labs, or using a \ngrant, as is the case for XSEDE. \nPUBLIC INFRASTRUCTURE\nBuilding a new HPC cluster would be a bespoke \nsolution, tailored to fit the NRC’s specific compute needs. \nThis approach would be relatively well-explored territory \nfor the federal government.64 The U.S. Department of \nEnergy (DOE) and the U.S. Department of Defense (DOD), \nalready regularly contract with a handful of companies \nto build HPC clusters every few years.65 The Department \nof Energy itself already uses two of the three fastest HPC \nclusters in the world and recently funded the development \nof two new supercomputers that, when completed, \nwill be the world’s fastest by a significant margin.66 The \nNational Science Foundation commonly issues grants \nfor the construction of high-performance computing \ninfrastructure.67 Given this familiarity, policymakers would \nhave reasonable estimates for how much a new HPC \ncluster for the NRC would cost and would already have \nrelationships with the companies that would submit bids \nfor the contract.\nThe hardware cost for such compute scale are, of \ncourse, substantial.68 For example, the IBM supercomputer \nused at Oak Ridge National Laboratory (ORNL) known as, \n“Summit”, cost $200 million.69 At the time of its completion \nin 2018, Summit was the fastest supercomputer in the \nworld, and as of 2020, is still the second fastest.70 Frontier, \nthe new Cray supercomputer being built at ORNL in 2021, \ncost $500 million. When completed, it is anticipated to \nbe the fastest supercomputer in the world at “up to 50 \ntimes” faster than Summit.71 Nonetheless, these large \nup-front costs could come with the benefit of computing \ninfrastructure specifically designed for AI research and the \nNRC’s needs. Such a system would be more efficient in cost \nper cycle over the long term, than subsidizing commercial \ncloud services. The NRC could also expand and upgrade \nmultiple clusters over time to meet the changing needs \nand scope of the program.\nIn addition, a dedicated cluster for the NRC has the \nadvantage of giving the federal government greater control \nover computational resources (e.g., reducing uncertainty \nover the products and platforms, such as the sudden \ndeprecation of required APIs). This level of control over \nthe hardware also allows policymakers greater flexibility \nwith NRC operations. Taking the public infrastructure \napproach (i.e., “making”, not “buying”) comes with several \nsignificant tradeoffs to weigh against the policy goals of \nthe NRC. First, building a new HPC cluster would take \nabout two years, in addition to the time it takes to solicit \nand evaluate proposals from potential contractors.72 If \nthe NRC hopes to quickly stimulate and help democratize \nAI research in the U.S., such a timeline for the program \nwould not be ideal, given how quickly AI discoveries \nadvance. Of course, contracting with cloud vendors or \nissuing grants for the construction of supercomputers \nwould also require a process. Yet, building a cluster could \nraise more challenging contracting issues, such as budget \noverruns and project delays.73 Contractors’ experience with \nbuilding this type of hardware may help mitigate some \nof these concerns, as well as their self-interest in being \nA Blueprint for the National Research Cloud 32\nCHAPTER 2\nconsidered for future government contracts. But the risks \nare nonetheless still present. \nSecond, the usability and the feature set of the \nsoftware stack for public infrastructure is by no means \nproven. One of the most common hurdles to researcher \nadoption of cloud computing lies in the usability of \nsystems,74 and public infrastructure has less of a track \nrecord of easing that onboarding path at the contemplated \nscale. This is why we recommend a pilot to assess whether \na national HPC center can be administered in a way to \nensure the ease of cloud transition and software stack that \nresearchers have gotten used to with private providers. \nThird, policymakers would also need to account for \ncosts of maintaining and administering the system.75 They \nwould need to find facilities to house and manage the \nhardware and need to account for the high energy costs \nof running an HPC cluster, as well as disaster prevention \nand recovery cost.76 These costs are significant. In 2021, \nthe Oak Ridge Leadership Computing Facility requested \n$225 million to operate all of its systems.77 The Argonne \nLeadership Computing Facility, in turn, requested $155 \nmillion.78 Furthermore, the lifecycle of DOE HPC systems \nhas traditionally been about seven years, after which new \nsystems are built and old ones decommissioned.79 While \nit is uncertain what the lifespan of newer systems will be, \nthis seven-year figure would lead us to argue that the NRC \nshould expect to either upgrade its systems or build new \nones with some degree of regularity. \nLast, giving the federal government greater control \nover the computing resources would not immediately \nmake the NRC safe from attacks.80 As with using \ncommercial cloud infrastructure, security will primarily be \ncontingent on the NRC’s implemented data access model.81\nWe discuss security issues in depth in Chapter Eight.\nKEY TAKEAWAYS\n Significant compute \npower: The Fugaku \nwas the fastest \nsupercomputer in \n2020.\n New application \nprocess for \ncompute power:\nApplications were \nsolicited to test out \nthe supercomputer \non a host of tasks \nand have control \nover who received \ncompute power. \nCASE STUDY: FUGAKU\nIn 2014, Japan’s Ministry of Education, Culture, Sports, Science and \nTechnology launched a public-private partnership between the government\u0002funded Riken Institute, the Research Organization for Information Science and \nTechnology (RIST), and Fujitsu, to create the supercomputer successor to the K \ncomputer that supports a wide range of scientific and societal applications.82 The \nresult was Fugaku, which was named the world’s fastest supercomputer in 2020.83\nThe technical aim of Fugaku was to be 100 times faster than the previous \nK computer, with a performance of 442 petaFLOPS in the TOP500’s FP64 high \nperformance LINPACK benchmark.84 It currently runs 2.9 times faster than the \nnext fastest system (IBM Summit)85 and is composed of slightly over 150,000 \nconnected CPUs, with each CPU using ARM-licensed computer chips.86 Despite \nhaving around 1.9 times more parts than its K computer predecessor, the Fugaku \nwas finished in three fewer months.87 The 6-year budget for the Fugaku was \naround $1 billion.88\nRIST solicited proposals for usage through the “Program for Promoting \nResearch on the Supercomputer Fugaku.” Under the program, the Fugaku has \nalready been used to study the effect of masks and respiratory droplets in order \nto inform Japanese policy during the COVID-19 pandemic.89 For FY 2021, 74 public \nand industrial projects were selected for full-scale access to Fugaku.90 Currently, \nRIST is still requesting proposals that fall under specific categories of usage, and \nany interested researcher may apply.91\nA Blueprint for the National Research Cloud 33\nCHAPTER 2\nCOST COMPARISON\nTo conclude this chapter, we provide a rough cost \ncomparison between a leading commercial cloud \nservice and a dedicated government HPC system (IBM \nSummit) (see Appendix A for details). We refer the reader \nto substantial work that has been published on the \neconomics of cloud computing for a fuller analysis, much \nof which emphasizes the variance in computing demand.92\nBuilding standalone public infrastructure is projected \nto be less expensive than implementing the NRC through \na vendor contracting arrangement over five years. At a \n10 percent discount on standard rates over five years, \nand under constant usage, AWS’s more powerful cloud \ncomputing option (known as P3 instances) could cost 7.5 \ntimes as much as Summit’s total estimated costs, using \ncomparable hardware. We use a 10 percent discount that \nwas negotiated by a major research university with a \ncommercial cloud provider. In contrast, the government \nwould need to negotiate an 88 percent discount for AWS \nto be cost-competitive with a dedicated HPC cluster in the \nlong run. Even in a scenario where NRC usage fluctuates \ndramatically, commercial cloud computing could cost 2.8 \ntimes Summit’s estimated cost. (While variability in usage \nfactors heavily into these estimates, the use of schedulers \ncan contribute to a smoothening out of demand.93) \nThese cost estimates have important limitations. First, \ngovernment may be able to negotiate the cost down. We \nhave used as a benchmark one major university’s enterprise \nagreement with AWS, which provides a 10 percent discount, \nrelative to market rates. But, unless the negotiated discount \nis orders of larger magnitude, the commercial cloud will \nremain significantly more expensive. Second, these cost \nestimates primarily focus on computing.94 As Compute \nCanada’s analysis showed, the cost difference in storage \nwas even greater. Third, the use of commercial rates is \nlikely more favorable to cloud vendors, as government \nsecurity standards typically increase rates due to \nregulatory requirements. For instance, a “data sovereignty” \nrequirement for data and hardware to reside within the \nUnited States, or private cloud requirements for certain \nagency datasets, may increase the cost of commercial cloud \ncomputing significantly. Fourth, this simple cost comparison \nis static, and does not reflect changes in hardware costs \nand pricing structures that are likely to occur over a five\u0002year period, with rapidly changing market conditions. \nBut, if the NRC in fact scales, systems would be procured \nincrementally over time, upgrading available resources \nand providing options at different price points, similar to \ncurrent commercial options. Last, as noted above, these \ncost estimates take into account maintenance as budgeted \nfor the Summit, but may not take into account all such \nnon-hardware costs, which is why we recommend a pilot \nto explore the ability to open up government computing \nfacilities to NRC users. \nIn short, we offer this simple comparison to highlight \nsome of the salient cost considerations to the make-or-buy \ndecision, which arrives at a very similar conclusion to the \nanalysis done by Compute Canada. \nA Blueprint for the National Research Cloud 34\nCHAPTER 2\nCASE STUDY: COMPUTE CANADA\nCompute Canada formed in 2006 as a partnership between Canada’s regional \nacademic HPC organizations to share infrastructure across Canada.95 The \norganization’s stated mission is to “enable excellence in research and innovation \nfor the benefit of Canada by effectively, efficiently, and sustainably deploying a \nstate-of-the-art advanced research computing network supported by world-class \nexpertise.”96\nCompute Canada’s infrastructure includes five HPC systems that are hosted \nat research universities across Canada.97 From 2015-2019, Compute Canada \nused about $125 million (CAD) in funding to build four of these systems.98 They \nalso investigated using commercial cloud resources instead of building these \nnew systems.99 However, they ultimately concluded relying on commercial \ncloud providers would be significantly more expensive and could not provide \nthe desired latency for large-scale, data-intensive research.100 In 2018, Compute \nCanada requested $61 million (CAD) to fund its operations, budgeting $41 million \n(CAD) for operating its HPC systems and $20 million (CAD) on support, training, \nand outreach.101 Demand for Compute Canada’s HPC resources far exceeds the \ninfrastructure’s current capacity and is expected to keep growing.102 In 2018, \nCompute Canada estimated they would need about $90 million (CAD) per year \nover five years to invest in expanding infrastructure to the point where it could \nmeet projected demands.103\nAbout 16,000 researchers from all scientific disciplines use Compute Canada’s \ninfrastructure to support their work.104 Compute Canada distributes its resources \nin two ways. First, Principal Investigators and sponsored users may request a \nscheduler-unprioritized resource allocation for their research group.105 Compute \nCanada finds that many research groups can meet their compute needs this way.106\nAlternatively, researchers who need more or prioritized resources may submit a \nproject proposal to the annual “Research Allocation Competitions.”107 Submitted \nproposals go through a scientific peer review and a technical staff review to \nrate their merits.108 Scientific review examines the scientific excellence and \nfeasibility of the specific research project, the appropriateness of the resources \nrequested to achieve the project’s objectives, and the likelihood that the resources \nrequested will be efficiently used.109 This review is conducted on a volunteer \nbasis by 80 discipline-specific experts from Canadian academic institutions.110\nTechnical review is conducted by Compute Canada staff itself, who verify the \naccuracy of the computational resources needed for each project, based on the \ntechnical requirements outlined in the application, and makes recommendations \nabout which resources should be allocated to meet the project’s needs.111 In \n2021, Compute Canada received 651 applications to the Research Allocation \nCompetition and fully reviewed all applications in the span of 5 months.112\nOne of the challenges Compute Canada has faced stems from the \ndecentralization into regional organizations. \nKEY TAKEAWAYS\n Default access \nwith tiers: All \nPIs are eligible \nfor access to \na scheduler\u0002unprioritized \ncompute resource \nallocation with \nan application \nprocess built in for \nrequesting more. \nMost researchers \nfind the default \nallocation sufficient \nfor their needs.\n Widely used \nand increasing \ndemand: Compute \nCanada’s \ninfrastructure \nis widely used \nacross academic \ndisciplines, with \ndemand constantly \nexceeding \nresources. \nCompute Canada \nintends to \ninvest heavily in \ninfrastructure to \nmeet increasing \ndemands.\nA Blueprint for the National Research Cloud 35\nCHAPTER 3\nAfter compute resources, the next critical design decision for the NRC is how to \nboth store and provide its users access to datasets: the “data access” goal of the NRC. \nIndeed, as articulated in the original NRC call to action, government agencies should \n“redouble their efforts to make more and better quality data available for public \nresearch at no cost,” as it will “fuel” unique breakthroughs in research.1 Investigating \nsome of the most socially meaningful problems hinges on large, but inaccessible \ndatasets in the public sector. From climate data housed by the National Oceanic and \nAtmospheric Administration (NOAA), health data from the country’s largest integrated \nhealth care system in the Department of Veterans Affairs (VA), or employment data \nin the Department of Labor (DOL), such data could fuel both fundamental research \nusing AI and refocus efforts away from consumer-focused projects (e.g., optimizing \nadvertising), to more socially pressing topics (e.g., climate change). \nAs noted in the Congressional charge, facilitating broad data access is a crucial \npillar of the NRC. Importantly, as we discuss below, we limit the scope of our \nrecommendations to facilitating access to public sector government data, which as \na condition of accessing government administrative data, NRC researchers should \nonly use for academic research purposes. NRC users should also be able to compute \non any private dataset available to them. There are available mechanisms for sharing \nsuch datasets, but we identify the NRC’s major challenge as providing access to \npreviously unavailable government data. \nGovernment data is intentionally decentralized. By design of the Privacy Act of \n1974, there is no centralized repository for US government data, or a core method for \nlinking data across government agencies.2\n The result is a sprawling, decentralized \ndata infrastructure with widely varying levels of funding, expertise, application of \nstandards, and access and sharing of policies. Thus, the NRC will have to develop a \nunified data strategy that can work with a wide range of agencies, unevenly adopted \nsecurity standards, and within existing data privacy legislation.\nPrevious efforts have sought to improve the access to and sharing of federal \ndata, both between agencies and with external researchers, but there are still \nsignificant barriers to enabling AI research access of the kind that the NRC demands.3\nBy linking data governance policies with access to compute, building on existing \nsuccessful models, and working with agencies to create interoperable systems that \nsatisfy security and privacy concerns, the NRC can enable increased access to data \nthat will aid AI researchers in answering pressing scientific and social questions and \nincrease AI innovation.4\nChapter 3: \nSecuring Data Access\nKEY \nTAKEAWAYS\n The NRC should adopt a tiered \nmodel for access to and storage \nof federal agency datasets. \nTiers should correspond to the \nsensitivity of the data.\n The NRC can help to harmonize \nthe fragmented federal data\u0002sharing landscape.\n The NRC should consider \nincentivizing agency \nparticipation by granting \nagencies that contribute data \nthe right to use NRC compute \nresources. \n The NRC should strategically \nsequence data acquisition \nby focusing first on low- to \nmoderate-risk datasets that are \ncurrently inaccessible.\n Due to legal constraints and \nmany outside options, the \nNRC should focus its efforts \non streamlining access \nto government datasets. \nResearchers should still \nbe permitted to use NRC \ncompute resources on \nprivate datasets, as long as \nresearchers certify they have \nrights to use such data.\nA Blueprint for the National Research Cloud 36\nCHAPTER 3\nWe will first explain why the NRC should focus its efforts \non facilitating federal government data sharing rather than \nprivate sector data sharing. We then examine how and why \nthe status quo for federal data sharing fails to realize the \nmassive potential of government data. While the concept \nof centralizing disparate data sources to unlock research \ninsights is not new,5 there are unique challenges for doing so \nwithin the context of the NRC. We will also discuss the key \nelements of our proposed model: (1) The use of FedRAMP as \na system for categorizing datasets based on their sensitivity, \nand for modifying access to them through tiered credentials \nfor NRC users; (2) Promotion of inter-agency standardization \nand harmonization efforts to modernize data-sharing \npractices; and (3) Strategic considerations regarding how \nto sequence efforts in streamlining access to particular \ndatasets. \nThe case studies included throughout this White Paper \nwere chosen as exemplars of successful data-sharing \ninitiatives6 and to illustrate the range of available design \ndecisions. While each individual case study provides a \nunique glimpse into different approaches, some common \nthemes emerge. First, many of the data-sharing entities \nwe studied not only have a single point of entry for \nresearchers to request access, but also allow government \nagencies to retain some control over access requirements \nto their data. As we discuss below, this conception of the \nNRC as a data intermediary would provide real benefits \nin streamlining data access while still maintaining trust \namong agencies that wish to protect their data. Second, \nsome initiatives use funding and personnel training as \ncarrots to incentivize agencies to engage in data sharing. \nThe NRC can learn from these initiatives in formulating its \nown set of incentives for agencies. \nPRIVATE DATA SHARING\nShould the NRC affirmatively facilitate private dataset \nsharing? While there are definite benefits to providing \nresearchers with access to private data,7 the NRC will \nhave its largest impact by focusing its efforts first on \nmechanisms to access and share government data. \nAs an initial matter, a variety of mechanisms for \ngeneral data sharing already exist.8 Private sector \nstakeholders, moreover, can and have often built their \nown in-house platforms to allow access to approved data \nsets while minimizing intellectual property concerns,9\n or \nprovide access to their application programming interfaces \n(APIs) to make open-source data more easily accessible.10\nBy focusing on providing access to public sector \ndata, notably administrative data that is traditionally \ninaccessible to most researchers,11 the NRC would play a \nunique and pertinent role for researchers across disciplines \nwithout having to deal with complex private-sector data \nconcerns or the need to incentivize participation by non\u0002government actors. \nComplex intellectual property \nconcerns would arise from the NRC \npermitting, facilitating, or even \nrequiring private sector stakeholders \nand independent researchers to \nshare their private data freely \nalongside public sector data. \nComplex intellectual property concerns would arise \nfrom the NRC permitting, facilitating, or even requiring, \nprivate sector stakeholders and independent researchers \nto share their private data freely alongside public sector \ndata. First, this would involve complex questions regarding \nwhat licenses should be available or mandated for \nNRC users in order to encourage data sharing, despite \napprehensions of how such sharing may affect future \nprofitability and commercialization. While mandating \nan open-source (e.g., Creative Commons) license would \nbenefit researchers most by providing the broadest access \nto data and would benefit NRC administrators by removing \nsome possible IP infringement concerns, private sector \nstakeholders may feel deterred from uploading as a result. \nConversely, if users have a choice to adopt a license that \nA Blueprint for the National Research Cloud 37\nCHAPTER 3\nallows them to preserve their IP rights, private sector \nstakeholders may feel more comfortable with sharing their \ndata, but this would shift some liability to users—or to the \nNRC itself—by relying on users to abide by the license. This \nwould involve an emphasis on enforcement, ranging from \nexplanations and user disclaimers to the industry standard \nof a full-blown notice-and-takedown system. \nData owners may want to prevent the uploading \nof copyrighted works by, for instance, having the NRC \nitself assess whether private data is already protected \nby copyright. Industry standards for conducting data \ndiligence, using either manual or automated tools, would \neither be very labor intensive12 or prohibitively expensive.13\nEven if these industry standards were met, researchers \nmay find an NRC data-sharing platform duplicative. \nNone of the above would prevent researchers from \nusing NRC compute resources on their own private \ndatasets. Like current cloud providers, the NRC can \nstipulate in an End User Licensing Agreement (EULA) \nthat researchers must agree they own the intellectual \nproperty rights on the data they are using.14 This EULA can \nalso assign liability to the end user, rather than the NRC \nitself, for any use of data that is encumbered by existing \nIP provisions. Additionally, the discussion above pertains \nto whether researchers should be required to share \ntheir private data, not to whether researchers should be \nrequired to share the outputs of their research conducted \non the NRC. The latter is discussed in Chapter Nine.\nTHE CURRENT PATCHWORK \nSYSTEM FOR ACCESSING \nFEDERAL DATA\nThe NRC could play a pivotal role streamlining \naccess to government data in a system that is currently \ndecentralized.15 In some cases, agencies may simply \nlack a standardized method for sharing data.16 Due to \nperceived legal constraints, risks, or security concerns, \nagencies often have little practical incentive to share their \ndata.17 Successful examples of researchers gaining access \nto government data from individual agencies frequently \nrely on the researchers having personal relationships \nwith administrators, and a willingness on the part of the \nadministrator to push against these constraints, in service \nof the research project.18 While this relationship-based \nprocess has produced some successes,19 the far more \ncommon outcome is that data is simply not shared or \naccessed at all by researchers.20 Indeed, one government \nofficial indicated that overcoming the obstacles to making \ncertain government data available for research was the \ngreatest challenge in a lengthy career.\nOne government official \nindicated that overcoming the \nobstacles to making certain \ngovernment data available \nfor research was the greatest \nchallenge in a lengthy career.\nAgencies typically require the recipient of the data \nto abide by a data-use agreement (DUA). These DUAs \nprescribe such limitations on data usage as the duration \nof use, the purpose of use, and guarantees on the privacy \nand security of data.21 However, DUAs suffer from a \ncentral problem: the process for negotiating DUAs is \nhighly fragmented and inconsistent across government \nagencies, drastically increasing the complexity in obtaining \napprovals for them.22 Some agencies have a designated \noffice or process to handle DUAs, but other agencies rely \non extemporaneous processes and ad hoc, quid-pro-quo \narrangements.23 One such example is the Research Data \nAssistance Center, a centralized unit within the Centers \nfor Medicare & Medicaid Services (CMS) dedicated to \nsupporting data access requests.24 In contrast, DUAs within \nthe Department of Housing and Urban Development and \nthe Department of Education are handled in decentralized \nbusiness units, each with different routing channels and \nlegal teams, which can confuse reviewers when multiple \nA Blueprint for the National Research Cloud 38\nCHAPTER 3\ndata requests between the same parties are routed \nsimultaneously, but separately.25 Indeed, university DUA \nnegotiators in one survey complained that the process was \na game of “bureaucratic hot potato” and wondered, “Why \nisn’t there just one template for everything?”26 Ultimately, \nthe lack of standardization means that DUAs often require \nextensive review and revision, creating substantial delays.\nAgency-by-agency requirements also impede data \nsharing. These requirements can range from mandating \nthat researchers only access data at an onsite facility, using \ngovernment-authorized equipment, to capping the amount \nof computational cycles that can be used to analyze data, \nor restricting the amount of data available simultaneously.27\nThese restrictions are particularly problematic, given that \nmodern AI models can require massive amounts of data and \ncomputation to be most effective. \nBroadly, the reasons for this dysfunction range \nfrom valid concerns about security and liability to the \nmundane and prosaic. Information technology systems \nwithin some agencies operate literally decades behind the \ntechnological frontier; a 2016 report from the Government \nAccountability Office (GAO) detailed examples of these \nlegacy systems, discussing how several agencies were \ndependent on hardware and software that were no longer \nupdateable, and required specialized staff to maintain.28\nA lack of incentives, a risk-averse culture, and an agency’s \nstatutory authority also all play an important role in \nenabling or obstructing data sharing.29\nWe are by no means the first observers to note \nthese problems. Advocates have been working for years \nto standardize and modernize government practices \naround data and technology.30 For example, the Federal \nData Strategy is the culmination of a multi-year effort to \npromulgate uniform data-sharing principles to address \nthe fact that the United States “lacks a robust, integrated \napproach to using data to deliver on mission, serve the \npublic, and steward resources.”31 However, substantial \nchallenges remain, particularly since the bulk of the efforts \nfocused on opening access to government data have \nnot been undertaken with the specific needs of machine \nlearning and AI in mind. \nTIERED DATA ACCESS AND STORAGE\nThe decentralized nature of government data has \ncascading implications across many aspects of the \ngovernment data ecosystem. One key area that will affect \nthe NRC is a lack of consistent storage and authentication \naccess protocols across government agencies. \nBecause many government datasets contain sensitive \ndata (e.g., high risk due to individual privacy concerns),32\na crucial component of the NRC’s data model will consist \nof a tiered storage taxonomy that distinguishes between \ndatasets based on their sensitivity and correspondingly \nrestricts access to different research groups. Interpreting \ntiered storage and access as two sides of the same coin, \nwe reference existing models that are based on dataset \nrisk levels and propose a framework for the NRC that \naims to achieve the dual goals of streamlining the process \nof enabling research access to government data while \nmaintaining privacy and security. \nFedRAMP: A tiered framework for data storage \non the cloud\nOne type of tiered storage taxonomy already exists for \nthird party government cloud services in one of the federal \ngovernment’s major cybersecurity frameworks, the Federal \nRisk and Authorization Management Program (FedRAMP).33\nEnacted in 2011, the framework was designed to govern \nall federal agency cloud deployments, with certain \nexceptions detailed in Chapter Eight of this White Paper. \nFedRAMP offers two paths for cloud services providers to \nreceive federal authorization. First, an individual agency \nmay issue what is known as an authority-to-operate (ATO) \nto a cloud service provider after the provider’s security \nauthorization package has been reviewed by the agency’s \nstaff and the agency has identified any shortcomings that \nneed to be addressed.34 These types of ATOs are valid for \neach vendor across multiple agencies, as other agencies \nare permitted to reuse an initial agency’s security package \nin granting ATOs. The second option available to cloud \nservices providers is to obtain a provisional ATO from the \nFedRAMP Joint Authorization Board, which consists of \nrepresentatives from the Department of Defense (DOD), \nthe Department of Homeland Security (DHS), and the \nA Blueprint for the National Research Cloud 39\nCHAPTER 3\nGeneral Services Administration (GSA). These provisional \nATOs offer assurances to agencies that DHS, DOD, and the \nGSA have reviewed security considerations, but before \nany specific agency is allowed to use a vendor’s services, \nthat agency must issue its own ATO.28 In both the first and \nsecond cases, FedRAMP categorizes systems into low, \nmoderate, or high impact levels (see Table).\nBecause FedRAMP requirements apply to all federal \nagencies when federal data is collected, maintained, \nprocessed, disseminated, or disposed of on the cloud, \nthe NRC itself will need to be compliant with FedRAMP \nsecurity standards irrespective of the organizational form \nit takes.35 Every dataset brought on to the NRC would \nneed to be reviewed under FedRAMP with appropriate \naccess levels. If a cloud service has already been evaluated \nunder FedRAMP because it was used in the past to house \nfederal data, the service can inherit the same FedRAMP \ncompliance level in the NRC without an additional \nevaluation.36\nBesides classifying datasets, the other function of \nFedRAMP is to identify a comprehensive set of “controls,” \ni.e., requirements and mechanisms that the cloud service \nproviders must implement before the government dataset \ncan be housed on them.37 They are based on the National \nInstitute of Standards and Technology (NIST) Special \nPublication 800-53, which provides standards and security \nrequirements for information systems used by the federal \ngovernment.38\nThese controls range widely and include requirements \nsuch as ensuring that the organization requesting \ncertification “automatically disables inactive accounts,” \n“establishes and administers privileged user accounts \nin accordance with a role-based access scheme that \norganizes system access and privileges into roles,” \n“provides security awareness training on recognizing \nand reporting potential indicators of insider threat,” or \ndevelops regular security plans in the event of a breach.39\nRequirements get more strenuous for FedRAMP “high \nimpact” data (e.g., creating system level air-gaps to protect \nsensitive data).40\nLEVEL TYPE OF DATA IMPACT OF DATA BREACH NUMBER OF \nCONTROLS\nLow-impact risk\n- Low baseline\n- Low-impact SaaS\nData intended \nfor public use\nLimited adverse effects; Preserves the safety, \nfinances, reputation, or mission of an agency 125\nModerate-impact risk\n- E.g. Personally \nidentifiable information\nControlled \nunclassified data \nnot available to \nthe public\nCan damage an agency’s operations 325\nHigh-impact risk\n- E.g. law enforcement, \nhealthcare, emergency \nservices\nSensitive federal \ninformation\nCatastrophic impacts, like shutting down an \nagency’s operations, causing financial ruin, or \nthreatening property or life\n421\nTable 2: FedRAMP levels are designated based on the degree of risk associated with the breach of an information system. The security baseline levels \nare based on confidentiality, availability, and integrity, as defined in Federal Information Processing Standard 199.41\nA Blueprint for the National Research Cloud 40\nCHAPTER 3\nThere can be significant costs with obtaining these \ncertifications and creating compliance plans, even if the \nunderlying technical specifications can be addressed or \nalready exist. A key issue for structuring the NRC is that the \nprincipal burdens of ensuring FedRAMP compliance should \nfall with NRC institutional staff, not originating agencies or \nindividual academic researchers. As part of the FedRAMP \ncertification process, NRC staff will have to consider how to \ngive access to PIs in compliance with FedRAMP rules, but \nthat process can and should avoid requiring originating \nagencies or individual universities to incur substantial \nexpenses associated with hiring consultants and attorneys \nto certify FedRAMP compliance.42\nWhile FedRAMP sets out common standards for \ncloud storage of government data within agencies,43 it \nis an exception to an otherwise balkanized federal data\u0002sharing standards landscape,44 though it does not facilitate \ndata exchange. The NRC needs to maintain compliance \nwith not only FedRAMP requirements, but also the \nrequirements of any agency it is partnering with for data \naccess.45 Advocates interested in increasing government \ndata availability have long fought to establish a universal \nFedRAMP equivalent across different agencies that \nprovides shared standards for data sharing based on data \nsensitivity.46 As we discuss in Chapter Eight, establishing \nsuch universal, “centralized” security standards not only \nensures internal uniformity, but also removes barriers to \ndata sharing.\nThe NRC’s implementation of FedRAMP standards can \nalso provide partnering agencies an important opportunity \nto re-examine their own standards and share best practices \nwith one another.47 This could involve raising or lowering \nrequirements that are out of date,48 given the current \nthreat to the environment and research needs. The NRC \ncan take inspiration from agencies’ best practices, as well \nas from FedRAMP to develop a common NRC standard for \ndetermining data to be high, moderate, or low risk, as well \nas what consequences should flow from that assessment. \nIn the later section on strategic considerations, we discuss \nhow to enable this process by incentivizing agencies to \nparticipate in the NRC and selecting datasets that present \na lower privacy and security risk. \nIn addition, given the diversity of data types and \nsources that could be stored on the platform, NRC \npolicy should ensure that standards and protections \nexist for data storage in areas where FedRAMP has \nblind spots. FedRAMP is in part animated by risks from \nmalicious actors like cybercriminals or adversarial foreign \ngovernments, but as we discuss in Chapter Six, privacy \nrisks may arise even for the intended use case of analysis \nby NRC researchers. Of particular concern are instances \nwhere disparate datasets are combined, which may allow \nnew inferences that make previously anonymous data \nindividually identifiable, even when the data itself did not \ncontain identifiable information.49 Such combinations \nmay also alter the original risk level of the data, creating \nan output that merits a higher risk classification. \nFurthermore, machine-learning models and \nrepresentations may unintentionally reveal properties of \nthe data used to train them,50 and dissemination of these \nmodels could pose privacy risks.\nThis is not a challenge unique to the NRC; the U.S. \nCensus Bureau and other government agencies engaged \nin data linkage have also had to develop means to \naddress this issue.51 One solution involves applying \nmethods of additional noise to the data (differential \nprivacy) in order to obfuscate individual data while \npreserving the data’s utility for research. We discuss it \nand other privacy-enhancing technologies in greater \ndetail in Chapter Seven.52 However, privacy-enhancing \ntechnologies are no panacea, and depending on the \nnature of the particular dataset, the goals of ensuring \nanonymization, while also enabling researchers to access \nfine-grained data can conflict. \nThe NRC can also draw from the “Five Safes” data \nsecurity framework used by the UK Data Service,53 the \nFederal Statistical Research Data Centers Network, \nand the Coleridge Initiative, a model centered on data, \nprojects, people, access settings, and outputs.54 The \nimplementation of the 2019 Evidence Act is already \nusing a similar Five Safes framework in making \ndeterminations around data linkage.55 Through a \ncombined framework, the NRC could place different \nanonymization requirements on datasets, depending \non the circumstances of their access and the privacy \nA Blueprint for the National Research Cloud 41\nCHAPTER 3\nagreements through which they were collected. Similarly, \nthe NRC could control the dissemination scope of models, \ncode, and data, depending on the sensitivity. Theoretical \nidentifiability is less likely to be a concern when access \nand dissemination is restricted and the data is of a less \nsensitive nature or is not about individuals at all.56\nFacilitating Researcher Access with a Tiered \nAccess Model\nHow should researchers gain access to specific data \nresources? Currently, approval proceeds on an agency\u0002by-agency basis.57 Just as the value of the NRC for \nsupporting AI research will depend, in part, on the extent \nto which it can bring together datasets from different \nagencies, it will also depend on the extent to which it can \nstreamline the process for accessing data. One way to \nachieve this streamlining will be through a tiered access \nsystem for the NRC users, similar to FedRAMP’s tiered \nsystem for storing federal data on the cloud, where higher \ntiers would enable access to higher-risk data, subject \nto the other requirements on compute and data use. \nWe discuss this access system in more depth in Chapter \nSeven.\nChapter Two made the case that compute access \nshould start with PIs at academic institutions. This \nauthorization can also serve as the baseline, where all \nNRC-registered PIs can freely access and use low-risk \ndatasets on the NRC. Additional tiers would impose more \nrequirements, such as citizenship, security clearance, \ndistribution restrictions, or compute and system \nrestrictions. These access tiers will be similar to those \nused for determining FedRAMP classification for data \nstorage, but while access and storage sensitivity may \ninvoke similar considerations; they might not necessarily \nbe the same. \nA Blueprint for the National Research Cloud 42\nCHAPTER 3\nKEY TAKEAWAYS\n Balances providing \nconsistent restricted \ndata access with agency \nrequirements: The ADRF \nbalances building in each \nrestricted data set’s access \nand export review form in \na consistent manner into \nthe data portal with agency \nrequirements for data \naccess and export. This \nallows agencies to control \naccess to their data, while \nproviding a single point \nof entry for researchers. \nCurrently, the platform only \nsupports data consistent \nwith a FedRAMP Moderate \ncertificate. \n Standardized for both users \nand data stewards: Along \nwith each data access for \nusers, a point of contact at \nthe agency providing the \ndata is given access to the \nplatform as well. This allows \neasy access to approve and \ntrack projects, and work \nwith the ADRF on access \nrequirements. \n Five Safes’ data security \nframework: The ADRF \nensures data security by \nfocusing on five aspects—\ndata, projects, people, \nsettings of access, and \noutputs.\n Training as a core function: \nthe ADRF hosts workshops \nand trains government \nemployees and other \nresearchers on data use.\nCASE STUDY: COLERIDGE INITIATIVE \n(ADMINISTRATIVE DATA RESEARCH \nFACILITY)\nIn partnership with the Census Bureau and funding from the Office \nof Management and Budget, the nonprofit organization, Coleridge \nInitiative, launched the Administrative Data Research Facility (ADRF), \na secure computing platform for governmental agencies to share and \nwork with agency micro-data.58 The ADRF is available on the Federal \nRisk and Authorization Management Program (FedRAMP) Marketplace \nand has a FedRAMP Moderate certification. Currently, the platform \nsupports over 100 datasets from 50 agencies.59\nThe ADRF provides access to agency-sponsored researchers and \nagency-affiliated researchers going through the ADRF training programs \nfor free. Over the past 3 years, over 500 employees from around 100 \nagencies have gone through ADRF training programs.60\nThe ADRF provides a shared workspace for projects and the \nData Explorer, a tool to view an overview and metadata (name, field \ndescription, and data type) of available datasets on the ADRF.61 In \norder to access restricted data, users must meet review requirements \nset by the agency providing the data. In order to export data, users \nmust go through a unique “Export Review” process.62 The ADRF has a \nhighly involved default review process, requiring researchers to submit \nall code and output for the project for approval to the data steward \nand generating additional charges, if requesting export of more than \n10 files.63 The agency providing the data can also amend the default \nreview process, if it wishes to do so. \nPrior to transferring data files, the ADRF provides an application for \ndata hashing to safely transmit data.64 The ADRF also follows the “Five \nSafes” security model used by other government agencies, such as the \nUK Data Service.65\nData stewardship for the ADRF is defined in compliance with \nthe Title III of the Evidence-Based Policymaking Act of 2018.66 Once \na restricted dataset is shared with the ADRF, one person within the \nagency will be assigned the data steward for all project requests. \nFrom there, procedures are developed with the agency, in terms of \nexpectations for how the data will be protected, authorized users, and \naudit procedures for continued compliance. \nData stewards have access to an online portal in the ADRF. All \nproject requests for specific data are routed to the data steward \nthrough this proposal. Once access has been granted, the data steward \nalso has options to monitor the project for compliance. \nA Blueprint for the National Research Cloud 43\nCHAPTER 3\nThe NRC can \nemulate both the \nColeridge Initiative \nand Stanford’s \nCenter for Population \nHealth Sciences \n(PHS), for instance, \nwhich serve as data \nintermediaries, in \nfacilitating access to \ngovernment data. \nKEY TAKEAWAYS\n Mixed data architecture, yet consistent user \nexperience: PHS utilizes a mix of on-premise \nand cloud data services, but still seeks to \nprovide a consistent user experience.\n Restricted data access has a single point \nof entry: The PHS Data Portal standardizes, \ncentralizes, and simplifies data access \nrequirements and trainings, rather than pointing \nusers to a time-consuming process working with \neach data steward directly.\n Reduces costs and time associated with \nprocuring data: PHS leverages existing \nrelationships with agencies to consolidate \ndatasets in a single portal, saving researchers \nthe time and money necessary to gain access \nthrough individual agency requests.\nExisting models for researcher access to sensitive datasets can help paint a \npicture of how the NRC might maintain and monitor a tiered access system. The \nNRC can emulate both the Coleridge Initiative and Stanford’s Center for Population \nHealth Sciences (PHS),67 for instance, which serve as data intermediaries, in \nfacilitating access to government data. Indeed, these intermediaries have been \ndocumented as effective means to overcome barriers to data-sharing because they, \nat their core, negotiate and streamline relationships between data contributors and \nusers.68 For example, as a trusted intermediary, the NRC could centralize the DUA \nintake process by promulgating a universal standard form for agency DUAs.69\nFurthermore, similar to the Coleridge Initiative example, a designated \nrepresentative(s) within the agency could be assigned as the data steward for all \nproject requests for a certain restricted dataset. Any project requiring access to data \nin higher tiers could commence only after its proposal was reviewed and approved \nby a relevant representative. Because NRC access begins with PIs, researchers would \nalso have to obtain approval from their university Institutional Review Boards (IRBs), \nas needed. After project approval and NRC researcher clearance, data would be \nmade available through the NRC’s secure portal. Any violations of the terms of use \nor subject privacy could result in penalties ranging from a demotion of access tier to \nremoval of NRC privileges or professional, civil, or criminal penalties, as relevant.\nCASE STUDY: STANFORD \nCENTER FOR POPULATION \nHEALTH SCIENCES\nThe Stanford Center for Population Health \nSciences (PHS) provides a growing set of population \nhealth-related datasets and access methods to \nStanford researchers and affiliates.70 The PHS Data \nEcosystem hosts high-value datasets, data linkages \nand filters, and analytical tools to aid researchers. \nThe PHS partners with a wide range of public, \nnonprofit, and private entities to license population\u0002level datasets for university researchers, ranging from \nlow-risk, public datasets to restricted data containing \nProtected Health Information (PHI) and Personally\u0002Identifiable Information (PII), such as Medicare, \ncommercial claims such as Optum and Marketscan \ndata,71 and electronic medical records. \nA Blueprint for the National Research Cloud 44\nCHAPTER 3\nCASE STUDY: STANFORD CENTER FOR POPULATION \nHEALTH SCIENCES (CONT’D)\nIn addition to secure data storage and computational tools for researchers, PHS provides standardized \nand well-documented data access and management protocols, which increases data proprietor comfort \nwith sharing data. PHS also has full-time staff who cultivate and maintain relationships with organizations \nholding data. This allows PHS to work with these groups to centralize data hosting and provide secure \naccess to a wide array of researchers. \nThe PHS Data Portal is hosted on a third-party platform which enables data discovery, exploration, and \nclearly delineated, standardized steps for data access. The third-party platform, Redivis, utilizes a four-tier \naccess system: (1) overview of data and basic documentation, (2) metadata access, including definitions, \ndescriptions, and characteristics, (3) a 1 or 5 percent sample of the dataset, and (4) full data access.72\nIf data is classified as public, researchers can access it using specialized software, or simply download \nit directly.73 For restricted data, the portal has forms integrated to easily apply for access.74 After identifying \nthe dataset, the researcher must apply for membership in the organization hosting the data.75 An \nadministrator of the organization owning the dataset can set member and study requirements that must \nbe met, including training and institutional qualification, in order to access the data. Member applications \ncan be set to auto-approval or require administrative approval. Once access has been granted to a data \nset, researchers can manipulate the data using specialized software. Usage restrictions are also specified \nindividually on each dataset to control whether full, partial, or no output can be exported, and what review \nlevel is required for exporting. All applications for data and export are handled directly on the Data Explorer \nplatform.\nCurrently, the PHS Data Portal is primarily for Stanford faculty, staff, students, or other affiliates.76 Even \nwith affiliate status, certain commercial datasets may require further data rider agreements for access. \nNon-Stanford collaborators must complete all of the same access requirements as Stanford affiliates, plus \nany requirements imposed by their own institution. Additionally, a “data rider” agreement on the original \nDUA is frequently necessary.77 \nTo work with restricted data, the PHS provides two computing services for high-risk data: (1) Nero, \nwith both an on-premise and Google Cloud Platform (GCP) platform versions and (2) PHS-Windows \nServer cluster.78 Both are managed by the Stanford Research Computing Center (SRCC). Both services are \nHIPAA compliant.79 Unrestricted data can be used on any of Stanford’s other computational environments \n(Sherlock, Oak) or simply downloaded to the researcher’s local machine.\nA Blueprint for the National Research Cloud 45\nCHAPTER 3\nPROMOTING INTER-AGENCY \nHARMONIZATION AND ADOPTION \nOF MODERN DATA ACCESS \nSTANDARDS\nThe federal data-sharing landscape suffers from \ndivergent standards and practices, and individual \nagencies, left alone, have traditionally faced high hurdles \nto harmonizing and modernizing their data access \nstandards.80 As we have discussed, this state of affairs \npresents formidable barriers to AI R&D from a researcher \nperspective, but is also problematic, both from an agency \nand societal perspective. As a report by the Administrative \nConference of the United States finds from surveying the \nuse of AI in the federal government, nearly half of agencies \nhave experimented with AI to improve decision-making \nand operational capabilities, but they often lack the \ntechnical infrastructure and data capacity to use modern \nAI techniques and tools.81 The lack of a modern, uniform \nstandard for data sharing in AI research, therefore, makes it \nharder for agencies to realize gains in accuracy, efficiency, \nand accountability, which subsequently impacts citizens \ndownstream, who are affected by agency decisions.82\nThe lack of a modern, uniform \nstandard for data sharing in \nAI research makes it harder \nfor agencies to realize gains \nin accuracy, efficiency, and \naccountability, which subsequently \nimpacts citizens downstream, who \nare affected by agency decisions.\nThe NRC can help overcome agency reluctance to \nshare data by enabling access to agencies to compute \non their own data. This would solve at least two crucial \nproblems for government agencies. First, access to the \nNRC’s collective computing resources would overcome \nsome difficulties that agencies have traditionally faced \nin setting up their own compute resources.83 Second, \nfacilitating agency access to modern data and compute \nresources would attract and build further in-house \ngovernment AI expertise.84 From a societal perspective, \nthis could increase the government’s capabilities in the \nresponsible adoption of AI, help reduce the cost of core \ngovernance functions, and increase agency efficiency, \neffectiveness, and accountability.85\nThe NRC can also learn from and align with other \ninitiatives to harmonize and modernize standards. The \nEvidence Act––which requires agencies to appoint Chief \nData and Evaluation Officers––is one example. The \nlegislation authorizing the creation of the NRC could \nprovide a federal mandate to encourage adoption of \nsharing best practices.86 However, as we discuss in Chapter \nFive, a federal mandate alone, without any additional \naid or incentives, may not be enough to incentivize \nharmonization of data access and sharing standards.87\nThe Task Force should therefore consider bundling the \nmandate with additional benefits, such as providing \nfunding to assist agencies in expanding their technical \nor staff capabilities in furtherance of the NRC and the \nNational AI Strategy. The NRC is aligned with the existing \nbipartisan case for the National Secure Data Service \n(NSDS) (described in the case study below), a service \nthat would facilitate researcher access to data with \nenhanced privacy and transparency, recommended by \nthe Commission on Evidence-Based Policymaking in 2018. \nBoth the NRC and NSDS are complementary data-sharing \ninitiatives that have the potential to considerably improve \npublic service operational effectiveness. We elaborate \nfurther on the NSDS proposal in Chapter Five. Lastly, \ntraining programs are promising avenues to increase NRC \nadoption and agency support. For example, as described \nin the case study above, the Coleridge Initiative has hosted \nworkshops to train over 500 employees from around 100 \nagencies on data use over the past 3 years. \nA Blueprint for the National Research Cloud 46\nCHAPTER 3\nCASE STUDY: THE EVIDENCE ACT\nIn pursuit of greater, more secure access to and linkage of \ngovernment administrative data, a bipartisan Commission on Evidence\u0002Based Policymaking was set up by Congress in March 2016. The \nCommission’s final report88 included 22 recommendations for the federal \ngovernment to build infrastructure, privacy-protecting mechanisms,89\nand institutional capacity to provide secure access to public data for \nstatistical and research purposes. One recommendation was to create \na ‘National Secure Data Service’ (NSDS) to facilitate access to data \nfor the purpose of building evidence, while maintaining privacy and \ntransparency. Through this service, the NSDS could help researchers by \ntemporarily linking existing data and providing secure access, without \nitself creating a data clearinghouse.\nThe Foundations for Evidence-Based Policymaking Act of \n201890 created some of the legislative footing for the Commission’s \nrecommendations. In particular, it created new roles for Chief Data, \nEvaluation, and Statistical Officials and sought to increase access and \nlinkage of datasets previously in scope of the Confidential Information \nProtection and Statistical Efficiency Act (CIPSEA).91\nFinally, the 2020 Federal Data Strategy and associated Action Plan92\nsought to put those legislative provisions into action. The Strategy \nincluded plans to improve data governance, to make data more \naccessible, to improve government use of data and to boost the use and \nquality of data inventories, metadata and data sensitivity.\nThe central remaining step envisioned by the initial Evidence-Based \nPolicymaking Commission is a National Secure Data Service (NSDS) \nmodelled on the UK’s Data Service.93 The UK’s Data Service provides \naccess to a range of public surveys, longitudinal studies, UK census \ndata, international aggregate data, business data, and qualitative \ndata. Alongside access, it provides guidance and training for data use, \ndevelops best practices and standards for privacy, and has specialized \nstaff who apply statistical control techniques to provide access to data \nthat are too detailed, sensitive, or confidential to be made available \nunder standard licenses.\nKEY TAKEAWAYS\n Priority data sharing where \nthere is a public service \noperational case: there is \nprecedent in large-scale \nadministrative data-sharing \ninitiatives justified on \ngrounds of improvements \nto public service \noperational efficiency, and \neffectiveness.\n National Secure Data \nService (NSDS) initiative: \nthe NSDS is bolstered by \nbipartisan support.\n Institutional models \nthat balance external, \ninnovative talent and \ninternal, cross-agency \ninfluence: a Federally \nFunded Research and \nDevelopment Center \n(FFRDC) model, housed \nwithin an existing agency \n(NSF), may balance the \nability to bring in external \ntalent with internal agency \ninfluence.\nA Blueprint for the National Research Cloud 47\nCHAPTER 3\nSEQUENCING INVESTMENT INTO \nDATA ASSETS\nGiven the significant hurdles in negotiating data \naccess, the NRC will need to strategically sequence which \nagencies and datasets to focus on for researcher use. \nThe federal government collects petabytes of data,94\neach with varying degrees of restrictions or openness. \nIn considering which datasets to prioritize, the NRC can \ndraw from the example of other data- sharing initiatives, \nas well as focus on data sets in the short term that do not \npose complex challenges with regards to data privacy \nor sharing. One private sector example is Google Earth \nEngine, which aggregated petabytes (approx. 1 million \ngigabytes) of satellite images and geospatial datasets, \nand then linked that access to Google’s cloud computing \nservices to allow scientists to answer a variety of crucial \nresearch questions.95 This process of aggregating complex \ndata and hosting it in a friendly computing infrastructure \nto facilitate research, demonstrates the compelling value \nof coupling compute and data. As another example, \nADR UK identifies specific areas of research that are of \npressing policy interest, such as “world of work,”96 and \nprioritizes data access for researchers working on those \ntopics. The UK Data Service offers datasets derived \nfrom survey, administrative and transaction sources, \nincluding productivity data from the Annual Respondents \nDatabase,97 innovation data from the UK Innovation \nSurvey,98 geospatial data from the Labour Force Survey,99\nUnderstanding Society,100 and sensitive data about \nchildhood development.101\nWhen prioritizing datasets and agencies for NRC \npartnership, we recommend the following criteria: \n• Data that is valuable to AI researchers, but is not \ncurrently available in a convenient form. For example, \nin a July 2019 request for comments, the Office of \nManagement and Budget (OMB) asked members \nof the public to provide input on characteristics of \nmodels that make them well suited to AI R&D, what \ndata are currently restricted, and how liberation \nof such data would accelerate high-quality AI \nR&D.102 In one response, the Data Coalition argued \nthat controlled release of private but structured \nindexed data in data.gov would be valuable for \nresearch.103 The Coalition also urged agencies to \nconsider releasing raw, unstructured datasets, such \nas agency call center logs, consumer inquiries and \ncomplaints, as well as regulatory inspection and \ninvestigative reports.104 Another example of data that \nis currently challenging to access, but is a matter of \npublic record, are, electronic court records housed \nin a system by the Administrative Office of the U.S. \nCourts.105\n• Data housed within agencies that have statutory \nauthority to share data and/or that have previous \ndata-sharing experience. The Census Bureau, for \ninstance, has greater existing statutory inter-agency \nlinkage than other agencies, and has pre-existing \nsubstantial in-house data analysis expertise.106\nThe U.S. Bureau of Labor Statistics has an existing \nprocess for sharing restricted datasets (in the \ncategories of employment and unemployment, \ncompensation and working conditions, and prices \nand living conditions) with researchers.107\n• Data with limited privacy implications. For example, \nagencies whose data concerns natural phenomena, \nrather than individuals, may be easier to manage \nfrom a privacy perspective – e.g., NASA, the US \nGeological Service, and the National Oceanic and \nAtmospheric Administration. Datasets like those \nhoused in NASA’s Planetary Data System,108 but that \nare not easily available to researchers may serve as \na valuable starting point for the NRC. Increasing the \navailability and interoperability of datasets from \nthese agencies would advance the core mission of \nthe NRC and could be done without jeopardizing \nindividual privacy.\nA Blueprint for the National Research Cloud 48\nCHAPTER 4\nChapter 4: \nOrganizational Design\nWhat institutional form should the NRC take? Two overarching considerations \nare: (1) Ease of access to data; and (2) Ease of coordination with compute resources.1 \nAs we discussed in Chapter Three and will detail in depth in Chapter Five, the federal \ndata-sharing landscape among agencies is highly fragmented, with many agencies \nreluctant to or legally constrained from sharing their data. The NRC will need to \ncoordinate between the entities supplying compute infrastructure and researchers \nthemselves. As the NRC’s goal is to provide researchers with access to government \ndata and high-performance computing power; one without the other will fall short \nof achieving the NRC’s mission.\nDrawing on extensive work in support of the Evidence Act, we recommend \nthe use of Federally Funded Research and Development Centers (FFRDCs) and \nprivate-public partnerships (PPPs) as possible organizational forms for the NRC. We \nrecommend the creation of an FFRDC at affiliated government agencies in the short \nterm, as we believe this path allows for the easiest facilitation of both the compute \ninfrastructure and access to government data. In the longer term, the establishment \nof a PPP could facilitate greater data sharing and access between the public and \nprivate sectors. Importantly, other options include creating an entirely new federal \nagency or bureau within an existing agency. While these options might simplify \ncoordination with compute resources, both pose challenges, with respect to data \naccessibility and inter-agency data sharing. \nFEDERALLY FUNDED RESEARCH AND \nDEVELOPMENT CENTER\nFFRDCs are quasi-governmental nonprofit corporations sponsored by a \nfederal agency, but operated by contractors, including universities, other nonprofit \norganizations, and private-sector firms.2 The FFRDC model confers the benefits of \na close agency relationship, alongside independent administration, in facilitating \naccess to data. Due to their intimate subcontracting relationships with their parent \nagency, all FFRDCs benefit from data access that goes “beyond that which is \ncommon to the normal contractual relationship, to Government and supplier data, \nincluding sensitive and proprietary data.”3\nA recent report by Hart and Potok on the National Secure Data Service (NSDS) \n(see case study in Chapter Three) also supports the FFRDC model as an optimal way \nto facilitate access to, and linkage of government administrative data.4\n The report \nconsidered FFRDCs, alongside such other institutional forms as creating an entirely \nnew agency, housing the NSDS in an existing agency, and developing a university\u0002KEY \nTAKEAWAYS\n In the short term, the NRC \nshould be instituted as a \nFederally Funded Research \nand Development Center \n(FFRDC), which reduce the \nsignificant costs of securing \ndata from federal agencies.\n In the longer term, a well\u0002designed, public-private \npartnership (PPP), governed \nby officers from Affiliated \nGovernment Agencies, \nacademic researchers, and \nrepresentatives from the \ntechnology sector, could \nincrease the quantity and \nquality of R&D, and reduce \nmaintenance costs. \n Instituting the NRC as a \nstandalone federal agency or \nbureau would face numerous \nchallenges, notably in securing \naccess to data housed in other \nagencies. \nA Blueprint for the National Research Cloud 49\nCHAPTER 4\nled, data-sharing service, but the report ultimately \nrecommended the FFRDC model for several reasons. An \nFFRDC can scale quickly, because it can access government \ndata and high-quality talent more easily than other \noptions.5 An FFRDC can also leverage existing government \nexpertise. The NSF, for instance, already sponsors five \nseparate FFRDCs and has extensive experience cultivating \nand maintaining networks of researchers.6\nHowever, the FFRDC model comes with a few \nlimitations. First, an FFRDC’s role is restricted to research \nand development for their sponsoring agency that “is \nclosely associated with the performance of inherently \ngovernmental functions.”7 Thus, it would be important to \nensure alignment during the contracting phase with the \nNRC’s core functions. \nSecond, the success of an FFRDC model for the NRC \nwill depend on the ability of the sponsoring agency to gain \ncooperation across the federal government to provide \ndata needed for research. One way to do this would be \nfor multiple agencies to co-sponsor the FFRDC, reducing \ncontracting friction for datasets.8 Another option would \nbe to create multiple FFRDCs housed in different agencies, \nincentivizing each of those agencies to share their data \nwith the respective FFRDC. An analogous example could \ninclude the National Labs as a network, where each \nNational Lab would be an instantiation of the NRC within \nits own relevant agency.9\nThird, multiple FFRDCs would require separate \nprocesses for compute resources. In the short term, \nthe NRC may alleviate this problem by contracting for \ncommercial cloud credits, which is likely already the short\u0002term solution for the NRC to provide compute access. As \ndiscussed earlier, private sector cloud providers already \nhave extensive experience in providing compute resources \nto the government10 and to academic institutions.11\nFamiliarity with these private cloud providers may reduce \nthe friction in allocating compute among researchers at \nmultiple FFRDCs.\nIn the longer term, the FFRDC model may not be the \nmost efficient. From a cost and sustainability perspective, \nFFRDCs have traditionally suffered from significant \noverruns, as they “operate under an inadequate, \ninconsistent patchwork of federal cost, accounting and \nauditing controls, whose deficiencies have contributed \nto the wasteful or inappropriate use of millions of federal \ndollars.”12 Another concern is that, historically, FFRDC \ninfrastructure has not been routinely updated. A 2017 \nDepartment of Energy report highlighted that FFRDC \ninfrastructure was inadequate to meet the mission.13 \nNASA’s Inspector General also highlighted that more \nthan 50 percent of the Jet Propulsion Laboratory (a NASA \nFFRDC) equipment was at least 50 years old.14 If an FFRDC \nversion of the NRC experiences these same challenges, \nwe recommend for the NRC in the long run, to switch to a \npublic-private partnership model.\nA Blueprint for the National Research Cloud 50\nCHAPTER 4\nKEY TAKEAWAYS\n Multiple agency \nco-sponsors: While \nSTPI’s primary \nsponsor is the \nNational Science \nFoundation, a \nnumber of other \nagencies also \nco-sponsor STPI, \nreducing difficulties \nin accessing data \nacross agencies.\n Expertise: While \nSTPI is staffed with \nits own employees, \nit can also tap into \nexpertise from \nthe hundreds of \nemployees at the \nInstitute for Defense \nAnalyses (IDA), the \norganization that \nmanages STPI. As \nan FFRDC, STPI can \nalso contract for \nadditional expertise \nas required.\nCASE STUDY: SCIENCE & TECHNOLOGY \nPOLICY INSTITUTE (STPI)\nSTPI is a FFRDC chartered by Congress in 1991 to provide rigorous objective \nadvice and analysis to the Office of Science and Technology Policy and other \nExecutive Branch agencies.15 STPI is managed by the Institute for Defense Analyses \n(IDA), a nonprofit organization that also manages two other FFRDCs: the Systems \nand Analyses Center and the Center for Communications and Computing.16 IDA has \nno other lines of business outside the FFRDC framework.17\nSTPI’s primary federal sponsor is the National Science Foundation, but \nresearch at STPI is also co-sponsored by other federal agencies, including the \nNational Institute of Health (NIH), Department of Energy (DOE), Department of \nTransportation (DOT), Department of Defense (DOD), and Department of Health and \nHuman Services (HHS).18 Due to the “unique relationship” between an FFRDC and its \nsponsors, STPI “enjoys unusual access to highly classified and sensitive government \nand corporate proprietary information.”19\nNSF appropriations provide the majority of funding for STPI, including $4.7 \nmillion in Fiscal Year 2020,20 but a limited amount of funding is also provided from \nother federal agencies.21 STPI has approximately 40 full-time employees and has \naccess to the expertise of IDA’s approximately 800 other employees.22 As an FFRDC, \nSTPI may also contract for expertise, as required for a particular project.23 The \nstatute specifying STPI’s duties also directs it to consult widely with representatives \nfrom private industry, academia, and nonprofit institutions, and to incorporate those \nviews in STPI’s work to the maximum extent practicable.24\nSTPI is also required to submit an annual report to the President on its \nactivities, in accordance with requirements prescribed by the President,25 which \nprovides additional accountability for the FFRDC. According to STPI’s 2020 report, \nSTPI worked across multiple federal agencies, supporting them on 48 separate \ntechnology policy analyses throughout 2020.26\nA PUBLIC-PRIVATE PARTNERSHIP (PPP)\nA Public-Private Partnership (PPP) would create a \npartnership between federal agencies and private-sector \norganizations to jointly house and manage data-sharing \nefforts and run compute infrastructure. Because different \nagencies and private-sector members may have different \ncontracting preferences, intellectual property goals, and \nsecurity allowances for data access, creating a data-sharing \npartnership within this patchwork framework could be \nchallenging in the immediate future. Nonetheless, PPPs \ncan provide a number of long-term benefits, as they have \nbeen used successfully as data clearinghouses to produce, \nanalyze, and share data between the public and private \nsector.27 Indeed, recognizing the benefits of the PPP model, \nthe European Union has launched a new initiative called \nthe Public Private Partnerships for Big Data that will offer \na secure environment for cross-sector collaboration and \nexperimentation using both commercial and public data.28\nIn general, PPPs for data-sharing can increase the quality \nand quantity of R&D, increase the value and efficiency \nof sharing public sector data, and reduce the long-run \ncost necessary to manage and maintain the data-sharing \ninfrastructure.29\nA Blueprint for the National Research Cloud 51\nCHAPTER 4\nKEY TAKEAWAYS\n Utilizing joint ventures: \nThe ADP PPP uses \na joint venture \nagreement to set the \nterms and conditions \nfor the partnership, \nwhereby one partner \nis the custodian for \ngovernment data, \nand the other is the \noperator that builds and \nmaintains the software \nthat facilitates data\u0002sharing.\n Revenue-sharing \nagreements: A shared \nrevenue model assures \ncontributions from, \nand realization of value \nto, each stakeholder. \nThe ADP subsequently \nreinvests its profits into \nimproving the system.\n Significant efficiencies: \nAccording to the \nADP, there are lower \ncosts to creating \nand maintaining the \nADP than under a \nconventional approach. \nCASE STUDY: ALBERTA DATA PARTNERSHIPS \n(ADP)\nFounded in 1997, the ADP PPP is designed to provide long-term \nmanagement of comprehensive digital data sets for the Alberta market.30\nThe PPP is structured as a joint venture between ADP, a nonprofit, and Altalis \nLtd. whereby the ADP is the “custodian” of government data and Altalis is \nthe “operator.”31 More specifically, geospatial data is owned by the provincial \ngovernment, but exclusive licensing arrangements are granted to ADP to \nallow for sales.32 Meanwhile, Altalis, under the direction and oversight of \nADP, builds software to securely load and distribute these provincial spatial \ndatasets to users. Altalis also provides training to end users and is responsible \nfor cleaning, updating, and standardizing datasets.33\nIn choosing its “operating partner” (i.e., Altalis) for the joint venture, the \nADP Board initially issued a “Request for Information” that solicited proposals \nfrom private-sector companies whose core business was the improvement, \nmaintenance, management, and distribution of spatial data.34 The ADP \nBoard ultimately chose Altalis; not only because it had the superior offering \nand existing capabilities, but also because Altalis was willing to take on all \nthe investment required, at its risk, to build and operate the ADP system in \naccordance with ADP specifications.35\nToday, all Altalis and ADP costs are covered by the operations of the \njoint venture.36 The joint venture earns revenues through, for instance, \ndirected project funding and data access fees from stakeholders, which \ninclude municipalities, regulatory agencies, energy, forestry, and mining \norganizations.37 Any profits from the joint venture are split roughly 80/20 \nbetween Altalis and ADP, respectively, and ADP subsequently uses its profit \nshare to reinvest in data and system improvements.38\nThe ADP PPP claims to have generated efficiencies for data sharing. For \ninstance, the ADP estimates that a traditional government-only approach to \nmaintaining and distributing datasets would have ranged between $65 million \nand $120 million cumulatively since ADP’s inception, and ADP claims to have \nprovided its users with $6.8 million in cost savings.39\nA Blueprint for the National Research Cloud 52\nCHAPTER 4\nA PPP model could reduce the friction of coordination \nbetween data and compute. One example of using a PPP \nfor compute resources is the COVID-19 High-Performance \nComputing Consortium, spearheaded by the Office of \nScience and Technology Policy, DOE, NSF, and IBM.40\nDrawing on the experience of XSEDE, the Consortium \nhas 43 members from the public and private sectors that \nvolunteer free compute resources to researchers with \nCOVID-19-related research proposals.41 The voluntary \nnature of compute provisioning, in this instance, provides \nbenefits to both the researchers, who gain immediate \naccess to compute, and the Consortium members, who \ncontribute to innovation and reap public relations benefits. \nWe also acknowledge that the evidence around the \nefficacy of PPPs is contested.42 Indeed, there is no one\u0002size-fits-all PPP model; PPPs differ vastly, according to the \nresponsibilities allocated between the private sector and \nthe public sector, and the success of a PPP can depend \non its structure.43 According to a RAND Report of 30 case \nstudies of successful public-private data clearinghouses, \nthese clearinghouses have widely different organizations, \naccess requirements, and strategies for managing data \nquality.44 Such decision points are crucial. For example, \nsome scholars emphasize the need for a trusted \nenvironment for the private and public sectors to handle \nprivacy and ethics violations in sensitive industries.45\nSimilarly, in the siloed federal data-sharing context, a PPP \nmust consider how to divide functions in tackling these \nadditional considerations in privacy, ethics, security, and \nintellectual property.\nTHE NRC AS A GOVERNMENT \nAGENCY\nThe NRC could also be constructed as a new \ngovernment agency or bureau. The main advantages \nto this model would be the development of a distinct \npublic-sector institution, devoted to AI compute and \ndata. The NRC could be to cloud and data what the U.S. \nDigital Service is to government information technology. \nSuch an agency would have to be established by statute \nor executive mandate. Enabling legislation could create \ndedicated, professional staff to build and develop the NRC, \nvest the NRC with authority to mandate inter-agency data \nsharing, and create a long-term plan that is informed by \nthe National AI Strategy. \nThere are, however, significant disadvantages to \ncreating a new agency or bureau. First, the NRC could \nlay claim to no government datasets at all, and could \nsubsequently encounter significant headway with having \nto negotiate with each originating agency for data, not to \nmention the constraints under the Privacy Act, discussed \nin Chapter Five. That said, enabling legislation could \nexempt the agency from the Privacy Act’s data linkage \nprohibitions and transfer litigation risk for data leakages \nto the new agency. Second, a new agency may face greater \nchallenges in recruiting top-flight talent.46 According to \nthe 2020 Survey on the Future of Government Service, a \nmajority of respondents at federal agencies agreed that \nthey often lose good candidates because of the time it \ntakes to hire, and less than half agreed that their agencies \nhave enough employees to do a quality job.47 Moreover, \nmany respondents highlighted inadequate career growth \nopportunities, inability to compete with private-sector \nsalaries, and lack of a proactive recruiting strategy as \nmajor factors contributing to an inadequately skilled \nworkforce in federal agencies.48 FFRDCs, in contrast, can be \nnegotiated with existing organizations, making the startup \ncosts potentially lower. Third, while national laboratories \nhave expertise contracting with entities to construct high\u0002performance computing facilities, it is unclear how a new \nfederal agency/office would approach such a task. It is \none thing for an entity like the U.S. Digital Service to help \ndevelop IT platforms for U.S. agencies; it is another to \nsimultaneously build a very large supercomputing facility \nand solve longstanding challenges with data access. \nFinally, it will be important to isolate the research mission \nof the NRC from political influence. To the extent that a \nnew agency might provide less isolation from changes \nin presidential administrations and politically appointed \nadministrators, is an important consideration.\nWhile these disadvantages are considerable, \nambitious legislative action could, in fact, make a new \ngovernment agency a viable option.\nA Blueprint for the National Research Cloud 53\nCHAPTER 5\nChapter 5: Data \nPrivacy Compliance\nThe vision motivating the NRC is to support academic research in AI by opening \naccess to both compute and data resources. Federal data can fuel basic AI research \ndiscoveries and reorient efforts from commercial domains toward public and social \nones. As stated in the NRC’s original call, “[r]esearchers could work with agencies to \ndevelop and test new methods of preserving data confidentiality and privacy, while \ngovernment data will provide the fuel for breakthroughs from healthcare to education \nto sustainability.”1\nBut is an NRC seeded with public sector data, particularly administrative data \nfrom U.S. government agencies, even possible given the legal constraints? Research \nproposals that sweep broadly across agencies for personally identifiable or otherwise \nsensitive data2 will rightly trigger concerns about potential privacy risk. The Privacy \nAct of 1974, the chief federal law governing the data collected by government \nagencies, fundamentally challenges the notion of an NRC as a one-stop shop for \nfederal data. Its research exceptions leave some uncertainty about open-ended \nresearch endeavors that go beyond statistical research or policy evaluation supporting \nan agency’s core mission. Even if agencies deemed such research possible, researchers \nwould be subject to access constraints and the data itself may potentially require \ntechnical privacy treatments.\nWe make the following recommendations regarding data privacy and the NRC. \nFirst, agencies may be able to share anonymized administrative data with the NRC \nwithin the boundaries of the Privacy Act for the purposes of AI research, based on the \nAct’s statistical research exemptions. Second, the NRC will require a staff of privacy \nprofessionals that include roles tasked with legal compliance, oversight, and technical \nexpertise. These professionals should build relationships with peers across agencies \nto facilitate data access. Third, the NRC should explore the design of virtual “data safe \nrooms” that enable researchers to access raw administrative microdata in a secure, \nmonitored, and cloud-based environment. Fourth, we recommend the NRC Task Force \nengage the policy and statistical research communities, and consider coordination \nwith proposals for a National Secure Data Service which has grappled extensively with \nthese issues. \nThis chapter proceeds as follows. We first review the existing laws that apply to \ngovernment agencies and the restrictions they impose on data access and sharing. We \nthen describe current agency practices for sharing data with researchers and agencies \nunder the Privacy Act. Last, we assess the implications of current legal constraints on \nNRC data sharing and the most important cognate proposal to promote data sharing \nunder the Evidence Act. \nKEY \nTAKEAWAYS\n Agencies may share \nanonymized administrative \ndata with the NRC under \nthe statistical research \nexemption of the Privacy \nAct.\n An agency’s willingness and \nability to share data may \ndepend on the extent to \nwhich a proposed research \nproject aligns with an \nagency’s core purpose.\n The NRC will require a staff \nof privacy professionals for \nlegal compliance, oversight, \nand technical expertise. \n Individually identifiable \nor sensitive data will face \nobstacles to release and \nmay warrant technical \nprivacy and/or tiered access \nmeasures. \nA Blueprint for the National Research Cloud 54\nCHAPTER 5\nEven when authorized or \nmandated to share data in limited \ncircumstances, federal agencies \nare often reluctant to do so due to a \nmyriad of factors, most prominently \na lack of adoption of consistent \ndata security standards, as well \nas difficulties with measuring and \nassessing privacy risks.\nWe note at the outset that this Chapter largely takes \nexisting statutory constraints as a given. At a macro level, \nhowever, the challenges in data sharing also suggest that \nan ambitious legislative intervention could overcome many \nexisting constraints, such as by statutorily (a) exempting \nthe NRC from the Privacy Act’s prohibition on data linkage; \n(b) granting the NRC the power to assume agency liabilities \nfor data breaches; (c) mandating that agencies transfer any \ndata that has been shared under a data use agreement or \nFreedom of Information Act (FOIA) request to the NRC; and \n(d) requiring IT modernization plans to include provisions \nfor data-sharing plans with the NRC.3\nTHE PRIVACY ACT\nData privacy issues are at the core of debates about \nsharing data, and the NRC will be no exception. Most data \nprivacy debates in the U.S. today focus on the consumer \ndata sector where data protection laws in the U.S. are \nlimited to nonexistent. In contrast, many U.S. government \nagencies are subject to a robust privacy law, the Privacy \nAct of 1974, that was passed in response to concerns \nabout government abuses of power.4 For nearly 50 years, \nthis legislation has been effective in its primary goal of \npreventing the U.S. government from centralizing and \nbroadly linking data about individuals across agencies. \nHowever, this approach has come at a cost, which is that \nmost government agencies are prevented from freely \nsharing and linking data across agency boundaries, \nwhich in turn hampers agency operational and research \nefforts.5 According to one government privacy expert, even \nwhen authorized or mandated to share data in limited \ncircumstances, federal agencies are often reluctant to do \nso due to a myriad of factors, most prominently a lack of \nadoption of consistent data security standards, as well as \ndifficulties with measuring and assessing privacy risks.6 To \nthat end, many agencies see promise in adopting technical \nprivacy measures, such as differential privacy, or the \ncreation of synthetic data sets as proxies for actual data, \nas a necessary precursor for enabling data sharing for both \nresearch purposes, as well as for inter-agency goals.7 \nIn the nearly 50 years since the Privacy Act’s \npassage, there have been periodic efforts to address \nthe government’s approach to data management, \nwhile preserving data privacy. Examples include the \nE-Government Act of 2002, 8 the Confidential Information \nProtection and Statistical Efficiency Act of 2002,9 and most \nrecently, the Foundations for Evidence Based Policymaking \nAct10 and the National Data Strategy.11 Most of these efforts \nhave been aimed at sharing government data for statistical \nanalysis and policy evaluation, and the scope of provisions \nmay need to be broadened to support AI research. We view \nthese efforts to be complementary: the NRC should build \non these efforts, while bringing increased attention to the \ncompute resources that enable both AI development, as \nwell as advanced data analysis.\nSTATUTORY CONSTRAINTS ON \nDATA SHARING\nOne vision of the NRC is for it to act as a data \nwarehouse for all government data. But that vision collides \nwith fundamental constraints from laws designed to \nhamper broad and unconstrained data sharing between \nU.S. government agencies. Lacking an overarching, \ncomprehensive privacy regime, similar to the European \nUnion’s General Data Protection Regulation (GDPR), the \nUS landscape is fragmented between a mix of sector\u0002specific consumer laws and certain government-specific \nlaws, such as the Privacy Act of 197412 and limited-scope \nfederal guidance, such as the Fair Information Practice \nA Blueprint for the National Research Cloud 55\nCHAPTER 5\nPrinciples.13 In particular, the Privacy Act, which focuses \nbroadly on data collection and usage by federal agencies, \nand restricts sharing between them, poses challenges \nto the ambitions of the NRC’s goal to make otherwise \nrestricted government datasets more widely available.\nExisting efforts, buttressed by such bills as the \nE-Government Act of 2002 and the Foundations of \nEvidence-Based Policymaking Act, have attempted to \nincrease access by researchers to government data assets. \nYet, these approaches were animated by the primary \npurposes of policy evaluation, not basic AI research. Nor \ndo they consider any ambitions on the part of agencies \nthemselves to pursue AI research and development.14\nApplication of these laws and regulations to the NRC, \nin part, hinge on three factors: (1) the institutional form \nof the NRC, as we discuss in Chapter Four; (2) whether \nNRC users can invoke the Privacy Act’s existing statistical \nresearch exception; and (3) whether researchers are \naccessing data from multiple federal agencies. Here we \nbriefly discuss the legal obligations of federal agencies. \nEven if the NRC does not take the form of a new standalone \nfederal agency, agencies contributing data will remain \nsubject to these constraints. \nTHE PRIVACY ACT’S LIMITATIONS AND EXEMPTIONS\nThe Privacy Act was enacted in response to growing \nanxiety about digitization, as well as the Watergate \nscandal during the Nixon Presidency. The Act was \nmotivated by concerns about the government’s ability \nto broadly collect data on citizens and centralize it into \ndigital databases, an emergent practice at the time. It \nis the primary limiting regulation for government data \nsharing, and has consequences for the NRC for both itself, \nbut more directly, for any government agency wishing to \nshare data with the NRC. \nData Linkage\nThe Privacy Act applies to systems of records, which \nare defined as: “a group of any records under the control of \nany agency from which information is retrieved by the name \nof the individual or by some identifying number, symbol, \nor other identifying particular assigned to the individual.”15\nImportantly, the Act places strict limits on “record \nmatching,” or linking between agencies, for the purposes of \nsharing information about individuals.16 Matching programs \nare only allowed when there is a written agreement in place \nbetween two agencies defining the purpose, legal authority, \nand the justification for the program; such agreements \ncan last for 18 months, with the option of renewal.17 These \nlimits were put in place in order to prevent the emergence \nof a centralized system of records that could track U.S. \ncitizens or permanent residents across multiple government \ndomains, as well as to limit the uses of data for the purposes \nit was collected. Indeed, while linkage across datasets may \nbe important for AI research,18 it could potentially enable \nabuse, surveillance, or the infringement of such rights such \nfree speech, by enabling persecution across the many areas \nin which a U.S. citizen or resident interacts with the federal \nsystem.19\nBecause the restriction on data linkages applies \nto linkages between agencies, the restriction applies in \ntwo particular scenarios for the NRC. First, if the NRC is \ninstituted as a federal agency, then agency data-sharing \nwith the NRC would run against the data linkages \nlimitation of the Privacy Act. Second, federal agency \nstaff access to the NRC could raise questions about inter\u0002agency data linkage under the Privacy Act. However, the \nrecommendation in Chapter Three is focused on granting \nagencies streamlined access to the computing resources \non the NRC and their own agency data, not to any multi\u0002agency data hosted on the NRC. If the NRC is not designed \nas a federal agency and does not grant agency members \naccess to inter-agency data, the Privacy Act’s restrictions \non data linkages may not apply. \nWe note that this approach to data management \nis both unusual and out of step with the private sector, \nas well as AI research specifically. The ability for both \nindustry20 and researchers21 to associate multiple data \nsources and data points with a specific (anonymized) \nindividual is common practice outside of government. In \nfact, this limitation is not one that many governments22\nor U.S. states23 place on their data systems. However, \nthe Privacy Act’s restrictions on data linkage remains \nuncontested, even in the various reform efforts we discuss \nA Blueprint for the National Research Cloud 56\nCHAPTER 5\nbelow. It is worth noting that the federal government’s \nbroad bar against data linkages does incur welfare costs. \nFor example, during the COVID-19 pandemic, the inability \nto share and link public health data created difficulties \ntracking the spread and severity of the virus.24 While \nprojects like Johns Hopkins’ Coronavirus Research Center25\nand the COVID Tracking Project26 attempted to aggregate \navailable data, the lack of data integration slowed \nimportant operational and research responses.27 Other \ncountries, for instance, integrated immigration and travel \nrecords to triage cases and prevent hospital outbreaks.28\nWe acknowledge the potential for data linkage \nto tackle important societal problems, without \nrecommending wholesale, unencumbered data linkage. \nBroad or unrestricted data linkage raises legitimate \nconcerns about both individual privacy and widespread \ngovernment surveillance,29 made concrete by the \ndisclosures of government whistle-blower Edward \nSnowden,30 among others. An initiative to link Federal \nAviation Administration (FAA) data with other agency \ndata for COVID-19 response, for instance, would meet \nresistance from the Privacy Act. The Task Force should \nappreciate these tensions and tradeoffs. Indeed, agencies \nview technical measures for privacy preservation a \nnecessary component of any government data strategy, as \nmethods such as multiparty computation or homomorphic \nencryption (which we discuss in Chapter Eight) may allow \nfor some forms of data linkages between agencies, without \nviolating the Privacy Act.\nNo Disclosure Without Consent\nAnother core restriction of the Privacy Act is the \n“No Disclosure Without Consent” Rule, which prohibits \ndisclosure of records to any agency or person without \nprior consent from the individual to whom the record \npertains.31 Because the NRC would disclose federal agency \ndata to researchers (i.e., to “person[s]”), this rule––unlike \nthe restriction on record linkage––is legally relevant and \nunavoidable. \nThe Privacy Act, however, contains a number of \nexceptions to this rule. Most pertinent to the NRC’s data\u0002sharing efforts are exemptions for: (1) “routine use”; (2) \nspecified agencies; and (3) statistical research. Under \nthe first exemption, the Privacy Act permits agencies to \ndisclose personally-identifiable administrative data when \nsuch disclosure is among one of the “routine uses” of the \ndata.32 A dataset’s “routine use” is defined on an agency\u0002to-agency basis, and is simply a specification filed with \nthe Federal Register on the agency’s plan to use and share \nits data.33 As a result, the more broadly an agency defines \n“routine use” of its data, the more broadly that agency can \nshare its data with other agencies without disclosure.34\nWhile courts have limited how broadly an agency can \ndescribe “routine uses,”35 a large number of use cases can \nstill be covered by a short, general statement.36 Further \nresearch should be conducted on the conditions for when \ndata sharing for research purposes constitutes routine use. \nImplications For Data Sharing with Researchers\nMuch will rest on the interpretation of the “statistical \nresearch” exception, as applied to AI research. Despite \nthe Privacy Act’s constraints on data sharing, researchers \nhave conventionally been able to access data directly from \nagencies, based on the statistical research exception to \nthe Privacy Act. This exception allows disclosure of records \n“to a recipient who has provided the agency with advance \nadequate written assurance that the record will be used \nsolely as a statistical research or reporting record, and the \nrecord is to be transferred in a form that is not individually \nidentifiable.”37 Doing so requires either access to an approved \nresearch dataset, or for the researcher to negotiate an MOU \ndirectly with the agency, a role we suggest the NRC may be \nable to fill as an intermediary, acting as a negotiating partner \nto facilitate access requests between multiple researchers \nand agencies (discussed in Chapter Three). \nWhile the Privacy Act does not define “statistical \nresearch,” subsequent laws and policies have elaborated \non the definition. For example, the E-Government Act \ndefines “statistical purpose” to include the development \nof technical procedures for the description, estimation, or \nanalysis of the characteristics of groups, without identifying \nthe individuals or organizations that comprise such \ngroups.38 Meanwhile, a “nonstatistical purpose” includes \nthe use of personally-identifiable information for any \nadministrative, regulatory, law enforcement, adjudicative, \nor other purpose that affects the rights, privileges or \nbenefits of any individual.39 That is, while researchers may \nA Blueprint for the National Research Cloud 57\nCHAPTER 5\nuse personally identifiable data for the broad purpose of \nanalyzing group characteristics, they cannot use such data \nfor targeted purposes to aid agencies with, for instance, \nspecific adjudicative or enforcement functions.\nThe precise meaning of “statistical purpose,” however, \nremains “obscure and the evaluation criteria may be \ndifficult to locate.”40 Yet, “statistical purpose” may well \nencompass data sharing for certain AI applications. The Act \nexplicitly designates the Bureau of Labor Statistics, Bureau \nof Economic Analysis, and the Census Bureau as statistical \nagencies that have heightened data-sharing powers for \nstatistical purposes.41 These agencies regularly use AI in \nconducting their statistical activities.42 While definitions \nof AI are themselves contested, statistical research may \nencapsulate at least some forms of machine learning and \nAI, if such research analyzes group characteristics43 and \ndoes not identify individuals. \nTo be sure, the NRC should not enable researchers or \nagencies to conduct an end-run around the Privacy Act. \nTo that end, the NRC will require staff devoted to privacy \ncompliance and oversight to ensure compliance. Key \nquestions regarding individual identifiability, sensitivity of \nthe data, or the potential for linkage and reidentification \nwill need to be assessed by such staff. \nImplications for agency data sharing with the NRC\nNotwithstanding the above avenues, agencies may \nnonetheless be reluctant to share data with the NRC and \nits researchers. Instances abound where federal agencies \nface constraints to sharing data, even if it is entirely legal \nor even federally mandated. For example, the Uniform \nFederal Crime Reporting Act of 1988 requires federal law \nenforcement agencies to report crime data to the FBI.44\nYet, no federal agencies appear to have shared their data \nwith the FBI under this law.45 Similarly, the Census Bureau \nis enabled by legislation that authorizes it to obtain \nadministrative data from any federal agency and requires \nit to try to obtain data from other agencies, whenever \npossible.46 However, the statute does not similarly require \nthe program agencies to provide their data to the Census \nBureau. That is, although the Census Bureau is required \nto ask other agencies for data, those agencies are not \nrequired to, and often do not, provide it.47\nFailure to engage in data sharing, even in the face of \na statutory authorization, can stem from risk aversion. \nAccording to a GAO report, agencies choose not to share \ndata because they tend to be “overly cautious” in their \ninterpretation of federal privacy requirements.48 Because \nlegal provisions authorizing or mandating data sharing \nare often ambiguous,49 agencies may err on the side of \ncaution and choose not to share their data for fear of the \ndownside risk that recipient use of the data may violate \nprivacy or security standards.50 To make matters worse, \nbecause agencies need to devote significant resources \nto facilitate data sharing, they may simply choose not to \nprioritize data sharing at all. The lack of resources poses a \nsignificant problem; according to a Bipartisan Policy Center \nstudy on agency data sharing, about half of agencies cited \ninadequate funding or inability to hire appropriate staff as \ntheir “most critical” barrier to data sharing.51\nThe NRC may overcome these hurdles by clarifying \nlegal provisions, ensuring that the benefits to agencies of \ndata sharing outweigh the risks and costs, and advocating \nfor resources. For instance, O’Hara and Medalia describe \nhow the Census Bureau was able to obtain food stamp and \nwelfare data from state agencies. In the face of ambiguous \nstatutes authorizing the U.S. Department of Agriculture \n(USDA) and the U.S. Department of Health and Human \nServices (HHS) to perform data linkages across federally \nsponsored programs, states originally arrived at different \nstatutory interpretations. Some states agreed to share their \ndata only after (1) the Office of General Counsel at both the \nUSDA and HHS issued a memo clarifying that data sharing \nwith the Census Bureau for statistical purposes was legal \nand encouraged, and (2) the states were convinced that data \nsharing would also enable evidence building that could help \nthem administer their programs.52\n[T]he NRC should not enable \nresearchers or agencies to \nconduct and end-run around \nthe Privacy Act.\nA Blueprint for the National Research Cloud 58\nCHAPTER 5\nBroader data sharing with the NRC that combines \nmultiple agency or external data sources may be facilitated \nby the passage of additional law requiring agencies to \nshare their data, subject to specific limitations on how \nthat data is used in the NRC. Even then, the effect of that \nrequirement is hardly a foregone conclusion. More is \nneeded by way of both clarifying the extent to which data \nsharing is permitted and providing benefits that incentivize \nagencies to share their data.\nCASE STUDY: ADMINISTRATIVE \nDATA RESEARCH UK\nAdministrative Data Research UK (ADR UK) is a new body, set up \nin July 2018, to facilitate secure, wide access to linked administrative \ndatasets from across government for the purpose of public research.53\nADR UK was set up as a central, coordinating point between four \nnational partnerships – ADR England, ADR Northern Ireland, ADR \nScotland, and ADR Wales – as well as the UK-wide national statistics \nagency, Office for National Statistics (ONS). ADR UK labels itself as \na ‘UK-wide Strategic Hub’: a central point that promotes the use of \nadministrative data for research, engages with government departments \nto facilitate secure access to data, and funds public good research that \nuses administrative data.54\nFunding for ADR UK came from a research council (Economic and \nSocial Research Council, ESRC) and was initially committed from July \n2018 to March 2022. A total of £59m was provided.55\nADR UK serves three core functions. First, the promotion of the value \nand availability of government administrative datasets for research. ADR \nUK acts as a general advocate for the use of administrative datasets \nfrom across the British government. It also acts as a specific driver of \nresearch for public good: it has identified specific areas of research that \nare of pressing policy interest (e.g., ‘world of work’56), and is focusing \non creating access to linked datasets for researchers who tackle those \npriority themes.\nThe second core function is serving as a coordination point \nto encourage government data sharing, standards, and linkage of \nadministrative datasets. Especially for its research calls, ADR UK is able \nto highlight multiple datasets, often spanning different government \ndepartments’ scope areas that can be linked and used in research. In \ndoing so, ADR UK plays an important role in facilitating research.\nFinally, to ensure compliance with the Privacy Act, as \nwell as to facilitate the NRC’s role as a data intermediary, \nthe NRC will require a staff of privacy professionals that \ninclude positions tasked with legal compliance, oversight, \nand technical methods expertise. These professionals \nshould build relationships with peers across agencies to \nfacilitate data access.\nKEY TAKEAWAYS\n Proactive advocacy for data \nuse and linkage: Given the \nrange of agencies and data \nsources in government, \nhaving a single, coordinated \nvoice of advocacy for data \nuse and linkage of public \ndatasets for public good is an \nimportant function.\n Bringing external talent \ninto government data use: \nADR UK has two schemes—\nResearch Fellowships and \nMethod Development \nGrants—that target \nexceptional, external talent \nwith the intention of building \nawareness and use of public \ndatasets in cutting-edge \nresearch.\n Small grant funding to \naccelerate research methods \nthat use large datasets: \nBy putting out calls for \nresearch that answers broad \nthemes, ADR UK is able to \ncorral a range of datasets \nin answering research \nquestions and avoids a single \ndisciplinary focus.\nA Blueprint for the National Research Cloud 59\nCHAPTER 5\nThird, ADR UK has a strategic funding approach to further the use of administrative datasets in research \nthat has comprised of three categories of funding:\n• Building new research datasets: ADR UK’s Strategic Hub Fund initially solicited invitation\u0002only bids for researchers who would build new research datasets of public significance in the \ncourse of their work.57 These new, research-ready datasets are now accessible to a wide range of \nresearchers.58\n• Research Fellowship Schemes: A major funding focus now is on funding research through \ncompetitive open-bid invitations under a Research Fellowship Scheme.59 Specific researchers are \nidentified through the competition. They are accredited for secure data access and placed right \nat the heart of government (with 10 Downing Street), with access to linked datasets to answer \nquestions of public significance.60\n• Methods Development Grants: Separately, ADR UK invites research proposals that further \nmethodological progress for the use of large-scale administrative datasets, such that the wider \nsocial science community can draw on developed methods in research.61\nPrivacy and security\nThe UK’s 2017 Digital Economy Act62 created a legal gateway for research access to secure government \ndata. Deidentified data held by a public authority in connection with the authority’s functions could be \ndisclosed for research, under the assurance that individual identities would not be specified.\nAny data shared with researchers is anonymized: personal identifiers are removed, and checks are \nmade to protect against re-identification.63 A rigorous accreditation process—for both the researcher and \nproposed research—is undertaken to ensure public benefit. Data access primarily takes place via a secure \nphysical facility, or a secure connection to that facility, provided by ADR UK’s constituent partners.64 There is \nclose monitoring of researcher activity and outputs, and any output is checked before release.65\nFrom a researcher’s point of view, access to ADR UK datasets requires the following steps:66\n• Researcher submits proposal for project to ADR UK\n• Project is approved by relevant panels\n• Researcher engages in training and may take assessment (e.g., access to linked data held by ONS \nrequired accreditation to ONS’ Secure Research Service,67 and can access data either in person or, \nwhere additionally accredited, through remote connection)\n• Required data is determined by ADR UK (through one of the four regional partners, or ONS), then \ningested by the relevant data center\n• De-identified data is made available through a secure data service (either at the ONS, or one of the \nfour regional partners)\n• Researcher conducts analysis; activity and outputs are monitored\n• Outputs are checked for subject privacy. Research serving the public good is published\nCASE STUDY: ADMINISTRATIVE DATA RESEARCH UK (CONT’D)\nA Blueprint for the National Research Cloud 60\nCHAPTER 5\nCOMPLEMENTARY EFFORTS TO \nIMPROVE THE FEDERAL APPROACH \nTO DATA MANAGEMENT\nThe barriers to data sharing created by the Privacy \nAct have long posed a challenge to researchers interested \nin using government data to evaluate or inform policy.68\nThe policy and statistical research communities, both \nwithin and outside the federal government, have engaged \nin admirable reform efforts to facilitate data sharing for \npolicy evaluation.69\nThe Foundations for Evidence Based Policymaking \nAct (EBPA) of 2018, which enacted reforms to improve \ndata access for evidence-based decision-making, is a key \nachievement of these efforts to date. However, several of \nthe provisions in the Act that helped to address some of \nthe barriers to data linking and sharing were not passed \nby Congress. These provisions—known collectively as \nthe National Secure Data Service (NSDS)—remain a high \npriority for facilitating further progress for sharing data \nfor research purposes. According to the nonprofit Data \nFoundation, one of the major supporters of the NSDS, its \npassage will “create the bridge across the government’s \ndecentralized data capabilities with a new entity that \njointly maximizes data access responsibilities with \nconfidentiality protections.”70\nThe NSDS is envisioned as an independent legal \nentity within the federal government that would have \nthe legal authority to acquire and use data. However, this \nauthority is currently conceived of as emanating from the \nEBPA, which focuses on using statistical data for evidence\u0002building purposes. A broader source of authority may be \nnecessary for AI research purposes under the NRC, which \nmay be distinct from agency obligations. One clear area of \noverlap is the proposal’s call for the Service to facilitate its \nown computing resources which could be harmonized with \nthe compute needs of the NRC. Similar to Chapter Four’s \ndiscussion of organizational options, NSDS supporters \nidentify a fundamental need for both a reliable funding \nsource as well as thoughtful placement of the Service \neither within an existing agency, or as an independent \nagency or FFRDC. The areas of common ground \nbetween the NRC and NSDS, as well as the expertise and \nmomentum behind the proposal, strongly suggest that the \nNRC engage and coordinate with these efforts. \nAnother complementary initiative is the Federal Data \nStrategy (FDS), launched in 2018 by the executive branch \nand led by the OMB. FDS is a government-wide effort \nto reform how the entire federal government manages \nits data. The plan both calls out the need for “safe data \nlinkage” through technical privacy techniques,71 as well as \nincorporates a directive from the 2019 Executive Order on \nMaintaining American Leadership in Artificial Intelligence \nto “[e]nhance access to high-quality and fully traceable \nfederal data, models, and computing resources to increase \nthe value of such resources for AI R&D, while maintaining \nsafety, security, privacy, and confidentiality protections, \nconsistent with applicable laws and policies.”72 The \nStrategy directs OMB to “identify barriers to access and \nquality limitations” and to [p]rovide technical schema \nformats on inventories,” with a focus on open data sources \n(i.e., non-sensitive or individually identifying data).73\nDatasets identified by this process could be key candidates \nfor populating the NRC. \nWhile both the NSDS and the FDS may promote data \nsharing, these efforts are presently focused primarily on \nfurthering policy evaluation purposes. Fortunately, there \nis much overlap and complementarity between these \ninitiatives and the NRC, illustrating the broad importance \nof more effective mechanisms to share federal data \nsecurely and in a privacy-protecting way. \nA Blueprint for the National Research Cloud 61\nCHAPTER 6\nChapter 6: Technical \nPrivacy and Virtual \nData Safe Rooms\nWe now discuss the role of technical privacy methods for the NRC. In the past \nseveral decades, researchers have devised a variety of computational methods that \nenable data analysis while preserving privacy. These methods hold considerable \npromise for enabling the sharing of government data for research purposes. We note \nat the outset that technical methods are merely one mechanism to strengthen privacy \nprotections. While effective, such methods may be neither sufficient nor universally \nappropriate. The application of any particular method does not obviate the need \nto inquire into whether the data itself adheres to articulated privacy standards. The \nmethods discussed here are not “replacements” for the recommendations discussed \nearlier and never themselves justify the collection of otherwise problematic data. \nUse of data from the NRC introduces two threats to individual privacy. The \nfirst type involves accidental disclosure by agencies (agency disclosure): an agency \nuploads a dataset to the NRC which lacks sufficient privacy protection and contains \nidentifying information about an individual. A researcher—either analyzing this \ndataset alone or in conjunction with other NRC datasets—discovers this information \nand re-identifies the individual.1\n The second type involves accidental disclosure by \nresearchers (researcher disclosure). Here, a researcher releases products computed on \nrestricted NRC data (e.g., trained machine learning models, publications). However, \nthe released products lack sufficient privacy protection, and an outside consumer of \nthe research product learns sensitive information about an individual or individuals in \nthe original dataset used by the researcher.2\nWe recommend that, due to the infancy and uncertainty surrounding uses of \nprivacy-enhancing technologies, privacy should primarily be approached via access \npolicies to data. While there will be circumstances that suggest, or even mandate, \ntechnical treatments, access policies, discussed in Chapter Three, are the primary line \nof defense: they ensure sensitive datasets are protected by controlling who can access \nthe data. We recommend a tiered access policy, with more sensitive datasets placed \nin more restricted tiers. For instance, highly restricted access data may correspond \nto individual health data from the VA, while minimally restricted access data may \ncorrespond to ocean measurements from NOAA. Proposals requesting access to highly \nrestricted data would face heightened standards of review, and researchers may \nbe limited to accessing only one restricted access dataset at a time. This approach \nmirrors current regimes where researchers undergo special training to work with \ncertain types of data.3\nKEY \nTAKEAWAYS\n Technical privacy measures \nare useful, but not substitutes \nfor securing data privacy \nthrough access policies. \n In some instances, the NRC or \nagencies may wish to make \naccess to data conditional \non use of technical privacy \nmeasures.\n Contributing agencies and \nthe NRC should collaborate to \ndetermine technical privacy \nmeasures based on dataset \nsensitivity, dataset utility, and \nequity implications. \n The NRC must have technical \nprivacy staff to administer \ntechnical privacy treatments, \nas well as to support \nadversarial privacy research.\n The NRC should explore \nadopting virtual “data \nsafe rooms” that enable \nresearchers to access \nraw administrative data \nor microdata in a secure, \nmonitored, and cloud-based \nenvironment.\nA Blueprint for the National Research Cloud 62\nCHAPTER 6\nTechnical treatments are a different line of defense: \nthey significantly reduce the chances of deanonymizing \na dataset. There are a range of technical methods which \ncan enable analysis while ensuring privacy:\n• Techniques like k-anonymity and ℓ-diversity \nattempt to offer group-based anonymization by \nreducing the granularity of individual records in \ntabular data.4 While effective in simple settings and \neasy to implement, both methods are susceptible \nto attacks by adversaries who possess additional \ninformation about the individuals in the dataset.\n• One of the most popular techniques is differential \nprivacy,5\n which provides provable guarantees \non privacy, even when an adversary possesses \nadditional information about records in the dataset. \nHowever, differential privacy requires adding \nrandom amounts of statistical “noise” to data and \ncan sometimes compromise the accuracy of data \nanalyses. Although differential privacy has become \na point of contention with respect to the Census \nBureau’s new disclosure avoidance system,6\n the \ntechnique remains a powerful defense against bad \nactors seeking to take advantage of public data for \nthe purposes of re-identification.\n• Researchers have also identified other promising \nmethods. Recent work has demonstrated that \nmachine learning can be used to generate \n“synthetic” datasets, which mirror real world \ndatasets in important ways, but consist of entirely \nsynthetic examples.7\n Other work has focused on \nthe incorporation of methods from cryptography, \nincluding secure multiparty computation8 and \nhomomorphic encryption.9\nMethods that obscure data introduce fundamental \ntensions with the way machine-learning researchers \ndevelop models. For example, when considering \nquestions of algorithmic fairness, in some instances \nprivacy protections can undercut the power to assess \nwhether such a technical method as differential privacy \nresults in demographic disparities, particularly for small \nsubgroups.10 Similarly, “error analysis”––the study of \nsamples over which a machine-learning model performs \npoorly––is central to how researchers improve models. It \nrequires understanding the attributes and characteristics \nof the data in order to better understand the deficiencies \nof an algorithm. Therefore, such methods as differential \nprivacy, which make raw data more opaque, will invariably \nimpede the process of error analysis. Synthetic data \ntypically captures relationships between variables, only \nif those relationships have been intentionally included \nin the statistical model that generated the data,11 and \nthus, may be poorly suited to certain AI models that \ndiscover unanticipated relationships among data. \nWhile homomorphic encryption may not require similar \nassumptions on data structure, existing methods are \ncomputationally expensive.\nWhile promising, understanding and applying these \nmethods is an evolving scientific process. The NRC is \npoised to contribute to their evolution by directly \nsupporting research into their application. \nCRITERIA AND PROCESS FOR \nADOPTION\nThe NRC will contain a rich array of datasets, each \npresenting unique privacy implications over different \ntypes of data formats (e.g., individual tabular records, \nunstructured text, images). Including a dataset on the \nNRC raises a question of choice: which technical privacy \ntreatment should be applied (e.g., k-anonymity vs. \ndifferential privacy), and how should it be applied? This \nquestion often requires technical determinations about \ndifferent algorithmic settings, but such technical choices \ncan also have important substantive consequences.12\nFirst, we recommend that these determinations are \nmade with respect to the following factors: \n• Dataset sensitivity: Different datasets will \npose privacy risks that range in type and \nmagnitude. Health records, for instance, are \nmore sensitive than weather patterns. The \nprivacy method chosen should reflect this \nsensitivity. As we discuss in Chapter Three, these \nprivacy methods should correspond and be tiered \nA Blueprint for the National Research Cloud 63\nCHAPTER 6\nto the appropriate FedRAMP classification for the \ndataset.\n• Dataset utility: As discussed above, applying \na privacy method can distort the original data, \ndiminishing the accuracy and utility of analysis. \nBecause different methods affect different levels \nof distortion, the choice of method should be \ninformed by the perceived utility of the data. \nHigh-utility datasets—where accurate analyses \nare highly important (e.g., medical diagnostic \ntools)—may necessitate methods which produce \nless distortion. \n• Equity: Certain privacy measures can \ndisproportionately impact underrepresented \nsubgroups in the data.13 In determining which \nmethod to apply, the presence of sensitive \nsubgroups and their relation to the objectives of \nthe dataset should be evaluated. \nFor any given dataset, we recommend that agencies \nproviding the data collaborate with NRC staff to identify and \nrecommend any privacy treatments. Originating agencies \nand NRC staff will possess domain and research expertise \nto make evaluations on the balance of privacy, utility, and \nequity, but should consult with NRC staff and researchers \non the most appropriate treatments. Given the cost of \nreview, such privacy treatments should be much less widely \nconsidered for low-risk datasets. \nVIRTUAL DATA SAFE ROOMS \nFor individual research proposals that would be greatly \nhampered by technical privacy measures, the NRC should \nexplore the use of virtual “data-safe rooms” that enable \nresearchers to access raw administrative microdata in a \nsecure, monitored environment. Currently, the Census \nBureau implements these safe rooms in physical locations \nand moderates access to raw inter-agency data through \nits network of Federal Statistical Research Data Centers \n(FSRDCs). However, the NRC should not adopt the FSRDC \nmodel wholesale. Indeed, the barriers to using FSRDCs \nare high, and “only the most persistent researchers are \nsuccessful.”14 For instance, applying for access and gaining \napproval to use an FSRDC takes at least six months, requires \nobtaining “Special Sworn Status”, which involves a Level \nTwo security clearance, and is limited to applicants who \nare either U.S. citizens or have been U.S. residents for \nthree years.15 To further complicate matters, agencies \nhave different review and approval processes for research \nprojects that wish to access agency data using an FSRDC.16\nFinally, even after approval is granted, researchers can only \naccess the data in person by going to secure locations, such \nas the FSRDC itself.17 \nTo be clear, some of these restrictions are unique to the \nCensus Bureau. U.S. law provides that any Census datasets \nthat do not fully protect confidentiality may only be used \nby Census staff.18 Researchers trying to access such data \ntherefore must go through the rigorous process of becoming \na sworn Census contractor. The extent that these restrictions \napply to the NRC will depend on whether the NRC \ninstitutionally houses itself in the Census Bureau, which we \nultimately do not recommend.19 Other problems, however, \nsuch as the lack of inter-agency uniformity in granting \naccess to datasets is not a problem unique to Census, but a \ncommon problem throughout the federal government (see \nChapter Three).\nAnother common problem––not necessarily tied to \nFSRDCs or the Census Bureau––is the use of a physical data \nroom to access raw microdata. The NRC should explore a \nvirtual safe room model, whereby researchers can remotely\naccess such microdata. For instance, in the private sector, \nthe nonpartisan and objective research organization, NORC, \nlocated at the University of Chicago, is a confidential, \nprotected environment where authorized researchers can \nsecurely store, access, and analyze sensitive microdata \nremotely.20 Some federal government agencies have also \nimplemented their own virtual data safe rooms. The Center \nfor Medicare and Medicaid Services’ Virtual Research Data \nCenter (VRDC), for instance, grants researchers direct \naccess to approved data files through a Virtual Private \nNetwork.21 In a 2019 Request for Information (RFI), the \nNational Institute of Health also solicited input for its own \nadministrative data enclave and whether such an enclave \nshould be physical or virtual.22 As articulated in responses \nto the RFI from the American Society for Biochemistry and \nMolecular Biology and the Federation for of American \nA Blueprint for the National Research Cloud 64\nCHAPTER 6\nSocieties for Experimental Biology, a virtual enclave would \ngreatly facilitate researcher access to data and can be \ndesigned and administered in a way to preserve privacy \nand security.23\nA National Research Cloud cannot function effectively \nif access to certain datasets is ultimately tied to a National \nResearch Room. \nCASE STUDY: \nCALIFORNIA POLICY LAB \nThe California Policy Lab (“CPL”) is a University \nof California research institute that provides \nresearch and data support to help California state \nand local governments craft evidence-based \npublic policy.24 CPL offers a variety of services to \ngovernments, including data analysis services and \nsecure infrastructure for hosting and linking the vast \namounts of data collected by government entities.25\nThese services help bridge the gap between \nacademia and government by helping policymakers \nget access to researchers and providing researchers \na secure way to access administrative data. CPL \naims to build trusting partnerships with government \nentities and enable them to make empirically \nsupported policy decisions.\nCPL enters data-use agreements with various \ngovernment entities around California, including, for \nexample, the California Department of Public Health \nand Los Angeles Homeless Services Authority.26\nThese agreements allow CPL to store administrative \ndata in a linkable format, promoting broad \nlongitudinal analyses across various public sector \ndomains.\nTo help manage the requirements of the various \ndata-use agreements and simplify compliance, \nCPL applies the strictest requirements for any \nindividual data across all data it stores.27 Each set of \nadministrative data is thus subject to strict technical \nrestrictions and thorough audits.28 CPL manages the \ndata in an on-premises data hub at UCLA. This data \nhub uses “virtual enclaves” modeled after air-gapped \nclean rooms typically used for sensitive government \ndata.29 Virtual enclaves are virtual machines that \nforbid any outbound connections.\nCPL creates a \nnew virtual enclave for \neach research project \nand only gives specific \nresearchers access \nto specific datasets \nfor each project.30\nResearchers can only \nwork with the data in \nthe enclave and can \nonly use tools provided \nin the environment. \nData access processes \nvary, based on the \nrequirements for the \ngovernment entities, and \nmost of CPL’s data-use \nagreements are purpose \nlimited and thus, require \napproval from the \nrelevant government \nentity, before being used \nin a project.31\nGenerally, CPL helps researchers understand how \nto gain access to various types of administrative data. \nFor some datasets, CPL has formalized applications \non its website.32 CPL prescreens project proposals and \nsends promising projects to its government partners \nfor final approval. Researchers then conduct these \napproved projects on CPL’s secure infrastructure. For \nother datasets without formalized access processes, \nCPL directs researchers towards individuals within \nthe government entities.33 CPL can then take over \nmanagement of approved projects that aim to use \ndata stored on its hub under their standing data-use \nagreements. Alternatively, the government entities \nand researchers themselves may craft new data-use \nagreements for specific projects.\nKEY TAKEAWAYS\n Virtual enclaves: \nThe CPL heavily \nutilizes secure \nvirtual enclaves \nfor researchers to \naccess, work with, \nand perform data \nlinkages across \nsensitive datasets. \n Acting as an \nintermediary: CPL \nfacilitates and \nstreamlines access \nto administrative \ndata by acting as \nan intermediary \nbetween researchers \nand relevant state \nagencies.\nA Blueprint for the National Research Cloud 65\nCHAPTER 6\nIMPLICATIONS FOR THE NRC\nDEDICATED STAFF\nAs discussed above, it will be critical for the NRC to \nmaintain a dedicated professional staff who specialize in \nprivacy technologies. First, not all agencies or departments \nwhich seek to place data into the NRC will have the expertise \nto both determine the privacy method that meets data \nutility expectations and data privacy demands, and apply \nit to the dataset of interest. Specialized NRC staff will be \nessential to assisting such agencies and departments. \nSecond, even where agencies and departments do \npossess the requisite expertise, NRC staff will bring a \nunique perspective from their collaborations across the \ngovernment. Where a specific department’s staff may \nonly foresee risks specific to the dataset, NRC staff will \nbe able to foresee instances where the presence of other \ndata in the NRC may raise other concerns. In fact, by \nworking with Affiliated Government Agencies and agency \nrepresentatives, the NRC staff can also help these agencies \ninternalize such benefits as helping them understand the \nfull range of privacy risks with respect to their data.34 Such \ncollaborative governance will be necessary to ensure that \nprivacy assessments consider the full implications of access \nand privacy technologies. Finally, it must not be overlooked \nthat while data management in general requires technical \nexpertise, these various privacy-enhancing technologies \nalso require very specific, highly skilled expertise. Using \nsynthetic data sets as an example, NRC staff could be asked \nto build synthetic data on an agency’s behalf, or need \nto validate the work performed at an agency to ensure \nit is done properly and well. Whatever the task, there \nare cascading effects downstream through the research \necosystem, if not carefully managed and executed.\nA FOCUS ON EVALUATING AND RESEARCHING PRIVACY \nENHANCING TECHNOLOGIES\nIt will be necessary to continually evaluate the \nstate of privacy protections on the NRC, either by NRC \nstaff members or by supporting privacy and security \nresearchers at academic institutions. Technical privacy \nand security research is by nature adversarial: researchers \nadopt the posture of adversaries in order to probe the \nweaknesses of a system/dataset. In the context of the \nNRC, this will require simulating attacks as researchers \ntry to reidentify individuals within specific NRC datasets. \nThis type of research is necessary to advance the field, \nand the NRC may be specially positioned to support a \nresearch center devoted to researching privacy-enhancing \ntechnologies. Doing so would allow the research \ncommunity to build stronger privacy methods to ensure \nanonymity, identify flaws, and self-regulate an evolving \ndata ecosystem.\nA National Research Cloud cannot \nfunction effectively if access to \ncertain datasets is ultimately tied \nto a National Research Room. \nA Blueprint for the National Research Cloud 66\nCHAPTER 7\nChapter 7: Safeguards \nfor Ethical Research \nThe pace of advances in AI has sparked ample debate about the principles that \nshould govern its development and implementation. Despite the technology’s promise \nfor economic growth and social benefits, AI also poses serious ethical and societal risks. \nFor example, studies have demonstrated AI systems can propagate disinformation,1\nharm labor and employment,2 demonstrate algorithmic bias along age, gender, race, and \ndisability,3 and perpetuate systemic inequalities.4\nThis chapter considers how the NRC should ensure its resources are deployed \nresponsibly and ethically. A growing body of research on AI fairness, accountability, and \ntransparency has raised serious and legitimate questions about the values implicated by \nAI research and its impact on society.5 The NRC’s focus on increasing access to sources of \npublic data and fostering non-commercial AI research is intended to help address these \nconcerns by enabling broader opportunities for academic research. At the same time, \nbroadening access to resources is not enough to assure that academic AI research does \nnot exacerbate existing inequalities or perpetuate systematic biases. In addition, the NRC \nmust also be prepared to handle and act upon complaints of unethical research practices \nby researchers. \nWhile there is an abundance of proposed ethics frameworks (see Appendix C for those \npublished by federal agencies) for AI, there is not a set of accepted principles enshrined \ninto law, like the Common Rule for human subjects research, that clearly establishes the \nboundaries for ethical research with AI.6 Lacking such guidance, a core question for the \nNRC is how to institutionalize the consideration of ethical concerns. This chapter starts by \ndiscussing two potential approaches for research proposals: ex ante review at the proposal \nstage for access to NRC resources (e.g., compute, dataset), and ex post review after \nresearch has concluded. Separately, we discuss guidance for the NRC on issues related to \nresearch practices. One of the virtues of starting with access by Principal Investigator (PI) \nstatus (Chapter Two) is that researchers will (a) often have undergone baseline training by \ntheir home institutions in research compliance, privacy, data security, and practices for \nresearch using human subjects; and (b) be subject to research standards and peer review \n(e.g., through IRB review when applicable). These mechanisms are insufficient to cover \nmany AI research projects, such as when human subjects review is deemed inapplicable. \nThus, we tailor our recommendations to the institutional design of the NRC. \nFirst, we recommend that the NRC require including an ethics impact statement for PIs \nrequesting access beyond base-level compute, or for research using restricted datasets. \nThis provides a layer of ethical review for the highest resource projects that are already \nrequired to undergo a custom application process. Second, for other categories of research \n(e.g., research conducted under base-level compute access, where no custom review is \nKEY \nTAKEAWAYS\n Researchers requesting \naccess to compute \nbeyond the default \nallocation and/or \nrestricted data (i.e., \nthose undergoing a \ncustom application \nprocess) should be \nrequired to provide an \nethics impact statement \nas part of their \napplication.\n The NRC should \nestablish a process \nto handle complaints \nabout unethical research \npractices or outputs. \n Eligibility based on \nPrincipal Investigator \nstatus will ensure \nsome review under the \nCommon Rule as well \nas through peer review, \nbut we recommend \nuniversities consider \nmore comprehensive \nmodels for assessing the \nethical implications of AI \nresearch.\nA Blueprint for the National Research Cloud 67\nCHAPTER 7\ncontemplated), we recommend that the NRC establish \na process for handling complaints that may arise out of \nunethical research practices and outputs. Third, given \nthe limitations of the prior mechanisms, we recommend \nthe exploration of a range of measures to address ethical \nconcerns in AI compute, such as the approach taken by the \nNational Institutes to Health to incentivize the embedding \nof bioethics in ongoing research. \nETHICS REVIEW MECHANISMS\nEX ANTE\nEx ante review assesses research yet to be performed.7\nFunding agencies and research councils worldwide rely on \nex ante peer reviews to evaluate the intellectual merit and \npotential societal impact of research proposals, based on \nset criteria.8 Institutional Review Boards (IRBs) commonly \nassess academic research involving human subjects prior \nto its initiation.9 However, much AI-related research may \nnot fall under IRB oversight, as the research may not use \nhuman subjects or rely on existing data (not collected by \nthe proposers) about people that is publicly available,10\nused with permission from the party that collected the \ndata, or is anonymized. Potential ethical issues may, \ntherefore, escape IRB review.11\nCreating an across-the-board ex ante ethics review \nprocess, however, would be challenging. First, as we \ndiscuss in Chapter Two, we recommend against case-by\u0002case review for all PI requests for access to NRC compute \nand data, as such a process would require substantial \nadministrative overhead. At the stage when researchers \nare simply applying for compute access, the research \nmay be so varied and early stage, that there is not much \nconcrete to review. And to the extent that every PI would \nrequire project-specific review, such a process would be \nonerous. \nSecond, ex ante review is unlikely to grapple with the \nmany ethical implications of design decisions that take \nplace after research commences.12 Research design can \nchange substantially from initial proposals as projects \nprogress. Ex ante review could identify some concerns, \nbut unlikely all.13 The nature of machine learning itself is \ninherently uncertain—and predictions can be challenging \nto explain—as well as highly dependent on the data used \nto build and train models.14 Ex ante proposal review alone \nmay not be sufficient to identify biased outcomes, and \nmay in fact require extensive documentation and review \nof the data used in a specific project to assess with any \nreliability.15\nThird, there are unique academic speech concerns \nabout government assessment of research. Authorizing the \ngovernment to conduct an ethics review (separate from \nIRB review under the Common Rule, which is typically \ndelegated to academic institutions) with vague standards \nmay implicate academic speech concerns, as well as \nsubject proposals to politically driven evaluation that can \nshift from administration to administration.\nIf the NRC were to create a review process for ex ante \nreview of research proposals for ethical concerns, such \na board would likely need to be composed of scientific \nand ethics experts, similar to how the NSF conducts their \nprocess, though perhaps with the addition of members \nfrom civil society organizations that focus on countering \nAI harms. The NSF convenes groups of experts from \nacademia, industry, private companies, and government \nagencies as peer reviewers, led by NSF program officers \nand division directors.16 However, the scope and range \nof NRC research proposals are likely to be both broad, as \nwell as highly interdisciplinary in nature, making ethics \nassessments challenging. \nEX POST\nEx post evaluations provide an assessment after \nresearch has concluded.17 In academia, researchers \nsubmit research results to journals or conferences for ex \npost peer-review; it is at this pre-publication stage that \nethical issues not identified by ex ante processes may be \nsurfaced by reviewers or editors. In the public sector, for \nexample, the Privacy and Civil Liberties Oversight Board \n(PCLOB) conducts ex post reviews on counterterrorism \npractices by executive branch departments and agencies \nto ensure they are consistent with governing laws, \nregulations, and policies regarding privacy and civil \nliberties.18 PCLOB has also recently begun to evaluate the \nA Blueprint for the National Research Cloud 68\nCHAPTER 7\nuse of new technologies in foreign intelligence collection \nand analysis,19 and identify legislative proposals might \nstrengthen its oversight of AI for counterterrorism.20\nRECOMMENDATIONS\nWhile we do not recommend across-the-board ex \nante review of research proposals, we do recommend that \nthe NRC establish a process to handle complaints about \nethical research practices and outputs. On that point, \nwe recommend the NRC collaborate with the Office of \nResearch Integrity (ORI) at the Department of Health and \nHuman Services to model their processes and procedures \nfor managing issues of research misconduct.21 The ORI \nhas substantial experience overseeing concerns about \nethical research practices. Parties could petition the NRC \nto revoke access when research is shown to manifestly \nviolate general ethical research standards or practices \napplicable to a researcher’s disciplinary domain. We note \nthat the NRC may want to adopt a high standard for such \na violation, given the academic speech considerations. For \nexample, federal agencies or external parties that wish to \nrevoke compute or data access from PIs would need to file \na written complaint, with supporting evidence. Decisions \nto revoke access should require input from NRC executive \nleadership and legal counsel. \nFor PIs requesting access beyond base-level compute \nor for restricted datasets, we recommend requiring the \ncompletion of ethics impact statements to be submitted \nwith research proposals. A recent proposal to address \nthe lack of “widely-applied professional ethical and \nsocietal review processes” in computing piloted such a \nrequirement in a grant process, requiring a description of \nthe potential social and ethical impacts and mitigation \nefforts by researchers.22 We limit this approach to \nproposals for compute access beyond default allocation \nor requests for access to restricted datasets, as the \nadministrability concerns are weaker for researchers \nthat are already applying for compute or data access \nbeyond the default levels. For those applications, a \nreview process of a specific proposal will already occur \nby an external review panel of experts (Chapter Two), \nand, much like the NSF requires statements of “Broader \nImpacts;”23 statements about the ethical considerations \nof the work could easily be included. It is important \nto note that ethics impact statements would be only \none component to NRC applications and should be \nweighed in conjunction with other application materials. \nIn addition to requiring researchers to carefully think \nthrough and document the potential impacts of their \nown work, the statements may also serve as useful \ndocumentation of potential negative impacts and be of \nuse to NRC staff when determining whether to provide \naccess to specific types of data. Such assessments may \nalso be helpful for journals, conferences, or universities \naddressing ex post concerns about ethical impacts. \nNext, we recommend that the NRC employ a \nprofessional staff devoted to ethics oversight, similar \nto what we propose regarding data privacy in Chapters \nFive and Six. In addition to staff devoted to handling \nlegal compliance issues, the NRC needs staff with \nspecialized training in AI ethics (as well as expertise in \nother subdomains) to provide expert internal consulting \nto NRC applicants, as well as to aid in evaluating ethics \nimpact statements. Similarly, data privacy experts can \nidentify ethical privacy issues specifically related to data, \nsuch as whether consent has been properly obtained \nand documented. To ensure that decisions are based on \nthe merits, the NRC staff overseeing these issues must \noperate independently of other federal agencies and be \ninsulated from political interference. \n[E]mbedded ethics approaches \nmay . . . identify[] and address[] \nissues as the research proceeds, \nin contrast to ex ante review, \nwhere it may be too early to spot \nan issue, and ex post review, \nwhich may be too late.\nA Blueprint for the National Research Cloud 69\nCHAPTER 7\nWe acknowledge that these ethics review mechanisms \nmay not identify all instances where researchers use \nthe NRC in a way to conduct research that raises ethical \nquestions. Few review mechanisms could, particularly in \nlight of considerable ambiguity, present in ethics standards \n(see Appendix C). Nonetheless, these mechanisms can \naugment key academic checkpoints (IRB review and peer \nreview) in an administrable fashion that does not raise \nserious concerns about academic speech. \nLastly, we recommend that non-NRC parties explore \na range of measures to address ethical concerns in AI \ncompute. These may include an ethics review process or \napproaches widely deployed in bioethics by the National \nInstitutes of Health, namely to incentivize the embedding \nof ethicists in research projects.24 Such embedded \nethics approaches may have the particular advantage \nof identifying and addressing issues as the research \nproceeds, in contrast to ex ante review, where it may be \ntoo early to spot an issue, and ex post review, which may \nbe too late. We expect this to be an active area of inquiry \nas new approaches are validated. The NRC, potentially in \nconjunction with the NSF, should consider offering funding \nfor projects that embed ethics domain experts into teams, \nin order to support this proposal.\nA Blueprint for the National Research Cloud 70\nCHAPTER 8\nChapter 8: Managing \nCybersecurity Risks \nWhile the NRC has the potential to level the \nplaying field for AI research, it will also create an \nalluring target for a vast array of bad actors. \nCybersecurity – the practices to protect \nsystems against incidents that may compromise \noperations or cause harm to relevant assets \nand parties – will be a critical focus of the NRC. \nIt will require a cybersecurity framework that \nmanages potential incidents throughout their \nlifecycle, spanning: (1) preparation; (2) detection \nand analysis; (3) containment, eradication, and \nrecovery; and (4) post-incident activity, which \ncollectively encompasses incident monitoring, \ndetection, recovery, and reporting.1\n Effective \ncybersecurity practices complement risk \nassessment based on impact, immediacy, and \nlikelihood, and will help gain the trust of users and thwart subversion and interference \nfrom foreign actors or other adversarial parties. Careful administrative design of the \nNRC with cybersecurity at the forefront will set a high standard as information systems \nbecome more central to our national infrastructure.\nIn this chapter we address these cybersecurity concerns. We first provide an \noverview of common types of vulnerabilities and attacks, and assess their relevance \nto the NRC. Next, we provide an overview of the federal government’s regulatory \nlandscape, as it pertains to cybersecurity, with a special focus on the FISMA and \nFedRAMP frameworks. Finally, we close with a discussion of the security and system \ndesign measures best suited to ensure that the integrity of the NRC is not compromised.\nMOTIVATIONS FOR POTENTIAL ATTACKS\nPossible attacks against the NRC could take a number of approaches, each \nof which would entail substantial consequences for the NRC.2 First, adversaries \ncould launch an attack against the NRC with the intention of disrupting the NRC’s \noperations or its ability to aid research. For example, adversaries could attack the \nNRC’s infrastructure directly by disabling or interfering with NRC servers. As a result, \nresearchers would be unable to access NRC servers or effectively utilize them. By \nlaunching such attacks, adversaries may throttle the NRC, thereby raising costs for \nthe federal government.3\n Alternatively, adversaries could seek to attack specific \nKEY \nTAKEAWAYS\n Deterring malicious actors \nfrom attacking the NRC will \nrequire more than adhering \nto current FISMA and \nFedRAMP standards.\n The NRC should centralize \nsecurity responsibilities for \ndatasets with the program’s \nstaff rather than deferring \nto originating agencies.\n Technical measures the \nNRC should investigate \ninclude confidential \nclouds, federated learning, \nand cryptography\u0002based measures such as \nhomomorphic encryption \nand secure multi-party \ncomputation.\nWhile the NRC \nhas the potential \nto level the \nplaying field for AI \nresearch, it will also \ncreate an alluring \ntarget for a vast \narray of bad actors.\nA Blueprint for the National Research Cloud 71\nCHAPTER 8\nresearch projects on the NRC, thereby slowing the pace of \nthat research or compromising the quality of the research \nfindings. They may also initiate “data-poisoning” attacks \non NRC datasets, thereby compromising the quality of \nresearch findings. \nSecond, bad actors could also launch cyber operations \nagainst the NRC, intending to steal computational \nresources. In this case, the purpose would not be to disrupt \nthe NRC, but to repurpose computational power toward \nillicit purposes (e.g., cryptocurrency mining).4 For instance, \nindividuals could pretend to be researchers, claiming to \nuse cloud credits for legitimate research purposes while \nactually using them for alternative ends. Individuals \ncould also infiltrate the NRC’s network, siphoning off \ncomputational resources from other projects and reducing \nthe functionality for legitimate users. \nThird, adversaries might pose a threat to the NRC out \nof a desire to steal or make use of the data and research \nproducts housed within the system. The NRC promises \nto be an attractive target because it will house data from \na range of different agencies. If the adversary wanted to \nsteal equivalent data from the agencies themselves, they \nwould need to break into each agency independently. \nHowever, the potential combination of datasets on the \nNRC, including researcher-owned datasets, may increase \nthe potential gains from accessing this information. \nAdditionally, adversaries may attempt to break into \nthe NRC in order to steal products generated by NRC \nresearchers. This could include trained machine-learning \nmodels or specific research findings.\nRelatedly, bad actors could determine that executing \nintrusions into the NRC is an effective way to target \nAffiliated Government Agencies. Because a participation \nincentive for agencies is the computing support that \nthe NRC will offer, one of the biggest cyber risks is of \nmalicious actors attempting to use the NRC to hack into \ntheir systems. For that reason, the cybersecurity risk to \nthe government may be substantial. On the other hand, as \nwe discussed in Chapter Three, the NRC also presents an \nopportunity to enhance and harmonize security standards \ncompliance, as agencies move into the cloud. \nA range of other motivations may exist. As a federal \nentity, successful operations against the NRC would carry \nsymbolic value and capture attention. Ransomware \nattacks could result in significant payoffs. The NRC could \nalso be a target for espionage, both on the part of nation\u0002state actors seeking to acquire sensitive datasets (e.g., \nenergy grid infrastructure), and on the part of private \nsector entities, looking to steal intellectual property or to \nmonitor the latest technological advances.\nIf successful, any of the attacks could undermine the \nNRC. For example, researchers would be deterred from \nusing the NRC and may invest their efforts in alternate \nprivate clouds. This could occur because researchers \nbelieve the NRC would be ineffective to use (e.g., on \naccount of frequent server outages), or because they \nbelieve their research products would be inadequately \nprotected. Federal agencies and departments could be \ndeterred from entrusting the NRC with sensitive datasets. \nFederal entities could risk embarrassment and face \nobstacles executing their policy objectives if datasets were \naccidentally leaked. If the NRC is insufficiently secure, such \nentities may prefer to avoid sharing data altogether.\nFISMA, FEDRAMP, AND EXISTING \nFEDERAL STANDARDS\nAs a federal entity, the NRC will be subject to federal \nstandards and regulations. In this section, we provide a \nhigh-level overview of the two most relevant regulations: \nthe Federal Information Systems Management Act (FISMA) \nand the Federal Risk and Authorization Management \nProgram (FedRAMP).5 FISMA traditionally applies to non\u0002cloud systems that support a single agency, whereas \nFedRAMP authorization is required for cloud systems.6\nWe finish by discussing critiques of these regulations. \nFISMA \nThe Federal Information Systems Management Act \n(FISMA) was first passed in 2002, with the purpose of \nproviding a comprehensive framework for ensuring the \neffectiveness of security controls for federal information \nsystems.7\n The law was later amended in 2014, and has \nA Blueprint for the National Research Cloud 72\nCHAPTER 8\nsince been augmented through other individual legislative \nand executive actions, and our discussion focuses on the \ncollective impact of FISMA compliance regulations.8\nFISMA applies to all federal agencies, contractors, \nor other sources that provide information security for \ninformation systems that support the operations and \nassets of the agency.9 It invests responsibility in several \ndifferent entities. First, the National Institute of Standards \nand Technology (NIST) is tasked with developing uniform \nstandards and guidelines for implementing security \ncontrols, evaluating the riskiness of different information \nsystems, and other methodologies.10 Second, the Office of \nManagement and Budget (OMB) is tasked with overseeing \nagency compliance with FISMA and reporting to Congress \non the state of FISMA compliance.11 Third, the Department \nof Homeland Security is tasked with administering the \nimplementation of agency information security policies \nand practices.12 Finally, federal agencies are required \nto develop and implement a risk-based information \nsecurity program in compliance with NIST standards and \nOMB policies.13 Agencies are further required to conduct \nperiodic assessments to ensure continued efficiency and \ncost effectiveness.14\nSeveral NIST requirements are worth mentioning \nhere. Pursuant to NIST SP 800-18, agencies are required \nto identify relevant information systems falling under the \npurview of FISMA. Agencies must also categorize each of \nthese systems into a risk level, following the guidance laid \nout in FIPS 199 and NIST 800-60.15 NIST 800-53 outlines \nboth the security controls that agencies should follow \nand the manner in which agencies should conduct risk \nassessments.16 Agencies must further summarize both \nthe security requirements and implemented controls \nin “security plans”, as outlined in NIST 800-18.17 Finally, \norganization officials are required to conduct annual \nsecurity reviews in accordance with NIST 800-37. \nFEDRAMP \nIn the late 2000s, federal agencies began expressing \nsecurity concerns as a barrier to cloud computing \nadoption.18 In response, Congress passed the 2011 Federal \nRisk and Authorization Management Program (FedRAMP) \nin order to provide a cost-effective, risk-based approach \nfor the adoption and use of cloud services by the federal \ngovernment.19 FedRAMP approval is exempted where: (i) \nthe cloud is private to the agency, (ii) the cloud is physically \nlocated within a federal facility, and (iii) the agency is not \nproviding cloud services from the cloud-based information \nsystem to any external entities.20 Like FISMA, FedRAMP \nsecurity requirements are governed by NIST standards, \nincluding NIST SP 800-53, FIPS 199, NIST 800-37, and \nothers.21 However, unlike FISMA, FedRAMP’s two tracks \nto receiving an authority-to-operate means that vendors \nworking with multiple agencies do not necessarily need to \nundergo the full approval process with each agency. This \nmeans that cloud services providers and agencies alike are \nable to save significant time and money.\nCRITICISMS OF FISMA AND FEDRAMP \nThese regulations are not without fault. Most notably, \ncritics point to the fact that despite their existence, cyber \nintrusions on government infrastructure are common \nand accelerating.22 A 2019 report by the U.S. Senate \nCommittee on Homeland Security and Governmental \nAffairs, investigating eight agencies, noted that the \nfederal government is failing its legislative mandate \nfrom FISMA.23 The errors identified included a failure to \nprotect personally identifiable information, inadequate \nIT documentation, poor remediation of bugs, a failure \nto upgrade legacy systems, and inadequate authority \nvested in agency chief information officers.24 Reports by \nthe Government Accountability Office (GAO) have reached \nsimilar conclusions.25 In turn, some have criticized the \ngovernment’s approach to cybersecurity wholesale, \narguing it places too much emphasis on merely detecting \nintrusions.26 They argue for a framework of “zero trust,” \nwhich assumes that intruders will penetrate a network and \ninstead focus on security controls which limit the ability of \nthose intruders to navigate the network.27\nFedRAMP faces its own criticisms. A recent study noted \nthat securing authorization can be time-consuming and \nexpensive—taking up to two years and costing millions \nof dollars in some cases.28 Even though parts of FedRAMP \nare designed to be reusable across agencies, agencies \noften delay the process by imposing separate, additional \nA Blueprint for the National Research Cloud 73\nCHAPTER 8\nrequirements. A variety of reasons for these deficiencies \nhave been noted, including an understaffed Joint \nAuthorization Board, a lack of trust between agencies, \nwith regards to Authorization to Operate (ATOs), and \noverly complex authorization process that leads to errors \nby agencies and Cloud Services Providers.29 Proposed \nrecommendations to address these deficiencies include \nincreased funding for FedRAMP’s Joint Authorization \nBoard, incentives to encourage reuse of ATOs, and \nmechanisms to improve the efficiency of the authorization \nprocess.30\nOn May 12, 2021, the Biden administration released \nan Executive Order (EO) on Improving the Nation’s \nCybersecurity,31 and OMB published a draft federal strategy \nfor public comment on September 7, 2021.32 Signed \nin the aftermath of the breach of the software vendor, \nSolarWinds, and the ransomware attack on Colonial \nPipeline, the EO presents several new initiatives. First, it \ncalls on the federal government to embrace “zero-trust \narchitecture” and improve post-attack investigation \nprocesses. Second, it seeks to improve collaboration \nbetween the public and private sectors by improving \ndisclosure requirements and establishing a private\u0002public Cybersecurity Safety Review Board (modeled \nafter the National Transportation Safety Board). Finally, \nit seeks a more cohesive government-wide approach \nto cybersecurity, calling for the creation of a playbook \nto standardize cyber response across federal agencies, \nalongside a government-wide detection and response \nsystem for attacks. \nThough it is too soon to determine whether the EO \nand the proposed strategy will be effective, it appears to \naddress deficiencies identified in the existing landscape. \nIt seeks to improve documentation and responsiveness \nto attacks and suggests a shift in cybersecurity thinking. \nIt is unclear, however, whether it will address the \nunderlying procurement issues and lack of inter-agency \ntrust that critics believe have hampered the effectiveness \nof FedRAMP. But given the potential for highly sensitive \ndata to be stored on the NRC, embracing a zero-trust \narchitecture at the outset is a crucial consideration for \nensuring its integrity.\nNRC SECURIT Y STANDARDS AND \nSYSTEM DESIGN MEASURES\nHere, we present recommendations on cybersecurity \npolicy for the NRC informed by the landscape of the \nexisting federal regulations and unique considerations that \na national research cloud will pose.\nPROCESS FOR RISK AND SECURITY DETERMINATIONS\nUnder the current regulatory landscape, agencies \nare responsible for determining the appropriate risk \ncategorizations and security controls for the datasets \nlocated on their servers. However, this raises a potential \nchallenge as agencies begin to share their data with the \nNRC—making it unclear who will maintain authority for \ncategorizing the risk of these datasets and determining \nappropriate security controls. \nOn the one hand, agencies themselves could continue \nto retain discretion over the security classification \nand controls for datasets they place into the NRC. In \nthis decentralized approach, much of the security \nresponsibilities assigned by FISMA would remain with \nthe agencies, irrespective of whether the data existed \non NRC servers. On the other hand, the NRC could take \nresponsibility for all security decisions. Datasets added to \nthe NRC would then be classified according to the NRC’s \nassessment of risk, and protected with controls that the \nNRC staff deems appropriate. This approach “centralizes” \nsecurity responsibilities by vesting it with the NRC after the \none-time negotiation for each dataset.\nThough both approaches have their merits, we \nrecommend the centralized approach for several reasons. \nFirst, the centralized approach ensures internal uniformity. \nThe paradox of federal cybersecurity regulation is that \nthough NIST has articulated a set of standards pertaining \nto risk and controls, agencies interpret these standards \ndifferently, leading to discrepancies in implementation and \nclassification across the federal government. Following \neach agency’s security classifications for data on the NRC \nwould produce unnecessarily complex and incoherent \nclassifications for a single system. This threatens \nto diminish the usability of the NRC, and the added \nA Blueprint for the National Research Cloud 74\nCHAPTER 8\ncomplexity could arguably weaken security by increasing \nthe likelihood of errors. Permitting the NRC to impose its \nown classifications allows for uniformity within the NRC \nand alignment with the access tiers suggested in Chapter \nThree of this White Paper. This approach may also simplify \nmanaging security practices across a potential mix of \ncloud compute providers.\nSecond, the NRC represents a valuable opportunity \nto harmonize federal cybersecurity standards across \ndifferent agencies. The assessments and implementations \nadopted by the NRC must generalize to the full diversity of \nfederal datasets. Hence, the NRC’s practices can serve as \na template for NIST’s guidelines, which any agency is free \nto adopt. \nThird, the centralized approach will remove hurdles \nfor data sharing. Security concerns often impede agency \ndata sharing. In a scheme where agencies retain control \nover all security determinations, agencies could demand \nsecurity classifications that are excessively high or \nimpractical to implement. The centralized approach \nwould place the burden on agencies to articulate with \nspecificity why the NRC’s security policies or classification \nguidelines are inadequate for a particular dataset. \nFinally, researchers should also have a voice in \ndetermining the appropriate security controls, since a \npublic resource of this magnitude that cannot attract users \nis bound to fail. As security controls implicate usability, \nthe NRC should not opt for controls which substantially \ninhibit or disincentivize researchers from leveraging its \nresources. The NRC needs to strike the right balance \nbetween usability and security.\nTECHNICAL CONSIDERATIONS\nThe federal government already possesses a range of \ntechnical options and countermeasures to cyber attacks. \nCybersecurity threats and defenses are, of course, actively \nevolving, so we discuss these only as a starting point—\nrobust, long-term cybersecurity comes through continued \nvigilance and prioritization that recognizes the shifting \nnature of the field. \nDATA STORAGE\nData storage mechanisms should ensure proper \nprotection from outside access. Encryption can be used \nto protect sensitive data at rest, to be later unencrypted \nwhen needed. Physical isolation through air-gapped \nenvironments is another design feature that can remove \nthe possibility of wireless network interfaces from being \nused to connect the data to malicious outside threats. \nHowever, even air gapping is not a foolproof solution: there \nare ways to “jump” air gaps such as through hiding in USB \nthumb drives (which is allegedly how the Stuxnet malware \nfamously compromised Iranian nuclear centrifuges).33 More \nrecent attacks bypass the need for electronic transmission \naltogether by leveraging other signals that leak data \nsuch as FM frequencies, audio, heat, light, and magnetic \nfields. These kinds of threats bring home the need for a \ncomprehensive and evolving approach to cybersecurity. \nNETWORKING PROTOCOLS\nData packets sent over networks are transmitted \naccording to a set of internationally standardized internet \nprotocols. Following the Open Systems Interconnection \n(OSI) model, the conceptual layers involved in computer \nnetworking can be categorized into seven dimensions: \nphysical, data link, network, transport, session, \npresentation, and application layers.34 \nRUNTIME SECURITY\nWhen considering runtime security technologies, \nthree design features that are relevant for the cloud \nenvironments are the use of confidential clouds, \nfederated learning, and cryptography-based measures \nsuch as homomorphic encryption and secure multi\u0002party computation. A growing number of vendors offer \n“confidential cloud” options as an emerging technical \nsolution to fully cyber secure cloud computation that is \nsecure throughout execution.35 Confidential clouds offer \nhigh-security, end-to-end isolated operation by executing \nworkloads within trusted execution environments. For \nexample, virtualization enables an operating system \nto run another operating system within it as a virtual \nenvironment with additional firewall or other network \nA Blueprint for the National Research Cloud 75\nCHAPTER 8\nbarriers, effectively simulating another device within the \nhost computer. \nDISTRIBUTED COMPUTING AND FEDERATED LEARNING\nAnother computing paradigm, known as distributed \ncomputing or federated learning, considers situations \nwhere multiple parties have individual shards of data \nwhich they are interested in leveraging in aggregate, \nwithout sharing outright. Federated learning addresses \nthis situation, for example, demonstrating how users’ \nmobile phones can send information—possibly \ndifferentially private—to central servers without exposing \nthe precise details of any one individual’s information. \nA second scenario more relevant to the large-scale \ndecentralized nature of the NRC is distributed computing—\nin which many institutions collectively share compute, \nakin in some respects to crowd-sourced computing. These \napproaches enable multiple parties to leverage existing \ncomputational infrastructure, while retaining some \nguarantees on privacy. \nCRYPTOGRAPHY-BASED MEASURES\nFinally, there are two types of cryptography-based \nmeasures worth noting. \nCryptography researchers have developed ways of \ncomputing mathematical operations over encrypted data, \nknown as homomorphic encryption. This impressive \nfeat has valuable implications because it obviates the \nneed for decryption, which can potentially expose the \nintermediate values of computation, and grant access to \npublic and secret encryption keys during computation. \nInitially, only partially homomorphic encryption schemes \nthat supported limited arithmetic operations like addition \nand multiplication were possible. But fully homomorphic \nencryption schemes have recently been developed which \nenable what is known as “arbitrary” computation for \npromising use cases in predictive medicine and machine \nlearning. That said, standardization is still under way \nto broader adoption, and homomorphic encryption (by \ndesign) is malleable—a property in cryptography that \nis usually undesirable, as it allows attackers to modify \nencrypted ciphertexts without needing to know their \ndecrypted value. These and other limitations of any \ntechnical approach are worth taking into account when \nconsidering which technologies to adopt and for what \npurpose. \nComplementing the distributed, decentralized \ncomputing model discussed throughout this White Paper \nis the subfield known as secure multi-party computation \n(also known as privacy-preserving computation), which \npresents methods for multiple parties to jointly compute \na function over all their respective inputs, while keeping \nthose inputs private from any other party. These methods \nhave matured in their origins from a theoretical curiosity \nto techniques with practical application in studies on tax \nand education records, cryptographic key management \nfor the cloud, and more.36 This makes secure multi\u0002party computation methods a potential candidate \nfor applications pertaining to secure, distributed \ncomputation. \nUltimately, it will be central for the NRC to \ncontinuously learn about the most effective security \nstandards (including such other creative strategies as red \nteaming or bug bounties37 to identify vulnerabilities) in this \nrapidly evolving space.\nA Blueprint for the National Research Cloud 76\nCHAPTER 9\nChapter 9: \nIntellectual Property\nWho should own the IP rights to outputs developed using NRC resources?1 When \nprivate research is funded, subsidized, or influenced by the federal government, the \nlaws and rules have evolved, so that both the researcher and the government have \ncertain rights in the intellectual property developed under the research. While IP \nprotection is theoretically designed to incentivize research and innovation, some signs \nindicate that AI researchers in particular are already amenable to sharing the fruits \nof their research. Indeed, over 2,000 researchers signed a 2018 petition to boycott \na new machine intelligence journal started by Nature, because it promised to place \nits articles behind a paywall.2 The Open Science and Open Research movements \nhave also encouraged AI researchers to make their machine-learning software and \nalgorithms publicly available.3 Furthermore, as we discuss below, the advancement of \ntechniques like transfer learning depend on researchers being able to distribute the \nfruits of their research freely. \nThis chapter surveys the existing IP-sharing agreements between researchers and \nthe government, and explores whether and to what extent the government should \nretain IP rights over researchers’’ outputs, as a condition of using the NRC.4 While the \nevidence on optimal IP rights varies, we recommend that: (1) academic researchers \nand universities should retain the same IP rights as the Bayh-Dole Act provides for \npatents developed under federally funded research, (2) the government should retain \nits copyrights and data rights under the Uniform Guidance, but contract around \nthose rights where applicable to incentivize NRC usage and AI innovation, and (3) \nthe government should consider conditions for requiring researchers to share their \nresearch outputs under an open-access license.\nKEY \nTAKEAWAYS\n To harmonize with the \nfederal grant process, the \nNRC should adopt the same \napproach to allocating \npatent rights, copyrights, \nand data rights to NRC \nusers as applies to federal \nfunding agreements.\n The NRC should contract \naround government \nintellectual property \nrights where applicable to \nincentivize NRC usage and \nAI innovation.\n The NRC should consider \nconditions for requiring \nresearchers to share \noutputs under an open\u0002source license.\n[A]cademic researchers and universities should retain \nthe same IP rights as the Bayh-Dole Act provides for \npatents developed under federally funded research.\nPATENTS RIGHTS \nA core question is whether NRC users should retain patent rights in inventions \nsupported by the NRC. The Bayh-Dole Act regulates patent rights for inventions \ndeveloped under federal funding agreements and its applicability depends on the \nA Blueprint for the National Research Cloud 77\nCHAPTER 9\nnature of NRC access––for instance, if cloud credits are \napportioned using federal grants, as described in Chapter \nTwo, they may be considered federal funding agreements.5\nIn such cases, Bayh-Dole Act permits the researchers \nto hold the title to the patent and to license the patent \nrights.6 However, these patent rights come with certain \nrestrictions: for example, the funding agency has a free, \nnonexclusive license to use the invention “for or on behalf \nof the United States,” and the agency may use “[m]arch-in \nrights” to grant additional licenses.7\nThe broader policy question about the government’s \nexercise of its patent rights is whether and how patents \nstimulate innovation in AI. Some commentators have \nargued that the U.S. suffers from over-patenting in \nsoftware,8\n and AI is no exception.9 The total number of AI \npatent applications received annually by the U.S. Patent \nand Trademark Office more than doubled from 30,000 in \n2002 to over 60,000 in 2018,10 and some argue that this \nproliferation of broad AI patents, especially those filed by \ncommercial companies, is hindering future innovation.11\nIn the Bayh-Dole context, researchers have also found that \nthe benefits of university patenting may justify the costs \nonly where industry licensees need exclusivity to justify \nundertaking the costs of commercialization, as, for instance, \nin the pharmaceutical context.12 For the substantial portion \nof university patenting, including AI, this rationale may not \ncarry much weight.13\nSome research shows that patents actually may not \nhave any net effect on the amount or quality of AI research \nconducted in the university context. In an empirical study \nof faculty at the top computer science and electrical \nengineering universities in the United States, research \nhas found that the prospect of obtaining patent rights to \nthe fruits of their research does not motivate researchers \nto conduct more or higher-quality research.14 Eighty-five \npercent of professors reported that patent rights were \nnot among the top four factors motivating their research \nactivities, and fifty-seven percent of professors reported \nthat they did not know whether or how their university \nshares licensing revenue with inventors.15 The patent \nscheme adopted by the NRC, therefore, may not have a \nstrong influence on researcher adoption. \nThat said, as a practical matter, there is a virtue \nto treating innovations stemming from NRC usage in a \nfashion that is consistent with Bayh-Dole. Particularly \nif cloud credits are awarded through the expansion of \nprograms like NSF CloudBank, it would be confusing to \nhave distinct patent rights out of the research and cloud \ngrant. In addition, many university tech transfer offices \nappear to have strong preferences on patent rights.16 To the \nextent that universities today view retaining patent rights \nas a condition for using the NRC, aligning NRC patent \nrights with Bayh-Dole may be preferred, but the evidence \nunderpinning this recommendation is not strong. \nCOPYRIGHT, DATA RIGHTS, AND \nTHE UNIFORM GUIDANCE\nThe Uniform Guidance (2 C.F.R. § 200) streamlines and \nconsolidates government requirements for receiving and \nusing federal awards to reduce administrative burden.17\nGrants.gov describes it as a “government-wide framework \nfor grants management,” a groundwork of rules for federal \nagencies in administering federal funding.18 The Uniform \nGuidance includes provisions on, for instance, cost \nprinciples, audit requirements, and requirements for the \ncontents of federal awards.19\nThe Uniform Guidance is applicable to “federal \nawards,”20 but IP provisions do not require the government \nto assert their rights over researcher outputs.21 Whether \nand how the government allocates its IP rights under the \nUniform Guidance is therefore an important question.\nThis section first covers government copyright \nand data rights to IP under the Uniform Guidance and \nThe government should . . . \nconsider conditions for requiring \nNRC researchers to disclose or \nshare their research outputs under \nan open-access license.\nA Blueprint for the National Research Cloud 78\nCHAPTER 9\ndiscusses how sharing copyright and data rights might \nimpact the AI innovation landscape. We then examine \nthe extent to which the government should retain its \nrights to research generated using the NRC. While the \nevidence is mixed, we ultimately recommend that the \ngovernment retain its copyrights and data rights under \nthe Uniform Guidance, but contract around those rights \nwhere applicable, to incentivize NRC usage and further AI \ninnovation. \nCOPYRIGHT\nUnder U.S. copyright law, NRC researchers can obtain \ncopyrights over various aspects of their work. For instance, \nNRC researchers may wish to copyright the software they \nused to build the model, since software is considered a \nliterary work under the Copyright Act.22 Researchers may \neven obtain copyrights over various aspects of the model, \nincluding the choices of training parameters, model \narchitectures, and training labels, if they can show that \nthose choices required creativity.23 Many scholars have \neven opined, without reaching consensus, on whether \noutputs such as text and art that are artificially generated \ncan be copyrighted.24\nUnder the Uniform Guidance,25 the recipient of \nfederal funds may copyright any work that was developed \nor acquired under a federal award. However, even \nif researchers are permitted to maintain copyrights, \nthe federal awarding agency reserves a “royalty-free, \nnonexclusive and irrevocable right to reproduce, publish, \nor otherwise use the work for federal purposes, and to \nauthorize others to do so.”26 Notably, this right is limited to \n“federal purposes,” meaning that third parties who acquire \nlicenses to the researchers’ copyrighted works cannot use \nthem for exclusively commercial purposes.27\nIt is unclear to what extent copyrights over NRC \noutputs should be fully vested in the researcher to \nstimulate basic AI research. One class of AI research \nand development output that has received significant \nacademic attention has been whether AI-generated \ncreative works like music from OpenAI’s Jukebox,28 can \nor should receive copyright protection.29 However, the \ntechnology and copyright community has hardly reached \na consensus on whether the public interest in AI research \nrequires granting copyright in these scenarios. On one \nhand, in a survey of AI scientists, tech policy experts, and \ncopyright scholars, roughly 54 percent of respondents \nagreed that copyright protection is an important incentive \nfor authors to make their work commercially available, \nand 63 percent agreed that an increase in the number \nof commercially available AI-produced works would \nstimulate further AI growth and research.30 On the other \nhand, in the same survey approximately 56 percent of \nrespondents agreed that the U.S. Copyright Office should \ndeny copyright protection to creative works produced \nindependently by AI, without creative intervention from a \nhuman author.31\nNotwithstanding the prominent debate about \ncopyright over creative works generated by AI models, such \nworks are only a subset of possible copyright protection \nin the AI context. As discussed above, researchers could \ntheoretically seek additional copyright protection over, \namong other things, their code, architecture, or model. \nHere, AI innovation may depend on sharing these \ncopyrightable elements. For instance, transfer learning \nuses existing ML models and “fine-tunes” those models for \na related target task,32 and various fine-tuning approaches \nhave emerged to perform transfer learning on different \nclasses of tasks.33\nDATA RIGHTS\nUnder the Uniform Guidance, when “data” is \n“produced” under a federal award, the government \nreserves the right to: (1) obtain, reproduce, publish or \notherwise use such data; and (2) authorize others to \nreceive, reproduce, publish or otherwise use such data.34\nNotably, this does not limit the use of such data for \nfederal government purposes. In other words, such data \ncan be promulgated for any use. The outstanding question, \ntherefore, is whether this “data,” which is not explicitly \ndefined in the Uniform Guidance, covers data generated \nfor AI and machine-learning purposes. Below, we examine \ntwo classes of data generated for AI purposes—synthetic \ndata and data labels—and how sharing this data could \nimpact AI innovation. \nA Blueprint for the National Research Cloud 79\nCHAPTER 9\nOne class of data generated for AI purposes is \nsynthetic data. Researchers have turned to deep \ngenerative models such as Variational Autoencoders35 and \nGenerative Adversarial Networks36 to generate synthetic \ndata to train their machine learning models. As noted by \nthe World Intellectual Property Organization, synthetic \ndata is an entirely new class of data that does not fit \nneatly under existing IP law.37 While a researcher may seek \ncopyright protection over the subset of synthetic data that \nis “creative,” therefore implicating the copyright provisions \nof the Uniform Guidance (described above), the broad \nclass of synthetic data, whether “creative” or not, may also \nimplicate the data rights provision. On one hand, training \ndata is often carefully guarded,38 so requirements to share \nsynthetic data, which is often used to train AI models, may \nbe a non-starter for NRC users. On the other hand, many \nscholars have written about the promise of synthetic \ndata to actually enable further data sharing by preserving \nprivacy and researchers’ trade secrets.39 In fact, sharing \nsynthetic datasets would spur additional research and \ninnovation in fields such as healthcare, where data sharing \nhas limited.40\n Another class of data generated for AI is labeled data, \nnamely data that has been tagged and classified to provide \nground truth for supervised machine learning models.41\nWhile techniques have been developed to decrease the \ncosts associated with data labeling,42 it nevertheless \nremains a resource and time-intensive task. For example, \nCognilytics Research reports that 25 percent of the total \ntime spent building machine learning models is devoted to \ndata labeling.43 Researchers using the NRC may therefore \nseek to protect their investment in data labeling, by \nopting not to share their labels with others, especially if \nthe underlying data is proprietary.44 However, recognizing \nthe difficulty of data labeling, some researchers have built \nonline platforms for sharing data labels.45 In the case of \nImageTagger, a data labeling and sharing platform for \nRoboCup Soccer, the developers wanted to solve the \nproblem that no single team, acting alone, could easily \nbuild its own high-quality training sets.46 Similarly, in \nthe NRC’s case, the sharing of labeled government data \n– where labeling may have been augmented by NRC \nresources47 – could act as a rising tide that lifts all boats, \nimproving the quality of not only the government data \nas a training dataset, but also all subsequent research \nusing that data. Furthermore, sharing data labels could \nbe instrumental in conducting bias and fairness of NRC \nresearch outputs where necessary, as discussed in Chapter \nSeven.48\nRETAINING IP RIGHTS IN THE UNIFORM GUIDANCE\nAs the preceding discussion suggests, sharing AI \nresearch output covered by copyrights and data rights \ncould be beneficial to AI innovation. We therefore \nrecommend that the NRC at least retain the same rights to \ncopyrights and data rights as under the Uniform Guidance, \nyielding several additional benefits. First, similar to our \nrecommendation in Chapter Three that federal agencies \nshould be allowed to use the NRC’s compute resources, \nretaining the same Uniform Guidance IP allocation scheme \ncould produce welfare benefits by improving government \ndecision-making using AI. For instance, federal agencies \ncan reduce the cost of core governance functions and \nincrease agency efficiency and effectiveness by using \ndata labels shared by NRC researchers or by fine-tuning \nmodels generated by NRC researchers. Second, retaining \nthe Uniform Guidance IP allocation scheme would result \nin more consistency across the federal award landscape. \nIndeed, as mentioned above in the patent context, it \ncould be confusing to diverge from the Uniform Guidance, \nespecially if the cloud credit grant is apportioned through \nprograms like CloudBank, but the research grant is \nadministered as a federal award.\nIn sum, we recommend that the government at least \nretain its copyrights and data rights under the Uniform \nGuidance. However, we also reiterate that the Uniform \nGuidance serves merely as a helpful framework, not as an \nimmutable rule. Where the Uniform Guidance IP allocation \nwould dissuade researchers from using the NRC or hinder \nAI innovation in specific scenarios, the government \ncan and should explicitly modify its rights and contract \nseparately with researchers on what rights the government \nretains, if any.\nA Blueprint for the National Research Cloud 80\nCHAPTER 9\nCONSIDERATIONS FOR OPEN\u0002SOURCING\nShould the government go beyond its rights and \nmandate that researchers share their NRC research \noutputs with others under an open-source license? As \nan initial matter, we note that agencies can modify the \nIP allocation schemes under the Uniform Guidance, but \nnot under the Bayh-Dole Act. Some federal agencies \nsupplement and/or replace the IP rights set out in the \nUniform Guidance with restrictions that are more specific \nto the IP being developed for that particular agency or \nunder a specific award.49 For instance, the Department of \nLabor requires that intellectual property developed under \na federal award must not only comply with the terms \nspecified in the Uniform Guidance, but also be available \nfor open licensing to the public.50 NSF grantees are also \nexpected to share their data with others.51 However, \nthe government cannot change the allocation of patent \nownership under the Bayh-Dole Act, unless the Act itself is \nmodified or unless the NRC isn’t administered as a federal \naward, rendering the Act inapplicable. \nRequiring researchers to open-source their research \noutputs may be possible, but the considerations around \nit are complex. On one hand, an open-source requirement \ncould negatively affect downstream commercialization, \ngiven the wide range of potential AI research.52 While the \nNRC might protect commercialization to some degree \nby adopting a restrictive open-source license,53 the mere \ndivergence from the Uniform Guidance or the Bayh-Dole \nAct could be confusing for researchers in navigating \nfederal awards and understanding open-source licensing \ninteractions across multiple situations.54 Furthermore, \nrequiring researchers to share research outputs comes \nwith its own host of privacy and cybersecurity issues.55 \nIf researchers are permitted to use the NRC to conduct \nclassified research,56 for instance, then keeping research \noutputs proprietary would serve the national interest.57\nIn this case, however, the NRC should consider limiting \nany open-source requirement to research that has fewer \nprivacy and security implications.\nOn the other hand, as discussed, sharing research \noutputs with other NRC researchers could be beneficial, \nand many scholars argue that AI researchers should \nopen-source their software to stimulate innovation.58 A \nrequirement to open-source software code, which can \nbe the subject of both copyrights and patent rights,59\nmay contravene Bayh-Dole, and face challenges from \nuniversities that seek to retain their patent rights, but \nsoftware patent disclosures alone are often limited, over\u0002broad, and fail to enhance social welfare.60 Requiring fuller \ndisclosure of code generated on the NRC can therefore \ndecrease the risk of over-patenting and increase AI \ninnovation. The growth of the robust open-source and \nopen science movements also suggests that an open\u0002sourcing requirement for the NRC would not be a complete \nbarrier to NRC usage.61\nA strong argument for mandating open-sourcing \nalso comes from the increasing private- sector reliance \non trade secrets for IP protection in AI.62 Some argue that \nthis heightened emphasis on trade secret protection \nconstitutes “artificial stupidity,”63 as it has stifled \ninnovation in AI by preventing disclosure, providing \nprotection for a potentially unlimited duration, and \nattaching immediately and broadly to any output with \nperceivable economic value.64 The reliance on secrecy, \ntherefore, contravenes many of the principles described \nabove—which argue that sharing code and data is crucial \nin AI—and results in significant AI industry consolidation \nand sub-optimal levels of AI innovation.65 This harkens \nback to the goal of the NRC discussed in Chapter One: To \naddress problems with AI research being concentrated \nin the hands of a few private-sector players. Because the \nNRC should explicitly avoid replicating these private\u0002sector challenges, this lends additional support to a \nrecommendation that the NRC should contemplate \nrequiring researchers to share their research outputs.\nIn sum, while AI raises a host of novel IP issues (e.g., \nwhether AI output is itself eligible for IP protection), we \nthink that the government can steer clear of many of these \ncomplications by tracking Bayh-Dole and the Uniform \nGuidance. The government should also additionally \nconsider conditions for requiring NRC researchers to \ndisclose or share their research outputs under an open\u0002access license.\nA Blueprint for the National Research Cloud 81\nConclusion \nAs we have articulated in this White Paper, the ambitious call for an NRC has transformative potential for the AI \nresearch landscape.\nIts biggest promise is to ensure more equitable access to core ingredients for AI research: compute and data. \nLeveling this playing field could shift the current ecosystem from one that focuses on narrow commercial problems to \none that fosters basic, noncommercial AI research to ensure long-term national competitiveness, to solve some of the \nmost pressing problems, and to rigorously interrogate AI models.\nAs we have spelled out in this White Paper, the NRC does raise a host of policy, legal, and normative questions. How \ncan such compute resources be provided in a way that is expeditious and user-friendly, but does not preclude the \npotential cost savings from a publicly owned resource? How can the NRC be designed to adhere to the Privacy Act of \n1974, which was animated by concerns about a national system of records that surveils on citizens? How can we ensure \nthat NRC mitigates, rather than heightens, concerns about the unethical use of AI? And how can one prevent the NRC \nfrom becoming the biggest target for cyberattacks?\nThese are tough questions, and we hope to have sketched out our initial attempt at answers above. We are \nhopeful that, if designed well, the NRC could help to realign the AI innovation space from one that is fixated with short\u0002term private profit, to one that is infused with long-term public values.\nA Blueprint for the National Research Cloud 82\nADP Alberta Data Partnerships\nADR UK Administrative Data Research UK\nADRF Administrative Data Research Facility\nAI artificial intelligence\nAPI application programming interface\nARPA Advanced Research Projects Agency\nARPANET Advanced Research Projects Agency \nNetwork\nATO authority-to-operate\nATO Authorization to Operate\nAWS Amazon Web Services\nCaaS Compute as a Service\nCIPSEA Confidential Information Protection\nand Statistical Efficiency Act\nCMS Centers for Medicare & Medicaid Services\nCPL California Policy Lab\nCPU central processing unit\nDFARS Defense Federal Acquisition Regulation \nSupplement\nDHS U.S. Department of Homeland Security\nDOD U.S. Department of Defense\nDOE U.S. Department of Energy\nDOT U.S. Department of Transportation\nDUA Data Use Agreement\nEBPA Foundations for Evidence Based \nPolicymaking Act or Evidence Act\nEO Executive Order\nESRC Economic and Social Research Council\nEULA End-User Licensing Agreement\nFAA Federal Aviation Administration\nFAR Federal Acquisition Regulation\nFDS Federal Data Strategy\nFedRAMP Federal Risk and Authorization Management \nProgram\nFFRDC Federally-Funded Research and Development \nCenter\nFIPS Federal Information Processing Standards\nFISMA Federal Information Security Modernization \nAct\nFSRDC Federal Statistical Research Data Center\nGAO U.S. Government Accountability Office\nGCP Google Cloud Platform\nGDPR General Data Protection Regulation\nGPS Global Positioning System\nGPU graphics processing unit\nGSA U.S. General Services Administration\nHAI Stanford Institute for Human-Centered \nArtificial Intelligence\nHECToR High-End Computing Terascale Resource\nHHS U.S. Department of Health and Human \nServices\nHIPAA Health Insurance Portability and \nAccountability Act\nHPC high-performance computing\nHTTP Hypertext Transfer Protocol\nHTTPS Hypertext Transfer Protocol Secure\nIC U.S. Intelligence Community\nIDA Institute for Defense Analyses\nIPTO Information Processing Techniques Office\nIRB Institutional Review Board\nJV joint venture\nLIDAR Light Detection and Ranging\nML machine learning\nMOU memorandum of understanding\nGlossary of Acronyms\n(Alphabetical order)\nA Blueprint for the National Research Cloud 83\nNAIRR National Artificial Intelligence Research \nResource Task Force Act\nNASA National Aeronautics and Space \nAdministration\nNDAA National Defense Authorization Act\nNIH National Institutes of Health\nNISE NSF’s Directorate for Computer and \nInformation Science and Engineering\nNIST National Institute of Standards and \nTechnology\nNIST SP NIST Special Publications\nNOAA National Oceanic and Atmospheric \nAdministration\nNORC National Opinion Research Center\nNRC National Research Cloud\nNSCAI National Security Commission on Artificial \nIntelligence\nNSDS National Secure Data Service\nNSF National Science Foundation\nODNI Office of the Director of National Intelligence\nOECD Organization for Economic Co-operation and \nDevelopment\nOMB U.S. Office of Management and Budget\nONS Office for National Statistics\nORNL Oak Ridge National Laboratory\nOLCF Oak Ridge Leadership Computing Facility\nOSI Open Systems Interconnection\nOT Other Transaction\nPCLOB Privacy and Civil Liberties Oversight Board\nPHI protected health information\nPHS Stanford Center for Population \nHealth Sciences\nPI principal investigator\nPII personally identifiable information\nPPP public-private partnership\nR&D research and development\nRFI Request for Information\nRFP Request for Proposal\nRIST Research Organization for Information \nScience and Technology\nSDSC San Diego Supercomputer Center\nSRCC Stanford Research Computing Center\nSSL Secure Sockets Layer\nSTPI Science & Technology Policy Institute\nTLS Transport Layer Security\nUC Berkeley University of California, Berkeley\nUC San Diego University of California, San Diego\nUCLA University of California, Los Angeles\nUSDA Department of Agriculture\nVRDC CMS’ Virtual Research Data Center\nWIPO World Intellectual Property Organization\nA Blueprint for the National Research Cloud 84\nAppendix\nA. COMPUTING INFRASTRUCTURE \nCOST COMPARISONS\nThis Appendix provides a sample cost-estimate \ncomparison between a commercial cloud service, AWS, and \na dedicated government HPC system, Summit. In sum, our \nestimations show that AWS P3 instances with comparable \nhardware to Summit would be 7.5 times as expensive \nas estimated costs under constant usage, and 2.8 times \nSummit’s estimated costs under fluctuating demand.\nTable 3 lists the three infrastructure models used \nin this comparison. Summit was used as the reference \ngovernment HPC system because it is one of the DOE’s \nnewest systems and has hardware well-suited for AI \nresearch.1 The other infrastructure model use is AWS EC2 \nP3.2 Both are commonly used in AI research and general \nHPC applications. Other commercial cloud platforms, \nsuch as GCP or Azure, could also feasibly provide the \ninfrastructure for the NRC. AWS EC2 P3 was used here \nbecause AWS has a robust cost calculator that allows for \nvariable workloads. \nThe number of AWS instances were set such that those \nmodels would have the exact same number of GPUs as \nSummit. GPUs were the fixed variable because GPUs are \nthe most important hardware for AI research applications, \nspecifically deep learning. Both Summit and AWS P3 \ninstances use NVIDIA V100 GPUs.\nWe conduct our cost comparison for the two \ninfrastructure models over five years, as Summit’s initial \nRFP documents include a five-year maintenance contract. \nAWS, however, only provides one-year or three-year pricing \nplans, so we extrapolated the five-year cost based on its \nthree-year plan.\nFor the cost estimate of Summit, we based our \ncalculation on the budget details in the original \nDepartment of Energy (DOE) Request for Proposal \n(RFP) in January 2014.3\n The RFP includes a $155 million \nmaximum budget for building Summit, an expected $15 \nmillion maximum for the non-recurring engineering cost,4\nand around $15 million for-five year maintenance,5 plus \ninterest based on the U.S. Treasury securities at five-year \nconstant maturity as specified in the price schedule.6 Upon \ncalculation, we estimated Summit costs around $192 \nmillion in total, which is consistent with public reporting of \nthe cost of Summit.7\nFor the cost estimate of AWS, we used the AWS pricing \ncalculator, choosing U.S. East (N. Virginia) as the data \ncenter and publicly available rates under the cheapest \npossible pricing plan (EC2 Instance Savings Plans). To \napproximate a negotiated discount, we applied a 10 \npercent discount based on the negotiated rate of one \nmajor university.\nSince commercial cloud platform costs scale with \nhow many instances are actually in use, two costs were \ncalculated for each AWS model representing usage \nextremes: (1) with the infrastructure under constant usage; \n(2) with the infrastructure under dramatically fluctuating \nusage each day. For the daily spike traffic calculation, we \nset the model to run five days a week with 8.4 hours of \neach day at peak performance. The maximum number of \ninstances used is the same as one would use for constant \nuse while the minimum number is zero. This workload \nsetting is based on the assumption that GPUs used for \ntraining AI models sit idle 30 percent of the time.8\n These \nestimates should provide hard upper and lower bounds on \ncosts for using each instance type. \nFigure 1 plots costs on the y-axis over a five-year \nperiod on the x-axis. The turquoise line indicates the \ncost to a Summit-like system and the purple and blue \nlines indicate the cost of the same AWS instances under \nvariable and constant usage. Overall, this simple analysis \ncorroborates the analysis conducted by Compute Canada, \nwhich found that commercial cloud “ranged from 4x to \n10x more than the cost of owning and operating our own \nclusters.”9\n Over five years and under constant usage, \nAWS P3 instances with comparable hardware to Summit \nwould be 7.5 times as expensive as estimated costs. Under \nfluctuating demand, AWS P3 instances would cost 2.8 \ntimes Summit’s estimated costs.\nA Blueprint for the National Research Cloud 85\nWe note that this simple analysis omits many potential factors (see discussion in Chapter Two), but provides a \nstarting point to understanding the considerable cost implications for the make-or-buy decision. \nFIGURE 1 – ESTIMATED COST OF AWS INSTANCES COMPARED TO SUMMIT OVER 3 YEARS\nTABLE 3 – SUMMIT & AWS COMPARISON\nSummit IBM AC922\nAWS p3dn.24xlarge\n(3456 nodes)\nGPUs RAM Network Bandwidth\n27,648\n(NVIDIA Volta V100)\n27,648\n(NVIDIA Volta V100)\n2.8 PB\n2.6 PB\n200 Gb/s\n100 Gb/s\n$1,600\n$1,400\n$1,200\n$1,000\n$800\n$600\n$400\n$200\n$0\n0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60\nMillions\nMonths\nAWS P3 (Constant Usage) AWS P3 (Daily Spike Traffic) Summit\nA Blueprint for the National Research Cloud 86\nB. FACILITATING PRIVATE DATASET \nSHARING\nUnique IP challenges arise if researchers are permitted \nto share their own private datasets with the NRC. Indeed, \nresearchers who “upload” proprietary data may be \nconcerned about how other NRC users utilize that data.10\nThrough interviews conducted for this White Paper, \ncorporate stakeholders representing the entertainment \nindustry, as well as other creative industries, have further \nexpressed fear that researchers may upload and share data \nto which they do not hold rights. However, if the NRC does \ndecide to facilitate private data-sharing, it should consider \nadopting two requirements to address these concerns: (1) \nThe NRC should require all users to affirm that they either \nhave the original IP rights to the data or that the data is \nalready in the public domain; (2) The NRC should have a \nscheme for its users to license their data.\n(a) NRC users must own IP rights to the data they are \nuploading\nResearchers uploading data need to agree that they \nown the intellectual property rights on the data prior to \nupload, or that the data is already in the public domain. \nThis should be the case whether researchers share the data \nbroadly with other researchers or simply use their data for \ntheir own private use. \nOf course, despite mandating that uploaders \nguarantee legitimate ownership or public domain status \nof their uploaded IP, uploaders may nevertheless upload \ndata they don’t own the IP rights to. This may happen \nbecause computer engineers and researchers are not \ninformed about IP law, anticipate that fair use will excuse \ntheir behavior, or simply hope not to get caught.11 Industry \nstakeholders were also concerned that AI researchers \nwould pull out “facts” from a copyrighted work (e.g., \ncertain melodies in the chorus of a song) or apply certain \nalgorithms to the work and “wrongly” claim a copyright \nover the transformed work. Whatever the case may \nbe, this assembly of protected input data the “clearest \ncopyright liability in the machine learning process” \nbecause assembling protected data violates the right to \nreproduction, and any preprocessing on the data could \nviolate the right to derivative works.12\nIn interviews, corporate stakeholders expressed a \ndesire to stymie the upload of copyrighted works by having \nthe NRC itself assess whether uploaded data is already \nprotected by copyright. Diligencing data can be completed \nmanually, or by using such automated systems as Content \nID, which is also used by corporations such as YouTube.\n13\nThe former option would be very labor intensive,14 whereas \nthe latter may be prohibitively expensive,15 so the value of \naddressing these concerns must be weighed against these \nburdensome costs.\nFinally, it is unclear the extent to which uploading and \nsharing copyrighted data for machine learning amounts \nto fair use.16 The most analogous case is Author’s Guild \nv. Google Books.\n17 In that case, Google scanned over 20 \nmillion books, many of which were copyright-protected, \nand assembled a corpus of machine-readable texts to \npower its Google Books service.18 The Second Circuit \nheld that Google Books’ unauthorized reproductions of \ncopyrighted works was transformative fair use, largely \nbecause Google Books provided information about\nbooks through small snippets, without threatening the \nrightsholders’ core protectable expression in the books.19\nWhile some have opined that the Author’s Guild holding \ncategorically protects using copyrighted material in \ndatasets for machine learning purposes,20 many legal \nscholars are not so sure about such a broad holding, \nespecially because fair use is so fact-intensive.21 Indeed, \nwhile Google Books used copyrighted works for a non\u0002expressive purpose, Sobel notes that machine learning \nmodels may increasingly be able to glean value from a \nwork’s expressive aspects.22 Therefore, until courts and \nlegislators provide more clarity on the applicability of fair \nuse in the machine learning context, the NRC should still \nrequire data uploaders to attest that they own the rights to \nthe data.\n(b) Users must be able to license their data to other \nusers. \nIf the NRC enabled private data sharing, users would \nneed to make clear what rights other NRC users have over \nthe uploaders’ shared data. The NRC would have two basic \nA Blueprint for the National Research Cloud 87\noptions for creating IP licensing schemes: (1) the NRC \ncould permit researchers to use whatever IP license they \nwish when sharing their private data; (2) the NRC could \nmandate a uniform license across the board for all data \nthat is uploaded.\n(1) Researcher’s Choice of License \nAllowing researchers to craft their own IP licensing \nagreements when sharing private data with other \nresearchers would be the most frictionless solution from \nthe perspective of the uploader; it would allow them to \nshare exactly what they want and restrict use to only \ncertain contexts. This choice of license seems to be \nimportant to data sharers.23 Indeed, many data scientists \nand engineers have written guides advising members \nof the open-source community on how they should go \nabout choosing specific licenses for their work.24 GitHub, \nan open-source code-sharing platform, permits its users \nto choose from dozens of licenses,25 and FigShare, a data\u0002sharing platform for researchers, likewise supports a host \nof different Creative Commons licenses.26 Some datasets \neven have their own custom IP licensing agreements. \nThe Twitter academic dataset, for instance, is licensed \naccording to Twitter’s own developer agreement and non\u0002commercial use policies, not to an existing open-source \nlicense.27\nHowever, there are also disadvantages to such \nflexibility. Just because different licenses might be allowed \ndoesn’t mean that these licenses will be fully understood \nby all users. Adopting multiple licenses may result in \nincreased accidental infringement. Indeed, a study \nconducted by the Institute of Electrical and Electronics \nEngineers found that “although [software] developers \nclearly understood cases involving one license, they \nstruggled when multiple licenses were involved,”28 and \nin particular, were found to “lack the knowledge and \nunderstanding to tease apart license interactions across \nmultiple situations.”29\nIn particular, researchers unfamiliar with the \nallowances provided by different data licenses, in \ncontexts where more than one license is implemented, \nmay lead to certain licenses being violated. For example, \nwhen researchers were surveyed regarding their \nunderstanding of copyright transfer agreements in the IP \ncommercialization process, they only demonstrated an \naverage 33 percent score on a knowledge-testing survey.30\n(2) Uniform Licensing Agreement \nThe second option available to the NRC would be to \nmandate that all private data be licensed under a single \nuniform license. For the NRC administration itself, this \nmay be the more straightforward option, since users \ncould be notified upon login about the appropriate use \nof data. The disadvantage of this strategy is that it may \ndeter would-be researchers who would share data under \na narrower license.31 Given the desire to allow researchers \nto innovate freely, there may be concerns about adopting \na restrictive licensing agreement. Nonetheless, several \noptions of licensing agreements would still be available \nfor adoption, and this pathway would require choosing a \nuniform agreement from these options, with the possibility \nof allowing potential opt-out of this default license. \nIf the NRC were to implement a uniform license, \nit could look to the licensing agreements leveraged \nby institutional research clouds, such as the Harvard \nDataverse as an analogy in determining best practices \nfor its own licensing agreements. The model adopted by \nthe Dataverse is a default use of the CC0 Public Domain \nDedication “because of its name recognition in the \nscientific community” and its “use by repositories as well \nas scientific journals that require the deposit of open \ndata.”32 Like an unrestricted Creative Commons or Open \nData license, a public domain license would allow the data \nit governs to be used in any context, even commercial \nones, and would also allow reproduction and creation of \nderivatives from the data. \nAlternatively, the NRC could have a default open \nlicense while also permitting researchers to choose from \na handful of more restrictive licenses if they wish. For \nexample, the Harvard Dataverse notably allows uploaders \nto optout of the CC0 if needed and specify custom terms \nof use. The Australian Research Data Commons and data\u0002sharing platform FigShare33 also use a default CC0 license \nbut nevertheless permit researchers to use a conditioned\nA Blueprint for the National Research Cloud 88\nCreative Commons license. These conditioned licenses \ncan, for instance, require attribution to the original owner, \nprevent exact reproduction, or only allow use for non\u0002commercial contexts. This may also help accommodate \nresearchers who seek to upload datasets incorporating \nthird-party data that holds a more restrictive license, \nsince a “combined dataset will adopt the most restrictive \ncondition(s) of its component parts.”34\nIf the NRC goes down this route of giving users the \nchoice of a narrower license, it would also shift some \nliability to users—or to the NRC itself—by relying on \nusers to abide by the license. Approaches to enforcement \nwould vary, depending on the amount of responsibility \nin enforcement, and by extension liability the NRC seeks \nto take on. For example, in the Harvard Dataverse, if an \nuploader decides to opt out of a default open license \nand pursue their own custom licensing agreement \nover uploaded data, the Dataverse’s General Terms of \nUse absolve this particular cloud from resource-heavy \nenforcement responsibilities by stating that it “has no \nobligation to aid or support either party of the Agreement \nin the execution or enforcement of the Data Use \nAgreement’s terms.”35\nC. CURRENT STATE OF AI ETHICS \nFRAMEWORKS\nAI ethics frameworks (or principles, guidelines) \nattempt to address the ethical concerns related to \nthe development, deployment, and use of AI within \nprospective organizations. We briefly discuss the current \nlandscape of AI ethics frameworks, while noting that this is \nstill an emergent topic without broad consensus.\nBetween 2015 and 2020, governments, technology \ncompanies, international organizations, professional \norganizations, and researchers around the world have \npublished some 117 documents related to AI ethics.36\nThese frameworks aim to tackle the disruptive potential \nof AI technologies by producing normative principles \nand “best practice” recommendations.37 Due to the \nprominence of essentially contested concepts in AI ethics—\ni.e., words that have different understandings to different \naudiences—such as fairness, equity, privacy,38 as well as \nthe lack of binding professional history and accountability \nmechanisms, those frameworks are often high level and \nself-regulatory, posing little threat to potential breaches to \nethical conduct.39\nFederal Frameworks\nIn the United States, there is not a central guiding \nframework on the responsible development and \napplication of AI across the federal government. Some \ngovernment agencies have adopted or are in the process \nof adopting their own AI framework, while others have not \npublished such guidelines. The following are all published \nfederal AI ethical frameworks as of August 2021:\n• After 15 months of deliberation with leading \nAI experts, the Department of Defense (DOD) \nadopted a series of ethical principles for the use \nof AI in February 2020 that align with the existing \nDOD mission and stakeholders.40\n• The General Services Administration (GSA), \ntasked by the Office of Management and Budget \n(OMB) in the Federal Data Strategy 2020 Action \nPlan, developed a Data Ethics Framework in \nFebruary 2020 to help federal personnel make \nethical decisions as they acquire, manage, and \nuse data.41\n• The Government Accountability Office (GAO) \ndeveloped an AI accountability framework \nin June 2020 for federal agencies and other \nentities involved in the design, development, \ndeployment, and continuous monitoring of \nAI systems to help ensure accountability and \nresponsible use of AI.42\n• The Office of the Director of National Intelligence \n(ODNI) released the Principles of AI Ethics for \nthe Intelligence Community in July 2020 to \nguide the intelligence community (IC)’s ethical \ndevelopment and use of AI to solve intelligence \nproblems.43\n• The National Security Commission on Artificial \nA Blueprint for the National Research Cloud 89\nIntelligence (NSCAI) published a set of best \npractices in July 2020 (later revised and \nintegrated into the Commission’s 2021 Final \nReport) for agencies critical to national security \nto implement as a paradigm for the responsible \ndevelopment and fielding of AI systems.44\nWhile these frameworks can help guide the NRC’s \napproach to ethics, we refrain from recommending a \nspecific framework for several reasons. First, despite \ngrowing calls for applied ethics in the AI community, \ndeveloping an AI ethics framework is still an emerging \narea. The lack of a unified government standard poses \nchallenges to the establishment of the NRC’s ethics review \nprocess.\nSecond, there are, in fact, significant differences \namong ethics frameworks published by various federal \nagencies. For example, NSCAI laid out differences between \nits recommended practices and those by DOD and IC.45\nMoreover, among the five frameworks above, the GSA \nFramework focused only on the ethical conduct of federal \nemployees when dealing with data while others focused \non the ethical development and application of AI systems \nspecifically. \nThird, the ethics framework for adopting AI technology \nmay be different from a framework for assessing research. \nMost federal agencies develop frameworks to guide the \nuse of AI-driven solutions for agency-specific tasks. For \nexample, DOD’s ethical principles only apply to defense\u0002specific combat or non-combat AI systems.46 In the \nabsence of a central federal guideline, the NRC should not \nadopt a framework by a particular agency because these \nframeworks are not necessarily designed for the wide \nrange of research contemplated for the NRC. The work on \nframeworks may nonetheless provide a useful starting \npoint for NRC’s ethics process.\nD. STAFFING AND EXPERTISE\nAs noted throughout this White Paper, the success of \nthe NRC will depend on human resources—both within \nthe NRC as well as across government— to solve the \nmany dimensions that the NRC promises to tackle. While \nwe refrain from providing an organizational chart, we \nlist the dimensions where staffing and expertise will be \ncritical to the success of the NRC. This list is not meant to \nbe exhaustive, but to highlight the importance of human \nresources.\nHuman Resource Areas\n• Computing\n°\n System administrators\n°\n Data center engineers\n°\n Research software engineers\n°\n Research application developers\n• Data\n°\n Data officers\n°\n Agency liaisons\n°\n Data architects\n°\n Data scientists\n• Grant administrators\n• Contracting officers\n• Support and training staff \n• Privacy Staff (technical and legal)\n• Ethics Staff\n• Cybersecurity Staff\nA Blueprint for the National Research Cloud 90\nExecutive Summary\n1 Klaus Schwab, The Fourth Industrial Revolution (2016).\n2 Tae Yano & Moonyoung Kang, Taking Advantage of Wikipedia in Natural Language Processing, Carnegie Mellon U. (2008), https://www.cs.cmu.\nedu/~taey/pub/wiki.pdf.\n3 See, e.g., Anthony Alford, Google Trains Two Billion Parameter AI Vision Model, InfoQ (June 22, 2021), https://www.infoq.com/news/2021/06/google\u0002vision-transformer/; Anthony Alford, OpenAI Announces GPT-3 AI Language Model with 175 Billion Parameters, InfoQ (June 2, 2020), https://www.infoq.\ncom/news/2020/06/openai-gpt3-language-model/.\n4 AlphaGo, DeepMind (2021), https://deepmind.com/research/case-studies/alphago-the-story-so-far/. \n5 Benjamin F. Jones & Lawrence H. Summers, A Calculation of the Social Returns to Innovation (Nat’l Bureau of Econ. Research, Working Paper No. \n27863, 2020); J.G. Tewksbury, M.S. Crandall & W.E. Crane, Measuring the Societal Benefits of Innovation, 209 Sci. Mag. 658-62 (1980); see also National \nAcademies of Sciences, Engineering, and Medicine, Returns to Federal Investments in the Innovation System (2017)\n6 Stuart Zweben & Betsy Bizot, 2019 Taulbee Survey: Total Undergrad CS Enrollment Rises Again, but with Fewer New Majors; Doctoral Degree \nProduction Recovers From Last Year’s Dip (2019). \n7 Jathan Sadowski, When Data is Capital: Datafication, Accumulation, and Extraction, 2019 Big Data & Soc’y 1 (2019). \n8 Amy O’Hara & Carla Medalia, Data Sharing in the Federal Statistical System: Impediments and Possibilities, 675 Annals Am. Acad. Pol. & Soc. Sci. 138, \n140-41 (2018).\n9 Nat’l Security Comm’n on Artificial Intelligence, Final Report 186 (2021).\n10 Stan. U. Inst. for Human-Centered Artificial Intelligence, 2021 Artificial Intelligence Index Report 118 (2021).\n11 Id.\n12 Id.\n13 Neil C. Thompson, Shuning Ge & Yash M. Sherry, Building the Algorithm Commons: Who Discovered the Algorithms that Underpin Computing in the \nModern Enterprise?, 11 Global Strategy J. 17-33 (2020).\n14 See, e.g., U.S. Gov’t Accountability Office, Federal Agencies Need to Address Aging Legacy Systems (2016); U.S. Gov’t Accountability Office, \nCloud Computing: Agencies Have Increased Usage and Realized Benefits, but Cost and Savings Data Need To Be Better Tracked (2019).\n15 David Freeman Engstrom, Daniel E. Ho, Catherine M. Sharkey & Mariano-Florentino Cuéllar, Government by Algorithm: Artificial \nIntelligence in Federal Administrative Agencies 6, 71-72 (2020).\n16 William M. (Mac) Thornberry National Defense Authorization Act for Fiscal Year 2021, Pub. L. No. 116-283, § 5106.\n17 The Biden Administration Launches the National Artificial Intelligence Research Resource Task Force, The White House (June 10, 2021), https://www.\nwhitehouse.gov/ostp/news-updates/2021/06/10/the-biden-administration-launches-the-national-artificial-intelligence-research-resource-task-force/. \n18 William M. (Mac) Thornberry National Defense Authorization Act for Fiscal Year 2021, Pub. L. No. 116-283, § 5107 (g).\n19 Nat’l Security Comm’n on Artificial Intelligence, supra note 9, at 191.\n20 See, e.g., Cloudbank, https://www.cloudbank.org; Fact Sheet: National Secure Data Service Act Advances Responsible Data Sharing in Government, \nData Coalition (May 13, 2021), https://www.datacoalition.org/fact-sheet-national-secure-data-service-act-advances-responsible-data-sharing-in\u0002government/. \n21 Steve Lohr, Universities and Tech Giants Back National Cloud Computing Project, N.Y. Times (June 30, 2020), https://www.nytimes.com/2020/06/30/\ntechnology/national-cloud-computing-project.html; John Etchemendy & Fei-Fei Li, National Research Cloud: Ensuring the Continuation of American \nInnovation, Stan. U. Inst. for Human-Centered Artificial Intelligence, (Mar. 28, 2020), https://hai.stanford.edu/news/national-research-cloud\u0002ensuring-continuation-american-innovation.\n22 Jennifer Villa & Dave Troiano, Choosing Your Deep Learning Infrastructure; The Cloud vs. On-Prem Debate, Determined AI (July 30, 2020), https://\ndetermined.ai/blog/cloud-v-onprem/; Is HPC Going to Cost Me a Fortune?, InsideHPC (last visited July 23, 2021), https://insidehpc.com/hpc-basic\u0002training/is-hpc-going-to-cost-me-a-fortune/.\n23 See, e.g., US Plans $1.8 Billion Spend on DOE Exascale Supercomputing, HPCwire (Apr. 11, 2018), https://www.hpcwire.com/2018/04/11/us-plans-1-\n8-billion-spend-on-doe-exascale-supercomputing/; Federal Government, Advanced HPC (last visited July 23, 2021), https://www.advancedhpc.com/\npages/federal-government; United States Continues to Lead World In Supercomputing, U.S. Dep’t. Energy (Nov. 18, 2019), https://www.energy.gov/\narticles/united-states-continues-lead-world-supercomputing.\n24 See NSF Funds Five New XSEDE-Allocated Systems, Nat’l Sci. Found. (Aug. 10, 2020), https://www.xsede.org/-/nsf-funds-five-new-xsede-allocated\u0002systems.\n25 Cloudbank, supra note 20.\n26 See, e.g., National Data Service, http://www.nationaldataservice.org; The Open Science Data Cloud, https://www.opensciencedatacloud.org; Harvard \nDataverse, https://dataverse.harvard.edu; FigShare, https://figshare.com. \n27 FedRAMP, https://www.fedramp.gov.\n28 See Fact Sheet: National Secure Data Service Act Advances Responsible Data Sharing in Government, Data Coalition (May 13, 2021), https://www.\ndatacoalition.org/fact-sheet-national-secure-data-service-act-advances-responsible-data-sharing-in-government/.\n29 See Administrative Data Research Facility, Coleridge Initiative, https://coleridgeinitiative.org/adrf/ (last visited July 26, 2021). \n30 See Landsat Data Access, U.S. Geological Survey, https://www.usgs.gov/core-science-systems/nli/landsat/landsat-data-access (last visited July 23, \n2021); Fed. Geographic Data Comm., The Value Proposition for Landsat Applications (2014); Crista L. Straub, Stephen R. Koontz & John B. Loomis, \nEconomic Valuation of Landsat Imagery (2019).\nEndnotes\nA Blueprint for the National Research Cloud 91\n31 See Bipartisan Pol’y Ctr., Barriers to Using Government Data: Extended Analysis of the U.S. Commission on Evidence-Based Policymaking’s \nSurvey of Federal Agencies and Offices 18-20 (2018); see also U.S. Dep’t of Health & Human Services, The State of Data Sharing at the U.S. \nDepartment of Health and Human Services 4 (2018) (describing how data at the agency is “largely kept in silos with a lack of organizational awareness \nof what data are collected across the Department and how to request access.”).\n32 Privacy Act, 5 U.S.C. § 552a (1974).\n33 Michael S. Bernstein et al., ESR: Ethics and Society Review of Artificial Intelligence Research, Cornell. U. (July 9, 2021), https://arxiv.org/\npdf/2106.11521.pdf.\n34 Courtenay R. Bruce et al., An Embedded Model for Ethics Consultation: Characteristics, Outcomes, and Challenges, 5 AJOB Empirical Bioethics 8 \n(2014).\nIntroduction\n1 National Research Cloud Call to Action, Stan. U. Inst. For Human-Centered Artificial Intelligence, https://hai.stanford.edu/national-re\u0002search-cloud-joint-letter.\n2 See id.; John Etchemendy & Fei-Fei Li, National Research Cloud: Ensuring the Continuation of American Innovation, Stan. U. Inst. For Human-Centered \nArtificial Intelligence (Mar. 28, 2020), https://hai.stanford.edu/news/national-research-cloud-ensuring-continuation-american-innovation.\n3 William M. (Mac) Thornberry National Defense Authorization Act for Fiscal Year 2021, Pub. L. No. 116-283, § 5106.\n4 The Biden Administration Launches the National Artificial Intelligence Research Resource Task Force, The White House (June 10, 2021), https://www.\nwhitehouse.gov/ostp/news-updates/2021/06/10/the-biden-administration-launches-the-national-artificial-intelligence-research-resource-task-force/. \n5 Privacy Act of 1974, 5 U.S.C. § 552a (2012).\n6 Foundations for Evidence-Based Policymaking Act of 2017, Pub. L. No. 115-435, 132 Stat. 5529 (2019).\n7 See Fact Sheet: National Secure Data Service Act Advances Responsible Data Sharing in Government, Data Coalition (May 13, 2021), https://www.data\u0002coalition.org/fact-sheet-national-secure-data-service-act-advances-responsible-data-sharing-in-government/.\n8 See, e.g., Facial Recognition and Biometric Technology Moratorium Act, S. 4084, 116th Cong. (2020); Bhaskar Chakravorti, Biden’s ‘Antitrust Revolution’ \nOverlooks AI—at Americans’ Peril, Wired (July 27, 2021), https://www.wired.com/story/opinion-bidens-antitrust-revolution-overlooks-ai-at-ameri\u0002cans-peril/.\nChapter 1\n1 See Stephen Breyer, Regulation and Its Reform (1982); Clifford Winston, Government Failure Versus Market Failure (2006).\n2 Largest Companies by Market Cap, Companies Market Cap (2021), https://companiesmarketcap.com.\n3 Stan. U. Inst. for Human-Centered Artificial Intelligence, 2021 Artificial Intelligence Index Report 93 (2021).\n4 See, e.g., Mary L. Gray & Siddarth Suri, Ghost Work: How to Stop Silicon Valley from Building a New Global Underclass (2019); Craig Webster & \nStanislav Ivanov, Robotics, Artificial Intelligence, and the Evolving Nature of Work 132-35 (2019); Weiyu Wang & Keng Siau, Artificial Intelligence, \nMachine Learning, Automation, Robotics, Future of Work and Future of Humanity: \nA Review and Research Agenda, 30 J. Database Mgmt. 61 (2019).\n5 AlphaFold: A Solution to a 50-Year-Old Grand Challenge in Biology, DeepMind (Nov. 30, 2020), https://deepmind.com/blog/article/alphafold-a-solution\u0002to-a-50-year-old-grand-challenge-in-biology.\n6 Tanha Talaviya et al., Implementation of Artificial Intelligence in Agriculture for Optimisation of Irrigation and Application of Pesticides and Herbicides, 4 \nArtificial Intelligence in Agriculture 58 (2020). \n7 Greg Allen & Taniel Chan, Artificial Intelligence and National Security, Harv. Kennedy Sch. Belfer Ctr. (July 2017), https://www.belfercenter.org/\npublication/artificial-intelligence-and-national-security. \n8 Stan. U. Inst. for Human-Centered Artificial Intelligence, supra note 3.\n9 Jeffrey Ding, Deciphering China’s AI Dream (2018).\n10 Fugaku is being used extensively for AI research initiatives. See Atsushi Nukariya et al., HPC and AI Initiatives for Supercomputer Fugaku and Future \nProspects, Fujitsu (Nov. 11, 2020), https://www.fujitsu.com/global/about/resources/publications/technicalreview/2020-03/article09.html.\n11 Eng’g & Physical Sciences Research Council, The Impact of HECToR (2014).\n12 Joshua New, Why the United States Needs a National Artificial Intelligence Strategy and What it Should Look Like (2018).\n13 Maggie Miller, White House Establishes National Artificial Intelligence Office, The Hill (Jan. 12, 2021), https://thehill.com/policy/cybersecurity/533922-\nwhite-house-establishes-national-artificial-intelligence-office.\n14 See Fast Track Action Comm. on Strategic Computing, National Strategic Computing Initiative Update: Pioneering the Future of Computing\n(2019), https://www.nitrd.gov/pubs/National-Strategic-Computing-Initiative-Update-2019.pdf.\n15 Nat’l Security Comm’n on Artificial Intelligence, Final Report (2021).\n16 The COVID-19 High Performance Computing Consortium, COVID-19 HPC Consortium, https://covid19-hpc-consortium.org.\n17 See Aaron L. Friedberg, Science, the Cold War, and the American State, 20 Diplomatic Hist. 107, 112 (1996); Sean Pool & Jennifer Erickson, The High \nReturn on Investment for Publicly Funded Research, Ctr. for Am. Progress (Dec. 10, 2012), https://www.americanprogress.org/issues/economy/\nreports/2012/12/10/47481/the-high-return-on-investment-for-publicly-funded-research/.\n18 Peter L. Singer, Federally Supported Innovations: 22 Examples of Major Technology Advances That Stem from Federal Research Support \n14-15 (2014).\n19 Nat’l Research Council, Government Support for Computing Research 136-55 (1999). \n20 Nat’l Security Comm’n on Artificial Intelligence, supra note 15, at 185.\n21 Philippe Aghion, Benjamin F. Jones & Charles I. Jones, Artificial Intelligence and Economic Growth, in The Economics of Artificial Intelligence: An \nAgenda 237 (2019).\nA Blueprint for the National Research Cloud 92\n22 Ian Moll, The Myth of the Fourth Industrial Revolution, 68 Theoria 1 (2021); see also Tim Unwin, 5 Problems with 4th Industrial Revolution, ICT Works \n(Mar. 23, 2019), https://www.ictworks.org/problems-fourth-industrial-revolution/.\n23 See, e.g., Geoffrey A. Manne & Joshua D. Wright, Google and the Limits of Antitrust: The Case Against the Antitrust Case Against Google, 34 Harv. J.L. & \nPub. Pol’y 1 (2011); Lina M. Khan, Amazon’s Antitrust Paradox, 126 Yale L.J. 710 (2016).\n24 David Patterson et al., Carbon Emissions and Large Neural Network Training, Cornell U. (Apr. 23, 2021), https://arxiv.org/pdf/2104.10350.pdf. To be \nclear, however, the study found that training other sophisticated but smaller NLP models such as Meena and T5 required approximately 96 and 48 tons \nof carbon dioxide, respectively. Id. Another study found that the training state-of-the-art NLP models produced approximately 626,000 pounds (313 \ntons) of carbon dioxide, five times the lifetime emissions of the average car in the United States. Emma Strubell, Ananya Ganesh & Andrew McCallum, \nEnergy and Policy Considerations for Deep Learning in NLP, Cornell U. (2019), https://arxiv.org/pdf/1906.02243.pdf.\n25 Calculate Your Carbon Footprint, The Nature Conservancy, https://www.nature.org/en-us/get-involved/how-to-help/carbon-footprint-calculator/.\n26 Economic studies in other fields also show that increasing access, supply, or quality of certain goods without appropriate pricing mechanisms or \nregulatory interventions can lead to over-use and waste. See, e.g., Chengri Ding & Shunfeng Song, Traffic Paradoxes and Economic Solutions, 1 J. Urban \nMgmt. 63 (2012) (roads and traffic congestion); Ari Mwachofi & Assaf F. Al-Assaf, Health Care Market Deviations from the Ideal Market, 11 Sultan Qaboos \nUniv. Med. J. 328 (2011) (doctors and quality of care).\n27 See Emily M. Bender et al., On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?, 2021 Proceedings ACM Conf. on Fairness, \nAccountability & Transparency 610 (2021).\n28 See Joy Buolamwini & Timnit Gebru, Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification, 81 Proceeding Machine \nLearning Res. 1 (2018); Inioluwa Deborah Raji & Joy Buolamwini, Actionable Auditing: Investigating the Impact of Publicly Naming Biased Performance \nResults of Commercial AI Products, 2019 Proceedings AAAI/ACM Conf. on AI, Ethics & Soc’y 429 (2019).\n29 See Virginia Eubanks, Automating Inequality (2018); Cathy O’Neil, Weapons of Math Destruction (2016).\n30 See Christopher Whyte, Deepfake News: AI-Enabled Disinformation as a Multi-Level Public Policy Challenge, 5 J. Cyber Pol’y 199 (2020); Jeffrey Dastin, \nAmazon Scrapes Secret AI Recruiting Tool that Showed Bias Against Women, Reuters (Oct. 10, 2018), https://www.reuters.com/article/us-amazon-com\u0002jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G; James Vincent, Google ‘Fixed’ its \nRacist Algorithm by Removing Gorillas from its Image-Labeling Tech, The Verge (Jan. 12, 2018), https://www.theverge.com/2018/1/12/16882408/google\u0002racist-gorillas-photo-recognition-algorithm-ai;.\n31 Kate Crawford, Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence 211 (2021) (“AI systems are built to see and \nintervene in the world in ways that primarily benefit the states, institutions, and corporations that they serve. In this sense, AI systems are expressions \nof power that emerge from wider economic and political forces, created to increase profits and centralize control for those who wield them.”).\n32 Elizabeth Gibney, Self-Taught AI is Best Yet at Strategy Game Go, Nature (Oct. 18, 2017), https://www.nature.com/news/self-taught-ai-is-best-yet-at\u0002strategy-game-go-1.22858.\n33 Bill Schackner, Carnegie Mellon’s Prestigious Computer Science School has a New Leader, Pittsburgh Post-Gazette (Aug. 8, 2019), https://www.post\u0002gazette.com/news/education/2019/08/08/Carnegie-Mellon-University-computer-science-Martial-Hebert-dean-artificial-intgelligence-google-robotics/\nstories/201908080096.\n34 Bipartisan Pol’y Ctr, Cementing American Artificial Intelligence Leadership: AI Research & Development (2020).\n35 Nur Ahmed & Muntasir Wahed, The De-Democratization of AI: Deep Learning and the Compute Divide in Artificial Intelligence Research, Cornell U.\n(Oct. 22, 2020), https://arxiv.org/pdf/2010.15581.pdf.\n36 Id.\n37 Fei-Fei Li, America’s Global Leadership in Human-Centered AI Can’t Come From Industry Alone, The Hill (July 6, 2021), https://thehill.com/opinion/\ntechnology/561638-americas-global-leadership-in-human-centered-ai-cant-come-from-industry?rl=1.\n38 Cade Metz, A.I. Researchers Are Making More Than $1 Million, Even at a Nonprofit, N.Y. Times (Apr. 19, 2018), https://www.nytimes.com/2018/04/19/\ntechnology/artificial-intelligence-salaries-openai.html. \n39 Stan. U. Inst. for Human-Centered Artificial Intelligence, supra note 3, at 118.\n40 Michael Gofman & Zhao Jin, Artificial Intelligence, Education, and Entrepreneurship, SSRN (Sept. 17, 2019), https://papers.ssrn.com/sol3/papers.\ncfm?abstract_id=3449440.\n41 Jathan Sadowski, When Data is Capital: Datafication, Accumulation, and Extraction, 2019 Big Data & Soc’y 1 (2019).\n42 For example, researchers have clamored for Facebook to share some of its proprietary data so they can better understand the effect of social media \non politics and societal discourse. Simon Hegelich, Facebook Needs to Share More with Researchers, Nature (Mar. 24, 2020), https://www.nature.com/\narticles/d41586-020-00828-5.\n43 Ashlee Vance, This Tech Bubble Is Different, Bloomberg (Apr. 14, 2011), https://www.bloomberg.com/news/articles/2011-04-14/this-tech-bubble-is\u0002different. \n44 Amy O’Hara & Carla Medalia, Data Sharing in the Federal Statistical System: Impediments and Possibilities, 675 Annals Am. Acad. Pol. & Soc. Sci. 138, \n140-41 (2018).\n45 Dario Amodei & Danny Hernandez, AI and Compute, Open AI (May 16, 2018), https://openai.com/blog/ai-and-compute/.\n46 See, e.g., Ahmed & Wahed, supra note 35; Ian Sample, ‘We Can’t Compete’: Why Universities Are Losing Their Best AI Scientists, The Guardian (Nov. 1, \n2017), https://www.theguardian.com/science/2017/nov/01/cant-compete-universities-losing-best-ai-scientists.\n47 Neil C. Thompson, Shuning Ge & Yash M. Sherry, Building the Algorithm Commons: Who Discovered the Algorithms that Underpin Computing in the \nModern Enterprise?, 11 Global Strategy J. 17-33 (2020).\n48 Minkyung Baek, RoseTTAFold: Accurate Protein Structure Prediction Accessible to All, U. Wash. Inst. for Protein Design (July 15, 2021), https://www.\nipd.uw.edu/2021/07/rosettafold-accurate-protein-structure-prediction-accessible-to-all/; Minkyung Baek et al., Accurate Prediction of Protein Structures \nand Interactions Using a Three-Track Neural Network, Sci. Mag. (July 15, 2021), https://science.sciencemag.org/content/sci/early/2021/07/19/science.\nabj8754.full.pdf.\n49 How Diplomacy Helped to End the Race to Sequence the Human Genome, Nature (June 24, 2020), https://www.nature.com/articles/d41586-020-\n01849-w.\n50 Joel Klinger et al., A Narrowing of AI Research?, Cornell U. (Nov. 17, 2020), https://arxiv.org/pdf/2009.10385.pdf.\n51 Id. \n52 Alex Tamkin et al., Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models, Cornell U. (Feb. 4, 2021), https://arxiv.\norg/pdf/2102.02503.pdf.\nA Blueprint for the National Research Cloud 93\n53 Those 5 were Boston, San Francisco, San Jose, Seattle and San Diego. See Robert D. Atkinson, Mark Muro & Jacob Whiton, The Case for Growth \nCenters: How to Spread Tech Innovation Across America, Brookings (Dec. 9, 2019), https://www.brookings.edu/research/growth-centers-how-to-spread\u0002tech-innovation-across-america/.\n54 Interview with Professor Erik Brynjolfsson, Director, Stanford Digital Economy Lab (2021). \n55 Solon Barocas & Andrew D. Selbst, Big Data’s Disparate Impact, 104 Cal. L. Rev. 671 (2016).\nChapter 2\n1 “Principal Investigator” status may differ from university to university, but typically represents the core faculty that are eligible to oversee research \nprojects at their home institutions.\n2 See Beth Jensen, AI Index Diversity Report: An Unmoving Needle, Stan. U. Inst. for Human-Centered Artificial Intelligence (May 3, 2021), https://hai.\nstanford.edu/news/ai-index-diversity-report-unmoving-needle.\n3 For a perspective, for instance, on the importance of modeling and simulation in physics, see Karen E. Wilcox, Omar Ghattas & Patrick Heimbach, The \nImperative of Physics-Based Modeling and Inverse Theory in Computational Science, 1 Nature Comp. Sci. 166 (2021).\n4 15 U.S.C. § 9415 (emphasis added). \n5 Id. (emphasis added). \n6 Contemporaneous accounts corroborate this core focus. The National Security Commission on AI, for instance, describes the proposal as “provid[ing] \nverified researchers and students subsidized access to scalable compute resources” with a specific reference to the “compute divide” that has left “middle\u0002and lower-tier universities [lacking] the resources necessary for cutting-edge AI research.” Nat’l Security Comm’n on A.I., Final Report 191, 197 (2021) \n(emphasis added). Upon the announcement of the NRC legislation, Jeff Dean, SVP of Google Research and Google Health, noted, “A National AI Research \nResource will help accelerate US progress in artificial intelligence and advanced technologies by providing academic researchers access to the cloud \ncomputing resources necessary for experiments at scale.” Brandi Vincent, Congress Inches Closer to Creating a National Cloud for AI Research, NextGov (July \n2, 2020), https://www.nextgov.com/emerging-tech/2020/07/congress-inches-closer-creating-national-cloud-ai-research/166624/ (emphasis added). Others \nhave suggested that “researchers” under NRC could include individuals at small businesses, start-up companies, non-profits, and certain technology firms. \nOne co-sponsor of the legislation, for instance, suggested that NRC resources should be provided to “developers” and “entrepreneurs.” Portman, Heinrich \nIntroduce Bipartisan Legislation to Develop National Cloud Computer for AI Research, Rob Portman, U.S. Senator for Ohio (June 4, 2020), https://www.\nportman.senate.gov/newsroom/press-releases/portman-heinrich-introduce-bipartisan-legislation-develop-national-cloud.\n7 Frequently Asked Questions About Small Businesses, U.S. Small Bus. Admin. Office of Advoc. (Oct. 2020), https://cdn.advocacy.sba.gov/wp-content/\nuploads/2020/11/05122043/Small-Business-FAQ-2020.pdf. \n8 Louise Balle, Information on Small Business Startups, Houston Chron., https://smallbusiness.chron.com/information-small-business-startups-2491.\nhtml. \n9 Such entities could potentially collaborate with academic partners, and the NRC would of course also need to set rules about collaborator eligibility. \n10 PI status provides a level of standardization across faculty compared to other metrics, such as tenure-track or designation as research faculty. For \nexample, the University of Michigan appoints individuals focused on full-time research as “research faculty,” which is not a tenure track position. In \ncontrast, research faculty at Purdue are eligible for tenure-track. Distinct from the categorization used by both universities, MIT designates full-time \nresearchers as “academic staff” rather than faculty. All three types of researchers, however, qualify for principal investigator status at their respective \nuniversities. Some universities go further by providing temporary PI status to non-PI status individuals affiliated with the university for a single project \n(including all three universities mentioned previously).\n11 Community & Education Resource Requests, CloudBank, https://www.cloudbank.org/training/cloudbank-community#toc-eligibilit-36nfpcrS. \n12 Apply for an Account, Compute Canada, https://www.computecanada.ca/research-portal/account-management/apply-for-an-account/. \n13 Nat’l Sci. Bd., Science & Engineering Indicators 2016, Academic Research and Development 72 (2016). \n14 Id.\n15 College Enrollment in the United States from 1965 to 2019 and Projections up to 2029 for Public and Private Colleges, Statista (Jan. 2021), https://www.\nstatista.com/statistics/183995/us-college-enrollment-and-projections-in-public-and-private-institutions/.\n16 Colaboratory – Frequently Asked Questions, Google, https://research.google.com/colaboratory/faq.html.\n17 Weekly Maximum GPU Usage, Kaggle (2019), https://www.kaggle.com/general/108481.\n18 Community & Education Resource Requests, supra note 11.\n19 Merit Review: Why You Should Volunteer to Serve as an NSF Reviewer, Nat’l Sci. Found., https://www.nsf.gov/bfa/dias/policy/merit_review/reviewer.\njsp#1.\n20 See XSEDE Campus Champions, XSEDE, https://www.xsede.org/community-engagement/campus-champions.\n21 Compute Canada, for instance, provides access to 15% of PIs to increased compute capacity based on a merit competition. In 2021, Compute \nCanada completed its review of 650 research submissions in about five months with only 80 volunteer reviewers from Canadian academic institutions \nto assess the scientific merit of the proposal. Resource Allocation Competitions, Compute Canada, https://www.computecanada.ca/research-portal/\naccessing-resources/resource-allocation-competitions/; 2021 Resource Allocations Competition Results, Compute Canada, https://www.computecanada.\nca/research-portal/accessing-resources/resource-allocation-competitions/rac-2021-results/. Compare this with CloudBank, which allocates compute \nresources by leveraging NSF’s grant administration process: In 2019, NSF needed 30,000 volunteer reviewers to handle over 40,000 proposals, with \neach proposal requiring about 10 months to process from start to finish. Nat’l. Sci. Found., Merit Review Process: Fiscal Year 2019 Digest (2020); NSF \nProposal and Award Process, Nat’l Sci. Found., https://www.nsf.gov/attachments/116169/public/nsf_proposal_and_award_process.pdf.\n22 Another boundary question will be the resource allocation to PIs that are affiliated both with universities and with private companies. As a default, \nNRC resources should go toward academic projects, and not subsidize work that is conducted in private researcher capacity. \n23 Resource Allocation Competitions, supra note 21.\n24 Enabling Access to Cloud Computing Resources for CISE Research and Education (Cloud Access), Nat’l Sci. Found., https://www.nsf.gov/pubs/2019/\nnsf19510/nsf19510.htm.\n25 Simplifying Cloud Services, Sci. Node (Dec. 2, 2019), https://sciencenode.org/feature/An%20easier%20cloud.php. \n26 Frequently Asked Questions (FAQ), CloudBank, https://www.cloudbank.org/faq.\nA Blueprint for the National Research Cloud 94\n27 Simplifying Cloud Services, supra note 25.\n28 Id.\n29 Id.\n30 Frequently Asked Questions (FAQ), supra note 26.\n31 Frequently Asked Questions (FAQs) for Budgeting for Cloud Computing Resources via CloudBank in NSF Proposals, Nat’l Sci. Found., https://www.nsf.\ngov/pubs/2020/nsf20108/nsf20108.jsp.\n32 Simplifying Access to Cloud Resources for Researchers: CloudBank, Amazon Web Serv. (Nov. 16, 2020), https://aws.amazon.com/blogs/publicsector/\nsimplifying-access-cloud-resources-researchers-cloudbank/.\n33 Community & Education Resource Requests, supra note 11.\n34 Larry Dignan, AWS Cloud Computing Ops, Data Centers, 1.3 Million Servers Creating Efficiency Flywheel, ZDNet (June 17, 2016), https://www.zdnet.\ncom/article/aws-cloud-computing-ops-data-centers-1-3-million-servers-creating-efficiency-flywheel/; Rich Miller, Ballmer: Microsoft Has 1 Million \nServers, Data Ctr. Knowledge (July 15, 2013), https://www.datacenterknowledge.com/archives/2013/07/15/ballmer-microsoft-has-1-million-servers; \nDaniel Oberhaus, Amazon, Google, Microsoft: Here’s Who Has the Greenest Cloud, Wired (Dec. 18, 2019), https://www.wired.com/story/amazon-google\u0002microsoft-green-clouds-and-hyperscale-data-centers/; Russell Brandom, Mapping out Amazon’s Invisible Server Empire, The Verge (May 10, 2019), \nhttps://www.theverge.com/2019/5/10/18563485/amazon-web-services-internet-location-map-data-center.\n35 See, e.g., AWS Pricing, Amazon Web Services, https://aws.amazon.com/pricing/; Overview of Cloud Billing Concepts, Google Cloud, https://cloud.\ngoogle.com/billing/docs/concepts; Azure Pricing, Azure, https://azure.microsoft.com/en-us/pricing/#product-pricing. \n36 Large research universities already negotiate enterprise agreements with cloud providers.\n37 What We Do, XSEDE, https://www.xsede.org/about/what-we-do (last visited Sept. 19, 2021).\n38 XSEDE Overall Organization, XSEDE Wiki, https://confluence.xsede.org/display/XT/XSEDE+Overall+Organization (last visited Sept. 19, 2021).\n39 XSEDE Allocations Info & Policies, XSEDE, https://portal.xsede.org/allocations/policies (last visited Sept. 19, 2021).\n40 Id.\n41 Id.\n42 Startup Allocations, XSEDE, https://portal.xsede.org/allocations/startup (last visited Sept. 19, 2021).\n43 Id.\n44 Id.\n45 Id.\n46 Research Allocations, XSEDE, https://portal.xsede.org/allocations/research (last visited Sept. 19, 2021).\n47 Id.\n48 Id.\n49 Id.\n50 XSEDE Allocations Info & Policies, supra note 36.\n51 XSEDE Campus Champions, supra note 20.\n52 Id.\n53 Id.\n54 XSEDE as a Collaborator on Proposals, XSEDE, https://www.xsede.org/about/collaborating-with-xsede (last visited Sept. 19, 2021).\n55 COVID-19 HPC Consortium, XSEDE, https://www.xsede.org/covid19-hpc-consortium (last visited Sept. 19, 2021).\n56 Amazon, for example, introduced its P4, P3, and P2 instances in 2020, 1997, and 1996, respectively. Frederic Lardinois, AWS Launches Its Next-Gen \nGPU Instances with 8 Nvidia A100 Tensor Core GPUs, TechCrunch (Nov. 2, 2020), https://social.techcrunch.com/2020/11/02/aws-launches-its-next-gen\u0002gpu-instances/; Ian C. Schafer, Amazon Elastic Compute Cloud P3 Launched alongside NVIDIA GPU Cloud, SD Times (Oct. 26, 2017), https://sdtimes.com/\nai/amazon-elastic-compute-cloud-p3-launched-alongside-nvidia-gpu-cloud/; Jeff Barr, New P2 Instance Type for Amazon EC2 – Up to 16 GPUs, Amazon \nWeb Services (Sept. 29, 2016), https://aws.amazon.com/blogs/aws/new-p2-instance-type-for-amazon-ec2-up-to-16-gpus/. The introduction years of \nthe P4 and P3 instances line up with the release of NVIDIA’s newest general purpose data center GPUs. \n57 See, e.g., Sarah Wang & Martin Casado, The Cost of Cloud, a Trillion Dollar Paradox, Andreessen Horowitz (May 27, 2021), https://a16z.\ncom/2021/05/27/cost-of-cloud-paradox-market-cap-cloud-lifecycle-scale-growth-repatriation-optimization/.\n58 Preston Smith et al., Community Clusters or the Cloud: Continuing Cost Assessment of On-Premises and Cloud HPC in Higher Education, 2019 \nProceedings Practice & Experience Advanced Res. Computing on Rise of the Machines 1 (2019). The amortized cost includes the annual compute \ncost, subsidized hardware cost, and power costs, but does not include personnel costs, as such costs are fixed and would be recurred regardless of \nwhether a cluster existed physically on-prem or on the cloud. Id.\n59 Craig A. Stewart et al., Return on Investment for Three Cyberinfrastructure Facilities: A Local Campus Supercomputer; the NSF-Funded Jetstream Cloud \nSystem; and XSEDE, 11 Int’l Conf. on Utility & Cloud Computing 223 (2018).\n60 Srijith Rajamohan & Robert E. Settlage, Informing the On/Off-prem Cloud Discussion in Higher Education, 2020 Practice & Experience Adv. Res. \nComputing 64 (2020). The cost sources include hardware, software services, software administration, electricity, and facilities but do not include \ncomputational scientists support, scientific software licenses, and data transfer costs. The study is also limited to Virginia Tech’s particular cloud \nworkload.\n61 Jennifer Villa & Dave Troiano, Choosing Your Deep Learning Infrastructure: The Cloud vs. On-Prem Debate, Determined AI (July 30, 2020), https://\ndetermined.ai/blog/cloud-v-onprem/; Is HPC Going to Cost Me a Fortune?, insideHPC, https://insidehpc.com/hpc-basic-training/is-hpc-going-to-cost\u0002me-a-fortune/.\n62 Interview with Suzanne Talon, Regional Director, Compute Canada (Jan. 14, 2021).\n63 Compute Canada, Cloud Computing for Researchers 1 (2016), https://www.computecanada.ca/wp-content/uploads/2015/02/CloudStrategy2016-\n2019-forresearchersEXTERNAL-1.pdf.\n64 US Plans $1.8 Billion Spend on DOE Exascale Supercomputing, HPCwire (Apr. 11, 2018), https://www.hpcwire.com/2018/04/11/us-plans-1-8-billion\u0002spend-on-doe-exascale-supercomputing/; Federal Government, Advanced HPC, https://www.advancedhpc.com/pages/federal-government; United \nStates Continues To Lead World In Supercomputing, Energy.gov, https://www.energy.gov/articles/united-states-continues-lead-world-supercomputing; \nHigh Performance Computing, Energy.gov, https://www.energy.gov/science/initiatives/high-performance-computing.\nA Blueprint for the National Research Cloud 95\n65 See, e.g., DOE Announces Five New Energy Projects at LLNL, LLNL (Nov. 13, 2020), https://www.llnl.gov/news/doe-announces-five-new-energy\u0002projects-llnl; New HPCMP System at the AFRL DSRC DoD Supercomputing Resource Center to Provide over Nine PetaFLOPS of Computing Power to \nAddress Physics, AI, and ML Applications for DoD Users, DOD HPC, https://www.hpc.mil/images/hpcdocs/newsroom/21-19_TI-21_web_announcement_\nAFRL_DSRC.pdf; Public Announcement, DOD HPC, https://www.hpc.mil/images/hpcdocs/newsroom/awards_and_press/HC101321D0002_PUBLIC_\nANNOUNCEMENT_20210505.pdf.\n66 Devin Coldewey, $600M Cray Supercomputer Will Tower Above the Rest — to Build Better Nukes, TechCrunch (Aug. 13, 2019), https://social.\ntechcrunch.com/2019/08/13/600m-cray-supercomputer-will-tower-above-the-rest-to-build-better-nukes/; CORAL-2 RFP, Oak Ridge Nat’l Laboratory\n(Apr. 9, 2018), https://procurement.ornl.gov/rfp/CORAL2/.\n67 See, e.g., NSF Funds Five New XSEDE-Allocated Systems, Nat’l Sci. Found. (Aug. 10, 2020), https://www.xsede.org/-/nsf-funds-five-new-xsede\u0002allocated-systems. \n68 Timothy Prickett Morgan, Bending The Supercomputing Cost Curve Down, The Next Platform (Dec. 2, 2019), http://www.nextplatform.\ncom/2019/12/02/bending-the-supercomputing-cost-curve-down/; Ben Dickson, The GPT-3 Economy, TechTalks (Sept. 21, 2020), https://bdtechtalks.\ncom/2020/09/21/gpt-3-economy-business-model/.\n69 Elijah Wolfson, The US Just Retook the Title of World’s Fastest Supercomputer from China, Quartz (June 9, 2018), https://qz.com/1301510/the-us-has\u0002the-worlds-fastest-supercomputer-again-the-200-petaflop-summit/.\n70 November 2020, TOP500 (Nov. 2020), https://www.top500.org/lists/top500/2020/11/.\n71 U.S. Department of Energy and Cray to Deliver Record-Setting Frontier Supercomputer at ORNL, Oak Ridge Nat’l Laboratory (May 7, 2019), https://\nwww.ornl.gov/news/us-department-energy-and-cray-deliver-record-setting-frontier-supercomputer-ornl.\n72 Coury Turczyn, Building an Exascale-Class Data Center, Oak Ridge Leadership Computing Facility (Dec. 11, 2020), https://www.olcf.ornl.\ngov/2020/12/11/building-an-exascale-class-data-center/.\n73 Don Clark, Intel Slips, and a High-Profile Supercomputer Is Delayed, N.Y. Times (Aug. 27, 2020), https://www.nytimes.com/2020/08/27/technology/\nintel-aurora-supercomputer.html; Mila Jasper, 10 of 15 of DOD’s Major IT Projects Are Behind Schedule, GAO Found, Nextgov (Jan. 4, 2021), https://www.\nnextgov.com/it-modernization/2021/01/10-15-dods-major-it-projects-are-behind-schedule-gao-found/171155/.\n74 See Nattakarn Phaphoom et al., A Survey Study on Major Technical Barriers Affecting the Decision to Adopt Cloud Services, 103 J. Systems & Software 167, \n171-72 (2015) (describing data portability, integration with existing systems, migration complexity, and availability as major barriers to cloud adoption); \nAbdulrahman Alharthi et al., An Overview of Cloud Services Adoption Challenges in Higher Education Institutions, 2 Proceedings of the Int’l Workshop \non Emerging Software as a Service & Analytics 102, 107-08 (2015) (acknowledging the low rate of cloud computing adoption in higher education and \nemphasizing that bolstering both the perceived ease of use and the actual usefulness of cloud computing can increase the adoption rate).\n75 See Dep’t of Energy, FY 2021 Budget Justification Volume 4: Science (2020).\n76 Joe Weinman, Cloudonomics: The Business Value of Cloud Computing (2012).\n77 OLCF supports and manages ORNL’s supercomputing resources, including Summit and eventually Frontier. This figure accounts for “operations and \nuser support at the LCF facilities–including power, space, leases, and staff. Id. at 37-38.\n78 ACLF supports and manages Argonne National Laboratory’s computing resources, including the Theta system and, later this year, the new Aurora \ncomputer, another DOE exascale HPC system. Id. \n79 OLCF operated its Titan HPC system for 7 years. See Coury Turczyn, supra note 72. ACLF also operated its Mira HPC system for 7 years. Argonne’s Mira \nSupercomputer to Retire After Years of Enabling Groundbreaking Science, HPCwire (Dec. 20, 2019), https://www.hpcwire.com/2019/12/20/argonnes\u0002mira-supercomputer-to-retire-after-years-of-enabling-groundbreaking-science/. If still operational, these systems would rank about the 19th and 29th \nfastest in the world, respectively. Compare November 2020, supra note 70, with TOP500 List - June 2019, TOP500 (June 2019), https://www.top500.org/\nlists/top500/list/2019/06/.\n80 See, e.g., Kim Zetter, Top Federal Lab Hacked in Spear-Phishing Attack, Wired (Apr. 20, 2011), https://www.wired.com/2011/04/oak-ridge-lab-hack/; \nNatasha Bertrand & Eric Wolff, Nuclear Weapons Agency Breached amid Massive Cyber Onslaught, Politico (Dec. 17, 2020), https://www.politico.com/\nnews/2020/12/17/nuclear-agency-hacked-officials-inform-congress-447855 (last visited Mar. 2, 2021); Ryan Lucas, List Of Federal Agencies Affected By \nA Major Cyberattack Continues To Grow, NPR (Dec. 18, 2020), https://www.npr.org/2020/12/18/948133260/list-of-federal-agencies-affected-by-a-major\u0002cyberattack-continues-to-grow (last visited Mar. 2, 2021).\n81 We discuss data access models in Chapter Three.\n82 See Ongoing Projects, RIKEN Ctr. for Computational Sci., https://www.r-ccs.riken.jp/en/fugaku/research/covid-19/projects/.\n83 Fugaku Retains Title as World’s Fastest Supercomputer, HPCWire (Nov. 17, 2020), https://www.hpcwire.com/off-the-wire/fugaku-retains-title-as\u0002worlds-fastest-supercomputer/.\n84 November 2020, supra note 70.\n85 Id.\n86 Behind the Scenes of Fugaku as the World’s Fastest Supercomputer, Fujitsu (Feb. 2, 2021), https://blog.global.fujitsu.com/fgb/2021-02-02/behind\u0002the-scenes-of-fugaku-as-the-worlds-fastest-supercomputer-1manufacturing/.\n87 Id.\n88 Don Clark, Japanese Supercomputer Is Crowned World’s Speediest, N.Y. Times (June 22, 2020), https://www.nytimes.com/2020/06/22/technology/\njapanese-supercomputer-fugaku-tops-american-chinese-machines.html.\n89 Justin McCurry, Non-Woven Masks Better to Stop Covid-19, Says Japanese Supercomputer, The Guardian (Aug. 26, 2020), http://www.theguardian.\ncom/world/2020/aug/26/non-woven-masks-better-to-stop-covid-19-says-japanese-supercomputer.\n90 Fujitsu and RIKEN Complete Joint Development of Japan’s Fugaku, the World’s Fastest Supercomputer, Fujitsu (Mar. 9, 2021), https://www.fujitsu.com/\nglobal/about/resources/news/press-releases/2021/0309-02.html.\n91 Id.\n92 See, e.g., Rolf Harms & Michael Yamartino, The Economics of the Cloud (2010); Srijith Rajamohan & Robert E. Settlage, Informing the On/Off-Prem \nCloud Discussion in Higher Education, 2020 Practice & Experience in Advanced Res. Computing 64 (2020); Byung Chul Tak et al., To Move or Not To Move: \nThe Economics of Cloud Computing, 3 USENIX Conf. on Hot Topics in Cloud Computing 1 (2011); Edward Walker, Walter Brisken & Jonathan Romney, \nTo Lease or Not To Lease from Storage Clouds, 43 Computer 44 (2010).\n93 See, e.g., Di Zhang et al., RLScheduler: An Automated HPC Batch Job Scheduler Using Reinforcement Learning, Cornell U. (Sept. 2, 2020), https://arxiv.\norg/pdf/1910.08925.pdf.\nA Blueprint for the National Research Cloud 96\n94 For instance, we have not been able to identify good estimates of electricity and cooling costs for DOE supercomputers. \n95 Hugh Couchman et al., Compute Canada — Calcul Canada: A Proposal to the Canada Foundation for Innovation – National Platforms Fund\n58 (2006).\n96 About, Compute Canada, https://www.computecanada.ca/about/.\n97 National Systems, Compute Canada, https://www.computecanada.ca/techrenewal/national-systems/.\n98 Compute Canada Technology Briefing, Compute Canada (Nov. 2017), https://www.computecanada.ca/wp-content/uploads/2015/02/Technology\u0002Briefing-November-2017.pdf. \n99 Cloud Computing for Researchers, Compute Canada (Dec. 2016), https://www.computecanada.ca/wp-content/uploads/2015/02/CloudStrategy2016-\n2019-forresearchersEXTERNAL-1.pdf.\n100 Id.\n101 Budget Submission 2018, Compute Canada (2018), https://www.computecanada.ca/wp-content/uploads/2015/02/UTF-8Compute20Canada20Budg\net20Submission202018.pdf at 5.\n102 Compute Canada projected it had only met about 55% of total demand for CPU compute hours in 2018. Id.\n103 Id.\n104 Compute Canada, Annual Report 2019-2020 4 (2020).\n105 Rapid Access Service, Compute Canada, https://www.computecanada.ca/research-portal/accessing-resources/rapid-access-service/.\n106 Id.\n107 Resource Allocation Competitions, supra note 21.\n108 Id.\n109 Id.\n110 Id.\n111 Id.\n112 2021 Resource Allocations Competition Results, supra note 21.\nChapter 3\n1 National Research Cloud Call to Action, Stan. U. Inst. for Human-Centered Artificial Intelligence (2020), https://hai.stanford.edu/national\u0002research-cloud-joint-letter. \n2 We discuss the Privacy Act and privacy considerations in more detail in Chapter Five.\n3 Amy O’Hara & Carla Medalia, Data Sharing in the Federal Statistical System: Impediments and Possibilities, 675 Annals Am. Acad. Pol. & Soc. Sci. 138, \n140-41 (2018); see also President’s Mgmt. Agenda, Federal Data Strategy 2020 Action Plan (2020).\n4 Improved data access would, as we describe below, also promote evidence-based policymaking and improve trust in science (as data access makes \nreplication efforts much easier). \n5 See, e.g., Nick Hart & Nancy Potok, Modernizing U.S. Data Infrastructure: Design Considerations for Implementing a National Secure Data \nService to Improve Statistics and Evidence Building (2020).\n6 These initiatives are successful in that they are sustainable and have been used by researchers to access multi-agency government data. The only \nexception is the National Secure Data Service (NSDS), which has not yet been implemented. We discuss the NSDS alongside the Census Bureau and \nthe Evidence-Based Policy-Making Act of 2018 below. Importantly, our focus in these case studies is not to evaluate their efforts or measure their exact \nlevels of success but to identify and understand some of the differences and similarities in the range of data-sharing efforts.\n7 For instance, private sector data may facilitate research regarding social media use, internet behavior, or fill in gaps for federal statistics research \nthrough big data analysis. See Robert M. Groves & Brian A. Harris-Kojetin, Using Private-Sector Data for Federal Statistics, Nat’l Ctr. for Biotechnology \nInfo. (Jan. 12, 2017), https://www.ncbi.nlm.nih.gov/books/NBK425876/.\n8 See, e.g., National Data Service, Nat’l Data Serv., http://www.nationaldataservice.org; The Open Science Data Cloud, Open Sci. Data Cloud, https://\nwww.opensciencedatacloud.org, Harvard Dataverse, Harv. Dataverse, https://dataverse.harvard.edu, FigShare, https://figshare.com. \n9 Facebook Data for Research provides access to a variety of libraries, via in-house platforms. See, e.g., Facebook Data For Good, Facebook (2020), https://\ndataforgood.fb.com/; What is the Facebook Ad Library and How do I Search it?, Facebook (2021), https://www.facebook.com/help/259468828226154; \nFacebook Disaster Maps Methodology, Facebook (May 15, 2019), https://research.fb.com/facebook-disaster-maps-methodology/.\n10 For example, Twitter has a Developer Portal that provides access to their API to allow researchers to use user data for noncommercial purposes. \nSee Twitter Developers, Twitter (2021), https://developer.twitter.com/en/portal/petition/academic/is-it-right-for-you; Take Your Research Further with \nTwitter Data, Twitter (2021), https://developer.twitter.com/en/solutions/academic-research. Thus, uploading Twitter data to a separate Cloud may \nprovide few incentives to researchers who can use the API route.\n11 See Nat’l Acad. of Sci., Innovations in Federal Statistics 31-42 (2017).\n12 See Jennifer M. Urban, Joe Karaganis & Brianna M. Schofield, Notice & Takedown in Everyday Practice 39 (2017) (illustrating the difficulty \nthat online service providers face in manually evaluating a large volume of data for potential infringement; for example, one online service provider \nexplained that “out of fear of failing to remove infringing material, and motivated by the threat of statutory damages, its staff will take “six passes to try \nto find the [identified content].”); see also Letter from Thom Tillis, Marsha Blackburn, Christopher A. Coons, Dianne Feinstein et. al, to Sundar Pichai, \nChief Executive Officer, Google Inc. (Sept. 3, 2019), https://www.ipwatchdog.com/wp-content/uploads/2019/09/9.3-Content-ID-Ltr.pdf (“We have \nheard from copyright holders who have been denied access to Content ID tools, and as a result, are at a significant disadvantage to prevent repeated \nuploading of content that they have previously identified as infringing. They are left with the choice of spending hours each week seeking out and \nsending notices about the same copyrighted works, or allowing their intellectual property to be misappropriated.”).\n13 To illustrate the costs of implementing Content ID on a large-scale platform, Google announced in a report in 2016 that YouTube had invested more \nthan $60 million in Content ID. See Google, How Google Fights Piracy 6 (2016).\n14 See, e.g., AWS Customer Agreement, Amazon (Nov. 30, 2020), https://aws.amazon.com/agreement/. \n15 For instance, across the 29 distinct agencies in the Department of Health and Human Services (HHS), data “are largely kept in silos with a lack of \norganizational awareness of what data are collected across the Department and how to request access. Each agency operates within its own statutory \nA Blueprint for the National Research Cloud 97\nauthority and each dataset can be governed by a particular set of regulations.” U.S. Dep’t of Health & Human Services, The State of Data Sharing at \nthe U.S. Department of Health and Human Services 4 (2018). \n16 See, e.g., id. at 8 (“HHS lacks consistent and standardized processes for one agency to request data from another agency.”).\n17 O’Hara & Medalia, supra note 3, at 140-41.\n18 See id. at 142 (“Most [data-sharing] agreements rely heavily on interpersonal relationships and informal quid pro quo arrangements, handling data \nrequests in a less centralized fashion.”).\n19 Jeffrey Mervis, How Two Economists Got Direct Access to IRS Tax Records, Sci. Mag. (May 22, 2014), https://www.sciencemag.org/news/2014/05/how\u0002two-economists-got-direct-access-irs-tax-records. \n20 See Robert M. Groves & Adam Neufeld, Accelerating the Sharing of Data Across Sectors to Advance the Common Good 17 (2017).\n21 See, e.g., Data Use Agreement, Dep’t Health & Human Services, https://www.hhs.gov/sites/default/files/ocio/eplc/EPLC%20Archive%20\nDocuments/55-Data%20Use%20Agreement%20%28DUA%29/eplc_dua_practices_guide.pdf. \n22 O’Hara & Medalia, supra note 3, at 138, 141.\n23 Bipartisan Pol’y Ctr., Barriers to Using Government Data: Extended Analysis of the U.S. Commission on Evidence-Based Policymaking’s \nSurvey of Federal Agencies and Offices 18-20 (2018).\n24 See Research Data Assistance Center (ResDAC), Ctr. for Medicare & Medicaid Services (Aug. 30, 2018), https://www.cms.gov/Research-Statistics\u0002Data-and-Systems/Research/ResearchGenInfo/ResearchDataAssistanceCenter. \n25 O’Hara & Medalia, supra note 3, at 141.\n26 Michelle Mello et al., Waiting for Data: Barriers to Executing Data Use Agreements, 367 Sci. Mag. 150 (Jan. 10, 2020), https://www.\nsciencemagazinedigital.org/sciencemagazine/10_january_2020/MobilePagedArticle.action?articleId=1552284#articleId1552284. \n27 Interview with Amy O’Hara, Executive Director, Georgetown Federal Statistical Research Data Center (Apr. 22, 2021); see also Special Sworn Research \nProgram, Bureau of Econ. Analysis, https://www.bea.gov/research/special-sworn-researcher-program; Nat’l Ctr. for Educ. Stat., Restricted-Use \nData Procedures Manual (2011). \n28 See U.S. Gov’t Accountability Office, Federal Agencies Need to Address Aging Legacy Systems (2016).\n29 O’Hara & Medalia, supra note 3, at 140-41.\n30 See, e.g., id.; U.S. Gov’t Accountability Office, supra note 28. \n31 President’s Mgmt. Agenda, supra note 3, at 11.\n32 Groves & Neufeld, supra note 20, at 12-13. For a precise definition of sensitive data, see Glossary: Sensitive Information, Nat’l Inst. Standards & \nTech., https://csrc.nist.gov/glossary/term/sensitive_information. \n33 Shanna Nasiri, FedRAMP Low, Moderate, High: Understanding Security Baseline Levels, Reciprocity (Sept. 24, 2019), https://reciprocity.com/fedramp\u0002low-moderate-high-understanding-security-baseline-levels/.\n34 Michael McLaughlin, Reforming FedRAMP: A Guide to Improving the Federal Procurement and Risk Management of Cloud Services, Info. Tech. & \nInnovation Found. (June 15, 2020), https://itif.org/publications/2020/06/15/reforming-fedramp-guide-improving-federal-procurement-and-risk\u0002management.\n35 Frequently Asked Questions, FedRAMP, https://www.fedramp.gov/faqs. \n36 Do Once, Use Many - How Agencies Can Reuse a FedRAMP Authorization, FedRAMP (May 7, 2020), https://www.fedramp.gov/how-agencies-can-reuse\u0002a-fedramp-authorization/. \n37 FedRAMP, FedRAMP Low, Moderate, and High Security Control Baselines (2021).\n38 Security and Privacy Controls for Information Systems and Organizations, Nat’l Inst. Standards & Tech. (Sept. 23, 2020), https://csrc.nist.gov/\npublications/detail/sp/800-53/rev-5/final.\n39 See, e.g., id.; NIST Risk Management Framework AC-2: Account Management, Nat’l Inst. Standards & Tech., https://csrc.nist.gov/Projects/risk\u0002management/sp800-53-controls/release-search#!/control?version=4.0&number=AC-2; NIST Risk Management Framework AC-3: Access Enforcement, \nNat’l Inst. Standards & Tech., https://csrc.nist.gov/Projects/risk-management/sp800-53-controls/release-search#!/control?version=5.1&number=AC-3.\n40 See Mark Bergen, Google Engineers Refused to Build Security Tool to Win Military Contracts, Bloomberg (June 21, 2018), https://www.bloomberg.com/\nnews/articles/2018-06-21/google-engineers-refused-to-build-security-tool-to-win-military-contracts.\n41 See Nat’l Inst. Standards & Tech., Standards for Security Categorization of Federal Information and Information Systems (2004).\n42 Partnering with FedRAMP, FedRAMP, https://www.fedramp.gov/cloud-service-providers/. While it may cost cloud service providers between $365,000 \nand $865,000 and take 6-12 months to receive FedRAMP compliance, Adam Isles, Securing Your Cloud Solutions: Research and Analysis on Meeting \nFedRAMP/Government Standards 21 (2017), such costs are borne by the cloud service providers themselves, not the providers’ customers. Indeed, \nFedRAMP uses a “do once, use many” model: Once a cloud service provider obtains an authorization to operate (ATO), that ATO can be leveraged and \nreciprocated across multiple customers, eliminating duplicative efforts and inconsistencies that would come from requiring multiple re-authorizations. \nId. at 11.\n43 Even within FedRAMP there are substantial amounts of variation in how different organizations ensure compliance with the relevant controls \nand standards, with many of the controls written broadly enough to give room for substantial interpretation. However, it does lay out a variety of \nconsiderations and requirements that are consistent across domains and allows a degree of predictability and reliance that is not present in other \naspects of federal data governance. \n44 O’Hara & Medalia, supra note 3, at 141 (“Data sharing is taking place on a mandatory or voluntary basis, and data requests are managed through a \ndesignated staff/process or diffusely through an organization.”).\n45 Bipartisan Pol’y Ctr., supra note 23, at 17 (“The lack of standard procedures or guidelines for sharing data across federal agencies that fund \nresearch makes efforts to link and share data difficult or inefficient.”).\n46 See, e.g., Amy O’Hara, US Federal Data Policy: An Update on The Federal Data Strategy and The Evidence Act, 5 Int’l J. Population Data Sci. 5 (2020).\n47 While existing federal efforts and initiatives are already aimed at harmonizing data sharing best practices, see, e.g., 2020 Action Plan, Federal Data \nStrategy (May 14, 2020), https://strategy.data.gov/action-plan/, the NRC can accelerate these efforts. Indeed, the development of clear, consistent \nstandards is crucial in facilitating data-sharing. David Crotty, Ida Sim & Michael Stebbins, Open Access to Federally Funded Research Data 7 (2020). \n48 These requirements are inconsistent and out-of-date due to difficulties in defining risk as well as risk aversion on the parts of agencies. See O’Hara \n& Medalia, supra note 3, at 140-41; see also David S. Johnson et al., The Opportunities and Challenges of Using Administrative Data Linkages to Evaluate \nMobility, 657 Annals Am. Acad. Pol. & Soc. Sci. 252-53 (2015).\n49 For a discussion of inference threats, see Nat’l Acad. of Sci., Eng’g & Med., Federal Statistics, Multiple Data Sources, and Privacy Protection: \nNext Steps 68 (2017).\nA Blueprint for the National Research Cloud 98\n50 Congzheng Song & Ananth Raghunathan, Information Leakage in Embedding Models, Cornell U. (Mar. 31, 2020), https://arxiv.org/abs/2004.00053. \n51 See, e.g., Statistical Safeguards, Census Bureau (July 1, 2021), https://www.census.gov/about/policies/privacy/statistical_safeguards.html. \n52 Alexandra Wood et al., Differential Privacy: A Primer for a Non-Technical Audience, 21 Vand. J. Ent. & Tech. L. 209 (2018).\n53 Regulating Access to Data, UK Data Serv., https://www.ukdataservice.ac.uk/manage-data/legal-ethical/access-control/five-safes.\n54 Administrative Data Research Facility, Coleridge Initiative, https://coleridgeinitiative.org/adrf/. \n55 See O’Hara, supra note 46. \n56 For additional discussion of the privacy implications of the NRC, see Chapter Five. \n57 See U.S. Office of Mgmt. & Budget, Barriers to Using Administrative Data for Evidence-Building 7 (2016).\n58 Administrative Data Research Facility, supra note 54.\n59 Id.\n60 Training, Coleridge Initiative, https://coleridgeinitiative.org/training/.\n61 ADRF User Guide: Data Explorer, Coleridge Initiative, https://coleridgeinitiative.org/adrf/documentation/using-the-adrf/data-explorer/. \n62 ADRF User Guide: Exporting Results, Coleridge Initiative, https://coleridgeinitiative.org/adrf/documentation/using-the-adrf/exporting-results/.\n63 Id.\n64 ADRF User Guide: Data Hashing Application, Coleridge Initiative, https://coleridgeinitiative.org/adrf/documentation/adrf-overview/data-hashing\u0002application/. \n65 ADRF User Guide: Security Model and Compliance, Coleridge Initiative, https://coleridgeinitiative.org/adrf/documentation/adrf-overview/security\u0002model-and-compliance/. \n66 Overview for Collaborators, Coleridge Initiative, https://coleridgeinitiative.org/collaborators/. \n67 Data, Stan. Med. Ctr. for Population Health Sci., https://med.stanford.edu/phs/data.html. \n68 Stanford Ctr. for Philanthropy & Civ. Soc’y, Trusted Data Intermediaries 2-3 (2018).\n69 Others have also recognized the benefit of universal DUA templates. See Mello et al., supra note 26, at 150; Guidance for Providing and Using \nAdministrative Data for Statistical Purposes, Office of Mgmt. & Budget (Feb. 14, 2014), https://obamawhitehouse.archives.gov/sites/default/files/omb/\nmemoranda/2014/m-14-06.pdf. \n70 Data | Center for Population Health Sciences | Stanford Medicine, Stan. Med. Ctr. for Population Health Sci., https://med.stanford.edu/phs/data.\nhtml.\n71 See Stanford PHS – Datasets, Redivis, https://redivis.com/StanfordPHS/datasets?orgDatasets-tags=109.medicare.\n72 Access Levels, Redivis (July 2020), https://docs.redivis.com/reference/data-access/access-levels.\n73 Step 1: Getting Access, Stan. Med. PHS Documentation, https://phsdocs.developerhub.io/start-here/getting-data-access.\n74 Id.\n75 Id.\n76 PHS Data-Use Workflow, Stan. Med. PHS Documentation, https://phsdocs.stanford.edu/start-here/phs-data-use-workflow.\n77 Id.\n78 PHS Computing Environment, Stan. Med. PHS Documentation, https://phsdocs.stanford.edu/computing-environment.\n79 Id.\n80 See U.S. Gov’t Accountability Office, Federal Agencies Need to Address Aging Legacy Systems 15 (2016) (noting that from 2010-2015, many \nfederal agencies increased their spending on operations and maintenance due to legacy systems).\n81 David Freeman Engstrom, Daniel E. Ho, Catherine M. Sharkey & Mariano-Florentino Cuéllar, Government by Algorithm: Artificial \nIntelligence in Federal Administrative Agencies 6, 71-72 (2020).\n82 Id. at 6-7.\n83 Id. at 71-72.\n84 Id. at 73.\n85 Id. at 6.\n86 See Results for America, The Promise of the Foundations for Evidence-Based Policymaking Act and Proposed Next Steps (2019) \n87 For example, the Uniform Federal Crime Reporting Act of 1988 requires federal law enforcement agencies to share crime data with the FBI. See 34 \nU.S.C. §§41303(c)(2), (3), (4). Unfortunately, though, no federal agencies apparently currently share their data with the FBI under this law. Nat’l Acad. of \nSci., supra note 11, at 41 (2017).\n88 Katharine G. Abraham & Ron Haskins, The Promise of Evidence-Based Policymaking; Comm’n on Evidence-Based Policymaking (2018).\n89 These privacy-preserving mechanisms are especially important in light of ongoing legal and political challenges in differential privacy application to \nFederal data. See, e.g., Dan Bouk & danah boyd, Democracy’s Data Infrastructure (2021).\n90 Foundations for Evidence-Based Policymaking Act of 2018, Pub. L. No. 115-435.\n91 Confidential Information Protection and Statistical Efficiency Act of 2002, Pub. L. No. 107-347.\n92 Overview, Federal Data Strategy (2020), https://strategy.data.gov/overview/.\n93 UK Data Service, UK Data Serv., https://www.ukdataservice.ac.uk/ (last visited Jun. 21, 2021).\n94 For example, the Social Security Administration alone has over 14 petabytes of data, stored in roughly 200 databases. Engstrom, Ho, Sharkey & \nCuéllar, supra note 81, at 72.\n95 Google Earth Engine, Google Earth Engine, https://earthengine.google.com (last visited Aug. 15, 2021). \n96 World of Work, ADR UK, https://www.adruk.org/our-work/world-of-work/.\n97 Annual Respondents Database, 1973-2008: Secure Access, UK Data Serv. (2020), https://beta.ukdataservice.ac.uk/datacatalogue/studies/\nstudy?id=6644. \n98 UK Innovation Survey, UK Data Serv. (2021), https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=6699. \n99 Quarterly Labour Force Survey, 1992-2021: Secure Access, UK Data Serv. (2021), https://beta.ukdataservice.ac.uk/datacatalogue/studies/\nstudy?id=6727.\n100 Understanding Society: Waves 1-10, 2009-2019 and Harmonised BHPS: Waves 1-18, 1991-2009: Secure Access, UK Data Serv. (2021), https://beta.\nukdataservice.ac.uk/datacatalogue/studies/study?id=6676.\n101 These datasets have helped researchers tackle some specific, public good questions. See, e.g., Francisco Perales, Why Does the Work Women Do Pay \nLess Than the Work Men Do?, UK Data Serv. (Dec. 8, 2011), https://beta.ukdataservice.ac.uk/impact/case-studies/case-study?id=62; Eva-Maria Bonin, Do \nParenting Programmes Reduce Conduct Disorder?, UK Data Serv. (Apr. 4, 2012), https://beta.ukdataservice.ac.uk/impact/case-studies/case-study?id=93. \nA Blueprint for the National Research Cloud 99\n102 Identifying Priority Access or Quality Improvements for Federal Data and Models for Artificial Intelligence Research and Development (R&D), and \nTesting; Request for Information, 84 Fed. Reg. 32962 (July 10, 2019). \n103 Nick Hart, Data Coalition Comments on AI Data and Model R&D RFI, Data Coalition (Aug. 9, 2019), http://www.datacoalition.org/wp-content/\nuploads/2019/09/Comment.RFI_.OMB_.2019-14618.DataCoalition.pdf. \n104 Id.\n105 See Adam R. Pah et al., How to Build a More Open Justice System, 369 Sci. 134 (2020); see also Seamus Hughes, The Federal Courts Are Running an \nOnline Scam, Politico (Mar. 20, 2019), https://www.politico.com/magazine/story/2019/03/20/pacer-court-records-225821/.\n106 Legal Authority and Policies for Data Linkage at Census, Census Bureau (Apr. 4, 2018), https://www.census.gov/about/adrm/linkage/about/\nauthority.html.\n107 BLS Restricted Data Access, U.S. Bureau of Lab. Stat., https://www.bls.gov/rda/restricted-data.htm (last updated May 20, 2021).\n108 Welcome to the PDS, NASA, https://pds.nasa.gov.\nChapter 4\n1 While we believe that these are the primary axes for consideration, some secondary considerations include organizational clout, talent retention, and \nbureaucratic overhead.\n2 Congressional Research Serv., Federally Funded Research and Development Centers (FFRDCs): Background and Issues for Congress 1 (2020). \n3 Id. See also About IDA, Inst. Defense Analyses, https://www.ida.org/about-ida (emphasizing that IDA, the private sector subcontractor that oper\u0002ates the Science & Technology Policy Institute and several other FFRDCs, “enjoys unusual access to classified government information and sensitive \ncorporate proprietary information.”); U.S. Gov’t Accountability Office, Federally Funded Research and Development Centers: Improved Oversight \nand Evaluation Needed for DOD’s Data Access Pilot Program 6 (2020) (discussing how the Department of Defense was able to establish a three-year \npilot program that allowed its FFRDC researchers to forgo having to obtain nondisclosure agreements with each data owner in order to streamline the \ndata-access process).\n4 Nick Hart & Nancy Potok, Modernizing U.S. Data Infrastructure: Design Considerations for Implementing a National Secure Data Service to \nImprove Statistics and Evidence Building (2020).\n5 Id. at 26.\n6 Id. at 26-27, 29-30.\n7 U.S. Gov’t Accountability Office, supra note 3, at 6. Note that while the FFRDC must operate to serve its sponsors, in establishing an FFRDC, the \nsponsor must ensure that it operates with substantial independence; the FFRDC must be “operated, managed, or administered by an autonomous or\u0002ganization or as an identifiably separate operating unit of a parent organization.” See Federal Acquisition Regulations [hereinafter “FAR”] § 35.017(a)(2).\n8 One example of this is the Science & Technology Policy Institute, which we discuss in a case study below.\n9 U.S. Dep’t of Energy, The State of the DOE National Laboratories 11-13 (2020).\n10 See, e.g., More Federal Agencies Head to the Cloud With Azure Government, Applied Info. Sci. (Feb. 23, 2018), https://www.ais.com/more-federal\u0002agencies-head-to-the-cloud-with-azure-government/; see also AWS GovCloud, Amazon, https://aws.amazon.com/govcloud-us/. Microsoft was also \npreviously awarded a $10 billion contract from the Pentagon. See Kate Conger, Microsoft Wins Pentagon’s $10 Billion JEDI Contract, Thwarting Amazon, \nN.Y. Times (Sept. 4, 2020), https://www.nytimes.com/2019/10/25/technology/dod-jedi-contract.html. However, this contract was recently canceled “due \nto evolving requirements, increased cloud conservancy and industry advances.” Ellie Kaufman & Zachary Cohen, Pentagon Cancels $10 Billion Cloud \nContract Given to Microsoft Over Amazon, CNN (July 6, 2021), https://www.cnn.com/2021/07/06/tech/defense-department-cancels-jedi-contract-am\u0002azon-microsoft/index.html. The Pentagon will now instead seek new bids for an updated Joint Warfighting Cloud Capability (JWCC) contract from \nAmazon and Microsoft. Id.\n11 See, e.g., Bram Bout, Helping Universities Build What’s Next with Google Cloud Platform, Google (Oct. 25, 2016), https://blog.google/outreach-ini\u0002tiatives/education/helping-universities-build-whats-next-google-cloud-platform; Cloud Computing for Education, Amazon, https://aws.amazon.com/\neducation/. \n12 Congressional Research Serv., supra note 2, at 11-12 (2020).\n13 U.S. Dep’t of Energy, Annual Report on the State of the DOE National Laboratories 87 (2017).\n14 Congressional Research Serv., supra note 2, at 19.\n15 Congressional Research Serv., Office of Science and Technology Policy (OSTP): History and Overview 9 (2020). STPI’s duties are also specified \nin 42 U.S.C. § 6686.\n16 What are FFRDCs?, Inst. Defense Analyses, https://www.ida.org/ida-ffrdcs. \n17 Id.\n18 Sponsors, Inst. Defense Analyses, https://www.ida.org/en/about-ida/sponsors. \n19 Id.\n20 Congressional Research Serv., supra note 15, at 9-10.\n21 For instance, from 2008-2012, these other federal agencies contributed a total of $9.8 million of funding to STPI while NSF contributed about $24 \nmillion. U.S. Gov’t Accountability Office, Federally Funded Research Centers: Agency Reviews of Employee Compensation and Center Perfor\u0002mance 43-44 (2014).\n22 Congressional Research Serv., supra note 15, at 9-10.\n23 Id.\n24 42 U.S.C. § 6686(d).\n25 42 U.S.C. § 6686(e).\n26 Sci. & Tech. Pol’y Inst., Report to the President Fiscal Year 2020 (2020).\n27 See, e.g., Open Government, Millennium Challenge Corp., https://www.mcc.gov/initiatives/initiative/open; Nat’l Geospatial Advisory Comm., \nAdvancing the National Spatial Data Infrastructure Through Public-Private Partnerships and Other Innovative Partnerships (2020); Nat’l \nAeronautics & Space Admin., Public-Private Partnerships for Space Capability Development 33-36 (2014).\n28 Big Data Value Public-Private Partnership, European Comm’n (Mar. 9, 2021), https://digital-strategy.ec.europa.eu/en/library/big-data-value-pub\u0002lic-private-partnership.\nA Blueprint for the National Research Cloud 100\n29 RAND, Public-Private Partnerships for Data-Sharing: A Dynamic Environment 33, 99 (2000).\n30 See Homepage - Alberta Data Partnerships, Alberta Data Partnerships, http://abdatapartnerships.ca (last visited Aug. 15, 2021). \n31 Alberta Data Partnerships, A P3 Success Story 1 (2017).\n32 Id.\n33 Id. at 19, 35.\n34 Id. at 15.\n35 Id.\n36 Id. at 1.\n37 Nat’l Geospatial Advisory Comm., Public-Private Partnership Use Case: Alberta Data Partnerships 1 (2020).\n38 Alberta Data Partnerships, supra note 31, at 15.\n39 Id. at 16.\n40 The COVID-19 High Performance Computing Consortium, COVID-19 HPC Consortium, https://covid19-hpc-consortium.org. \n41 Id.\n42 See, e.g., David Hall, Why Public-Private Partnerships Don’t Work (2015); Disadvantages and Pitfalls of the PPP Option, APMG Int’l, https://\nppp-certification.com/ppp-certification-guide/54-disadvantages-and-pitfalls-ppp-option. \n43 Graeme A. Hodge, Carsten Greve & Anthony E. Boardman, International Handbook on Public–Private Partnerships, 187-90 (2012).\n44 For example, on one end of a spectrum, the California Teale Data Center creates, owns, maintains, and archives its own datasets for private sector \nuse. In contrast, the Pennsylvania Spatial Data Access houses metadata, requiring users to ask the actual data sources for access. RAND, supra note 29, \nat 102-03. We encourage the Task Force to examine this comprehensive report to assess the various organizational options for a PPP data clearinghouse \nmodel. \n45 Angela Ballantyne & Cameron Stewart, Big Data and Public-Private Partnerships in Healthcare and Research, 11 Asian Bioethics R. 315, 315 (2019).\n46 See Gov’t Accountability Office, Human Capital: Improving Federal Recruiting and Hiring Efforts; see also Catch and Retain: Improving Recruit\u0002ing and Retention at Government Agencies, Salesforce, https://www.salesforce.com/solutions/industries/government/resources/government-recruit\u0002ment-software/.\n47 Partnership for Public Service, Survey on the Future of Government Service 2 (2020).\n48 Id.\nChapter 5\n1 National Research Cloud Call to Action, Stan. U. Inst. for Human-Centered Artificial Intelligence (2020), https://hai.stanford.edu/national-re\u0002search-cloud-joint-letter.\n2 Sensitive information, as defined by the National Institute of Standards and Technology, is information where the loss, misuse, or unauthorized \naccess or modification could adversely affect the national interest or the conduct of federal programs, or the privacy to which individuals are entitled \nunder 5 U.S.C. § 552a (the Privacy Act); that has not been specifically authorized under criteria established by an Executive Order or an Act of Congress \nto be kept classified in the interest of national defense or foreign policy. See Glossary: Sensitive Information, Nat’l Inst. Standards & Tech., \nhttps://csrc.nist.gov/glossary/term/sensitive_information.\n3 We thank Mark Krass for these insights. \n4 Agencies covered by the Act include “any Executive department, military department, Government corporation, Government controlled corporation, \nor other establishment in the executive branch of the [federal] Government (including the Executive Office of the President), or any independent regula\u0002tory agency.” 5 U.S.C. § 552(f)(1).\n5 U.S. General Accounting Office, Record Linkage and Privacy: Issues in Creating New Federal Research and Statistical Information 10 (2001). \n6 Interview with Marc Groman, Former Senior Advisor for Privacy, White House Office of Management and Budget (Feb. 18, 2021); see also Bipartisan \nPol’y Ctr., Barriers to Using Government Data: Extended Analysis of the U.S. Commission on Evidence-Based Policymaking’s Survey of Federal \nAgencies and Offices 10 (2018).\n7 See Joseph Near & David Darais, Differentially Private Synthetic Data, Nat’l Inst. Standards & Tech. (May 3, 2021), https://www.nist.gov/blogs/cyber\u0002security-insights/differentially-private-synthetic-data; see also Steven M. Bellovin et al., Privacy and Synthetic Datasets, 22 Stan. L. Rev. 1 (2019).\n8 E-Government Act of 2002, Pub. L. No. 107-347. \n9 Confidential Information Protection and Statistical Efficiency Act of 2002, 44 U.S.C. § 3501 (2012).\n10 Foundations for Evidence-Based Policymaking Act of 2017, Pub. L. No. 115-435, 132 Stat. 5529 (2019).\n11 President’s Mgmt. Agenda, Federal Data Strategy 2020 Action Plan (2020).\n12 Privacy Act of 1974, 5 U.S.C. § 552a (2012).\n13 There are many versions of the Fair Information Practice Principles, and the U.S. government has not institutionalized a specific version, though the \nversion used by the Department of Homeland Security is commonly referenced (available at: https://www.dhs.gov/publication/privacy-policy-guid\u0002ance-memorandum-2008-01-fair-information-practice-principles). The Organisation for Economic Cooperation and Development produced an influen\u0002tial version of them in 1980 (revised in 2013), which remains an authoritative source. OECD Guidelines on the Protection of Privacy and Transborder Flows \nof Personal Data, OECD (2013), https://www.oecd.org/digital/ieconomy/oecdguidelinesontheprotectionofprivacyandtransborderflowsofpersonaldata.\nhtm.\n14 See David Freeman Engstrom, Daniel E. Ho, Catherine M. Sharkey & Mariano-Florentino Cuéllar, Government by Algorithm: Artificial Intelli\u0002gence in Federal Administrative Agencies (2020) (documenting present use of AI by government agencies).\n15 5 U.S.C. § 552a (a)(5).\n16 5 U.S.C. §§ 552a(a)(8)(A)(i)(I), (II).\n17 The Privacy Act of 1974, Elec. Privacy Info. Ctr., https://epic.org/privacy/1974act/ (last visited Aug. 15, 2021).\n18 See Fact Sheet: National Secure Data Service Act Advances Responsible Data Sharing in Government, Data Coalition (May 13, 2021), https://www.\ndatacoalition.org/fact-sheet-national-secure-data-service-act-advances-responsible-data-sharing-in-government/; U.S Gov’t Accountability Office, \nRecord Linkage and Privacy: Issues in Creating New Federal Research and Statistical Information (2001).\nA Blueprint for the National Research Cloud 101\n19 It is no small irony that private companies in the U.S. have fulfilled that mission today. In fact, the U.S. government now approaches private industry, \neither through legal process or through procurement, when it requires data about individuals that the government itself does not collect. Senator Ron \nWyden has proposed legislation to prevent the government from making these purchases. Wyden, Paul and Bipartisan Members of Congress Introduce \nThe Fourth Amendment Is Not For Sale Act, Ron Wyden U.S. Senator for Or. (Apr. 21, 2021), https://www.wyden.senate.gov/news/press-releases/\nwyden-paul-and-bipartisan-members-of-congress-introduce-the-fourth-amendment-is-not-for-sale-act-.\n20 See, e.g., World Econ. Forum, The Next Generation of Data-Sharing in Financial Services (2019).\n21 See, e.g., Stacie Dusetzina et al., Linking Data for Health Services Research: A Framework and Instructional Guide, Agency for Healthcare Research & \nQuality (Sept. 1, 2014), https://www.ncbi.nlm.nih.gov/books/NBK253315/.\n22 See, e.g., European Comm’n, A European Strategy for Data (2020) (arguing for cross-border data aggregation and linkage of both private and pub\u0002lic sector data); M Sanni Ali et al., Administrative Data Linkage in Brazil: Potentials for Health Technology Assessment, 10 Frontiers in Pharmacology 984 \n(2019); Data Linkage, Australian Inst. of Health & Welfare (Jan. 4, 2020), https://www.aihw.gov.au/our-services/data-linkage. \n23 See, e.g., Elsa Augustine, Vikash Reddy & Jesse Rothstein, Linking Administrative Data: Strategies and Methods (2018) (describing tips for \nconducting data linkages in California); see also U.S. Dep’t of Health & Human Services, Status of State Efforts to Integrate Health and Human \nServices Systems and Data (2016).\n24 Ben Moscovitch, How President Biden Can Improve Health Data Sharing For COVID-19 And Beyond, Health Affairs (Mar. 1, 2021), https://www.\nhealthaffairs.org/do/10.1377/hblog20210223.611803/full/.\n25 Home, Johns Hopkins Coronavirus Resource Ctr., https://coronavirus.jhu.edu/.\n26 The COVID Tracking Project, https://covidtracking.com/. \n27 Fred Bazzoli, COVID-19 Emergency Shows Limitations of Nationwide Data Sharing Infrastructure, Healthcare IT News (June 2, 2020), https://www.\nhealthcareitnews.com/news/covid-19-emergency-shows-limitations-nationwide-data-sharing-infrastructure.\n28 See, e.g., C. Jason Wang et al., Response to COVID-19 in Taiwan: Big Data Analytics, New Technology, and Proactive Testing, JAMA (Mar. 3, 2020), https://\njamanetwork.com/journals/jama/fullarticle/2762689/; Fang-Ming Chen et al., Big Data Integration and Analytics to Prevent a Potential Hospital Outbreak \nof COVID-19 in Taiwan, 54 J. Microbiology, Immunology & Infection 129-30 (2020).\n29 See, e.g., Q&A on the Pentagon’s “Total Information Awareness” Program, Am. C.L. Union, https://www.aclu.org/other/qa-pentagons-total-informa\u0002tion-awareness-program; The Five Problems with CAPPS II: Why the Airline Passenger Profiling Proposal Should Be Abandoned, Am. C.L. Union, https://\nwww.aclu.org/other/five-problems-capps-ii. \n30 See, e.g., Barton Gellman, Dark Mirror: Edward Snowden and the American Surveillance State (2020); Edward Snowden, Permanent Record \n(2019).\n31 5 U.S.C. § 552(b).\n32 5 U.S.C. § 552(b)(3).\n33 The Privacy Act also contains specific carve-outs for disclosures to the Census Bureau and to the National Archives and Records Administration. \nHowever, the carve-outs for these two agencies require that the disclosures be made for the purposes of a census survey and of recording historical \nvalue, respectively. Because the NRC’s explicit purpose is to democratize AI innovation, it is unlikely that the NRC can take advantage of this existing \nexception to dataset disclosures under the Privacy Act.\n34 For example, the Federal Emergency Management Agency’s list of routine uses includes broad disclosure “[t]o an agency or organization for the \npurpose of performing audit or oversight operations as authorized by law, but only such information as is necessary and relevant to such audit or over\u0002sight function.” Privacy Act of 1974; Department of Homeland Security Federal Emergency Management Agency-008 Disaster Recovery Assistance Files \nSystem of Records, 78 Fed. Reg. 25282 (May 30, 2013). \n35 See, e.g., Britt v. Naval Investigative Service, 886 F.2d 544 (3d Cir. 1989).\n36 The Privacy Act of 1974, supra note 17. \n37 5 U.S.C. § 552(b)(5). \n38 5 U.S.C. §§ 552a(a)(8)(B)(i), (ii) (emphasis added).\n39 44 U.S.C. § 3561(8), (12).\n40 U.S. Dep’t of Health & Human Services, The State of Data Sharing at the U.S. Department of Health and Human Services 16 (2018).\n41 44 U.S.C. § 3575(4).\n42 See Engstrom, Ho, Sharkey & Cuéllar, supra note 14, at 16 (finding that the Bureau of Labor Statistics is one of the top ten agencies that use \nartificial intelligence); Machine Learning, Census Bureau (Apr. 17, 2019), https://www.census.gov/topics/research/data-science/about-machine-learn\u0002ing.html (asserting that the Census Bureau “needs” machine learning capabilities); Bureau of Econ. Analysis, 2020 Strategic Action Plan 7 (2020) \n(highlighting the importance of artificial intelligence and machine learning to BEA’s strategy).\n43 Group level data analyses also have inherent privacy risks and harms. See, e.g., Linnet Taylor, Safety in Numbers? Group Privacy and Big Data Analyt\u0002ics in the Developing World, in Group Privacy: New Challenges of Data Technologies 13 (2017).\n44 See 34 U.S.C. §§ 41303(c)(2), (3), (4).\n45 Nat’l Acad. of Sci., Innovations in Federal Statistics 41 (2017).\n46 See 13 U.S.C. § 6. \n47 Nat’l Acad. of Sci., supra note 45, at 40.\n48 According to the study, “an agency’s legal counsel may advise against sharing data as a precautionary measure rather than because of an explicit \nprohibition.” U.S. Gov’t Accountability Office, Sustained and Coordinated Efforts Could Facilitate Data Sharing While Protecting Privacy 1 \n(2013).\n49 See Amy O’Hara & Carla Medalia, Data Sharing in the Federal Statistical System: Impediments and Possibilities, 675 Annals Am. Acad. Pol. & Soc. Sci.\n138, 141 (2018).\n50 Robert M. Groves & Adam Neufeld, Accelerating the Sharing of Data Across Sectors to Advance the Common Good 12 (2017). \n51 Bipartisan Pol’y Ctr., supra note 6, at 18-20.\n52 See O’Hara & Medalia, supra note 49, at 141.\n53 Administrative Data Research UK, ADR UK, https://www.adruk.org.\n54 About ADR UK, ADR UK, https://www.adruk.org/about-us/about-adr-uk/.\n55 Id.\n56 See World of Work, ADR UK, https://www.adruk.org/our-work/world-of-work/.\nA Blueprint for the National Research Cloud 102\n57 Funding Opportunities, ADR UK, https://www.adruk.org/news-publications/funding-opportunities/.\n58 Id.\n59 Id.\n60 Funding Opportunity: A Unique Chance to Shape Data Science at the Heart of UK Government, ADR UK (Apr. 8, 2021), https://www.adruk.org/\nnews-publications/news-blogs/funding-opportunity-a-unique-chance-to-shape-data-science-at-the-heart-of-uk-government-384/. \n61 Funding Opportunities, supra note 57.\n62 Digital Economy Act 2017 (Gr. Br.).\n63 ADR UK, Trust, Security and Public Interest: Striking the Balance 28 (2020).\n64 Id.\n65 Id.\n66 How Do We Work with Researchers?, ADR UK, https://www.adruk.org/our-mission/working-with-researchers/.\n67 Accessing Secure Research Data as an Accredited Researcher, Off. for Nat’l Stat., https://www.ons.gov.uk/aboutus/whatwedo/statistics/requesting\u0002statistics/approvedresearcherscheme. \n68 See Nick Hart & Nancy Potok, Modernizing U.S. Data Infrastructure: Design Considerations for Implementing a National Secure Data Service \nto Improve Statistics and Evidence Building 17, 21 (2020).\n69 Id.\n70 Id. at 15.\n71 President’s Mgmt. Agenda, supra note 11, at 9.\n72 Id. at 31.\n73 See What is Open Data?, Open Data Handbook, https://opendatahandbook.org/guide/en/what-is-open-data/. \nChapter 6\n1 See Keeping Secrets: Anonymous Data Isn’t Always Anonymous, Berkeley Sch. of Info. (Mar. 15, 2014), https://ischoolonline.berkeley.edu/blog/anony\u0002mous-data/; Arvind Narayanan & Vitaly Shmatikov, How to Break Anonymity of the Netflix Prize Dataset, Cornell U. (Nov. 22, 2007), https://arxiv.org/pdf/\ncs/0610105.pdf.\n2 Matt Fredrikson, Somesh Jha & Thomas Ristenpart, Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures, 22 Pro\u0002ceedings of the ACM Special Interest Group on Security, Audit & Control 1322 (2015); Nicholas Carlini et al., Extracting Training Data from Large \nLanguage Models, Cornell U. (June 15, 2021), https://arxiv.org/pdf/2012.07805.pdf. \n3 See, e.g., HIPAA Training, Certification, and Compliance, HIPAA Training, https://www.hipaatraining.com/; Research Data Management, UK Data Serv., \nhttps://ukdataservice.ac.uk/learning-hub/research-data-management/.\n4 Ashwin Machanavajjhala et al., L-Diversity: Privacy Beyond K-Anonymity, 22 Int’l Conf. Data Eng’g 24 (2006). \n5 Cynthia Dwork & Aaron Roth, The Algorithmic Foundations of Differential Privacy (2014). \n6 See, e.g., Tara Bahrampour & Marissa J. Lang, New System to Protect Census Data May Compromise Accuracy, Some Experts Way, Wash. \nPost (June 1, 2021), https://www.washingtonpost.com/local/social-issues/2020-census-differential-privacy-ipums/2021/06/01/6c\u000294b46e-c30d-11eb-93f5-ee9558eecf4b_story.html; Kelly Percival, Court Rejects Alabama Challenge to Census Plans for Redistricting and Privacy, \nBrennan Ctr. (June 30, 2021), https://www.brennancenter.org/our-work/analysis-opinion/court-rejects-alabama-challenge-census-plans-redistrict\u0002ing-and-privacy.\n7 See, e.g., Leonard E. Burman et al., Safely Expanding Research Access to Administrative Tax Data: Creating a Synthetic Public Use File and a \nValidation Server (2018); see also The Synthetic Data Vault, https://sdv.dev. \n8 Valerie Chen, Valerio Pastro & Mariana Raykova, Secure Computation for Machine Learning with SPDZ, Cornell U. (Jan. 2, 2019), https://arxiv.org/\npdf/1901.00329.pdf. \n9 Louis J. M. Aslett et al., A Review of Homomorphic Encryption and Software Tools for Encrypted Statistical Machine Learning, Cornell U. (Aug. 26, 2015), \nhttps://arxiv.org/pdf/1508.06574.pdf. \n10 See Hongyan Chang & Reza Shokri, On the Privacy Risks of Algorithmic Fairness, Cornell U. (Apr. 7, 2021), https://arxiv.org/pdf/2011.03731.pdf.\n11 Ruggles et al., Differential Privacy and Census Data: Implications for Social and Economic Research, 109 Am. Econ. Ass’n Papers & Proceedings 403, \n406 (2019).\n12 In Computer Science literature, such algorithmic settings are often referred to as hyperparameters. For instance, k is a hyperparameter for k-ano\u0002nymity. By setting k to different values (e.g., 5, 10, 100), practitioners can modulate the amount of anonymity afforded to records in the data. As we note \nhowever, the choice of hyperparameters controls both the privacy effected on a dataset as well as the fidelity of that data. \n13 See Differential Privacy for Census Data Explained, Nat’l Conf. of State Legislatures (July 1, 2021), https://www.ncsl.org/research/redistricting/dif\u0002ferential-privacy-for-census-data-explained.aspx; Hongyan Chang & Reza Shokri, On the Privacy Risks of Algorithmic Fairness, Cornell U. (Apr. 7, 2021), \nhttps://arxiv.org/pdf/2011.03731.pdf. Some scholars even find that the incorporation of differential privacy into machine learning algorithms can have \ndisparate impact on underrepresented groups. See Eugene Bagdasaryan & Vitaly Shmatikov, Differential Privacy Has Disparate Impact on Model Accura\u0002cy, Cornell U. (Oct. 27, 2019), https://arxiv.org/pdf/1905.12101.pdf.\n14 Steven Ruggles, Differential Privacy and Census Data: Implications for Social and Economic Research 17.\n15 Id. at 18-19.\n16 Nat’l Acad. of Sci., Innovations in Federal Statistics 86 (2017). The fragmented FSRDC review process is similar to the fragmented data access \nregime we discussed in Chapter Three.\n17 Special Sworn Researcher Program, Bureau of Econ. Analysis, https://www.bea.gov/research/special-sworn-researcher-program (last updated July \n23, 2021).\n18 13 U.S.C. § 9.\n19 The institutional form of the NRC is discussed in depth in Chapter Four.\n20 NORC Data Enclave, NORC, https://www.norc.org/PDFs/BD-Brochures/2016/Data%20Enclave%20One%20Sheet.pdf. \n21 CMS Virtual Research Data Center (VRDC), Research Data Assistance Ctr., https://resdac.org/cms-virtual-research-data-center-vrdc. \nA Blueprint for the National Research Cloud 103\n22 Request for Information (RFI) Seeking Stakeholder Input on the Need for an NIH Administrative Data Enclave, Nat’l Inst. of Health (Mar. 1, 2019), \nhttps://grants.nih.gov/grants/guide/notice-files/NOT-OD-19-085.html. \n23 See FASEB Response to NIH Request for Information (RFI): Seeking Stakeholder Input on the Need for an NIH Administrative Data Enclave, Fed’n of Am. \nSocieties for Experimental Biology (2019), https://www.faseb.org/Portals/2/PDFs/opa/2019/FASEB_Response_Data_Enclave_RFI_NOT-OD-19-085.\npdf; Am. Soc’y of Biochemistry & Molecular Biology (May 30, 2019), https://www.asbmb.org/getmedia/e3401ed5-3210-4ed2-a82a-7363cb86071d/\nASBMB-Response-to-NIH-RFI-NOT-09-19-085.pdf. \n24 What We Do, Cal. Pol’y Lab, https://www.capolicylab.org/what-we-do/.\n25 Id.\n26 CPL Roadmap to Government Administrative Data in California, Cal. Pol’y Lab, https://www.capolicylab.org/data-resources/california-data-road\u0002map/.\n27 Interview with Evan White, Executive Director, California Policy Lab (Apr. 29, 2021).\n28 Id.\n29 Id.; see, e.g., Policy Evaluation and Research Linkage Initiative (PERLI), Cal. Pol’y Lab, https://www.capolicylab.org/data-resources/perli/; University \nof California Consumer Credit Panel, Cal. Pol’y Lab, https://www.capolicylab.org/data-resources/university-of-california-consumer-credit-panel/.\n30 Interview with Evan White, supra note 27.\n31 Id.\n32 See, e.g., Life Course Dataset, Cal. Pol’y Lab, https://www.capolicylab.org/life-course-dataset/.\n33 See CPL Roadmap to Government Administrative Data in California, supra note 26.\n34 We note that it is possible that the organizational form could affect the authority of NRC staff to speak to the legality of data transfers. \nChapter 7\n1 Christopher Whyte, Deepfake News: AI-Enabled Disinformation as a Multi-Level Public Policy Challenge, 5 J. Cyber Pol’y 199 (2020); Don Fallis, What Is \nDisinformation?, 63 Libr. Trends 601 (2015). \n2 Mary L. Gray & Siddharth Suri, Ghost Work: How to Stop Silicon Valley From Building a New Global Underclass (2019); Science Must Examine \nthe Future of Work, Nature (Oct. 19, 2017), \nhttps://www.nature.com/articles/550301b. \n3 David Danks & Alex John London, Algorithmic Bias in Autonomous Systems, 26 Int’l Joint Conf. on Artificial Intelligence 4691 (2017); Joy Buolam\u0002wini & Timnit Gebru, Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification, 81 Proceeding of Machine Learning Res.\n1 (2018); Ben Hutchinson et al., Unintended Machine Learning Biases as Social Barriers for Persons with Disabilities, 125 ACM SIGACCESS Accessibility & \nComputing 1 (2020).\n4 Oscar H. Gandy Jr., The Panoptic Sort: A Political Economy of Personal Information (1993); Virginia Eubanks, Automating Inequality (2018); \nRashida Richardson, Racial Segregation and the Data-Driven Society: How Our Failure to Reckon with Root Causes Perpetuates Separate and Unequal \nRealities, 36 Berkeley Tech. L. J. 101 (2021). \n5 For approaches to improve machine learning practices, see Timnit Gebru et al., Datasheets for Datasets, Cornell U. (Mar. 19, 2020), https://arxiv.org/\npdf/1803.09010.pdf; Margaret Mitchell et al., Model Cards for Model Reporting, 2019 Proceedings ACM Conf. on Fairness, Accountability & Transpar\u0002ency 220 (2019); Kenneth Holstein et al., Improving Fairness in Machine Learning Systems: What do Industry Practitioners Need?, 2019 CHI Conf. on Hum. \nFactors in Computing Sys. 1 (2019); Michael A Madaio et al., Co-Designing Checklists to Understand Organizational Challenges and Opportunities Around \nFairness in AI, 2020 CHI Conf. on Hum. Factors in Computing Sys. 318 (2019). The literature on AI’s societal impacts and fairness, accountability, and \ntransparency of AI is vast, but see Michael Kearns & Aaron Roth, The Ethical Algorithm: The Science of Socially Aware Algorithm Design (2019); \nEubanks, supra note 4; Solon Barocas, Moritz Hardt & Arvind Narayanan, Fairness and Machine Learning (2019); Cathy O’Neil, Weapons of Math \nDestruction (2016).\n6 45 C.F.R §§ 46.101-124.\n7 J. Britt Holbrook & Robert Frodeman, Peer Review and the Ex Ante Assessment of Societal Impacts, 20 Res. Evaluation 239 (2011). \n8 Id.\n9 Institutional Review Boards (IRBs) and Protection of Human Subjects in Clinical Trials, U.S. Food & Drug Admin., https://www.fda.gov/about-fda/cen\u0002ter-drug-evaluation-and-research-cder/institutional-review-boards-irbs-and-protection-human-subjects-clinical-trials (last updated Sept. 11, 2019).\n10 There are crucial questions with regards to consent even with data considered “publicly” available. See generally Casey Fiesler & Nicholas Proferes, \n“Participant” Perceptions of Twitter Research Ethics, 4 Social Media + Society 1 (2018); Sarah Gilbert, Jessica Vitak & Katie Shilton, Measuring Americans’ \nComfort with Research Uses of Their Social Media Data, 7 Social Media + Society 1 (2021). \n11 Sara R. Jordan, Future of Privacy Forum, Designing an Artificial Intelligence Research Review Committee (2019), https://fpf.org/wp-content/\nuploads/2019/10/DesigningAIResearchReviewCommittee.pdf.\n12 Agatta Feretti et al., Ethics Review of Big Data Research: What Should Stay and What Should Be Reformed?, 22 BMC Medical Ethics 1, 6 (2021); Kathryn \nM. Porter et al., The Emergence of Clinical Research Ethics Consultation: Insights from a National Collaborative, 2018 Am. J. Bioethics 39 (2018). \n13 Feretti et al., supra note 12.\n14 See, e.g., Mark Diaz et al., Addressing Age-Related Bias in Sentiment Analysis, 2018 Proceedings CHI Conf. on Hum. Factors in Computing Sys. 1 \n(2018); Buolamwini & Gebru, supra note 3.\n15 See, e.g., Timnit Gebru et al., Datasheets for Datasets, Cornell U. (Mar. 19, 2020), https://arxiv.org/pdf/1803.09010.pdf; Margaret Mitchell et al., Model \nCards for Model Reporting, 2019 Proceedings ACM Conf. on Fairness, Accountability & Transparency 220 (2019); Emily M. Bender et al., On the Dan\u0002gers of Stochastic Parrots: Can Language Models Be Too Big?, 2021 Proceedings ACM Conf. on Fairness, Accountability & Transparency 610 (2021); \nChristo Wilson et al., Building and Auditing Fair Algorithms: A Case Study in Candidate Screening, 2021 Proceedings ACM Conf. on Fairness, Account\u0002ability & Transparency 666 (2021); Pauline T. Kim, Auditing Algorithms for Discrimination, 166 U. Pa. L. Rev. Online 189 (2017).\n16 Phase II: Proposal Review and Processing, Nat’l Sci. Found., https://www.nsf.gov/bfa/dias/policy/merit_review/phase2.jsp#select.\n17 Harvey A. Averch, Criteria for Evaluating Research Projects and Portfolios, in Evaluating R&D Impacts: Methods and Practice 263 (1993).\n18 Nat’l Security Comm’n on Artificial Intelligence, Final Report 141-54 (2021). \n19 History and Mission, U.S. Privacy & Civil Liberties Oversight Bd., https://www.pclob.gov/About/HistoryMission.\nA Blueprint for the National Research Cloud 104\n20 AI in Counterterrorism Oversight Enhancement Act of 2021, H.R. 4469, 117th Cong. (2021). \n21 In instances where a researcher is using data obtained from one of the agencies that falls under ORI’s oversight, it may make sense to have ORI adju\u0002dicate those cases directly. For more information about the ORI, see ORI, Office Of Research Integrity, https://ori.hhs.gov/.\n22 Michael S. Bernstein et al., ESR: Ethics and Society Review of Artificial Intelligence Research, Cornell U. (July 9, 2021), https://arxiv.org/\npdf/2106.11521.pdf.\n23 Nat’l Sci. Found., Broader Impacts, https://www.nsf.gov/od/oia/special/broaderimpacts/.\n24 See, e.g., Notice of Special Interest: Administrative Supplement for Research and Capacity Building Efforts Related to Bioethical Issues, Nat’l Inst. of \nHealth (Nov. 17, 2020), https://grants.nih.gov/grants/guide/notice-files/NOT-OD-21-020.html; Notice of Special Interest: Administrative Supplement \nfor Research on Bioethical Issues, Nat’l Inst. of Health (Dec. 30, 2019), https://grants.nih.gov/grants/guide/notice-files/NOT-OD-20-038.html; see also \nCourtenay R. Bruce et al., An Embedded Model for Ethics Consultation: Characteristics, Outcomes, and Challenges, 5 AJOB Empirical Bioethics 8 (2014); \nSharon Begley, In a Lab Pushing the Boundaries of Biology, an Embedded Ethicist Keeps Scientists in Check, Stat (Feb. 23, 2017), https://www.statnews.\ncom/2017/02/23/bioethics-harvard-george-church/. Private foundations also promote the use of embedded bioethicists. See, e.g., Making a Differ\u0002ence Request for Proposals – Fall 2021, The Greenwall Found. (2021), https://greenwall.org/making-a-difference-grants/request-for-proposals-MAD\u0002fall-2021.\nChapter 8\n1 Paul Cichonski et al., Computer Security Incident Handling Guide (2012). \n2 Putative attacks could include the deployment of ransomware, phishing schemes, gaining root access (the highest level of privilege available which \ngives users access to all commands and files by default), exposure of secret credentials, data poisoning, data exfiltration, as well as other types of \nunauthorized network intrusions.\n3 Karen Hao, AI Consumes a Lot of Energy. Hackers Could Make it Consume More, MIT Tech. R. (May 6, 2021), https://www.technologyreview.\ncom/2021/05/06/1024654/ai-energy-hack-adversarial-attack/.\n4 Catalin Cimpanu, Vast Majority of Cyber-Attacks on Cloud Servers Aim to Mine Cryptocurrency, ZDNet (Sept. 14, 2020), https://www.zdnet.com/article/\nvast-majority-of-cyber-attacks-on-cloud-servers-aim-to-mine-cryptocurrency/.\n5 We note that the NRC will likely need to comply with data specific security regulations as well. For instance, medical data security will need to comply \nwith HIPAA, and financial data will need to comply with The Gramm-Leach-Bliley Act.\n6 Ray Dunham, FISMA Compliance: Security Standards & Guidelines Overview, Linford & Co. (Nov. 29, 2017), https://linfordco.com/blog/fisma-compliance/.\n7 Amy J. Frontz, Review of the Department of Health and Human Services Compliance with the Federal Information Security Modernization Act \nof 2014 for Fiscal Year 2020 (2021). \n8 U.S. Senate Comm. on Homeland Security & Governmental Affairs, Federal Cybersecurity: America’s Data at Risk 18 (2019). \n9 Federal Information Security Modernization Act (FISMA) Background, Nat’l Inst. Standards & Tech., https://csrc.nist.gov/projects/risk-management/\nfisma-background (last updated Aug. 4, 2021).\n10 Dunham, supra note 6.\n11 U.S. Senate Comm. on Homeland Security & Governmental Affairs, supra note 8, at 19.\n12 Id. at 18.\n13 Id. at 19.\n14 Id. at 20.\n15 Kevin Stine, et al., Guide for Mapping Types of Information and Information Systems to Security Categories (2008). Specifically, FISMA defines \ncompliance in terms of three levels: low impact, moderate impact, and high impact. Low impact indicates that the loss of confidentiality, integrity, or \navailability of the system will have a limited adverse effect, while high impact indicates that such losses will have severe or catastrophic effects. See Sar\u0002ah Harvey, 3 FISMA Compliance Levels: Low, Moderate, High, KirkpatrickPrice (Apr. 24, 2020), https://kirkpatrickprice.com/blog/fisma-compliance-lev\u0002els-low-moderate-high/.\n16 Nat’l Inst. Standards & Tech., Security and Privacy Controls for Information Systems and Organizations (2020). \n17 Marianne Swanson et al., Guide for Developing Security Plans for Federal Information Systems (2006). \n18 Michael McLaughlin, Reforming FedRAMP: A Guide to Improving the Federal Procurement and Risk Management of Cloud Services, Info. Tech. & Innova\u0002tion Found. (June 15, 2020), https://itif.org/publications/2020/06/15/reforming-fedramp-guide-improving-federal-procurement-and-risk-management.\n19 Program Basics, FedRAMP, https://www.fedramp.gov/program-basics/; see also Steven VanRoekel, Security Authorization of Information Sys\u0002tems in Cloud Computing Environments (2011). \n20 FISMA vs. FedRAMP and NIST: Making Sense of Government Compliance Standards, Foresite, https://foresite.com/fisma-vs-fedramp-and-nist-making\u0002sense-of-government-compliance-standards/. However, we note that FedRAMP approval is exempted for certain types of cloud models: (i) where the \ncloud is private to the agency, (ii) where the cloud is physically located within a Federal facility, (iii) where the agency is not providing cloud services \nfrom the cloud-based information system to any external entities. See VanRoekel, supra note 19.\n21 FedRAMP, FedRAMP Security Assessment Framework 5 (2017).\n22 Doina Chiacu, White House Warns Companies to Step Up Cybersecurity: ‘We Can’t Do it Alone’, Reuters (June 3, 2021), https://www.reuters.com/tech\u0002nology/white-house-warns-companies-step-up-cybersecurity-2021-06-03/; see also Significant Cyber Incidents, Ctr. Strategic & Int’l Studies, https://\nwww.csis.org/programs/strategic-technologies-program/significant-cyber-incidents (last visited Aug. 19, 2021).\n23 U.S. Senate Comm. Homeland Security & Governmental Affairs, supra note 8, at 5.\n24 Id. at 6.\n25 Frontz, supra note 7.\n26 Jonathan Reiber & Matt Glenn, The U.S. Government Needs to Overhaul Cybersecurity. Here’s How., Lawfare (Apr. 9, 2021), https://www.lawfareblog.\ncom/us-government-needs-overhaul-cybersecurity-heres-how.\n27 Nat’l Security Agency, Embracing a Zero Trust Security Model (2021).\n28 McLaughlin, supra note 18.\n29 Id.\n30 Id.\nA Blueprint for the National Research Cloud 105\n31 Exec. Order No. 14,028, 86 Fed. Reg. 26633 (May 17, 2021).\n32 U.S. Office of Mgmt. & Budget, Moving the U.S. Government Towards Zero Trust Cybersecurity Principles (2021).\n33 See, e.g., David Kushner, The Real Story of Stuxnet, IEEE Spectrum (Feb. 26, 2013), https://spectrum.ieee.org/the-real-story-of-stuxnet.\n34 HTTP is the protocol at the highest level of abstraction targeting the application layer, and its secure variant HTTPS additionally encrypts the data \nusing an encryption protocol. Without encryption, HTTP is insecure and should not be used. The encryption protocol in original use was SSL but this \nhas since been deprecated in the realm of network security in favor of its newer version, TLS. Both SSL and TLS rely on public key certificates signed by \na trusted certificate authority. When these certificates have expired, the websites providing them can no longer necessarily be trusted. Although these \nmeasures have their own limitations, not adopting them can only be less secure.\n35 See, e.g., Azure Confidential Computing, Microsoft, https://azure.microsoft.com/en-ca/solutions/confidential-compute/; Nataraj Nagaratnam, \nConfidential Computing, IBM (Oct. 16, 2020), \nhttps://www.ibm.com/cloud/learn/confidential-computing; Confidential Computing, Google Cloud, https://cloud.google.com/confidential-computing.\n36 David Archer et al., From Keys to Databases—Real-World Applications of Secure Multi-Party Computation, 61 Computer J. 1749 (2018). \n37 Amit Elazari Bar On, We Need Bug Bounties for Bad Algorithms, Motherboard (May 3, 2018) https://www.vice.com/en/article/8xkyj3/we-need-bug\u0002bounties-for-bad-algorithms.\nChapter 9\n1 Importantly, this chapter discusses the extent to which researchers should be required to share their research outputs, not the extent to which re\u0002searchers should be required to share their private data. The latter was discussed in Chapter Three.\n2 Dan Robitzski, AI Researchers Are Boycotting A New Journal Because It’s Not Open Access, Futurism (May 3, 2018), https://futurism.com/artificial-intel\u0002ligence-journal-boycot-open-access.\n3 Mikio L. Braun & Cheng Soon Ong, Open Science in Machine Learning (2014).\n4 Since researchers using the NRC are not “contractors” under FAR/DFARS, and since evidence is lacking on the value of Other Transactions to AI re\u0002searchers, we do not cover FAR/DFARS and Other Transactions in this section. \n5 Under the Bayh-Dole Act, Aa “federal funding agreement” is defined as “any contract, grant, or cooperative agreement entered into between any \nFederal agency, other than the Tennessee Valley Authority, and any contractor for the performance of experimental, developmental, or research work \nfunded in whole or in part by the Federal Government.” 35 U.S.C. § 201.\n6 35 U.S.C. § 202.\n7 35 U.S.C. § 203.\n8 See, e.g., Mark A. Lemley & Julie E. Cohen, Patent Scope and Innovation in the Software Industry, 89 Cal. L. Rev. 1 (2001); Mark A. Lemley, Software \nPatents and the Return of Functional Claiming, 2013 Wis. L. Rev. 905 (2013).\n9 Jeremy Gillula & Daniel Nazer, Stupid Patent of the Month: Will Patents Slow Artificial Intelligence?, Elec. Frontier Found. (Sept. 29, 2017), https://\nwww.eff.org/deeplinks/2017/09/stupid-patent-month-will-patents-slow-artificial-intelligence.\n10 U.S. Patent & Trademark Off., Inventing AI: Tracing the Diffusion of Artificial Intelligence with Patents 2 (2020). \n11 See, e.g., Mike James, Google Files AI Patents, I Programmer (July 8, 2015), https://www.i-programmer.info/news/105-artificial-intelli\u0002gence/8765-google-files-ai-patents.html. This is especially problematic because companies represent 26 out of the top 30 AI patent applicants world\u0002wide, while only four are universities or public research organizations. World Intell. Prop. Org., Artificial Intelligence 7 (2019).\n12 Lisa Ouellette & Rebecca Weires, University Patenting: Is Private Law Serving Public Values?, 2019 Mich. St. L. Rev. 1329 (2019).\n13 Id. at 1331; see also Arti Kaur Rai, Regulating Scientific Research: Intellectual Property Rights and the Norms of Science, 94 Nw. U. L. Rev. 77, 136 (1999).\n14 See Brian J. Love, Do University Patents Pay Off? Evidence From a Survey of University Inventors in Computer Science and Electrical Engineering, 16 Yale \nJ. L & Tech. 285 (2014).\n15 See id. at 286.\n16 See, e.g., Tech Transfer FAQ, U. Mich., https://techtransfer.umich.edu/for-inventors/resources/inventor-faq/ (“We carefully review the commercial \npotential for an invention before investing in the patent process. However, because the need for commencing a patent filing usually precedes finding a \nlicensee, we look for creative and cost-effective ways to seek early protections for as many promising inventions as possible”); What is Technology Trans\u0002fer, Princeton U., https://patents.princeton.edu/about-us/what-technology-transfer (“[T]echnologies and everyday products are possible because of \ntechnology transfer . . . Because the discoveries emerging from university research tend to be early-stage, high-risk inventions, successful university \ntechnology transfer transactions require a patent system that protects such innovations.”).\n17 The Uniform Guidance for intellectual property is laid out in 2 C.F.R. § 200.315.\n18 Uniform Administrative Requirements, Cost Principles, and Audit Requirements for Federal Awards, Grants.gov, https://www.grants.gov/learn-grants/\ngrant-policies/omb-uniform-guidance-2014.html (last visited Aug. 27, 2021).\n19 See Key Sections of the Uniform Guidance, AICPA.org, https://www.aicpa.org/interestareas/governmentalauditquality/resources/singleaudit/uni\u0002formguidanceforfederalrewards/key-sections-uniform-guidance.html.\n20 2 C.F.R. § 200.315. A “federal award” under the Uniform Guidance includes, among other things, “the federal financial assistance that a recipient \nreceives directly from a Federal awarding agency or indirectly from a pass-through entity;” or “the cost-reimbursement contract under the Federal \nAcquisition Regulations;” or a “grant agreement, cooperative agreement, [or] other agreement [for federal financial assistance].” 2 C.F.R. § 200.1.\n21 2 C.F.R. §§ 200.315(b), (c). These provisions specify that the government merely “reserves” its “right” to copyright and data rights over research \nproduced under the federal award. \n22 U.S. Copyright Office, Compendium of U.S. Copyright Office Practices 35 (2021, 3d ed.). \n23 Wil Michiels, How Do You Protect Your Machine Learning Investment?, EETimes (Mar. 31, 2020), https://www.eetimes.com/how-do-you-protect-your\u0002machine-learning-investment-part-ii/. \n24 See, e.g., Tabrez Y. Ebrahim, Data-Centric Technologies: Patent and Copyright Doctrinal Disruptions, 43 Nova L. Rev. 287, 304; Daryl Lim, AI & IP: Innova\u0002tion & Creativity in an Age of Accelerated Change, 52 Akron L. Rev. 813, 835 (2018)\n25 2 C.F.R. § 200.315(b).\n26 Id.\nA Blueprint for the National Research Cloud 106\n27 For a comprehensive report on how artificial intelligence is used in various government agencies, see David Freeman Engstrom, Daniel E. Ho, Cath\u0002erine M. Sharkey & Mariano-Florentino Cuéllar, Government by Algorithm: Artificial Intelligence in Federal Administrative Agencies (2020).\n28 Jukebox, OpenAI (Apr. 30, 2020), https://openai.com/blog/jukebox/.\n29 See, e.g., Shlomit Yanisky-Ravid, Generating Rembrandt: Artificial Intelligence, Copyright, and Accountability in the 3A Era--the Human-Like Authors \nare Already Here--a New Model, 27 Mich. St. L. Rev. 659 (2017); Kalin Hristov, Artificial Intelligence and the Copyright Dilemma, 57 J. Franklin Pierce Ctr. \nIntell. Prop. 431 (2017). \n30 Kalin Hristov, Artificial Intelligence and the Copyright Survey, 16 J. Sci. Pol’y & Governance 1, 14-15 (2020).\n31 Id. at 16.\n32 See What is Transfer Learning?, TensorFlow (Mar. 31, 2020), https://www.tensorflow.org/js/tutorials/transfer/what_is_transfer_learning. \n33 See, e.g., Yunhui Guo et al., SpotTune: Transfer Learning Through Adaptive Fine-Tuning, Cornell U. (Nov. 2018), https://arxiv.org/pdf/1811.08737.pdf. \n34 2 C.F.R. § 200.315(d).\n35 See Zhiqiang Wan, Yazhou Zhang & Haibo He, Variational Autoencoder Based Synthetic Data Generation for Imbalanced Learning, IEEE (2017).\n36 See Noseong Park, Mahmoud Mohammadi & Kshitij Gorde, Data Synthesis Based on Generative Adversarial Networks, 11 Proc. VLDB Endowment\n1071 (2018).\n37 See Ron Bakker, Impact of Artificial Intelligence on IP Policy 12. \n38 See Marta Duque Lizarralde, A Guideline to Artificial Intelligence, Machine Learning and Intellectual Property 4-7 (2020). \n39 Steven M. Bellovin et al., Privacy and Synthetic Datasets, 22 Stan. Tech. L. Rev. 1, 2-3 (2019); see also Fida K. Dankar & Mahmoud Ibrahim, Fake It Till \nYou Make It: Guidelines for Effective Synthetic Data Generation, 5 Applied Sci. 11 (2021); but see Theresa Stadler et al., Synthetic Data – Anonymisation \nGroundhog Day, Cornell U. (July 8, 2021), https://arxiv.org/pdf/2011.07018.pdf.\n40 See, e.g., Daniel S. Quintana, A Synthetic Dataset Primer for the Biobehavioural Sciences to Promote Reproducibility and Hypothesis Generation, 9 eLife\n1 (2020).\n41 Yuji Roh et al., A Survey on Data Collection for Machine Learning, Cornell U. (Aug. 12, 2019), https://arxiv.org/pdf/1811.03402.pdf.\n42 See, e.g., Hang Qiu et al., Minimum Cost Active Labeling, Cornell U. (June 24, 2020), https://arxiv.org/pdf/2006.13999.pdf; Eric Horvitz, Machine \nLearning, Reasoning, and Intelligence in Daily Life: Directions and Challenges, 18 Proceeding of the Conf. on Uncertainty in Artificial Intelligence 3 \n(2007).\n43 Cognilytics Research, Data Engineering, Preparation, and Labeling for AI 2019 3 (2019).\n44 See Wil Michiels, How Do You Protect Your Machine Learning Investment?, EETimes (Mar. 26, 2020), https://www.eetimes.com/how-do-you-protect\u0002your-machine-learning-investment/. In fact, in the European Union, labeled datasets are awarded with database rights protections. Mauritz Kop, \nMachine Learning & EU Data Sharing Practices, Stan.-Vienna Transatlantic Tech. L. F.(Mar. 24, 2020), https://ttlfnews.wordpress.com/2020/03/24/\nmachine-learning-eu-data-sharing-practices/.\n45 See, e.g., Niklas Fiedler et al., ImageTagger: An Open Source Online Platform for Collaborative Image Labeling, 11374 Lecture Notes in Computer Sci. \n162 (2019).\n46 Id. at 162.\n47 Researchers may, for instance, use NRC data and compute resources to implement active learning strategies, procedures to manually label a subset \nof available data and infer the remaining labels automatically using a machine learning model. See, e.g., Oscar Reyes et al., Effective Active Learning \nStrategy for Multi-Label Learning, 273 Neurocomputing 494 (2018). Similarly, researchers may augment existing public sector data with valuable labels. \n48 See, e.g., Pedro Saleiro et al., Aequitas: A Bias and Fairness Audit Toolkit, Cornell U. (Apr. 29, 2019), https://arxiv.org/pdf/1811.05577.pdf; Florian \nTramèr et al., FairTest: Discovering Unwarranted Associations in Data-Driven Applications, Cornell U. (Aug. 16, 2019), https://arxiv.org/pdf/1510.02377.\npdf.\n49 While we do not discuss the idiosyncratic modifications to the Uniform Guidance that vary from agency-to-agency, we encourage the task force to \nassess these modifications if it decides to implement the NRC through a particular agency. If the NRC is administered through multiple agencies, the \ncomplex amalgam of agency-specific IP rules may increase the friction in using the NRC if researchers must context-switch from one set of regulations \nto the next depending on the funding agency.\n50 2 C.F.R. § 2900.13. Previously, the Department of Labor explicitly required IP generated under a federal award to be licensed under a Creative \nCommons Attribution license, but this rule was changed in April 2021 to replace the proprietary term “Creative Commons Attribution license” with the \nindustry-recognized standard “open license.” 86 Fed. Reg. 22107 (Apr. 27, 2021).\n51 Dissemination and Sharing of Research Results - NSF Data Management Plan Requirements, Nat’l Sci. Found., https://www.nsf.gov/bfa/dias/policy/\ndmp.jsp. \n52 See, e.g., Aidan Courtney et al., Balancing Open Source Stem Cell Science with Commercialization, Nature Biotechnology (Feb. 7, 2011), https://\nwww.nature.com/articles/nbt.1773.\n53 See Klint Finley, When Open Source Software Comes with a Few Catches, Wired (July 31, 2019), https://www.wired.com/story/when-open-source-soft\u0002ware-comes-with-catches/; Guide to Open Source Licenses, Synopsys (Oct. 7, 2016), https://www.synopsys.com/blogs/software-security/open-source-li\u0002censes/.\n54 See Daniel A. Almeida et. al, Do Software Developers Understand Open Source Licenses?, 25 IEEE Int’l Conf. on Program Comprehension 1 (2017) \n(finding that software developers “struggle[] when multiple [open-source] licenses [are] involved” and “lack the knowledge and understanding to tease \napart license interactions across multiple situations.”).\n55 See, e.g., Alexandra Theben et al., Challenges and Limits of an Open Source Approach to Artificial Intelligence 14 (2021); Stadler et al., supra \nnote 39; Milad Nasr et al., Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Feder\u0002ated Learning, Cornell U. (June 6, 2020), https://arxiv.org/abs/1812.00910.pdf.\n56 Some universities have decided to eliminate classified research. See, e.g., At the Hands of Radicals, Stan. Mag. (Jan. 2009), https://stanfordmag.org/\ncontents/at-the-hands-of-the-radicals.\n57 See Donald Kennedy, Science and Secrecy, 289 Sci. 724 (2000); Peter J. Westwick, Secret Science: A Classified Community in the National Laboratories, \n38 Minerva 363 (2000).\n58 See Braun & Ong, supra note 3; Sören Sonnenburg et al., The Need for Open Source Software in Machine Learning, 8 J. Machine Learning Res. 2443 \n(2007); see also Katie Malone & Richard Wolski, Doing Data Science on the Shoulders of Giants: The Value of Open Source Software for the Data Science \nCommunity, HDSR (May 31, 2020), https://hdsr.mitpress.mit.edu/pub/xsrt4zs2/release/4. \nA Blueprint for the National Research Cloud 107\n59 See Laura A. Heymann, Overlapping Intellectual Property Doctrines: Election of Rights Versus Selection of Remedies, 17 Stan. Tech. L. Rev. 239, 240 \n(2013); Oracle Am. Inc. v. Google Inc., 750 F.3d 1339 (Fed. Cir. 2014) (accepting that software is both patentable and copyrightable).\n60 Robert E. Thomas, Debugging Software Patents: Increasing Innovation and Reducing Uncertainty in the Judicial Reform of Software Patent Law, 25 \nSanta Clara Computer & High Tech. L.J. 191, 222-23 (2008).\n61 See, e.g., Joaquin Vanschorin et al., OpenML: Networked Science in Machine Learning, Cornell U. (Aug. 1, 2014), https://arxiv.org/pdf/1407.7722.pdf \n(developing a collaboration platform through which scientists can automatically share, organize and discuss machine learning experiments, data, and \nalgorithms); see also Sarah O’Meara, AI Researchers in China Want to Keep the Global-Sharing Culture Alive, Nature (May 29, 2019), https://www.nature.\ncom/articles/d41586-019-01681-x; Shuai Zhao et al., Packaging and Sharing Machine Learning Models via the Acumos AI Open Platform, 17 ICMLA (2018).\n62 Jeanne C. Fromer, Machines as the New Oompa-Loompas: Trade Secrecy, the Cloud, Machine Learning, and Automation, 94 N.Y.U. L. Rev. 706, 712 \n(2019); Jordan R. Raffe et al., The Rising Importance of Trade Secret Protection for AI- Related Intellectual Property 1, 5-6 (2020); Jessica \nM. Meyers, Artificial Intelligence and Trade Secrets, Am. Bar Ass’n (Feb. 2019), https://www.americanbar.org/groups/intellectual_property_law/pub\u0002lications/landslide/2018-19/january-february/artificial-intelligence-trade-secrets-webinar/; AIPLA Comments Regarding “Request for Comments on \nIntellectual Property Protection for Artificial Intelligence Innovation”, Am. Intell. Prop. L. Ass’n (Jan. 10, 2020), https://www.uspto.gov/sites/default/files/\ndocuments/AIPLA_RFC-84-FR-58141.pdf.\n63 Clark D. Asay, Artificial Stupidity, 61 Wm. & Mary L. Rev. 1187, 1197, 1241-42 (2020).\n64 See id.; Am. Intell. Prop. L. Ass’n, supra note 62, at 16.\n65 See Asay, supra note 63, at 1242.\nAppendix\n1 Department of Energy Awards $425 Million for Next Generation Supercomputing Technologies, Energy.gov (Nov. 14, 2014), https://www.energy.gov/\narticles/department-energy-awards-425-million-next-generation-supercomputing-technologies.\n2 Amazon EC2 P3 Instances, Amazon, https://aws.amazon.com/ec2/instance-types/p3/ (last visited Sept. 9, 2021).\n3 CORAL Request for Proposal B604142, Lawrence Livermore Nat’l Laboratory (2014), https://web.archive.org/web/20140816181824/ https://asc.llnl.\ngov/CORAL/. We note that we were not able to locate the final award documents, nor is Summit budgeted in sufficient detail to back out cost from the \nDOE budget statements. Our cost estimates here, however, are comparable to publicly reported estimates for the total cost of the Summit system. \n4 This is based on a $30M maximum in the DOE Office of Science contract for non-recurring engineering (NRE) costs for the systems at Argonne National \nLaboratory and Oak Ridge National Laboratory. \n5 This is based on the difference in the RFP terms between the inclusion of maintenance under the Lawrence Livermore National Laboratory system \n(with a maximum budget of $170M) and the exclusion of maintenance under the systems for the Oak Ridge National Laboratory and the Argonne \nNational Laboratory (with a maximum budget for the build contract of $155M). This is likely an upper bound on maintenance, given that the difference \nreflects the combination of NRE and 5-year maintenance. \n6 See CORAL Price Schedule, Lawrence Livermore Nat’l Laboratory (2014), https://web.archive.org/web/20140816181824/ https://asc.llnl.gov/CORAL/\nRFP_components/04_CORAL_Price_Schedule_ANL_ORNL_tabs.xlsx. We used 1.62% as the interest rate to calculate the cost over 60 months. It is the \n5-year Treasury constant maturity rate on November 14, 2014, see Selected Interest Rates (Daily) – H.15, Fed. Res., https://www.federalreserve.gov/\nreleases/H15/default.htm, when DOE announced the award of the HPC system, see Department of Energy Awards $425 Million for Next Generation Super\u0002computing Technologies, supra 1.\n7 For instance, this estimate is in line with the cost of $200M reported by the New York Times. Steve Lohr, Move Over, China: U.S. is Again Home to World’s \nSpeediest Supercomputer, N.Y. Times (June 8, 2018), https://www.nytimes.com/2018/06/08/technology/supercomputer-china-us.html. Some reporting \nconflates the procurement of multiple systems that occurred contemporaneously. \n8 Research shows that for training compute-intensive deep learning models, such as ResNet-101, the GPU utilization is around 70%. Jingoo Han et \nal., A Quantitative Study of Deep Learning Training on Heterogeneous Supercomputers, 2019 IEEE Conf. on Cluster Computing 1, 5 (2019). However, \nResNet-50 has a GPU utilization of approximately 40%, see id., and other accounts report that GPUs are utilized only 15-30% of the time, see, e.g., Lukas \nBiewald, Monitor and Improve GPU Usage for Training Deep Learning Models, Towards Data Sci. (Mar. 27, 2019), https://towardsdatascience.com/mea\u0002suring-actual-gpu-usage-for-deep-learning-training-e2bf3654bcfd; Janet Morss, Giving Your Data Scientists a Boost with GPUaaS, CIO (June 2, 2020), \nhttps://www.cio.com/article/3561090/giving-your-data-scientists-a-boost-with-gpuaas.html.\n9 Compute Canada, Cloud Computing for Researchers 1 (2016), https://www.computecanada.ca/wp-content/uploads/2015/02/CloudStrate\u0002gy2016-2019-forresearchersEXTERNAL-1.pdf.\n10 Jennifer Shkabatur, The Global Commons of Data, 22 Stan. Tech. L.R. 407, 407-09 (2019).\n11 Benjamin Sobel, Artificial Intelligence’s Fair Use Crisis, 41 Colum. J.L. & Arts 61 (2017).\n12 Id.\n13 See Protecting What We Love About the Internet: Our Efforts to Stop Online Piracy, Google Pub. Pol’y Blog (Nov. 7, 2019), https://www.blog.google/\noutreach-initiatives/public-policy/protecting-what-we-love-about-internet-our-efforts-stop-online-piracy/. \n14 See Jennifer M. Urban, Joe Karaganis & Brianna M. Schofield, Notice & Takedown in Everyday Practice 39 (2017) (illustrating the difficulty that \nonline service providers face in manually evaluating a large volume of data for potential infringement; for example, one online service provider ex\u0002plained that “out of fear of failing to remove infringing material, and motivated by the threat of statutory damages, its staff will take “six passes to try to \nfind the [identified content].”); see also Letter from Thom Tillis, Marsha Blackburn, Christopher A. Coons, Dianne Feinstein et. al, to Sundar Pichai, Chief \nExecutive Officer, Google Inc. (Sept. 3, 2019), https://www.ipwatchdog.com/wp-content/uploads/2019/09/9.3-Content-ID-Ltr.pdf (“We have heard from \ncopyright holders who have been denied access to Content ID tools, and as a result, are at a significant disadvantage to prevent repeated uploading of \ncontent that they have previously identified as infringing. They are left with the choice of spending hours each week seeking out and sending notices \nabout the same copyrighted works, or allowing their intellectual property to be misappropriated.”).\n15 See Google, How Google Fights Piracy 6 (2016). To illustrate the costs of implementing Content ID on a large-scale platform, Google announced in \na report in 2016 that YouTube had invested more than $60 million in Content ID. \n16 See Sobel, supra note 11, at 66-79.\n17 See Authors Guild v. Google Inc., 804 F.3d 202 (2d Cir. 2015).\n18 Id.\nA Blueprint for the National Research Cloud 108\n19 Id. at 216-17.\n20 Matthew Stewart, The Most Important Court Decision For Data Science and Machine Learning, Towards Data Sci. (Oct. 31, 2019), https://towardsdata\u0002science.com/the-most-important-supreme-court-decision-for-data-science-and-machine-learning-44cfc1c1bcaf. \n21 See, e.g., James Grimmelmann, Copyright for Literate Robots, 101 Iowa L. Rev. 657, 661; Sobel, supra note 11, at 51-57.\n22 See Sobel, supra note 11, at 57.\n23 See Anna I. Krylov et. al. What is the Price of Open Source Software? 6 J. Physical Chemistry Letters 2751, 2753 (2015) (explaining that budding \nresearchers considering commercialization may be particularly concerned about what licenses are available, since a “strictly open-source environment \nmay furthermore disincentivize young researchers to make new code available right away, lest their ability to publish papers be short-circuited by a \nmore senior researcher with an army of postdocs poised to take advantage of any new code.”).\n24 See, e.g., A Data Scientist’s Guide to Open-Source Licensing, Towards Data Sci. (Nov. 4, 2018), https://towardsdatascience.com/a-data-scientists\u0002guide-to-open-source-licensing-c70d5fe42079; Choose an Open-Source License, https://choosealicense.com. \n25 Licensing a Repository, GitHub, https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/licensing-a-repository. \n26 What is the Most Appropriate Licence for My Data?, FigShare, https://help.figshare.com/article/what-is-the-most-appropriate-licence-for-my-data. \n27 See Developer Agreement, Twitter (Mar. 10, 2020), https://developer.twitter.com/en/developer-terms/agreement; Non-commercial Use of the Twitter \nAPI, Twitter, https://developer.twitter.com/en/developer-terms/commercial-terms.\n28 See Daniel A. Almeida et. al, Do Software Developers Understand Open Source Licenses?, 25 IEEE Int’l Conf. on Program Comprehension 1 (2017). \n29 Id. at 9.\n30 Alexandra Kohn & Jessica Lange, Confused About Copyright? Assessing Researchers’ Comprehension of Copyright Transfer Agreements, 6 J. Librarian\u0002ship & Scholarly Commc’n. 1, 9 (2018). \n31 See Will Frass, Jo Cross & Victoria Gardner, Taylor & Francis Open Access Survey June 2014 15 (2014). Note that lack of IP literacy could act as \nan additional deterrent to uploaders. The Taylor and Francis Open Access Survey of 2014 found that “63% of respondents indicated a lack of under\u0002standing of publisher policy as an important or very important factor in failing to deposit an article in an IR [Institutional Repository].” Id.\n32 Dataverse Community Norms, Harv. Dataverse, https://dataverse.org/best-practices/dataverse-community-norms.\n33 Copyright and License Policy, FigShare, https://help.figshare.com/article/copyright-and-license-policy. \n34 Australian Data Research Commons, Research Data Rights Managing Guide 6 (2019).\n35 See Harvard Dataverse General Terms of Use, Harv. Dataverse (2021), https://dataverse.org/best-practices/harvard-dataverse-general-terms-use.\n36 Stan. U. Inst. of Human-Centered Artificial Intelligence, Artificial Intelligence Index Report 2021 125-34 (2021).\n37 Thilo Hagendorff, The Ethics of AI Ethics: An Evaluation of Guidelines, 30 Minds & Machines 99 (2020). \n38 Andrew D. Selbst, An Institutional View of Algorithmic Impact Assessments, 35 Harv. J.L. & Tech. 1, 66 (forthcoming 2021).\n39 Brent Mittlestadt, Principles Alone Cannot Guarantee Ethical AI, 1 Nature Mach. Intelligence 501 (2019). \n40 DOD Adopts Ethical Principles for Artificial Intelligence, U.S. Dep’t Defense (Feb. 24, 2020), https://www.defense.gov/Newsroom/Releases/Release/\nArticle/2091996/dod-adopts-ethical-principles-for-artificial-intelligence/. \n41 President’s Mgmt. Agenda, Federal Data Strategy: Data Ethics Framework (2020).\n42 Artificial Intelligence: An Accountability Framework for Federal Agencies and Other Entities, U.S. Gov’t Accountability Office (June 30, 2021), https://\nwww.gao.gov/products/gao-21-519sp.\n43 Principles of Artificial Intelligence Ethics for the Intelligence Community, Office of the Director of Nat’l Intelligence, https://www.odni.gov/index.\nphp/features/2763-principles-of-artificial-intelligence-ethics-for-the-intelligence-community. \n44 Key Considerations for Responsible Development and Fielding of Artificial Intelligence, Nat’l Security Comm’n Artificial Intelligence (2021), https://\nwww.nscai.gov/key-considerations/. \n45 Recommended Practices, Nat’l Security Comm’n Artificial Intelligence, https://www.nscai.gov/wp-content/uploads/2021/01/Key-Consider\u0002ations-Supporting-Visuals.pdf. \n46 Defense Innovation Bd., AI Principles: Recommendations on the Ethical Use of Artificial Intelligence by the Department of Defense (2019). \n",
    "length": 431966,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Advancing Science- and Evidence-Based AI Policy",
    "url": "https://law.stanford.edu/publications/advancing-science-and-evidence-based-ai-policy/",
    "text": "Advancing Science- and Evidence-Based AI Policy - Journal Article - Stanford Law School\nlogo-footerlogo-fulllogo-stanford-universitylogomenu-closemenuClose IconIcon with an X to denote closing.Play IconPlay icon in a circular border.\n[Skip to main content] \n![Sls logo] \n# Advancing Science- and Evidence-Based AI Policy\n### Abstract\nPolicy-makers around the world are grappling with how to govern increasingly powerful artificial intelligence (AI) technology. Some jurisdictions, like the European Union (EU), have made substantial progress enacting regulations to promote responsible AI. Others, like the administration of US President Donald Trump, have prioritized “enhancing America’s dominance in AI.” Although these approaches appear to diverge in their fundamental values and objectives, they share a crucial commonality: Effectively steering outcomes for and through AI will require thoughtful, evidence-based policy development (1). Though it may seem self-evident that evidence should inform policy, this is far from inevitable in the inherently messy policy process. As a multidisciplinary group of experts on AI policy, we put forward a vision for evidence-based AI policy, aimed at addressing three core questions: (i) How should evidence inform AI policy? (ii) What is the current state of evidence? (iii) How can policy accelerate evidence generation?\nAI policy should advance AI innovation by ensuring that its potential benefits are responsibly realized and widely shared. To achieve this, AI policy-making should place a premium on evidence: Scientific understanding and systematic analysis should inform policy, and policy should accelerate evidence generation. But policy outcomes reflect institutional constraints, political dynamics, electoral pressures, stakeholder interests, media environment, economic considerations, cultural contexts, and leadership perspectives. Adding to this complexity is the reality that the broad reach of AI may mean that evidence and policy are misaligned: Although some evidence and policy squarely address AI, much more partially intersects with AI. Well-designed policy should integrate evidence that reflects scientific understanding rather than hype (2). An increasing number of efforts address this problem by often either (i) contributing research into the risks of AI and their effective mitigation or (ii) advocating for policy to address these risks. This paper tackles the hard problem of how to optimize the relationship between evidence and policy (3) to address the opportunities and challenges of increasingly powerful AI.\n### Details\nAuthor(s):\n* Rishi Bommasani\n* Sanjeev Arora\n* Jennifer Chayes\n* Yejin Choi\n* Mariano-Florentino Cuéllar\n* Li Fei-Fei\n* [Daniel E. Ho] \n* Dan Jurafsky\n* Sanmi Koyejo\n* Hima Lakkaraju\n* Arvind Narayanan\n* Alondra Nelson\n* Emma Pierson\n* Joelle Pineau\n* Scott Singer\n* Gaël Varoquaux\n* Suresh Venkatasubramanian\n* Ion Stoica\n* Percy Liang\n* Dawn Song\nPublish Date:July 31, 2025Publication Title:ScienceFormat:Journal ArticleVolume389Issue6759Page(s)459-461Citation(s):\n* Rishi Bommasani,Sanjeev Arora,Jennifer Chayes,Yejin Choi,Mariano-Florentino Cuéllar,Li Fei-Fei, Daniel E. Ho, Dan Jurafsky, Sanmi Koyejo, Hima Lakkaraju, Arvind Narayanan, Alondra Nelson, Emma Pierson, Joelle Pineau, Scott Singer, Gaël Varoquaux, Suresh Venkatasubramanian, Ion Stoica, Percy Liang &amp; Dawn Song,*Advancing Science- and Evidence-Based AI Policy*, 389Science459 (2025). Attachment(s):[**Download Now] ### Other Publications By\n[Daniel E. Ho] \n**Back to the Top",
    "length": 3550,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Expansion of GenAI Services Coming Soon",
    "url": "https://itcommunity.stanford.edu/news/expansion-genai-services-coming-soon",
    "text": "Expansion of GenAI Services Coming Soon | IT Community[Skip to main content] \n[![Stanford University]] \n[SUNet Login] \n[News] \n# Expansion of GenAI Services Coming Soon\nMore AI options deliver more opportunities for innovation at Stanford\nMonday, December 1, 2025\n**Get UIT services and training to support AI innovation at Stanford**\nFind more help for your AI needs—and curiosity—at[uit.stanford.edu/ai].\nYou’ll get access to:\n* Training\n* The free Stanford AI Playground\n* Service details\n* Indexes of GenAI efforts, topics, services, and tools being developed\n[Visit UIT's AI site] \nJust in time to ring in the giving season, we will soon be offering more AI services for the Stanford University community.\n**Beginning Dec. 16, all active SUNet ID holders will gain access to an expanded set of free GenAI tools from Microsoft and Google, with for-purchase options and reduced licensing costs for certain Microsoft AI services.**\nThis move will empower faculty, students, and staff with cutting-edge AI capabilities and stay aligned with Stanford's rigorous data security and privacy standards.\nWhile more details will be provided closer to the expansion date, this preview will help you understand what will be available and how our Stanford community can engage with these services.\n## Adoption strengthened by security and privacy practices\nThis expansion is powered by our firm data security and privacy positioning. Each of the newly available service options has a corresponding data classification rationale. For more on these match-ups, visit our[Responsible AI site] and the[GenAI Tool Evaluation Matrix].\nBy connecting our security and privacy policies with vendor management negotiations, Stanford is able to support the varied use cases across campus, within our policies and guidelines.\nNote: Be careful not to use tools with Stanford data, unless you are authenticated with your Stanford login. And for each tool, align your usage with Stanford’s[data classifications] as indicated in the “Data Approval” columns below.\n## Expanded Microsoft AI services\nWith Microsoft, we will be able to offer Microsoft Copilot Chat (free, authenticated), Microsoft 365 Copilot (paid, within Stanford’s Microsoft 365 tenant), and Github Copilot (within Cardinal Cloud Github).\n|Tool Name|Availability &amp; Purchase Options|Details|Data Approval|How to Use|\n**Microsoft Copilot Chat\\***|\nAvailable starting\nDec. 16.\nFree for all active SUNet IDs.\n|The authenticated version of Microsoft Copilot Chat; formerly known as Bing Chat Enterprise.|\nApproved for:\n* Low-Risk Data\n* Moderate-Risk Data|Visit[m365.cloud.microsoft/chat] and log in with your SUNet ID.|\n**Microsoft 365 Copilot**|\nAvailable now, with new reduced rates.\n[View purchasing details].\n|\n* For users and departments requiring more AI capabilities\n* Use within the Stanford University Microsoft 365 tenant\n* The premier solution for handling sensitive data within the Microsoft ecosystem.|\nApproved for:\n* Low-Risk Data\n* Moderate-Risk Data\n* High-Risk: non-PHI Data\n* High-Risk: PHI Data\nThe Stanford University Microsoft 365 tenant has been approved (exclusively for the listed[in-scope applications]) for handling High-Risk and PHI data, based on a security and privacy review.\n|Available as a browser-based tool, a mobile phone application, and as part of the Office 365 suite of applications.|\n**Github Copilot**|\nAvailable now.\n[Purchase by request]; billed monthly in arrears to a departmental PTA.\n|An AI coding assistant that suggests code completions, functions, and documentation directly in the selected IDE.|\nApproved for:\n* Low-Risk Data\n* Moderate-Risk Data|Only approved for use within[Cardinal Cloud GitHub] and requires an active Cardinal Cloud Azure subscription.|\n*\\* Similar to the OpenAI ChatGPT models also available in the*[*Stanford AI Playground*] *for free.*\n## Expanded Google AI services\nWith Google, we will be making available Google Gemini (free, authenticated) and Google NotebookLM (free, authenticated).\n|Tool Name|Availability &amp; Purchase Options|Details|Data Approval|How to Use|\n**Google Gemini\\***|\nAvailable starting\nDec. 16.\nFree for all active SUNet IDs.\n|Free, basic AI chatbot powered by Google Gemini AI models. It can answer questions, write content, summarize documents, and generate images, but it is not integrated into Google Workspace.|\nApproved for:\n* Low-Risk Data\n* Moderate-Risk Data|Available as a browser-based tool and mobile phone application, when authenticated with your SUNet ID.|\n**NotebookLM**|\nAvailable starting\nDec. 16.\nFree for all active SUNet IDs.\n|AI-powered research and note taking tool that lets users upload documents and ask questions about them.|\nApproved for:\n* Low-Risk Data\n* Moderate-Risk Data|Available as a browser-based tool and mobile phone application, when authenticated with your SUNet ID.|\n*\\* Similar to the Google Gemini models also available in the*[*Stanford AI Playground*] *for free.*\n## Rates and purchasing details to note\nSome community members might have questions on these specific topics:\n**Microsoft 365 Copilot reduced cost timing:**For departments that have already purchased Microsoft 365 Copilot licenses, the newly negotiated cost reduction will be applied automatically on future invoices, effective Nov. 1, 2025.​\n**Paid versions of Google Gemini for Workspace and Google NotebookLM Plus:**At this time, paid versions of Google Gemini and NotebookLM cannot be purchased for use with Stanford accounts due to vendor licensing limitations. UIT will continue to work with them to find solutions in the future.\n## Get help\nCloser to the rollout of these services, UIT will publish instructions for each tool. These service details will be available on[UIT’s AI site] under Services.\nIn the meantime:\n* For purchasing questions, see the purchase details above for each service. please submit a[Help request].\n* For technical assistance with existing services, please[contact the Service Desk].\n* For general questions about AI, visit[UIT’s AI site].\n*DISCLAIMER: IT Community News is accurate on the publication date. We do not update information in past news items. We do make every effort to keep our webpages up-to-date.*\n[![Stanford University]] \n* [Stanford Home] \n* [Maps & Directions] \n* [Search Stanford] \n* [Emergency Info] \n* [Terms of Use] \n* [Privacy] \n* [Copyright] \n* [Trademark] \n* [Non-Discrimination] \n* [Accessibility] \n©CopyrightStanford University.Stanford,California94305.",
    "length": 6467,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Shared - AIWG Chairs & Deans Presentation 10_20_25",
    "url": "https://studentaffairs.stanford.edu/sites/g/files/sbiybj19271/files/media/file/shared-aiwg_chairs_deans_presentation_10_20_25_1.pdf",
    "text": "AIWG - Fall Recommendations\nOctober 20, 2025\nPresentation Accessibility Note: please download as MS PowerPoint.\nScan QR Code\nAIWG Membership\nStudents\n● John Sandoval, (GR) MLA\n● Ryan Loo, (GR) Computer Science\n● Emily Chan, (UG) Art & Art History\nFaculty\n● Brian Conrad, Math\n● Chris Gregg, Computer Science\nStaff & Partner Offices\n● Vy Hoang, OCS\n● Cassandra Volpe Horii, CTL (VPUE + VPGE)\n● Karis Chi, OGC\n● Crystal Hill, OAE\n● Retnika Devasher, OAE/CTC\n● Deanna Graesser Ledoux, DoS\nCo-Chairs\n● Jennifer Schwartz Poehlmann, Senior Lecturer, Chemistry - AIWG Faculty Co-Chair\n● Xavier Arturo Millan, Undergraduate, Computer Science - AIWG Student Co-Chair\nAgenda\n• Academic Landscape\n• Key Department Actions\n• Proctoring Pilot Status\n• Opportunities to engage with AIWG\n• Scope of the Problem of Academic Dishonesty\n• Many students express concern about the extent of cheating on campus\n• Students report that use of unpermitted collaboration/aid is far more common on assignments completed outside \nthe classroom rather than in-person assessments\n• Students do not wish to have to report on each other\n• Root Causes of Academic Dishonesty\n• Student rationalization based on perceived cheating by other students, the need to keep up with peers\n• Many stressors (personal and academic) contribute to poor decision making\n• Disconnect between faculty and students on what are reasonable expectations outside of the classroom\n• Interplay of Academic Integrity and Pedagogy, including Generative AI\n• Faculty and Student conversations on pedagogy are needed to create buy-in on reasonable standards of work and \nuse of collaboration, AI , or other forms of aid on different assignment types\nAcademic Landscape\nTakeaways from AIWG focus groups and surveys\n➔ There is a need for ongoing faculty-student conversations to collaboratively agree on \nand maintain standards for academic integrity.\nStudent Opinions around Generative AI\n• Students want explicit AI policies in syllabi and examples of permitted and not \npermitted use. \n• Faculty vary greatly in permitted or recommended AI use, sometimes between sections of same \ncourse, causing confusion and leading students to perceive policies as inconsistent or unfair\n• Students want policy to differentiate between AI use for legitimate educational support \nvs cheating.\n• Some students perceive ChatGPT and other LLMs as a means to seek academic help rather than as \nan avenue for cheating, a “24-7 TA”\n• Because some instructors encourage use of AI for brainstorming, viewing examples of \nproblem-solving, etc., students are frustrated when others ban use without an explanation\n• Many students feel that so long as the userʼs personal understanding is represented in the final \nwork, AI is a helpful learning tool. \n• Students want help building capacity for Ethical AI Use and better AI literacy. \n• They are looking for in-class activities, discussion, or examples that develop AI literacy skills for \nfuture careers\nCurrent AI Policy statement from BCA\nGuidance adopted on February 16, 2023\nAbsent a clear statement from a course instructor, use of or consultation with generative AI shall be\ntreated analogously to assistance from another person. In particular, using generative AI tools to\nsubstantially complete an assignment or exam ӿe.g. by entering exam or assignment questionsԀ is\nnot permitted. Students should acknowledge the use of generative AI ӿother than incidental useԀ and\ndefault to disclosing such assistance when in doubt.\nIndividual course instructors are free to set their own policies regulating the use of generative AI tools\nin their courses, including allowing or disallowing some or all uses of such tools. Course instructors\nshould set such policies in their course syllabi and clearly communicate such policies to students.\nStudents who are unsure of policies regarding generative AI tools are encouraged to ask their\ninstructors for clarification.\nKey Actions Departments can consider\n1\nCreate and clearly communicate consistent policies and expectations on \nacademic integrity and generative AI across similar courses and departments\n2\nEngage students in a dialogue on learning goals and use (or non-use) of AI \nwithin your discipline\n3\nSupport faculty in development of pedagogical innovations that support \nstudent learning and career preparation in light of AI applications\n4\nMove more high-stakes assessments (or portions of large assignments) to \nin-person formats, where feasible or beneficial \n1) Create and clearly communicate policies\n1. Communicate and discuss academic integrity policies for their course with students, \nincluding details on AI\n2. Increase coordination and consistency across multiple offerings of a course (i.e. \nMultiple sections within a single type of course, courses offered in multiple quarters)\n• Example of Department Policy From Math - launched this summer across all courses\n3. Schedule a School or Department Consultation with CTL\n• Customized workshops and facilitated discussions on AI: e.g., 15 minute intro during a faculty\nmeeting, 60-90 minute workshop on its own or during a retreat\n• Individual teaching consultations for specific courses/instructors\n4. Resources for developing policies\nAIMES (AI Meets Education at Stanford) - CTL & VPUE:\n● AIMES Library of Examples: find example course and department AI policies, assignments, etc.\n● Critical AI Literacy for Instructors: self-paced Canvas resource for instructors\n● AI Teaching Strategies: generalized patterns - what is working\n● AI at Stanford Advisory Committee (AISAC) Report (January 2025)\nFind Examples at AIMES: \nAI Meets Education at Stanford\nExample from CS145: Narayanan Shivakumar \n The Why: https://cs145-bigdata.web.app/Section1-SQL/learning-outcomes.html\n The What - for Projects: https://cs145-bigdata.web.app/projects/projects.html\nStrategies When Assigning AI Use\nStrategies When Limiting AI Use\nStrategies When Prohibiting AI Use\nFundamentals of AI policy\nAspects to consider \n2 Pedagogical reasoning\n● What are your learning goals for the course? In what \nways does AI help or hurt student learning? \n● Why should students follow your proposed path?\n● How will they know if they are using AI effectively? Or \ntoo much?\n1 Level of Use\n● AI Free zone\n● Restricted/specific use cases\n● Open AI use\nExample use cases\n● Use AI to provide feedback, coach a process, run \nsimulations, provide example practice problems, …\n● Create guided activities or prompts for students to \nuse that promote AI literacy\n3\nDirections for citation\n● How should students acknowledge the use of AI?\n● Citation of primary sources\n● Statement of AI use and prompts\n4\n2) Engage students in discussion of policies\n1. Host listening sessions or surveys to hear concerns, create buy-in on policies\n• Engage students in developing a shared understanding of reasonable standards for \nacademic work and appropriate levels of collaboration across different assignment types\n• Courses might consider offering participation credit to avoid selection bias in students\n2. Attend upcoming AIWG listening sessions and encourage your faculty to do so\n• Faculty AI policy \n• Joint Faculty-Student \n3. Participate in Instructor Student Pedagogy Partnership Program through CTL\n• Partners instructors with undergraduate students to gain direct feedback on the student \nexperience in introductory undergraduate courses\n4. Encourage students to educate themselves and take advantage of resources\n• AI and Your Learning: A Guide for Students\n• Consider creating workshops or short learning modules for students on ethical AI use, tailored to \nyour discipline (e.g., ChatGPT in humanities vs. coding assistance in CS). \n3) Support Faculty in Pedagogical development\n1. Faculty will need support in adapting to evolving student needs \n• Develop policies, pedagogical strategies and expectations with students in class\n• Incorporate effective AI use and literacy development into course assignments\n• Bring process and development of ideas into class\n• Create out of class work that provides learning opportunities for students to \nprepare for in-person assessments\n2. Share your policies and teaching strategies that are working! (CTL - AIMES is collecting \na growing body of examples.)\n3. Resources for developing teaching innovations in the AI-evolving landscape\n• Stanford Accelerator For Learning - AI\n• Human Centered Artificial Intelligence - involved research and grants\n• CTL consultations and custom workshops: CTL General Consultation Request Form\n• AIMES, AI Meets Education at Stanford\nAI and Your \nLearning:\nA Guide for \nStudents\nAIMES \nLibrary of \nExamples\nAI \nTeaching \nStrategies\nCritical AI \nLiteracy \nFor Instructors\nCanvas Resource\n4) Move towards In-Person assessments\n→ In-person assessments have higher adherence to the Honor Code than at-home work\n→ Proctored in-person assessments have become more vital given increase in phone/AI use\n1. Reflect broadly on graded work \n• Distribute weight of assignments to encourage students to demonstrate mastery of \nindependent skills in a way they can be fully accountable: AIMES Library Example\n• Reframe home assignments as learning opportunities, less for the grade\n• Move high-stakes assessments to in-person formats where helpful: AIMES Library Example\n• Consider alternate formats of assessment including (Examples in AIMES):\n■ The Oxford style tutorial system (small group discussion and feedback)\n■ Oral Presentations or in-person discussions\n■ Segment writing process into multiple in-person activities\n2. AIWG Resources for In-Person Assessments\n• Logistics to Consider when Transitioning to In-Person Assessments\n• Exam Integrity Best Practices\n• IF approved to participate in the proctoring pilot: Proctoring Guidelines\nParticipating Courses: Autumn \nScience & Econ\n• CHEM 31A & 31B\n• CHEM 121\n• ECON 1\n• ECON 102A\n• ECON 50\n• ECON 51\n• HUMBIO 2A\n• HUMBIO 2B\n• PHYS 41\n• PHYS 41E\n• PHYS 45\n• STATS 110\n• STATS 118\nGSB\n• FINANCE 205 \nHumanities/Social Science\n• COLLEGE 101\n• COMPLIT 121\n• COMPLIT 147P\n• FRENLANG 1\n• HIST 40A/ 140A\n• PHIL 49\n• PSYCH 1\nEngineering\n• AA100\n• BIOE 101/210\n• CHEMENG 110A\n• CS 103\n• CS 106B\n• CS 161\n• CS 229\n• ENGR 14\n• ME 70\n• ME 80\nMath\n• MATH 18, 19, 20, & 21\n• MATH 51 & 53\n• MATH 61CM & 61 DM\n• MATH 104\n• MATH 110\n• MATH 113\n• MATH 115\n• MATH 120\n• MATH 136\n• MATH 143\n• MATH 145\n• MATH 155\n• MATH 171\n• MATH 193X\n• MATH 215A\n• MATH 220A/CME 303\n• Inclusion of COLLEGE & introductory sequences \nin Chemistry, HumBio, Math, Physics & Psych \nmean that almost all frosh will have opportunities \nto participate in proctored courses\nEnrollments total \nover 7,500 \nCritical Campus Partnerships\nCareful expansion of pilot still required to plan for specialty situations\nOffice of Accessible Education (OAE)\n(15% of UG students)\n• Build out Centralized Testing Center and create data-management system \nto enable equitable proctoring of accommodated exams\nStudent-Athletes\n(11% of UG students)\n• Training for coaches and staff who proctor exams for traveling athletes\n• Creation of documented process to coordinate exam requirements and \nmaterials between coach & course staff\nCenter for Global & Online Education (CGOE), formerly SCPD\n(4% for classes with CGOE students)\n• Study of and adaptation of pilot for off-campus students. \n➔ Proctoring CANNOT expand without thoughtful planning and resources. \nExample breakdown in\n 200 person class\nCGOE (8)↴\nAthletes\n (22)\nOAE (30)\n Standard \nAdministration\n (144)\nBuilding Infrastructure & Processes for Scale\nMeeting Accommodation Challenges\nCentralized Testing Center (CTC) concurrently created and \nlaunched to provide a subset of exam accommodations such as: \n● Approved for a “private room” OR\n● Scrubbed device (not internet enabled) OR\n● Furniture accommodations OR\n● Special lighting OR\n● 100% extended time (distraction reduced) OR\n● Other complex cases\nAccommodations managed by faculty:\n● 50% extended time\n● 50% extended time and distraction reduced\n➔ Increased space and staffing of the CTC is required before we expand proctoring.\nMajor contributions from Proctoring Pilot\n1. Partnerships with OAE, Athletics, and CGOE to create clear policies and coordination of \nexam proctoring for these students\n2. AIWG Proctoring Guidelines to create consistent student experience\n3. Determined and shared Best Practices/Tips for bolstering Academic Integrity \n4. Coordinated expansion of Centralized Testing Center (CTC) in partnership with OAE - \nnow seating up to 72 students, will add additional 7 private rooms by winter\n➔ Focus groups and surveys revealed the importance of consistency in proctoring \nexperience for students.\n➔ Proctor Training and application of guidelines across all courses provides clear \nexpectations for students.\nCurrent AY 25-26 Goals for Pilot\n1. Systematically expand capacity of Proctoring Pilot\n2. Identify and create processes for other special cases\n• Humanities/writing courses\n• World languages, Digital Language Lab\n• Make-up exams or time conflicts\n• Short “in-class” quizzes (time sensitive material) \n• Incoming UG Placement exams\n• Graduate courses and Graduate Qualifying Exams\n3. Share findings and vote on: Does Stanford move to \nfull scale proctoring across the University?\n4. Support faculty-student conversations around & \nassessment of Academic Integrity on campus, \nincluding intersection with Generative AI\nEngagement Opportunities \n❏ Faculty Listening Sessions: aiwg.stanford.edu/FacultySessions\n❏ Nov. 3 & Nov. 4 \n❏ AIWG representative can visit faculty meetings\n❏ Discuss department specific Academic Integrity concerns \n❏ Answer questions on participating in the proctoring pilot\n❏ You may also send any feedback, concerns, etc through the form\n❏ CTL General Consultation Request Form for CTL consultations and \ncustom workshops\n❏ AY 25-26 Course Pilot Volunteer Form: aiwg.stanford.edu/signup\n❏ Taking submissions for Winter and Spring\n❏ CTC capacity is expanding to allow for more course participation Scan QR code to access \npilot volunteer form\nScan QR code to access \nfaculty listening sessions\nThank you.\nQuestions? Scan QR Code\nAdditional Information\nAppendix of Additional Data and Background Information\nAdvance Reference | \n Interim Report from the AIWG (page 1 of 3)\n● AIWG: Academic Integrity Working Group\n● VPSA: Vice Provost for Student Affairs\n○ DoS: Dean of Students\n○ OAE: Office of Accessible Education\n■ CTC: Centralized Testing Center\n○ OCS: Office of Community Standards\n● BCA: Board on Conduct Affairs\n● Committee of 12 (C-12) rewrote the Honor \nCode, Judicial Charter, and Interpretations of \nthe Fundamental Standard\n● OGC: Office of General Counsel\n● CTL: Center for Teaching and Learning\n● VPUE: Vice Provost for Undergraduate \nEducation\n● ASSU: Associated Students of Stanford \nUniversity\nExecutive Summary\nThe AIWG is charged with addressing a variety of issues related to academic integrity at Stanford University. It aims to study and \nunderstand the causes of academic dishonesty in contemporary settings and to assess the interplay of academic integrity and \npedagogical practices both here and elsewhere. It will ultimately recommend policies and other measures to address these \nissues.\nAcronym Key, Arranged by Organization \nAdvance Reference | \n Interim Report from the AIWG (page 2 of 3)\nProctoring Pilot Overview\nTimeline\nLaunched Spring 2024 with seven initial courses, expanded to nineteen by Fall 2024. Courses spanned various departments, \nlevels, and enrollment sizes. The proctoring pilot will take place for 2-4 years. \nObjective\nTo assess the feasibility, challenges, and impact of in-person proctoring as a tool to support academic integrity during \nassessments.\nData Collection\nFaculty and student surveys, OAE feedback, course implementation reports, student listening sessions, and data on \nproctored exam logistics.\nAdvance Reference | \n Interim Report from the AIWG (page 3 of 3)\nKey Findings from the Proctoring Pilot\nLarger Courses Present Unique Challenges\nCourses with 100+ students increase logistical complexity, requiring scalable proctoring infrastructure and staffing models.\nRising Accommodation Needs Require Greater Flexibility\nIncreasing numbers of students with approved accommodations require enhanced coordination with the OAE and the \nCentralized Testing Center.\nConsistency in Proctoring Improves Student Experience\nClear and uniform guidelines reduce confusion and create a more equitable and supportive exam environment for students.\nFaculty Involvement Is Critical to Success\nContinued pilot participation, listening sessions, and conversations will shape effective and sustainable academic integrity \nstrategies.",
    "length": 16652,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Generative AI for the Future of Learning",
    "url": "https://seedfunding.stanford.edu/opportunities/generative-ai-future-learning",
    "text": "Generative AI for the Future of Learning | Stanford Seed Funding\n[Skip to main content] \n[Skip to navigation] \n[Stanford University] \n[\nSOLO\nSOLO\n] \n### Warning!\n**Archiving this opportunity has collateral effects.**If you archive this opportunity,153active application(s) will be archived.*Archived*applications cannot be managed by Program Officers, and they cannot be viewed or managed by applicants.\n[Proceed] [Cancel] \nClose\n# Generative AI for the Future of Learning\nSponsored by\n[Stanford Accelerator for Learning] \n[Stanford Institute for Human-Centered Artificial Intelligence] \nFunding:\nSeed grants up to $100,000 for innovative designs and/or research on critical issues and applications of generative AI in learning contexts.\nEligibility:\nAcademic Council Faculty\nClinician Educators\nInstructors\nMedical Center Line Faculty\nPostdocs\nStaff\nStudents\nOpportunity Type:\nResearch\nProof-of-Concept\nTranslational\nApplications closed\nApplications closed on March 1, 2023\nApproximate Offer Date:\nApril 3rd, 2023\n## **Generative AI for the Future of Learning**\n### ***Request for Proposals***\n**The Stanford Accelerator for Learning and the Stanford Institute for Human-Centered Artificial Intelligence invite proposals for innovative designs and/or research on critical issues and applications of generative AI in learning contexts.**\n**The[Stanford Accelerator for Learning] seeks to accelerate solutions to the most pressing challenges facing learners. Housed at Stanford Graduate School of Education, the Stanford Accelerator for Learning is the first university-wide initiative connecting scholars across disciplines and with external partners to bridge research, innovation, practice, and policy, and bring quality scalable and equitable learning experiences to all learners, throughout the lifespan.**\n**The[Institute for Human-Centered Artificial Intelligence] ’s vision for the future is led by the commitment to promote human-centered uses of AI, design for it using human-centered methods, and ensure that humanity benefits from the technology and that the benefits are broadly shared. In support of these goals, our research falls into three key focus areas:[Human Impact],[Augment Human Capabilities], and[Intelligence].**\n**Generative AI is artificial intelligence that can generate novel content by utilizing existing text, audio files, or images. Generative AI has now reached a tipping point where it can produce high quality output that can support many different kinds of tasks. For example, ChatGPT can write essays and code, DALL-E can create images and art, while other forms of generative AI can produce recipes, music, and videos.[These new forms of generative AI have the capacity to change how we think, create, teach, and also learn]. They may also change our perspective on what is important to learn.**\n**Most generative AI tools were not built for educational purposes, and advances in the technology have outpaced research and design of their application to learning contexts. We invite scholars and students from across Stanford University to submit proposals for innovative designs and/or studies that explore how generative AI can be applied in novel ways to support learning and/or investigate[critical issues in learning contexts].**\n**There is an opportunity for research and design solutions to shape future applications of this emerging technology in an ethical, equitable, and safe manner. This seed grant will fund early and exploratory stages of this work, such as designs, prototypes, and pilot studies that have the potential to scale or have broad impact. In line with this goal, we will accept the following types of proposals:**\n* **Design proposals which produce a working prototype of an AI-based learning tool or an intervention that applies an existing AI learning tool. Designs should be grounded in user/stakeholder contexts and needs.**\n* **Empirical research proposals that investigate questions or hypotheses around generative AI and the future of learning.**\n* **A combination of design and empirical work.**\n**Proposals may target any type of learner (e.g., worker, student, teacher, family) in any setting (e.g., workplaces, museums, classrooms, homes). We especially welcome proposals that focus on one or more of the Accelerator’s areas of concentration including:**\n* **early childhood learning and development**\n* **learning differences and the future of special education**\n* **equity and learners**\n* **workforce learning**\n**Proposals that support student RAs are also encouraged. However, all proposals that investigate generative AI in learning contexts will be considered for funding.**\n**ACCELERATOR STUDIO SUPPORT SERVICES**\n**The Stanford Accelerator for Learning is dedicated to accelerating solutions anchored in the science and design of learning, and offers technical, research, and partnership consulting for grantees to aid their work. This is implemented through the Accelerator Studio, a new service model that provides extensive support services to help guide the design of effective, scalable solutions anchored in the rapidly growing research from the learning sciences. For example, an instructor with a great learning innovation may not have experience researching its effects on learning. Alternatively, a faculty member may have an idea for a new digital learning tool or application, but not have access to technologists to bring it to life.  The Stanford Accelerator for Learning is equipped to provide additional support from Accelerator Studio staff with relevant expertise.  If awarded a grant, the specific needs and allocation of support will be determined through consultation with the Accelerator Studio. Examples of Accelerator Studio support include:**\n* **Technology support. Ideation, storyboarding, prototyping, interface design, instructional design, app development and testing, cloud services, and media production.**\n* **Research support.  Research conceptualization and measure design, execution of quantitative and/or qualitative research in the lab or field, data use agreements and storage, guidance for working with Stanford’s Institutional Review Board.**\n* **Science and design of learning. Evidence-based strategies for improving learning experiences, as well as guidance on avoiding common mistakes that interfere with learning.**\n* **Partnership support. Finding and helping to broker prospective participants and partner organizations outside of Stanford (e.g., middle school students, community garden partners, tech companies, etc.).**\n****\n**ELIGIBILITY &amp; AWARD AMOUNTS**\n***Faculty Seed Grants***\n**Two tiers of funding are available to faculty:**\n* ***Up to $100,000, with option for supplemental Accelerator Studio service support.*This level of funding is open to collaborative proposals that include either (a) two or more Stanford PI-eligible faculty from different departments, schools, or other academic units or (b) one Stanford PI-eligible faculty member plus an external partner. External partners may include schools, community-based organizations, technology based firms, businesses, non-profits, and other organizations who are interested in advancing learning opportunities in generative AI and invested in partnership with experts at Stanford University.**\n* ***Up to $50,000, with option for supplemental Accelerator Studio service support*. This level of funding is open to Stanford PI-eligible faculty and may or may not include a collaboration.**\n**See guidelines on PI Eligibility in the[Stanford Research Policy Handbook Chapter 2.1].**\n**Faculty selected to receive funds will be enrolled as Faculty Affiliates with the Stanford Accelerator for Learning and the Institute for Human-Centered Artificial Intelligence, opening the doors to additional engagement opportunities and resources. To support faculty who are new to aspects of technology development and/or human subjects research, applicants may also request supplemental support to be used only for Accelerator Studio service support (see above). Grant proposals that seek supplemental support services should include a separate section explaining their potential support needs and whether those are technical, research, learning design, partnership, or some combination. If awarded a grant, we will work together to determine the specific allocation of support through consultation, so please do not include these funds in your budget.**\n**Academic Staff Seed Grants**\n***Up to $10,000, with option for supplemental Accelerator Studio service support***\n**Full-time academic staff, including lecturers, may apply for funding if they receive approval from their supervisor.  To support academic staff who are new to aspects of technology development and/or human subjects research, applicants may also request supplemental funds to be used only for Accelerator Studio service support (see above). Grant proposals that seek supplemental funding for support services, should include a separate section explaining their potential support needs, and whether those are technical, research, learning design, partnership, or some combination. If awarded a grant, we will work together to determine the specific allocation of support through consultation, so please do not include these funds in your budget.**\n**Student and Postdoc Grants**\n***Up to $5,000***\n**Postdocs, students, or teams may apply. Projects must include at least one Stanford student (undergrad or grad) or postdoc and can include collaborations with external organizations or institutions. (Post-docs will need approval from their faculty supervisors.)  For students interested in diving deeper into learning design and educational technology, the[Digital Learning Design Challenge] offers workshops and potential for mentorship and additional funding.**\n****\n**TIMELINE**\n**Funded projects will have*15 months to spend their award*. Faculty and academic staff seed grant project periods must start no earlier than April 3, 2023 and finish by June 30, 2024.**\n**Student seed grant project periods must start no earlier than April 3, 2023 and finish by April 3, 2024 or end of quarter of graduation, whichever is earlier.**\n**Key dates:**\n* ***Proposals Due: March 1, 2023 11:59 pm PT***\n* ***Awards Announced and grant period begins: April 3, 2023\\****\n**\\* If human subjects research is involved, funds cannot be released until IRB approval has been obtained**\n**\\* Funds cannot be released until proposals have sufficiently addressed any ethical and societal risks**\n****\n**REQUIREMENTS**\n* **Participation in 2-3 seed grant recipient meetings**\n* **A short interim report and a final report**\n**GRANT PROPOSAL SECTIONS**\n* **Project abstract**\n* **Background and problem statement**\n* **Design and/or research plan with timeline**\n* **Potential for broader impact**\n* **Ethics and Society Review (ESR) statement****: (1)****Detail the ethical challenges and possible negative societal impacts of the proposed research. What are the possible long-term consequences of this research and (2)****Articulate general principles that you will use to eliminate or mitigate these issues. Then, translate those principles into the specific design decisions you are making in your proposed research.**\n* **​​​​​****[Read more details about the ESR statement and process.] **\n* **Roles and contributions of all project personnel and contributions of collaborators, if applicable.**\n* **If Accelerator Studio support services are requested, explain any technical, research, or partnership service needs.**\n* **Requested funding and budget plan (e.g., 25% RA 2qtrs, travel $3K, equipment $4K). Budget MUST include an 8% infrastructure charge. Capital equipment ($5k+) will not be funded.**\n**\\*Proposal sections can be entered and submitted on the[Stanford Seed Funding website].[Each proposal section is limited to 200-300 words.] **\n\\*Note that proposals must comply with all applicable laws, University Policies (such as the[University's code of conduct] and[intellectual property policy]), and contractual terms, including the terms of service for generative AI tools or applications*.*\n**SELECTION CRITERIA**\n**The Stanford Accelerator for Learning and the Institute for Human-Centered Artificial Intelligence will review proposals based on:**\n* **Likelihood to make a positive impact on our education system, our learners, our communities, and/or our lives**\n* **Novelty or innovation in the application, development or study of generative AI in support of learning**\n* **Intellectual merit of the proposal, balanced with quality and creativity**\n* **Involving multiple disciplines**\n* **Whether the proposed team has the proper credentials to complete the project**\n* **Alignment with one or more of the areas of concentration listed above**\n* **Ability to address and mitigate ethical and societal risks**\n**MORE INFORMATION**\n**Seed Grant Information Sessions will be held via Zoom to provide an overview of the project and answer questions from prospective applicants.  These optional sessions will be held on:**\n* **February 9, 12:00pm - 1:00pm PT**\n* **February 21, 2:00pm - 3:00pm PT**\n**If you cannot attend the information sessions and/or have other questions please contact Cathy Chase ([cchase@stanford.edu]), senior research scholar and seed program manager, Stanford Accelerator for Learning.**\n**Proposals may be submitted via the[Stanford Seed Funding website].**\nEligibility:\nPI-eligible faculty, academic staff, postdocs, students\nRequirements:\n* **PI participation in 2-3 seed grant recipient meetings**\n* **An interim and final report**\nAmount\n$100000\nHow can the funds be used?\nFunds may be used for salary support of faculty, students, and other research or technical support staff, tuition for student RAs, supplies and equipment, participant support, prototyping expenses, and travel directly associated with the research activity. Funds will not support general staff or administrative support. Funds cannot support capital equipment purchases ($5k or more).\nAward amounts will be based on an analysis of the budget request and planned research/design activities.\nOpportunity Type:\nResearch\nProof-of-Concept\nTranslational\n![Stanford Accelerator for Learning and Stanford Institute for Human-Centered Artificial Intelligence] \n#### Contact:\nCatherine Chase\n[cchase@stanford.edu] \n[![Stanford University]] \n* [Stanford Home] \n* [Maps & Directions] \n* [Search Stanford] \n* [Emergency Info] \n* [Terms of Use] \n* [Privacy] \n* [Copyright] \n* [Trademarks] \n* [Non-Discrimination] \n* [Accessibility] \n&copy;Stanford University,Stanford,California94305.",
    "length": 14633,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "",
    "url": "https://hai.stanford.edu/assets/files/2024-06/HAI-Policy-Brief-Responsible-Development-LLMs-Psychotherapy.pdf",
    "text": "Key Takeaways\nLarge language models (LLMs) \nhold promise for supporting, \naugmenting, and even \nautomating psychotherapy \nthrough tasks ranging \nfrom note-taking during \ninterviews to assessment \nand delivering therapy. \nHowever, psychotherapy is \na uniquely complex, high\u0002stakes domain. The use of \nLLMs in this field poses \nwide-ranging safety, legal, \nand ethical concerns.\nWe propose a framework \nfor evaluating and reporting \non whether AI applications \nare ready for clinical \ndeployment in behavioral \nhealth contexts based on \nsafety, confidentiality/privacy, \nequity, effectiveness, and \nimplementation concerns. \nPolicymakers and behavioral \nhealth practitioners should \nproceed cautiously when \nintegrating LLMs into \npsychotherapy. Product \ndevelopers should \nintegrate evidence-based \npsychotherapy expertise \nand conduct comprehensive \neffectiveness and safety \nevaluations of clinical LLMs.\nPolicy Brief\nHAI Policy & Society\nJune 2024\nToward Responsible \nDevelopment and \nEvaluation of LLMs \nin Psychotherapy\nElizabeth C. Stade, Shannon Wiltsey Stirman, Lyle Ungar, \nCody L. Boland, H. Andrew Schwartz, David B. Yaden, \nJoão Sedoc, Robert J. DeRubeis, Robb Willer, Jane P. Kim, \nand Johannes C. Eichstaedt\nTHERE IS GROWING ENTHUSIASM ABOUT THE POTENTIAL OF \nOPENAI’S GPT-4, Google’s Gemini, Anthropic’s Claude, and other large \nlanguage models (LLMs) to support, augment, and even fully automate \npsychotherapy. By serving as conversational agents, LLMs could help \naddress the shortage of mental healthcare services, problems with \nindividual access to care, and other challenges. In fact, behavioral \nhealthcare specialists are beginning to use LLMs for tasks such as note\u0002taking, while consumers are already conversing with LLM-powered \ntherapy chatbots.\nHowever, psychotherapy is a uniquely complex, high-stakes domain. \nResponsible and evidence-based therapy requires nuanced expertise. \nWhile the stakes involved with using an LLM for productivity purposes may \nbe failing to maximize efficiency, in behavioral healthcare, the stakes may \ninclude the improper handling of suicide risk.\n1\n2\nPolicy Brief \nToward Responsible Development and \nEvaluation of LLMs in Psychotherapy\nOur paper, “Large Language Models Could Change \nthe Future of Behavioral Healthcare,” provides a \nroad map for the responsible application of clinical \nLLMs in psychotherapy. We provide an overview of \nthe current landscape of clinical LLM applications \nand analyze the different stages of integration into \npsychotherapy. We discuss the risks of these LLM \napplications and offer recommendations for guiding \ntheir responsible development. \nIn a more recent paper, “Readiness for AI Deployment \nand Implementation (READI): A Proposed Framework \nfor the Evaluation of AI-Mental Health Applications,” \nwe build on our prior work and propose a new \nframework for evaluating whether AI mental health \napplications are ready for clinical deployment.\nThis work underscores the need for policymakers to \nunderstand the nuances of how LLMs are already, \nor could soon be, integrated in psychotherapy \nenvironments as researchers and industry race to \ndevelop AI mental health applications. Policymakers \nhave the opportunity and responsibility to ensure that \nthe field evaluates these innovations carefully, taking \ninto consideration their potential limitations, ethical \nconsiderations, and risks.\nIntroduction\nThe use of AI in psychotherapy is not a new \nphenomenon. Decades before the emergence of \nmainstream LLMs, researchers and practitioners used \nAI applications, such as natural language processing \nmodels, in behavioral health settings. For instance, \nvarious research experiments used machine learning \nand natural language processing to detect suicide \nrisk, identify homework resulting from psychotherapy \nsessions, and evaluate patient emotions. More \nrecently, mental health chatbots such as Woebot and \nTessa have applied rules-based AI techniques to target \ndepression and eating pathology. Yet they frequently \nstruggle to respond to user inputs and have high \ndropout rates and low user engagement.\nLLMs have the potential to fill some of these gaps \nand change many aspects of psychotherapy care \nthanks to their ability to parse human language, \ngenerate human-like and context-dependent \nresponses, annotate text, and flexibly adopt different \nconversational styles. \nHowever, while LLMs show vast promise in performing \ncertain tasks and skills associated with psychotherapy, \nclinical LLM products and prototypes are not yet \nsophisticated enough to replace psychotherapy. \nThere is a gap between simulating therapy skills and \nimplementing them to alleviate patient suffering. To \nLLMs hold the potential \nto fill gaps in mental health \ntreatment and change many \naspects of psychotherapy \ncare delivery.\n3\nPolicy Brief \nToward Responsible Development and \nEvaluation of LLMs in Psychotherapy\nachieve the implementation piece, clinical LLMs need \nto be tailored to psychotherapy contexts using prompt \nengineering—structuring a set of instructions so they \ncan be understood by an AI model—or fine-tuning \ntechniques that use curated datasets to train the LLM. \nAs LLMs are increasingly used in psychotherapy, \nit is essential to understand the complexity and \nstakes at play: In the worst-case scenario, an \n“LLM co-pilot” functioning poorly could lead to \nthe improper handling of the risk of suicide or \nhomicide. While clinical LLMs are, of course, not the \nonly AI applications that may involve life-or-death \ndecisions—consider self-driving cars, for example—\npredicting and mitigating risk in psychotherapy \nis unique. It requires conceptualizing complex \ncases, considering social and cultural contexts, and \naddressing unpredictable human behavior. Poor \noutcomes or ethical transgressions from clinical \nLLMs could seriously harm individuals and undermine \npublic trust in behavioral healthcare as a field, as has \nbeen seen in other domains.\nBeginning with an overview of the clinical LLMs in use \ntoday, our first paper reviews the current landscape of \nclinical LLM development. We examine how clinical \nLLMs progress across different stages of integration \nand identify specific ethical and other concerns related \nto their use in different scenarios. We then make \nrecommendations for how to responsibly approach \nthe development of LLMs for use in behavioral health \nsettings. In our second paper, we propose a framework \nthat could be used by developers, researchers, \nclinicians, and policymakers to evaluate and report \non the readiness of generative AI mental health \napplications for clinical deployment.\nClinical Integration \nof LLMs\nClinical LLMs can take multiple forms. These include \napplications that are patient-facing (e.g., providing \npsychoeducation for patients), therapist-facing (e.g., \noffering intervention options), trainee-facing (e.g., \ngiving feedback on trainees’ performance), and \nsupervisor- or consultant-facing (e.g., summarizing \nhigh-level takeaways from a session).\nMuch like scholars have done for the autonomous \nvehicle industry, we classify the integration of clinical \nLLMs into psychotherapy into three main stages: \n• Assistive (“machine in the loop”): LLMs that \nassist clinical providers and researchers by \nperforming low-level, concrete, and low-risk \ntasks, such as conversing with patients to \ncollect information about their symptoms.\n• Collaborative (“human in the loop”): LLMs \nthat provide treatment suggestions for \npsychotherapists to review, such as producing \nan overview of a person’s symptoms and \nexperiences, and curating a list of therapy \nexercises from which the provider can select.\n• Fully autonomous: LLMs that perform a full \nrange of clinical skills and interventions without \ndirect oversight from a provider, such as \nconducting assessments, presenting feedback, \nselecting an appropriate intervention, and \ndelivering a course of therapy. \n4\nPolicy Brief \nToward Responsible Development and \nEvaluation of LLMs in Psychotherapy\nSeveral promising assistive- and collaborative-stage \napplications of clinical LLMs that relate to the provision \nof, training in, and research on psychotherapy already \nexist or are imminently feasible. These include:\n• Automating clinical administration tasks, \nsuch as writing session transcripts or \nconducting chart reviews.\n• Measuring treatment fidelity, including a \ntherapist’s adherence to evidence-based \npractices (EBPs) and specific modalities, as well \nas their overall counseling skills.\n• Offering feedback on therapy worksheets and \nhomework, including real-time clarifications or \nproblem solving.\n• Automating aspects of supervision and \ntraining, including supporting peer counselors\nor psychotherapy trainees with corrections \nand suggestions.\nIn the long-term, fully autonomous clinical care may \ntheoretically be possible. In addition to the fully \nautonomous applications described above, these \nmay also include LLMs that act as a decision aid for \nPoor outcomes or ethical \ntransgressions from clinical LLMs \ncould seriously harm individuals \nand undermine public trust in \nbehavioral healthcare as a field.\nexisting EBPs, for example by analyzing transcripts \nfrom therapy sessions and offering guidance \ntailored to the individual, and LLMs that support \nthe development of new therapeutic techniques \nand EBPs, for example by detecting therapeutic \ntechniques associated with objective outcomes and \n“reverse-engineering” new EBPs.\nThese potential applications of clinical LLMs may \nhelp move the behavioral healthcare field toward \n5\nPolicy Brief \nToward Responsible Development and \nEvaluation of LLMs in Psychotherapy\npersonalized treatment approaches that optimize \nexisting evidence-based psychotherapy, identify \nnew therapeutic approaches, and improve \nunderstanding of mechanisms of change. The goal \nis to enhance practitioners’ ability to identify which \npsychotherapy treatments work best, for whom, \nand under what circumstances.\nPotential Risks of \nClinical LLMs\nEach stage of clinical LLM integration poses its own \nrisks or costs. Assistive AI may increase overhead for \ntherapists, because these systems require significant \nsupervision. Collaborative AI applications may require \ntime-intensive review and corrections that fail to \nsave therapists time or, worse, could lead to patients \nreceiving clinical interventions that have not been \nassessed or tailored by their therapists because they \nlacked sufficient time to review LLM outputs. \nFully autonomous AI, for its part, could miss critical \ninformation in a clinical setting that could lead \nto inappropriate or harmful recommendations. \nFor example, these systems may not be able to \ncarry out case conceptualization on patients with \ncomplex symptoms or to take into account important \ncontextual information such as past suicidality and \nlife circumstances. At present, LLMs can’t pick up \nnonverbal behavior or appropriately challenge patients. \nIt is also unclear if LLMs can effectively engage \npatients in the long term.\nResearch has shown that humans can develop \ntherapeutic alliances with chatbots, but the long-term \nviability of these relationships—and whether they have \nharmful downstream effects—is an open question. \nLLM chatbots have also been found to exhibit \nnarcissistic tendencies and have the potential to \nunduly influence humans. Questions of accountability \nand liability, such as in cases where a clinical LLM is \ninvolved in malpractice, pose additional challenges.\nIt remains to be seen whether fully autonomous clinical \nLLMs will ever be deemed safe enough for deployment \nand whether complete automation is even desired. \nGiven the wide-ranging safety, legal, philosophical, \nand ethical concerns around fully autonomous \nclinical LLMs, it is likely, at least in the short term, \nthat assistive or collaborative AI will be the primary \napplications in behavioral healthcare. \nEvaluating Clinical LLMs\nA principled method for evaluating and reporting \non generative AI applications in behavioral health \nThese potential applications \nof clinical LLMs may help move \nthe behavioral healthcare \nfield toward personalized \ntreatment approaches.\n6\nPolicy Brief \nToward Responsible Development and \nEvaluation of LLMs in Psychotherapy\nengaging (neither too much nor too little), with \nengagement levels determined by patients’ \nindividual needs.\n• Effectiveness: Application integrates clinical \nscience principles and is clinically effective, \ni.e., decreases symptoms and functional \nimpairment, and increases well-being and \nquality of life.\n• Implementation: Application integrates well \ninto clinical practice, existing technologies, and \nworkflows, is cost-effective.\nFor example, an AI-based mental health chatbot \nfor treating depression might meet several READI \ncriteria, such as safety (e.g., monitoring systems \ndetect suicidality, self-harm, abuse, and violence as it \nrelates to the human user) and privacy/confidentiality\n(e.g., HIPAA-level data safeguards ensure that usage \nof the application is not contingent upon allowing \nthird-party access to health information), but falls \nshort of other criteria, including effectiveness\n(e.g., there is no evidence that the application’s \nIt remains to be seen whether \nfully autonomous clinical LLMs \nwill ever be deemed safe \nenough for deployment and \nwhether complete automation \nis even desired.\ncontexts is needed to ensure the responsible \ndeployment of these systems. Several frameworks \nalready exist that could be employed to evaluate \nLLM-based applications, ranging from medical and \npsychology ethics, implementation science, digital \nmental health, and health equity frameworks to more \ngeneral AI governance frameworks. However, none is \nsufficient for evaluating the specific risks of AI mental \nhealth applications. They either focus too narrowly on \nparticular medical domains without addressing the \nunique considerations of LLMs or focus too broadly \non AI applications without addressing healthcare\u0002specific concerns.\nTo fill this gap, we introduce the READI (readiness for \nAI deployment and implementation) framework for \nevaluating AI-mental health applications. Based on the \nfoundational principles of transparency and consumer \nautonomy, our framework outlines six criteria to help \nindividuals and organizations make informed decisions \nabout the appropriateness and potential for successful \nimplementation of specific AI applications:\n• Safety: Application prevents dangerous human \nbehaviors and is “healthy” itself, i.e., does not \nexhibit inflammatory or extreme traits.\n• Privacy/Confidentiality: Application keeps \npatient information private and confidential, i.e., \ndoes not disclose health information without \npatient authorization and allows individuals to \naccess their health information.\n• Equity: Application is unbiased in its \ncommunication, engagement, and \neffectiveness; is equally usable across \nall demographic groups; and is culturally \nresponsive.\n• Engagement: Application is appropriately \n7\nPolicy Brief \nToward Responsible Development and \nEvaluation of LLMs in Psychotherapy\nEvidence-based treatments and techniques have \nalready been identified for specific mental disorders \n(e.g., major depressive disorder, PTSD), stressors (e.g., \nbereavement, job loss, divorce), and populations (e.g., \nLGBTQ+ individuals, older adults). Clinical LLMs that \ndon’t focus on evidence-based techniques may fail to \nreflect current knowledge and even cause harm.\nRigorous evaluation and transparent reporting is \ncrucial to ensuring that consumers and healthcare \norganizations can maintain autonomy and make \ninformed decisions about the use of AI technologies. \nWithout a standardized set of evaluation criteria, \ncompanies may optimize for business objectives \nwithout fully considering clinical effectiveness or \npatient rights. For example, engagement alone is not an \nappropriate measure for using an LLM in psychotherapy \nbecause it does not necessarily entail clinically \nmeaningful change. Instead, the primary goals for \ntraining a clinical LLM should be clinical effectiveness \nand safety. Meanwhile, researchers or healthcare \norganizations may overlook important considerations \naround usability, engagement, effectiveness, and \napplicability for different populations. \nDevelopers and health \npractitioners must steer \naway from “black box”-type \nLLM-identified interventions.\nintervention is better than no treatment) and \nengagement (e.g., high daily usage rates suggest the \napplication may be too engaging). \nWe recommend using the READI criteria to evaluate \nnew LLMs or other generative AI technologies before\nlarge-scale clinical deployment—and on an ongoing \nbasis after deployment since the technology and the \ncontexts into which these tools are deployed can \nchange rapidly. \nPolicy Recommendations\nWhile LLMs hold considerable potential for helping \nimprove the quality, accessibility, consistency, and \nscalability of therapeutic interventions and clinical \nresearch, the integration of LLMs into psychotherapy \nwarrants caution. Developers, behavioral health \npractitioners, and policymakers must understand the \nvast implications of integrating clinical LLMs into \npsychotherapy—and the need to do so responsibly to \navoid serious harm.\nExplainability and transparency are key. To ensure \nthe clinical community can appropriately integrate \nand vet LLM-based advances, developers and health \npractitioners must steer away from “black box”-type \nLLM-identified interventions. Developers could design \nLLM systems such that they generate inspectable \nrepresentations of the LLMs’ decisions that clinicians \ncan examine and choose to implement.\nPolicymakers, developers, and clinicians should \nalso work to ensure that clinical LLMs are based on \nthe best available evidence for specific conditions. \n8\nPolicy Brief \nToward Responsible Development and \nEvaluation of LLMs in Psychotherapy\nWidespread adoption of a framework such as READI, \nwhich can be applied across academic and private \ndomains, is therefore crucial. Application developers \nshould work together with researchers and end-users \n(e.g., healthcare organizations) to collect and provide \ninformation relating to the criteria in plain language. In \nparticular, evaluation metrics should include escalation \nfor suicidality, non-suicidal self-harm, and risk of harm \nto others, as well as comparing LLM effectiveness to \nstandard treatments. Developers and clinicians should \nalso commit to the systematic collection of data on \nadverse events, including when the LLM behaves \nunexpectedly or fails to detect high-risk situations.\nInterdisciplinary collaboration between clinical \nscientists, engineers, and technologists will also \nbe crucial in the development of clinical LLMs. For \nexample, as behavioral health experts design LLM \nsystems, they will benefit from bringing together \ntechnologists, scientists, industry partners, and \npolicymakers to ensure that new LLM technologies \nhelp patients and that both therapists and patients \ntrust them.\nAs LLMs advance quickly and move toward the \nclinical domain, it is vital for policymakers to foster a \nthoughtful, risk-based approach to integrating these \ntechnologies into psychotherapy. Only through careful, \nresponsible design and rigorous risk monitoring can \npolicymakers, behavioral health practitioners, and \ntechnologists harness the promise of clinical LLMs \nwhile avoiding harm to patients.\n9\nElizabeth C. Stade is a \npostdoctoral researcher \nat the Stanford University \nInstitute for Human\u0002Centered Artificial \nIntelligence (HAI).\nH. Andrew Schwartz is \nan associate professor \nof computer science at \nStony Brook University.\nRobb Willer is a \nprofessor of sociology \nand, by courtesy, \npsychology and business \nat Stanford University.\nShannon Wiltsey \nStirman is a professor \nin the department \nof psychiatry and \nbehavioral sciences at \nStanford University.\nDavid B. Yaden is an \nassistant professor at the \nJohns Hopkins University \nSchool of Medicine.\nJane P. Kim is a clinical \nassociate professor \nof psychiatry and \nbehavioral sciences at \nStanford University.\nLyle Ungar is a \nprofessor of computer \nand information science \nat the University of \nPennsylvania.\nJoão Sedoc is an \nassistant professor of \ntechnology, operations, \nand statistics at New \nYork University.\nJohannes C. Eichstaedt\nis an assistant professor \nof psychology and the \nRam and Vijay Shriram \nHAI Faculty Fellow at \nStanford University.\nCody L. Boland works \nat the National Center \nfor PTSD in the VA \nPalo Alto Health \nCare System.\nRobert J. DeRubeis is a \nprofessor of psychology \nand director of clinical \ntraining at the University \nof Pennsylvania.\nReference: The first original article is \naccessible at Elizabeth Stade et al., \n“Large Language Models Could Change \nthe Future of Behavioral Healthcare: \nA Proposal for Responsible Development \nand Evaluation,” npj Mental Health \nResearch, 3 (April 2024): 12, https://www.\nnature.com/articles/s44184-024-00056-z.\nThe second original article is accessible \nat Elizabeth Stade et al., “Readiness for \nAI Deployment and Implementation \n(READI): A Proposed Framework for \nthe Evaluation of AI-Mental Health \nApplications,” PsyArXiv, March 29, 2024, \nhttps://osf.io/preprints/psyarxiv/8zqhw.\nStanford University’s Institute for Human\u0002Centered Artificial Intelligence (HAI)\napplies rigorous analysis and research \nto pressing policy questions on artificial \nintelligence. A pillar of HAI is to inform \npolicymakers, industry leaders, and civil \nsociety by disseminating scholarship to \na wide audience. HAI is a nonpartisan \nresearch institute, representing a range of \nvoices. The views expressed in this policy \nbrief reflect the views of the authors. \nFor further information, please contact \nHAI-Policy@stanford.edu. \nStanford HAI: 353 Jane Stanford Way, Stanford CA 94305-5008 \nT 650.725.4537 F 650.123.4567 E HAI-Policy@stanford.edu hai.stanford.edu",
    "length": 22007,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Strategies for Limiting AI Use",
    "url": "https://ctl.stanford.edu/strategies-limiting-ai-use",
    "text": "Strategies for Limiting AI Use | Center for Teaching and Learning\n[Skip to main content] [Skip to secondary navigation] \n[Stanford University(link is external)] \n[\nStanford\nCenter forTeaching and Learning\n] \nSearch this siteSubmit Search\n![AI-Limited] \n# Strategies for Limiting AI Use\nMain content start\nWhile the[AIMES Library of Examples] allows you to filter and explore real course policies, assignments, and other teaching artifacts that limit AI use, this page provides a brief overview of strategies. AI use in higher education is a rapidly evolving domain with new research emerging frequently. We do not yet have an extensive body of evidence to draw from, but the strategies included here are consistent with broader educational research findings, as well as having been found useful by instructors at Stanford. Please consider them “emerging practices” rather than “best practices.”\n## General Approaches to Limiting AI Use\n### **Increase transparency and motivation:**\nShare with students why they should follow your guidance about when it is and is not appropriate to use AI in your course or assignment. What skills do you want them to develop? How will those skills serve them in their studies, life, and work? What kinds of activities could AI productively enhance in this particular academic context?\nRevisit these reasons throughout the course–for example, in the context of specific course learning goals, class activities, assignments, and assessments. Check in with students about how they are experiencing the different cases in their learning.\nProvide a table or list that clearly breaks down different circumstances where AI may and may not be used.\nGuide students on how to disclose and cite AI use. Some instructors require a disclosure with every assignment; it can also include metacognition about how and why the students approached their work on the assignment and what kinds of tools were most and least helpful.\nNormalize cognitive effort and struggle, which are crucial for learning, as well as places where AI may assist students in deep learning, and how to tell the difference between uses of AI that support their learning and uses of AI that shortchange their learning.\nEmphasize students’ progress on both AI assisted and non-AI assisted thinking and skills through individual feedback and discussion with the whole class.\nConsider ways in which limited use of AI may support students with learning differences.\n### Emphasize alignment and process:\nEnsure that course policies are clear and aligned with the goals and reasons you have for students to limit AI to particular use cases or assignments.\nRequire that students show their processes of doing the work of the course (e.g., problem solving process, writing process, analysis process, coding process, reasoning process) as components of assignments. If some use of AI is allowed, that can be included as part of students’ documentation of their processes.\nFor major assignments such as projects, long papers, and capstones, include several stages that provide scaffolding and feedback on the way to the completed product. Feedback and discussion along the way can increase students’ engagement and accountability for the development of their own ideas, ensure accountability for their work and appropriate disclosure and citation of sources and AI use, and lead to strong final products. Logs or journals that accompany major projects and are reviewed with the instructor or TA periodically can also help.\n### Model ways of working safely and ethically with AI, and without AI:\nGuide students to go to the Stanford AI Playground for instances when they do use AI in the course or assignment. Discuss the risks of sharing sensitive data or information. Open up a conversation about ethical use of AI in ways that tap into disciplinary perspectives represented in the course.\nBring some of the important intellectual and academic work into class, where you can provide guidance and modeling of productive strategies with and without AI, and the difference between them.\nBecome familiar with where and how students may encounter AI assistance built into software, searches, and other platforms they use for coursework. Demonstrate and discuss the drawbacks of relying on AI assistance that is limited in your class.\nDiscuss and model examples of prompting and fact-checking AI results in ways that are specific and appropriate to your discipline, course, and the AI use cases that you allow.\n### Design assignments and assessments that demonstrate learning:\nFollow up on assignments completed outside of class, e.g., by asking students to explain their reasoning, discuss an example from their work, explain a technique they used, and articulate how and why they used AI.\nConsider whether conducting some assessments in class would support your learning goals (e.g., blue books or other in-class exam formats). As of fall 2025, proctored exams are only allowed as part of the[Academic Integrity Working Group Proctoring Pilot], but additional courses may request to join the pilot each quarter.\nHelp students prepare for in-class and/or proctored assessments. Remind them of the conditions they will encounter and encourage them to practice under similar conditions before the exam. Scaffold learning experiences leading up to timed, in-class writing so that a higher-stakes assessment is not the first time students experience the format.\n## Further Reading\nThe following publications highlight recent discussion about limiting AI use in higher education courses. This list is not a comprehensive bibliography.\n* Kinsey, V. (2025, March 13). Teaching with AI: Resources, Fears, Reflections, Invitation. In the Moment, Stanford Teaching Writing.[teachingwriting.stanford.edu/news/teaching-ai-resources-fears-reflections-invitation] \n* Perkins, Furze, Roe &amp; MacVaugh (2024). The Al Assessment Scale.[aiassessmentscale.com/] (includes several peer-reviewed publications using the scale)\n* McGee, N. J., Kozleski, E., Lemons, C. J., &amp; Hau, I. C. (2025). AI+Learning Differences: Designing a Future with No Boundaries. White Paper, Stanford Accelerator for Learning.[https://acceleratelearning.stanford.edu/app/uploads/2025/07/AI-Learning-Differences-Designing-a-Future-with-No-Boundaries\\_Final.pdf] ## Last updated\nDate\nBack to Top",
    "length": 6331,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Generative AI and Its Educational Implications",
    "url": "https://scale.stanford.edu/ai/repository/generative-ai-and-its-educational-implications",
    "text": "Generative AI and Its Educational Implications | SCALE Initiative\n[Skip to main content] \n[![Stanford University]] \n## Search and Filter\nWhat is the application?\nTeaching –Instructional Materials[**] \nTeaching –Assessment and Feedback[**] \nTeaching –Professional Learning[**] \nLearning –Student Support[**] \nCommunicating / Social Tools[**] \nOrganizing[**] \nAnalyzing[**] \nWho is the user?\nStudent[**] \nParent/Caregiver[**] \nEducator[**] \nSchool Leader[**] \nOthers[**] \nWhich age?\n0-3 years\nElementary (PK5)\nMiddle School (6-8)\nHigh School (9-12)\nPost-Secondary\nAdult\nWhy use AI?\nEfficiency[**] \nOutcomes –Literacy[**] \nOutcomes –Numeracy[**] \nOutcomes –Other Academic[**] \nOutcomes –Differentiation[**] \nOutcomes –Social Emotional[**] \nOutcomes –Durable Skills[**] \nReimagined Schooling[**] \nOther[**] \nStudy design\nDescriptive –Implementation and Use[**] \nDescriptive –Product Development[**] \nImpact –Randomized Controlled Trial[**] \nImpact –Quasi-experimental[**] \nSystematic Review[**] \nTechnical –Computational\nQuantitative –Others\n## Submit a research study\nContribute to the repository:\n[Add a paper] \n# Generative AI and Its Educational Implications\nAuthors\nKacper Lodzikowski,\nPeter W. Foltz,\nJohn T. Behrens\nDate\n01/2024\nPublisher\narXiv\nLink\n[https://arxiv.org/pdf/2401.08659] \nWe discuss the implications of generative AI on education across four critical sections: the historical development of AI in education, its contemporary applications in learning, societal repercussions, and strategic recommendations for researchers. We propose ways in which generative AI can transform the educational landscape, primarily via its ability to conduct assessment of complex cognitive performances and create personalized content. We also address the challenges of effective educational tool deployment, data bias, design transparency, and accurate output verification. Acknowledging the societal impact, we emphasize the need for updating curricula, redefining communicative trust, and adjusting to transformed social norms. We end by outlining the ways in which educational stakeholders can actively engage with generative AI, develop fluency with its capacities and limitations, and apply these insights to steer educational practices in a rapidly advancing digital landscape.\nWhat is the application?\n[Teaching –Instructional Materials],\n[Teaching –Assessment and Feedback],\n[Learning –Student Support],\n[Communicating / Social Tools] \nWho is the user?\n[Student],\n[Educator] \nWho age?\n[Elementary (PK5)],\n[Middle School (6-8)],\n[High School (9-12)],\n[Post-Secondary],\n[Adult] \nWhy use AI?\n[Efficiency],\n[Outcomes –Literacy],\n[Outcomes –Numeracy],\n[Outcomes –Other Academic],\n[Outcomes –Differentiation],\n[Outcomes –Durable Skills],\n[Reimagined Schooling] \nStudy design\n[Descriptive –Implementation and Use],\n[Descriptive –Product Development] \n[![Stanford University]] \n* [Stanford Home] \n* [Maps &amp; Directions] \n* [Search Stanford] \n* [Emergency Info] \n* [Privacy] \n* [Copyright] \n* [Trademarks] \n* [Non-Discrimination] \n* [Accessibility] \n©Stanford University. Stanford, California 94305.",
    "length": 3100,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Addressing AI-Generated Child Sexual Abuse Material: Opportunities for Educational Policy | Stanford HAI",
    "url": "https://hai.stanford.edu/policy/addressing-ai-generated-child-sexual-abuse-material-opportunities-for-educational-policy",
    "text": "Addressing AI-Generated Child Sexual Abuse Material: Opportunities for Educational Policy | Stanford HAI\n### Stay Up To Date\nGet the latest news, advances in research, policy work, and education program updates from HAI in your inbox weekly.\n[\n**Sign Up**For Latest News\n] \n[] \n* [] \n* [] \n* [] \n* [] \n* [] \n* [] \n###### Navigate\n* [\nAbout\n] \n* [\nEvents\n] \n* [\nCareers\n] \n* [\nSearch\n] \n###### Participate\n* [\nGet Involved\n] \n* [\nSupport HAI\n] \n* [\nContact Us\n] \n[Skip to content] \n[] \n[] \n* [] \n* [] \n* [] \n* [] \n* [] \n* [] \n[] \n[\npolicyPolicy Brief\n] # Addressing AI-Generated Child Sexual Abuse Material: Opportunities for Educational Policy\nDate\nJuly 21, 2025\nTopics\n[\nPrivacy, Safety, Security\n] [\nEducation, Skills\n] \n![] \n[**Read Paper**] \nabstract\nThis brief explores student misuse of AI-powered “nudify” apps to create child sexual abuse material and highlights gaps in school response and policy.\n### Key Takeaways\n* Most schools are not talking to students about the risks of AI-generated child sexual abuse material (CSAM), specifically via “nudify” apps; nor are they training educators how to respond to incidents of students making and circulating so-called “deepfake nudes” of other students.\n* While many states have recently criminalized AI CSAM, most fail to address how schools should establish appropriate frameworks for handling child offenders who create or share deepfake nudes.\n* To ensure schools respond proactively and appropriately, states should update mandated reporting and school discipline policies to clarify whether educators must report deepfake nude incidents, and consider explicitly defining such behavior as cyberbullying.\n* Criminalization is not a one-size-fits-all solution for minors; state responses to student-on-student AI CSAM incidents should prioritize behavioral interventions over punitive measures, grounded in child development, trauma-informed practices, and educational equity.\n### Executive Summary\nStarting in 2023, researchers found that generative AI models were being misused to create sexually explicit images of children. AI-generated child sexual abuse material (CSAM) has become easier to create thanks to the proliferation of generative AI software programs that are commonly called “nudify,” “undress,” or “face-swapping” apps, which are purpose-built to let unskilled users make pornographic images. Some of those users are children themselves.\nIn our paper, “[AI-Generated Child Sexual Abuse Material: Insights from Educators, Platforms, Law Enforcement, Legislators, and Victims],” we assess how several stakeholder groups are thinking about and responding to AI CSAM. Through 52 interviews conducted between mid-2024 and early 2025 and a review of documents from four public school districts, we find that the prevalence of AI CSAM in schools remains unclear but appears to be not overwhelmingly high at present. Schools therefore have a chance to proactively prepare their AI CSAM prevention and response strategies.\nThe AI CSAM phenomenon is testing the existing legal regimes that govern various affected sectors of society, illuminating some gaps and ambiguities. While legislators in Congress and around the United States have taken action in recent years to address some aspects of the AI CSAM problem, opportunities for further regulation or clarification remain. In particular, there is a need for policymakers at the state level to decide what to do about children who create and disseminate AI CSAM of other children, and, relatedly, to elucidate schools’ obligations with respect to such incidents.\n### The AI CSAM Problem\nAI image generation models are abused to create CSAM in several ways. Some AI-generated imagery depicts children who do not exist in real life, though the AI models used to create such material are commonly trained on actual abuse imagery. Another type of AI-generated CSAM involves real, identifiable children, such as known abuse victims from existing CSAM series, famous children (e.g., actors or influencers), or a child known to the person who generated the image. AI tools are used to modify an innocuous image of the child to appear as though the child is engaged in sexually explicit conduct. This type of CSAM is commonly referred to as “morphed” imagery.\nThe difficulty of making AI CSAM varies. Many mainstream generative AI platforms have committed to combating the abuse of their services for CSAM purposes. Creating bespoke AI-generated imagery depicting a specific child sex abuse scenario thus still entails some amount of technical know-how, such as prompt engineering or fine-tuning open-source models. By contrast, nudify apps, which are trained on datasets of pornographic imagery, take an uploaded photo of a clothed person (either snapped by the perpetrator, or sourced from a social media account, school website, etc.) and quickly return a realistic-looking but fake nude image.\nNudify apps enable those with no particular skills in AI or computer graphics to create so-called “deepfake nudes” or “deepfake porn”—rapidly and potentially of numerous individuals at scale, and typically without the depicted person’s consent. What’s more, nudify apps do not consistently prohibit the upload of images of underage individuals either in their terms of service or in practice. Their ease of use and lack of restrictions have made them an avenue for the expeditious creation of AI CSAM—including by users who are themselves underage.\nBeginning in mid-2023, male students at several U.S. middle and high schools (both public and private) reportedly used AI to make deepfake nudes of their female classmates. A high-profile case in New Jersey was followed by widely reported incidents in Texas, Washington, Florida, Pennsylvania, and multiple incidents in Southern California. More recently, media have reported on additional incidents in Pennsylvania, Florida, and Iowa. There have also been several reported occurrences internationally since 2023.\nIt is not clear what these cases imply about the prevalence of student-on-student incidents involving deepfake nudes. That they are so spread out geographically could be read to indicate a widespread problem in schools. On the other hand, the number of reported incidents nationwide remains minuscule for a country with over 54 million schoolchildren.\n[**Read Paper**] \nShare\n[] [] [] \nLink copied to clipboard!\nAuthors\n* [\n![Riana Pfefferkorn] \n###### Riana Pfefferkorn\n] \n### Related Publications\n##### [Jen King&#x27;s Testimony Before the U.S. House Committee on Energy and Commerce Oversight and Investigations Subcommittee] \n[Jennifer King] \nQuick ReadNov 18, 2025\nTestimony\n![] \nIn this testimony presented to the U.S. House Committee on Energy and Commerce’s Subcommittee on Oversights and Investigations hearing titled “Innovation with Integrity: Examining the Risks and Benefits of AI Chatbots,” Jen King shares insights on data privacy concerns connected with the use of chatbots. She highlights opportunities for congressional action to protect chatbot users from related harms.\nTestimony\n![] \n#### [Jen King&#x27;s Testimony Before the U.S. House Committee on Energy and Commerce Oversight and Investigations Subcommittee] \n[Jennifer King] \n[Privacy, Safety, Security] Quick ReadNov 18\nIn this testimony presented to the U.S. House Committee on Energy and Commerce’s Subcommittee on Oversights and Investigations hearing titled “Innovation with Integrity: Examining the Risks and Benefits of AI Chatbots,” Jen King shares insights on data privacy concerns connected with the use of chatbots. She highlights opportunities for congressional action to protect chatbot users from related harms.\n##### [Validating Claims About AI: A Policymaker’s Guide] \n[Olawale Salaudeen,] [Anka Reuel,] [Angelina Wang,] [Sanmi Koyejo] \nQuick ReadSep 24, 2025\nPolicy Brief\n![] \nThis brief proposes a practical validation framework to help policymakers separate legitimate claims about AI systems from unsupported claims.\nPolicy Brief\n![] \n#### [Validating Claims About AI: A Policymaker’s Guide] \n[Olawale Salaudeen,] [Anka Reuel,] [Angelina Wang,] [Sanmi Koyejo] \n[Foundation Models] [Privacy, Safety, Security] Quick ReadSep 24\nThis brief proposes a practical validation framework to help policymakers separate legitimate claims about AI systems from unsupported claims.\n##### [Response to the Department of Education’s Request for Information on AI in Education] \n[Victor R. Lee,] [Vanessa Parli,] [Isabelle Hau,] [Patrick Hynes,] [Daniel Zhang] \nQuick ReadAug 20, 2025\nResponse to Request\n![] \nStanford scholars respond to a federal RFI on advancing AI in education, urging policymakers to anchor their approach in proven research.\nResponse to Request\n![] \n#### [Response to the Department of Education’s Request for Information on AI in Education] \n[Victor R. Lee,] [Vanessa Parli,] [Isabelle Hau,] [Patrick Hynes,] [Daniel Zhang] \n[Education, Skills] [Regulation, Policy, Governance] Quick ReadAug 20\nStanford scholars respond to a federal RFI on advancing AI in education, urging policymakers to anchor their approach in proven research.\n##### [Adverse Event Reporting for AI: Developing the Information Infrastructure Government Needs to Learn and Act] \n[Lindsey A. Gailmard,] [Drew Spence,] [Daniel E. Ho] \nQuick ReadJun 30, 2025\nIssue Brief\n![] \nThis brief assesses the benefits of and provides policy recommendations for adverse event reporting systems for AI that report failures and harms post deployment.\nIssue Brief\n![] \n#### [Adverse Event Reporting for AI: Developing the Information Infrastructure Government Needs to Learn and Act] \n[Lindsey A. Gailmard,] [Drew Spence,] [Daniel E. Ho] \n[Regulation, Policy, Governance] [Privacy, Safety, Security] Quick ReadJun 30\nThis brief assesses the benefits of and provides policy recommendations for adverse event reporting systems for AI that report failures and harms post deployment.",
    "length": 9912,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "",
    "url": "https://setr.stanford.edu/sites/default/files/2025-02/SETR2025_01-AI_web-240128.pdf",
    "text": "Copyright © 2025 by the Board of Trustees of the Leland Stanford Junior University. All rights reserved.\nCopyright © 2025 by the Board of Trustees of the Leland Stanford Junior University. All rights reserved.\n21\nOverview\nArtificial intelligence (AI), a term coined by computer \nscientist and Stanford professor John McCarthy in \n1955, was originally defined as “the science and \nengineering of making intelligent machines.” In \nturn, intelligence might be defined as the ability to \nlearn and perform suitable techniques to solve prob\u0002lems and achieve goals, appropriate to the context \nin an uncertain, ever-varying world.1\n AI could be said \nto refer to a computer’s ability to display this type of \nintelligence.\nThe emphasis today in AI is on machines that can \nlearn as well as humans can learn, or at least some\u0002what comparably so. However, because machines \nare not limited by the constraints of human biology, \nAI systems may be able to run at much higher speeds \nand digest larger volumes and types of information \nthan are possible with human capabilities.\nKEY TAKEAWAYS\n° Artificial intelligence (AI) is a foundational tech\u0002nology that is supercharging other scientific fields \nand, like electricity and the internet, has the \npotential to transform societies, economies, and \npolitics worldwide.\n° Despite rapid progress in the past several years, \neven the most advanced AI still has many failure \nmodes that are unpredictable, not widely appre\u0002ciated, not easily fixed, not explainable, and \ncapable of leading to unintended consequences.\n° Mandatory governance regimes for AI, even \nthose to stave off catastrophic risks, will face stiff \nopposition from AI researchers and companies, \nbut voluntary regimes calling for self-governance \nare more likely to gain support. \nARTIFICIAL INTELLIGENCE\n01\nCopyright © 2025 by the Board of Trustees of the Leland Stanford Junior University. All rights reserved.\n22 STANFORD EMERGING TECHNOLOGY REVIEW\nproductivity growth by 1.5 percent over a ten-year \nperiod if it is adopted widely.10 Private funding for \ngenerative AI start-ups surged to $25.2 billion in \n2023, a nearly ninefold increase from 2022, and \naccounted for around a quarter of all private invest\u0002ments related to AI in 2023.11\nThe question of what subfields are considered part \nof AI is a matter of ongoing debate, and the bound\u0002aries between these fields are often fluid. Some of \nthe core subfields are the following:\n° Computer vision, enabling machines to recog\u0002nize and understand visual information from the \nworld, convert it into digital data, and make deci\u0002sions based on these data\n° Machine learning (ML), enabling computers \nto perform tasks without explicit instructions, \noften by generalizing from patterns in data. This \nincludes deep learning that relies on multilayered \nartificial neural networks—which process infor\u0002mation in a way inspired by the human brain—to \nmodel and understand complex relationships \nwithin data. \n° Natural language processing, equipping machines \nwith capabilities to understand, interpret, and \nproduce spoken words and written texts\nMost of today’s AI is based on ML, though it draws \non other subfields as well. ML requires data and \ncomputing power—often called compute12—and \nmuch of today’s AI research requires access to these \non an enormous scale.\nIn October 2024, the Royal Swedish Academy of \nSciences awarded the Nobel Prize in Physics for \n2024 to John Hopfield and Geoffrey Hinton for \ntheir work in applying tools and concepts from sta\u0002tistical mechanics to develop “foundational discov\u0002eries and inventions that enable machine learning \nwith artificial neural networks”13 (further discussed \nbelow). Underscoring the importance of AI-based \ntechniques in advancing science, it also awarded the \nToday, AI promises to be a fundamental enabler of \ntechnological advancement in many fields, arguably \nof comparable importance to electricity in an earlier \nera or the internet in more recent years. The science \nof computing, worldwide availability of networks, \nand civilization-scale data—all that collectively \nunderlies the AI of today and tomorrow—are poised \nto have similar impact on technological progress in \nthe future. Moreover, the users of AI will not be lim\u0002ited to those with specialized training; instead, the \naverage person on the street will increasingly inter\u0002act directly with sophisticated AI applications for a \nmultitude of everyday activities.\nThe global AI market was worth $196.63 billion in \n2023, with North America receiving 30.9 percent of \ntotal AI revenues.2\n The Stanford Institute for Human\u0002Centered Artificial Intelligence (HAI) AI Index 2024 \nAnnual Report found that private investment in all AI \nstart-ups totaled $95.99 billion in 2023, marking the \nsecond consecutive year of decline since a record \nhigh of over $120 billion in 2021.3\n Amid a 42 per\u0002cent fall in overall global venture funding across all \nsectors in 2023,4\n AI start-ups raised $42.5 billion in \nventure capital that year, marking only a 10 percent \ndecrease5\n from 2022.6\nMany tech companies are significantly ramping up \ninvestments in AI infrastructure, such as larger and \nmore powerful computing clusters to meet the grow\u0002ing demand for AI capabilities. Companies such as \nAmazon and Meta have begun revamping their data \ncenters,7\n and BlackRock, Microsoft, and the technol\u0002ogy investor MGX, which is backed by the United Arab \nEmirates, announced in September 2024 the new \nGlobal AI Infrastructure Investment Partnership fund, \nwhich seeks to raise $30 billion in private equity cap\u0002ital to finance data centers and other projects that \nspan the AI infrastructure ecosystem.8\n The fund may \nultimately invest up to $100 billion over time.9\nOne estimate forecasts that generative AI—which \ncan create novel text, images, and audio output and \nis discussed in more detail later in this chapter—\ncould raise global GDP by $7  trillion and raise \nCopyright © 2025 by the Board of Trustees of the Leland Stanford Junior University. All rights reserved.\n01 Artifi\nhardware components were likely also needed—\nsuggests the overall hardware costs for GPT-4 were \nat least a few hundred million dollars. And the chips \nunderlying this hardware are specialty chips often \nfabricated offshore.16 (Chapter 8 on semiconductors \ndiscusses this point at greater length.)\nLastly, AI models consume a lot of energy. Consider \nfirst the training phase: One estimate of the elec\u0002tricity required to train a foundation model such \nas GPT-4 pegs the figure at about fifty  million \nkilowatt-hours (kWh).17 The average American house\u0002hold uses about 11,000 kWh per year, meaning the \nenergy needed to train GPT-4 was approximately \nthe same as that used by 4,500 average homes \nin a year. Paying for this energy adds significant \ncost, even before a single person actually uses a \nmodel.\nThen, once a model is up and running, the cost of \nenergy used to power queries can add up fast. This \nis known as the inference phase. For ChatGPT, \nthe energy used per query is around 0.002 of a \nkilowatt-hour, or 2 watt-hours.18 (For comparison, \na single Google search requires about 0.3 watt\u0002hours,19 and an alkaline AAA battery contains about \n2 watt-hours of energy.) Given hundreds of millions \nof queries per day, the operating energy require\u0002ment of ChatGPT might be a few hundred thousand \nkilowatt-hours per day, at a cost of several tens of \nthousands of dollars. \nAI can automate a wide range of tasks. But it also \nhas particular promise in augmenting human capa\u0002bilities and further enabling people to do what they \nare best at doing.20 AI systems can work alongside \nhumans, complementing and assisting their work \nrather than replacing them. Some present-day \nexamples are discussed below.\nHealthcare\n° Medical diagnostics An AI system that can pre\u0002dict and detect the onset of strokes qualified for \nMedicare reimbursement in 2020.21\nNobel Prize in Chemistry for 2024 to Demis Hassabis \nand John M. Jumper for AI-based protein structure \nprediction,14 an important and long-standing prob\u0002lem in biology and chemistry involving the predic\u0002tion of the three-dimensional shape a protein would \nassume given only the DNA sequence associated \nwith it.\nMachine learning also requires large amounts of \ndata from which it can learn. These data can take \nvarious forms, including text, images, videos, sensor \nreadings, and more. Learning from these data is \ncalled training the AI model.\nThe quality and quantity of data play a crucial role \nin determining the performance and capabilities \nof AI systems. Without sufficient and high-quality \ndata, models may generate inaccurate or biased \noutcomes. (Roughly speaking, a traditional ML \nmodel is developed to solve a particular problem—\ndifferent problems call for different models; for prob\u0002lems sufficiently different from each other, entirely \nnew models need to be developed. Foundation \nmodels, discussed below, break this tradition to \nsome extent.) Research continues on how to train \nsystems incrementally, starting from existing models \nand using a much smaller amount of specially curated \ndata to refine those models’ performance for special\u0002ized purposes.\nFor a sense of scale, estimates of the data required \nto train GPT-4, OpenAI’s large language model \n(LLM) released in March 2023 and the base on which \nsome versions of ChatGPT were built, suggest that \nits training database consisted of the textual equiv\u0002alent of around 100 million books, or about 10 tril\u0002lion words, drawn from billions of web pages and \nscanned books. (LLMs are discussed further below.) \nThe hardware requirements for computing power \nare also substantial. The costs to compute the training \nof GPT-4, for example, were enormous. Reports indi\u0002cate that the training took about twenty-five thousand \nNvidia A100 GPU deep-learning chips—at a cost\nof $10,000 each—running for about one hundred \ndays.15 Doing the math—and noting that other \nCopyright © 2025 by the Board of Trustees of the Leland Stanford Junior University. All rights reserved.\n24 STANFORD EMERGING TECHNOLOGY REVIEW\n° Autonomous trucking Multiple companies col\u0002laborated in a consortium that arranged for trucks \ncarrying tires to drive autonomously for over \nfifty thousand long-haul trucking miles in the \nperiod from January to August 2024.28 If this and \nother demonstrations continue to be successful, \nit is possible that long-haul drives—the most\nboring and time-consuming aspect of a truck \ndriver’s job—can be automated; at the same time, \naspects of such jobs requiring human-centered \ninteractions, including navigating the first miles \nout of the factory and the last miles of delivering \ngoods to customers, could be retained.\nLaw\n° Legal transcription AI enables the real-time \ntranscription of legal proceedings and client \nmeetings with reasonably high accuracy, and \nsome of these services are free of charge.29\n° Legal review AI-based systems can reduce \nthe time lawyers spend on contract review by as \nmuch as 60  percent. Further, such systems can \nenable lawyers to search case databases more \nrapidly than online human searches—and even \nwrite case summaries.30\nKey Developments\nFoundation Models\nFoundation models dominated the conversation \nabout AI in both 2023 and 2024. These models \nare large-scale systems trained on vast amounts \nof diverse data that can handle a variety of tasks.31\nThey often contain billions or trillions of parame\u0002ters,32 and their massive size allows them to capture \nmore complex patterns and relationships. Trained \non these datasets, foundation models can develop \nbroad capabilities33 and are thus sometimes called \ngeneral-purpose models. They excel at transfer \n° Drug discovery An AI-enabled search iden\u0002tified a compound that inhibits the growth of a \nbacterium responsible for many drug-resistant \ninfections, such as pneumonia and meningitis, by \nsifting through a library of seven thousand poten\u0002tial drug compounds for an appropriate chemical \nstructure.22\n° Patient safety Smart AI sensors and cameras \ncan improve patient safety in intensive care units, \noperating rooms, and even at home by improv\u0002ing healthcare providers’ and caregivers’ ability \nto monitor and react to patient health develop\u0002ments, including falls and injuries.23\n° Robotic assistants Mobile robots using AI can \ncarry out healthcare-related tasks such as making \nspecialized deliveries, disinfecting hospital wards, \nand assisting physical therapists, thus supporting \nnurses and enabling them to spend more time \nhaving face-to-face human interactions.24\nAgriculture\n° Production optimization AI-enabled computer \nvision helps some salmon farmers pick out fish \nthat are the right size to keep, thus off-loading \nthe labor-intensive task of sorting them.25\n° Crop management Some farmers are using AI \nto detect and destroy weeds in a targeted manner, \nsignificantly decreasing environmental harm by \nusing herbicides only on undesired vegetation \nrather than entire fields, in some cases reducing \nherbicide use by as much as 90 percent.26\nLogistics and Transportation\n° Resource allocation AI enables some commer\u0002cial shipping companies to predict ship arrivals \nfive days into the future with high accuracy, thus \nallowing real-time allocations of personnel and \nschedule adjustments.27\nCopyright © 2025 by the Board of Trustees of the Leland Stanford Junior University. All rights reserved.\n01 Artifi\nmedical advice, and they outscore the median \nhuman performance on clinical examination in obstet\u0002rics and gynecology,34 on standardized tests of \ndivergent thinking,35 and on other standardized tests \nsuch as the LSAT, sections of the GRE, and various \nAP exams.36 However, models do not necessarily \nexcel at the actual tasks or skills that these tests are \ntrying to capture and, as discussed below, still pro\u0002duce errors and fail in all sorts of other ways, many \nof them unexpected.\nWell-known closed-source LLMs include OpenAI’s \nGPT models (e.g., GPT-3, GPT-3.5, and GPT-4), \nAnthropic’s Claude, and Google’s Gemini. Well\u0002known open-source LLMs include Meta’s Llama, Big\u0002Science’s BLOOM, EleutherAI’s GPT-J, and Google’s \nBERT and T5.\nSpecialized foundation models have also been \ndeveloped in other modalities such as audio, video, \nand images:\n° Foundation models for images are able to gen\u0002erate new images based on a user’s text input. \nNovel methods for handling images, combined \nwith using very large collections of pictures and \ntext for training, have led to models that can turn \nwritten descriptions into images that are quickly \nbecoming comparable to—and sometimes indis\u0002tinguishable from—real-life photographs and \nartwork created by humans. Examples include \nOpenAI’s DALL-E 3, the open-source Stable \nDiffusion, Google’s Imagen, Adobe Firefly, and \nMeta’s Make-A-Scene.\n° An example of a foundation model for audio \nis UniAudio, which handles all audio types and \nemploys predictive algorithms to generate \nhigh-quality speech, sound, and music, sur\u0002passing leading methods in tasks such as text \nto speech, speech enhancement, and voice \nconversion.\n° Foundation models in video such as Meta’s Emu \nVideo represent a significant advancement in \nlearning—applying knowledge learned in one con\u0002text to another—making them more flexible and effi\u0002cient than traditional task-specific models. A single \nfoundation model is often fine-tuned for various \ntasks, reducing the need to train separate models \nfrom scratch. \nThese models are generally classified as closed \nsource or open source. A closed-source model is \na proprietary one developed and maintained by \na specific organization, usually a for-profit com\u0002pany, with its source code, data, and architecture \nkept confidential. Access to these models is typi\u0002cally restricted through technically enforced usage \npermissions, such as application programming \ninterfaces, allowing the developers to control the \nmodel’s distribution, usage, and updates. By con\u0002trast, an open-source model is one whose code, \ndata, and underlying architecture are publicly acces\u0002sible, allowing anyone to use, modify, and distribute \nit freely. \nThe most familiar type of foundation model is an \nLLM—a system trained on very large volumes of \ntextual content. LLMs are an example of generative \nAI, a type of AI that can produce new material based \non how it has been trained and the inputs it is given. \nModels trained on text can generate new text based \non a statistical analysis that makes predictions about \nwhat other words are likely to be found immediately \nafter the occurrence of certain words. \nThese models do not think or feel like humans do, \neven though their responses may make it seem like \nthey do. Instead, LLMs use statistical analysis based \non training data. For example, because the word \nsequence “thank you” is far more likely to occur than \n“thank zebras,” a person’s query to an LLM asking it \nto draft a thank-you note to a colleague is unlikely to \ngenerate the response “thank zebras.”\nThese models generate linguistic output surpris\u0002ingly similar to that of humans across a wide range \nof subjects. For example, LLMs can generate useful \ncomputer code, poetry, legal case summaries, and \nCopyright © 2025 by the Board of Trustees of the Leland Stanford Junior University. All rights reserved.\n26 STANFORD EMERGING TECHNOLOGY REVIEW\nto various formats and learner types, improving \nengagement and comprehension. When integrated \nwith virtual and augmented reality, it can create \nimmersive, highly realistic training environments that \nare particularly valuable in fields like healthcare. The \nadvent of multimodal AI is also set to further trans\u0002form human-computer interactions, enabling more \nintuitive communication and expanding the range of \ntasks that AI systems can handle. \nEmbodied AI\nEmbodied AI involves integrating AI systems into \nrobots or other physical devices. This approach aims \nto bridge the gap between the digital and physical \nrealms. Embodied AI has the potential to enhance \nrobotic capabilities and expand the range of inter\u0002actions robots have with the physical world. These \nrobot-plus-AI systems could potentially address \nknowledge tasks, physical tasks, or combinations \nof both. (This topic is explored further in chapter 7\non robotics.) As research progresses in AI autonomy \nand reasoning, embodied AI systems may be able to \nhandle increasingly complex tasks with greater inde\u0002pendence. This could lead to applications in various \nfields such as logistics and domestic assistance.\nvideo generation. Emu first generates an image \nfrom text input and then creates a video based \non both the text and the generated image. Emu \nVideo has demonstrated superior performance \nover previous state-of-the-art methods in terms \nof image quality, faithfulness to text instructions, \nand evaluations from humans. \nMultimodal Models\nAI systems that incorporate multiple modalities—text,\nimages, and sound—within single models are \nbecoming increasingly popular. This multimodal \napproach, shown in figure 1.1, aims to create more \nhumanlike experiences by leveraging various senses \nsuch as sight, speech, and hearing to mirror how \nhumans interact with the world. \nMultimodal AI systems have diverse applications \nacross sectors. They can enhance accessibility for \npeople with disabilities through real-time transcrip\u0002tion, sign language translation, and detailed image \ndescriptions. They can also eliminate language \nbarriers via cost-effective, near-real-time transla\u0002tion services. In education, multimodal AI can sup\u0002port personalized learning by adapting content \nSingle-modal AI model\nInput Process Output\nText AI Text\nMultimodal AI model\nProcess\nInput Output\nText\nImage AI\nSound\nText\nImages\nSounds\nFIGURE 1.1  Multimodal AI systems can transform one type of input into a different type of output\nCopyright © 2025 by the Board of Trustees of the Leland Stanford Junior University. All rights reserved.\n01 Artifi\nA National AI Research Resource\nLLMs such as GPT-4, Claude, Gemini, and Llama \ncan be developed only by large companies with the \nresources to build and operate very large data and \ncompute centers. For a sense of scale, Princeton \nUniversity announced in March 2024 that it would dip \ninto its endowment to purchase 300 advanced Nvidia \nchips to use for research at a total estimated cost \nof about $9 million.40 By contrast, Meta announced \nat the start of 2024 that it intended to purchase \n350,000 such chips by the end of the year41—over \none thousand times as many chips as Princeton and \nwith a likely price tag of nearly $10 billion. \nTraditionally, academics and others in civil society \nhave undertaken research to understand the poten\u0002tial societal ramifications of AI, but with large com\u0002panies controlling access to these AI systems, they \ncan no longer do so independently. In July 2023, a \nbipartisan bill (S.2714, the CREATE AI Act of 2023)42\nwas proposed to establish the National Artificial \nIntelligence Research Resource (NAIRR) as a shared \nnational research infrastructure that would provide \ncivil society researchers greater access to the com\u0002plex resources, data, and tools needed to support \nresearch on safe and trustworthy AI. The bill’s text did \nnot mention funding levels, but the final NAIRR task \nforce report, released in January 2023, indicated that \nNAIRR should be funded at a level of $2.6 billion over \nits initial six-year span.43 In January 2024, the National \nScience Foundation established the NAIRR pilot to \nestablish proof of concept for the full-scale NAIRR. \nExistential Concerns About AI\nLLMs have generated considerable attention because \nof their apparent sophistication. Indeed, their capa\u0002bilities have led some to suggest that they are the \ninitial sparks of artificial general intelligence (AGI).37\nAGI is AI that is capable of performing any intellec\u0002tual task that a human can perform, including learn\u0002ing. But, according to this argument, because an \nelectronic AGI would run on electronic circuits rather \nthan biological ones, it is likely to learn much faster \nthan biological human intelligences—rapidly out\u0002stripping their capabilities.\nThe belief in some quarters that AGI will soon be \nachieved has led to substantial debate about its \nrisks. Scholars have continued to argue over the past \nyear about whether current models present initial \nsparks of AGI,38 although there hasn’t been substan\u0002tial evidence presented that proves they possess \nsuch capabilities.\nOthers suggest that focusing on low-probability \ndoomsday scenarios distracts from the real and imme\u0002diate risks AI poses today.39 Instead, society should \nbe prioritizing efforts to address the harms that AI \nsystems are already causing, like biased decision\u0002making, hallucinations (error-ridden responses that \nappear to provide accurate information), and job \ndisplacement. Those who support this view argue \nthat these problems are the ones on which govern\u0002ments and regulators should be concentrating their \nefforts.\nThe advent of multimodal AI is . . . set to further \ntransform human-computer interactions, enabling \nmore intuitive communication and expanding the \nrange of tasks that AI systems can handle.\nCopyright © 2025 by the Board of Trustees of the Leland Stanford Junior University. All rights reserved.\n28 STANFORD EMERGING TECHNOLOGY REVIEW\nalways relevant, but in certain cases, such as medical \ndecision-making, they may be critical so that users \ncan have confidence in an AI system’s output.\nBias and fairness Because ML models are trained \non existing datasets, they are likely to encode any \nbiases present in these datasets. (Bias should be \nunderstood here as a property of the data that is \ncommonly regarded as societally undesirable.) For \nexample, if a facial recognition system is primarily \ntrained on images of individuals from one ethnic \ngroup, its accuracy at identifying people from other \nethnic groups may be reduced.45 Use of such a system \ncould well lead to disproportionate singling out of \nindividuals in those other groups. To the extent that \nthese datasets reflect historical approaches, they will \nalso reflect the biases embedded in that history, and \nan ML model based on such datasets will also reflect \nthese biases. \nVulnerability to spoofing It is possible to tweak \ndata inputs to fool many AI models into drawing \nfalse conclusions. For example, in figure 1.2, chang\u0002ing a small number of pixels in a visual image of a \ntraffic stop sign can lead to its being classified as \nAs a point of comparison to the fledgling NAIRR \neffort, investments from high-tech companies for AI \nexceeded $27 billion in 2023 alone.44\nOver the Horizon\nImpact of New AI Technologies\nPotential positive impacts of new AI technologies \nare most likely to be seen in the applications they \nenable for societal use, as described in detail above. \nOn the other hand, no technology is an unalloyed \ngood. Potential negative impacts from AI will likely \nemerge from known problems with current state-of\u0002the-art AI and from technical advances in the future. \nSome of the known issues with today’s leading AI \nmodels include the following:\nExplainability This is the ability to explain the rea\u0002soning behind—and describe the data underlying—\nan AI system’s conclusions. Today’s AI is largely \nincapable of explaining the basis on which it arrives \nat any particular conclusion. Explanations are not \nChange a few\npixels\nNew AI classi\u001fcation:\n“yield”\nOriginal AI classi\u001fcation:\n”stop”\nSource: Derived from figure 1 in Fabio Carrara, Fabrizio Falchi, Giuseppe Amato, Rudy Becarelli, and Roberto \nCaldelli, “Detecting Adversarial Inputs by Looking in the Black Box,” in “Transparency in Algorithmic Decision \nMaking,” special issue, ERCIM News 116 (January 2019): 16–19.\nFIGURE 1.2  Changing a few pixels can fool AI into thinking a picture of a stop \nsign is a picture of a yield sign\nCopyright © 2025 by the Board of Trustees of the Leland Stanford Junior University. All rights reserved.\n01 Artifi\nelection by buying votes.47 In January 2024, voters in \nNew Hampshire received robocalls that used a voice \nsounding like President Biden’s telling them not to \nvote in the state’s presidential primary.48 In elections \nin India in early 2024, deepfake videos were used \nto depict deceased politicians as though they were \nstill alive (see figure 1.3).49 All of these deepfakes are \nmuch more sophisticated than attempts such as the \n“dumbfake” video of Representative Nancy Pelosi \n(D-CA) that involved merely slowing down an exist\u0002ing video of her to make her look drunk.50\nPrivacy Many LLMs are trained on data found \non the internet rather indiscriminately, and such \ndata may include personal information of individu\u0002als. When incorporated into LLMs, this information \ncould be publicly disclosed more often.\nOvertrust and overreliance If AI systems become \ncommonplace in society, their novelty will inevitably \ndiminish for users. The level of trust in computer out\u0002puts often increases with familiarity. But skepticism \nabout answers received from a system is essential if \none is to challenge the correctness of these outputs. \nAs trust in AI grows, reducing skepticism, there’s a \na yield sign, even though this fuzzing of the image \nis invisible to the naked eye. That example seems \ninnocuous, but as AI models are used increasingly \nin applications from medical treatment to intelli\u0002gence and military operations, the potential harms \ncould be substantial. It is also possible that an attack \ntargeting one AI model could work against other \nmodels performing the same task—a phenomenon \nknown as transferability. One study reports that as \noften as 80 percent of the time, transferability allows \nattackers to create an attack on a surrogate model \nand then apply it to their intended target, too.46\nData poisoning An attacker manipulating the \ndataset used to train an ML model can damage its \nperformance and even create predictable errors.\nDeepfakes AI provides the capability for gener\u0002ating highly realistic but entirely inauthentic audio \nand video imagery. This has obvious implications for \nevidence presented in courtrooms and for efforts to \nmanipulate political contests. In September 2023, just \nbefore elections took place in Slovakia, a deepfake \naudio was posted to Facebook in which a candidate \nwas heard discussing with a journalist how to rig the \nPhoto from the late Indian Congress leader\nH. Vasanthakumar‘s funeral in 2020\nScreenshot from a deepfake video of\nH. Vasanthakumar endorsing his son‘s\nparliamentary candidacy in 2024\nSource: (Left) PTI Photo / R. Senthil Kumar; (right) “H Vasantha Kumar,” posted April 16, 2024, by Vasanth TV, YouTube, https://\nwww.youtube.com/watch?v=98_K-Ag7p2M\nFIGURE 1.3  Deepfake videos of deceased Indian politicians speaking as if they were alive were used \nin India’s 2024 elections\nCopyright © 2025 by the Board of Trustees of the Leland Stanford Junior University. All rights reserved.\n30 STANFORD EMERGING TECHNOLOGY REVIEW\nand metadata in building and offering the prod\u0002ucts Stable Diffusion (an application that generates \nimages from text) and DreamStudio (the app that \nserves as a user interface to Stable Diffusion).52 In \nlate 2023, the New York Times sued OpenAI and \nMicrosoft over their alleged use of millions of arti\u0002cles published by the Times to train the companies’ \nLLMs.53 In June 2024, music labels Sony Music, \nUniversal Music Group, and Warner Records sued \nAI start-ups Suno and Udio for copyright infringe\u0002ment, alleging that the companies had trained their \nmusic-generation systems on protected content.54\nAI researchers are cognizant of issues such as these, \nand in many cases work has been done—or is being \ndone—to develop corrective measures. However, \nin most cases, these defenses don’t apply very well \nto instances beyond the specific problems that they \nwere designed to solve.\nChallenges of Innovation and \nImplementation \nThe primary challenge of bringing AI innovation into \noperation is risk management. It is often said that \nAI, and especially ML, brings a new conceptual par\u0002adigm for how systems can exploit information to \ngain advantage, relying on pattern recognition in the \nbroadest sense rather than on explicit understand\u0002ing of situations that are likely to occur. Because \nthere have been significant recent advances in AI, \nthe people who would make decisions to deploy \nAI-based systems may not have a good under\u0002standing of the risks that could accompany such \ndeployment.\nConsider, for example, AI as an important approach \nfor improving the effectiveness of military oper\u0002ations. Despite broad agreement by the military \nservices and the US Department of Defense (DOD) \nthat AI would be of great benefit, the actual inte\u0002gration of AI-enabled capabilities into military forces \nhas proceeded at a slow pace. Certainly, it is well \nunderstood that technical risks of underperformance \nand error in new technologies take time to mitigate. \nhigher risk that errors, mishaps, and unforeseen inci\u0002dents will be overlooked. One recent experiment \nshowed that developers with access to an AI-based \ncoding assistant wrote code that was significantly less \nsecure than those without an AI-based assistant—\neven though the former were more likely to believe \nthey had written secure code.51\nHallucinations As noted earlier, AI hallucina\u0002tions refer to situations where an AI model gen\u0002erates results or answers that are plausible but do \nnot correspond to reality. In other words, models \ncan simply make things up, but human users will \nnot be aware they have done this. The results are \nplausible because they are constructed based on \nstatistical patterns that the model has learned to \nrecognize from its training data. But they may not \ncorrespond to reality because the model does not \nhave an understanding of the real world. For exam\u0002ple, in September 2024, a Stanford professor asked \nan AI model to name ten publications she had writ\u0002ten. The AI responded with five correct publications \nand five that she had never actually written—but the \nAI results included titles and summaries that made \nthem seem real. When she told the model that “the \nlast two entries are hallucinations,” it simply pro\u0002vided two new results that were also hallucinations.\nOut-of-distribution inputs All ML systems must \nbe trained on a large volume of data. If the inputs \nsubsequently given to a system are substantially \ndifferent from the training data—a situation known \nas being out-of-distribution—the system may draw \nconclusions that are more unreliable than if the \ninputs were similar to the training data. \nCopyright violations Some AI-based models have \nbeen trained on large volumes of data found online. \nThese data have generally been used without the \nconsent or permission of their owners, thereby raising \nimportant questions about appropriately compen\u0002sating and acknowledging those owners. For exam\u0002ple, in January 2023, Getty Images sued Stability AI \nin an English court for infringing on the copyrights of \nmillions of photographs, their associated captions, \nCopyright © 2025 by the Board of Trustees of the Leland Stanford Junior University. All rights reserved.\n01 Artifi\nthe humans it replaces.58 In at least some cases, \ncompanies are deciding that the cost savings of \neliminating human workers outweigh the draw\u0002backs of mediocre AI performance.\n° Training displaced workers to be more compet\u0002itive in an AI-enabled economy does not solve \nthe problem if new jobs are not available. The \nnature and extent of new roles resulting from \nwidespread AI deployment are not clear at this \npoint, although historically the introduction of \nnew technologies has not resulted in a long-term \nnet loss of jobs.59\nGOVERNANCE AND REGULATION OF AI\nGovernments around the world have been increas\u0002ingly focused on establishing regulations and guide\u0002lines for AI. Research on foundational AI technologies \nis difficult to regulate across international bound\u0002aries even among like-minded nations, especially \nwhen other nations have strong incentives to carry \non regardless of actions taken by US policymakers. It \nis even more difficult, and may well be impossible, to \nreach agreement between nations that regard each \nother as strategic competitors and adversaries. The \nsame applies to voluntary restrictions on research \nby companies concerned about competition from \nless constrained foreign rivals. Regulation of specific \napplications of AI may be more easily implemented, \nin part because of existing regulatory frameworks in \ndomains such as healthcare, finance, and law. \nThe most ambitious attempt to regulate AI came \ninto force in August 2024 with the European Union’s \nAI Act. This forbids certain applications of AI, such \nas individual predictive policing based solely on a \nperson’s data profile or tracking of their emotional \nstate in the workplace and educational institutions, \nunless for medical or safety reasons.60 Additionally, \nit imposes a number of requirements on what the \nAI Act calls “high-risk” systems. (The legislation pro\u0002vides a very technical definition of such systems, but \ngenerally they include those that could pose a sig\u0002nificant risk to health, safety, or fundamental rights.) \nBut another important reason for the slow pace is \nthat the DOD acquisition system has largely been \ndesigned to minimize the likelihood of program\u0002matic failure, fraud, unfairness, waste, and abuse—in \nshort, to minimize risk. It is therefore not surprising \nthat the incentives at every level of the bureaucracy \nare aligned in that manner. For new approaches like \nAI to take root, a greater degree of programmatic \nrisk acceptance may be necessary, especially in light \nof the possibility that other nations could adopt the \ntechnology faster, achieving military advantages \nover US forces.\nPolicy, Legal, and Regulatory Issues\nTHE FUTURE OF WORK\nSome researchers expect that, within the next five \nto ten years, more and more workers will have AI \nadded to their workflows to enhance productivity \nor will even be replaced by AI systems, which may \ncause significant disruptions to the job market in the \nnear future.55 LLMs have already demonstrated how \nthey can be used in a wide variety of fields, includ\u0002ing law, customer support, coding, and journalism. \nThese demonstrations have led to concerns that \nthe impact of AI on employment will be substan\u0002tial, especially on jobs that involve knowledge work. \nHowever, uncertainty abounds. What and how many \npresent-day jobs will disappear? Which tasks could \nbest be handled by AI? And what new jobs might be \ncreated by the technology today and in the future?\nSome broad outlines and trends are clear: \n° Individuals whose jobs entail routine white-collar \nwork may be more affected than those whose \njobs require physical labor; some will experience \npainful shifts in the short term.56\n° AI is helping some workers to increase their pro\u0002ductivity and job satisfaction.57 At the same time, \nother workers are already losing their jobs as AI \ndemonstrates adequate competence for business \noperations, despite potentially underperforming \nCopyright © 2025 by the Board of Trustees of the Leland Stanford Junior University. All rights reserved.\n32 STANFORD EMERGING TECHNOLOGY REVIEW\nin September 2024. The act sought to hold the cre\u0002ators of advanced AI models liable in civil court for \ncausing catastrophic harms unless they had taken \ncertain advance measures to forestall such an out\u0002come. Opposition to the bill was based on con\u0002cerns about a technologically deficient definition of \nadvanced AI models, the burden that the bill would \nplace on small start-ups and academia, and the \nunfairness of holding model developers responsible \nfor harmful applications that others build using the \ndevelopers’ models.\nOther important developments regarding AI gov\u0002ernance include the AI Safety Summit, held on \nNovember 1–2, 2023, at Bletchley Park in the United \nKingdom,62 which issued the Bletchley Declaration, \nand the AI Seoul Summit of May 2024. In the Bletchley \nDeclaration, the European Union and twenty-eight \nnations collectively endorsed international coopera\u0002tion to manage risks associated with highly capable \ngeneral-purpose AI models. Signatories commit\u0002ted to ensuring that AI systems are developed and \ndeployed safely and responsibly. The summit also \nled to the establishment of the United Kingdom’s \nAI Safety Institute and the US Artificial Intelligence \nSafety Institute, located within the National Institute \nof Standards and Technology.\nThe Seoul Declaration from the AI Seoul Summit \n2024 built on the Bletchley Declaration to acknowl\u0002edge the importance of interoperability between \nnational AI governance frameworks to maximize \nbenefits and minimize risks from advanced AI sys\u0002tems. In addition, sixteen major AI organizations \nThese requirements address data quality, documen\u0002tation and traceability, transparency and explainabil\u0002ity, human oversight, accuracy, cybersecurity, and \nrobustness.\nIn the United States, the president’s Executive Order \non the Safe, Secure, and Trustworthy Development \nand Use of Artificial Intelligence was issued on \nOctober 30, 2023.61 The order addressed actions to \nadvance AI safety and security; privacy; equity and \ncivil rights; consumer, patient, student, and worker \ninterests; the promotion of innovation and com\u0002petition, as well as American leadership; and gov\u0002ernment use of AI. Of particular note is the order’s \nrequirement that developers of advanced AI systems \nposing a serious risk to national security, national eco\u0002nomic security, or national public health and safety \ninform the US government when training them and \nshare with it all results from internal safety testing \nconducted by red teams. (A red team is a team of \nexperts that attempts to subvert or break the system \nit is asked to test. It then reports its findings to the \nowner of the system so that the owner can take cor\u0002rective action.) The order also requires government \nactions to develop guidance to help protect against \nthe use of AI to develop biological threats and to \nadvance the use of AI to protect against cyberse\u0002curity threats, to help detect AI-generated content, \nand to authenticate official content.\nAt the state level in the United States, an attempt \nto pass an AI regulatory bill in California (SB 1047, \nthe Safe and Secure Innovation for Frontier Artificial \nIntelligence Models Act) was vetoed by the governor \nThe resources needed to train GPT-4 far exceed those \navailable through grants or any other sources to any \nreasonably sized group of the top US research universities.\nCopyright © 2025 by the Board of Trustees of the Leland Stanford Junior University. All rights reserved.\n01 Artifi\nof weapons or in making decisions about the use \nof deadly force. Notably, in late 2023, press reports \nindicated that President Biden and Chinese President \nXi Jinping had considered entering into a dialogue \nabout AI in nuclear command and control, but such \nan arrangement was never formalized.66\nTALENT\nThe United States is eating its seed corn with respect \nto the AI talent pool. As noted in the Foreword, fac\u0002ulty at Stanford and other universities report that the \nnumber of students studying in AI who are joining \nthe industry, particularly start-ups, is increasing at \nthe expense of those pursuing academic careers and \ncontributing to foundational AI research. \nMany factors are contributing to this trend. One \nis that industry careers come with compensation \npackages that far outstrip those offered by aca\u0002demia. Academic researchers must also obtain \nfunding to pay for research equipment, computing \ncapability, and personnel like staff scientists, tech\u0002nicians, and programmers. This involves searching \nfor government grants, which are typically small \ncompared to what large companies might be will\u0002ing to invest in their own researchers. Consider, for \nexample, that the resources needed to build and \ntrain GPT-4 far exceed those available through \ngrants or any other sources to any reasonably sized \ngroup of the top US research universities, let alone \nany single university.\nIndustry often makes decisions more rapidly than \ngovernment grant makers and imposes fewer reg\u0002ulations on the conduct of research. Large com\u0002panies are at an advantage because they have \nresearch-supporting infrastructure in place, such as \ncompute facilities and data warehouses.\nOne important consequence is that academic access \nto research infrastructure is limited, so US-based stu\u0002dents are unable to train on state-of-the-art systems—\nat least this is the case if their universities do not \nhave access to the facilities of the corporate sector. \nagreed on the Frontier AI Safety Commitments, a \nset of voluntary guidelines regarding the publication \nof safety frameworks for frontier AI models and the \nsetting of thresholds for intolerable risks, among \nother things. \nNATIONAL SECURITY\nAI is expected to have a profound impact on mil\u0002itaries worldwide.63 Weapons systems, command \nand control, logistics, acquisition, and training will \nall seek to leverage multiple AI technologies to \noperate more effectively and efficiently, at lower \ncost and with less risk to friendly forces. Trying \nto overcome decades of institutional inertia, the \nDOD is dedicating billions of dollars to institutional \nreforms and research advances aimed at integrat\u0002ing AI into its warfighting and war preparation \nstrategies. Senior military officials recognize that \nfailure to adapt to the emerging opportunities and \nchallenges presented by AI would pose significant \nnational security risks, particularly considering that \nboth Russia and China are heavily investing in AI \ncapabilities.\nIn adopting a set of guiding principles that address \nresponsibility, equity, traceability, reliability, and \ngovernability in and for AI,64 the DOD has taken \nan important first step in meeting its obligation to \nproceed ethically with the development of AI capa\u0002bilities; eventually, these principles will have to be \noperationalized in specific use cases. An additional \nimportant concern, subsumed under these principles \nbut worth calling out, is determining where the use \nof AI may or may not be appropriate—for example, \nwhether AI is appropriate in nuclear command and \ncontrol. The United States, the United Kingdom, and \nFrance have made explicit commitments to maintain \nhuman control over nuclear weapons.65\nMeanwhile, other countries are also adopting AI, \nand nations such as Russia and China are unlikely \nto make the same operational and ethical decisions \nas Western countries about the appropriate roles \nof AI vis-à-vis humans in controlling the operation \nCopyright © 2025 by the Board of Trustees of the Leland Stanford Junior University. All rights reserved.\n34 STANFORD EMERGING TECHNOLOGY REVIEW\nFigure 1.4 shows that most notable ML systems are \nnow released by industry, while very few are released \nby academic institutions.\nAt the same time, China’s efforts to recruit top sci\u0002entific talent offer further temptations for scientists \nto leave the United States. These efforts are often \ntargeted toward ethnic Chinese in the US—ranging \nfrom well-established researchers to those just fin\u0002ishing graduate degrees—and offer recruitment \npackages that promise benefits comparable to those \navailable from private industry, such as high salaries, \nlavish research funding, and apparent freedom from \nbureaucracy. \nAll of these factors are leading to an AI “brain drain” \nthat does not favor the US research enterprise. 2003 2004 2005 2006 2007 2008 2009 2010\n2011201220132014201520162017201820192020202120222023\n50\n40\n30\n20\n10\n0\nNumber of notable machine-learning models by sector, 2003–23 Number of notable machine-learning models\nIndustry Industry-academia collaboration Academia\n51\n21\n15\nSource: Adapted from Nestor Maslej, Loredana Fattorini, Raymond Perrault, et al., The AI Index 2024 Annual Report, AI Index Steering Committee, \nInstitute for Human-Centered AI, Stanford University, Stanford, CA, April 2024. Data from Epoch, 2023\nFIGURE 1.4  Most notable machine-learning models are now released by industry\nNOTES\n1. Christopher Manning, “Artificial Intelligence Definitions,” Insti\u0002tute for Human-Centered AI, Stanford University, September 2020, \nhttps://hai.stanford.edu/sites/default/files/2020-09/AI-Definitions\n-HAI.pdf.\n2. Grand View Research, “Artificial Intelligence Market Size, \nShare, Growth Report 2024–2030,” accessed September 23, 2024, \nhttps://www.grandviewresearch.com/industry-analysis/artificial\n-intelligence-ai-market.\n3. Nestor Maslej, Loredana Fattorini, Raymond Perrault, et al., \nThe AI Index 2024 Annual Report, AI Index Steering Committee, \nInstitute for Human-Centered AI, Stanford University, Stanford, CA, \nApril 2024, https://aiindex.stanford.edu/wp-content/uploads/2024\n/05/HAI_AI-Index-Report-2024.pdf.\n4. CB Insights, “State of Venture 2023 Report,” February 1, 2024, \nhttps://www.cbinsights.com/research/report/venture-trends-2023/.\n5. Reputable sources disagree as to whether global venture fund\u0002ing for AI start-ups increased or decreased in 2023. We have fol\u0002lowed the majority view.\n6. CB Insights, “State of AI 2023 Report,” February 1, 2024, https://\nwww.cbinsights.com/research/report/ai-trends-2023/.\n7. Karen Weise, “In Race to Build A.I., Tech Plans a Big Plumbing \nUpgrade,” New York Times, April 27, 2024, https://www.nytimes\n.com/2024/04/27/technology/ai-big-tech-spending.html.\nCopyright © 2025 by the Board of Trustees of the Leland Stanford Junior University. All rights reserved.\n01 Artifi\n8. Jack Pitcher and Connor Hart, “BlackRock, Microsoft, Others \nForm AI and Energy Infrastructure Investment Partnership,” \nWall Street Journal, September 17, 2024, https://www.wsj.com\n/tech/ai/blackrock-global-infrastructure-partners-microsoft\n-mgx-launch-ai-partnership-1d00e09f.\n9. BlackRock, “BlackRock, Global Infrastructure Partners, Micro\u0002soft, and MGX Launch New AI Partnership to Invest in Data Cen\u0002ters and Supporting Power Infrastructure,” September 17, 2024, \nhttps://ir.blackrock.com/news-and-events/press-releases/press\n-releases-details/2024/BlackRock-Global-Infrastructure-Partners\n-Microsoft-and-MGX-Launch-New-AI-Partnership-to-Invest-in\n-Data-Centers-and-Supporting-Power-Infrastructure/default.aspx.\n10. Goldman Sachs, “Generative AI Could Raise Global GDP by \n7%,” April 5, 2023, https://www.goldmansachs.com/intelligence\n/pages/generative-ai-could-raise-global-gdp-by-7-percent.html.\n11. Maslej et al., The AI Index 2024 Report.\n12. Jafar Alzubi, Anand Nayyar, and Akshi Kumar, “Machine \nLearning from Theory to Algorithms: An Overview,” Journal of \nPhysics: Conference Series 1142, Second National Conference \non Computational Intelligence (December 2018), https://doi\n.org/10.1088/1742-6596/1142/1/012012.\n13. The Nobel Prize in Physics 2024, “Summary,” The Nobel Prize, \nOctober 12, 2024, https://www.nobelprize.org/prizes/physics\n/2024/summary/.\n14. The Nobel Prize in Chemistry 2024, “Summary,” The Nobel Prize, \nOctober 12, 2024, https://www.nobelprize.org/prizes/chemistry\n/2024/summary/.\n15. Kif Leswing, “Meet the $10,000 Nvidia Chip Powering the \nRace for A.I.,” CNBC, February 23, 2023, https://www.cnbc\n.com/2023/02/23/nvidias-a100-is-the-10000-chip-powering-the\n-race-for-ai-.html; Kasper Groes Albin Ludvigsen, “The Carbon \nFootprint of GPT-4,” Medium, July 18, 2023, https://towardsdata\nscience.com/the-carbon-footprint-of-gpt-4-d6c676eb21ae.\n16. Darian Woods and Adrian Ma, “The Semiconductor Founding \nFather,” December 21, 2021, in The Indicator from Planet Money, \npodcast produced by NPR, MP3 audio, 10:14, https://www.npr\n.org/transcripts/1066548023.\n17. Ludvigsen, “The Carbon Footprint.”\n18. Kasper Groes Albin Ludvigsen, “ChatGPT’s Electricity Con\u0002sumption,” Medium, July 12, 2023, https://towardsdatascience\n.com/chatgpts-electricity-consumption-7873483feac4. Different \nsources provide somewhat different numbers for the energy cost \nper query, but they all are in the range of a few watt-hours.\n19. EPRI, “Powering Intelligence: Analyzing Artificial Intelligence \nand Data Center Energy Consumption,” Technology Innovation, \naccessed September 23, 2023, https://www.epri.com/research\n/products/3002028905.\n20. Hope Reese, “A Human-Centered Approach to the AI \nRevolution,” Institute for Human-Centered AI, Stanford Uni\u0002versity, October 17, 2022, https://hai.stanford.edu/news/human\n-centered-approach-ai-revolution.\n21. Viz.ai, “Viz.ai Receives New Technology Add-on Payment \n(NTAP) Renewal for Stroke AI Software from CMS,” August 4, \n2021, https://www.viz.ai/news/ntap-renewal-for-stroke-software.\n22. Gary Liu, Denise B. Catacutan, Khushi Rathod, et al., “Deep \nLearning-Guided Discovery of an Antibiotic Targeting Acineto\u0002bacter baumannii,” Nature Chemical Biology (2023), https://doi\n.org/10.1038/s41589-023-01349-8.\n23. Albert Haque, Arnold Milstein, and Fei-Fei Li, “Illuminating \nthe Dark Spaces of Healthcare with Ambient Intelligence,” Nature\n585 (2020): 193–202, https://doi.org/10.1038/s41586-020-2669-y.\n24. Khari Johnson, “Hospital Robots Are Helping Combat a Wave \nof Nurse Burnout,” Wired, April 19, 2022, https://www.wired.com\n/story/moxi-hospital-robot-nurse-burnout-health-care.\n25. Fish Site, “Innovasea Launches AI-Powered Biomass Camera \nfor Salmon,” August 17, 2023, https://thefishsite.com/articles\n/innovasea-launches-ai-powered-biomass-camera-for-salmon.\n26. Itransition, “Machine Learning in Agriculture: Use Cases and \nApplications,” February 1, 2023, https://www.itransition.com/\nmachine-learning/agriculture.\n27. GateHouse Maritime, “Vessel Tracking Giving Full Journey \nVisibility,” accessed August 15, 2023, https://gatehousemaritime\n.com/data-services/vessel-tracking.\n28. Kodiak, “J.B. Hunt, Bridgestone and Kodiak Surpass 50,000 \nAutonomous Long-Haul Trucking Miles in Delivery Collabo\u0002ration,” August 7, 2024, https://kodiak.ai/news/jb-hunt-and\n-kodiak-collaborate.\n29. JD Supra, “Artificial Intelligence in Law: How AI Can Reshape \nthe Legal Industry,” September 12, 2023, https://www.jdsupra\n.com/legalnews/artificial-intelligence-in-law-how-ai-8475732.\n30. Steve Lohr, “A.I. Is Doing Legal Work. But It Won’t Replace Law\u0002yers, Yet,” New York Times, March 19, 2017, https://www.nytimes\n.com/2017/03/19/technology/lawyers-artificial-intelligence.html.\n31. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, et al., “On \nthe Opportunities and Risks of Foundation Models,” arXiv, Stan\u0002ford University, July 12, 2022, https://doi.org/10.48550/arXiv\n.2108.07258. \n32. Parameters are like the building blocks of knowledge that \nmake up an AI system’s understanding. You can think of them as \ntiny bits of information the AI uses to make sense of data and gen\u0002erate responses. When we say AI models have billions or trillions \nof parameters, it means they have an enormous number of these \ninformation pieces to work with. This allows them to understand \nand generate more sophisticated and nuanced content.\n33. Bommasani et al., “On the Opportunities and Risks.”\n34. Sarah W. Li, Matthew W. Kemp, Susan J. S. Logan, Sebastian \nE. Illanes, and Mahesh A. Choolani, “ChatGPT Outscored Human \nCandidates in a Virtual Objective Structured Clinical Examination \nin Obstetrics and Gynecology,” American Journal of Obstetrics & \nGynecology 229, no. 2 (August 2023): 172.E1-172.E12, https://\ndoi.org/10.1016/j.ajog.2023.04.020.\n35. Kent F. Hubert, Kim N. Awa, and Darya L. Zabelina, “The Cur\u0002rent State of Artificial Intelligence Generative Language Models \nIs More Creative Than Humans on Divergent Thinking Tasks,” \nScientific Reports 14, no. 3440 (February 2024), https://doi\n.org/10.1038/s41598-024-53303-w.\n36. Josh Achiam, Steven Adler, Sandhini Agarwal, et al., “GPT-4 \nTechnical Report,” arXiv, March 4, 2024, https://doi.org/10.48550\n/arXiv.2303.08774. \n37. Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, et \nal., “Sparks of Artificial General Intelligence: Early Experiments \nwith GPT-4,” arXiv, Cornell University, April 13, 2023, https://doi\n.org/10.48550/arXiv.2303.12712.\n38. For an argument that they are, see Bubeck et al., “Sparks of \nArtificial General Intelligence”; for an argument that they are not, \nsee Thomas Macaulay, “Meta’s AI Chief: LLMs Will Never Reach \nCopyright © 2025 by the Board of Trustees of the Leland Stanford Junior University. All rights reserved.\n36 STANFORD EMERGING TECHNOLOGY REVIEW\nHuman-Level Intelligence,” The Next Web, April 10, 2024, https://\nthenextweb.com/news/meta-yann-lecun-ai-behind-human\n-intelligence.\n39. ”Stop Talking about Tomorrow’s AI Doomsday When AI Poses \nRisks Today,” editorial, Nature 618 (June 2023): 885–86, https://\nwww.nature.com/articles/d41586-023-02094-7.\n40. Poornima Apte, “Princeton Invests in New 300-GPU Clus\u0002ter for Academic AI Research,” AI at Princeton, Princeton Uni\u0002versity, March 15, 2024, https://ai.princeton.edu/news/2024\n/princeton-invests-new-300-gpu-cluster-academic-ai-research.\n41. Tae Kim, “Mark Zuckerberg Says Meta Will Own Billions \nWorth of Nvidia H100 GPUs by Year End,” Barrons, January 19, \n2024, https://www.barrons.com/articles/meta-stock-price-nvidia\n-zuckerberg-b0632fed.\n42. The bill’s full name is Creating Resources for Every American to \nExperiment with Artificial Intelligence Act of 2023. Congress.gov, \n“S.2714 – 118th Congress (2023–2024): CREATE AI Act of 2023,” \nJuly 27, 2023, https://www.congress.gov/bill/118th-congress/senate\n-bill/2714. \n43. White House, National Artificial Intelligence Research Resource \nTask Force Releases Final Report, Office of Science and Tech\u0002nology Policy, News & Updates, Press Releases, https://www\n.whitehouse.gov/ostp/news-updates/2023/01/24/national\n-artificial-intelligence-research-resource-task-force-releases\n-final-report/.\n44. Karen Kwok, “AI Firms Lead Quest for Intelligent Business \nModel,” Reuters, December 12, 2023, https://www.reuters.com\n/breakingviews/ai-firms-lead-quest-intelligent-business-model\n-2023-12-12/.\n45. Joy Buolamwini and Timnit Gebru, “Gender Shades: Intersec\u0002tional Accuracy Disparities in Commercial Gender Classification,” \nProceedings of Machine Learning Research 81, Conference on Fair\u0002ness, Accountability, and Transparency (February 2018): 1–15, https://\nwww.media.mit.edu/publications/gender-shades-intersectional\n-accuracy-disparities-in-commercial-gender-classification.\n46. Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow, \n“Transferability in Machine Learning: From Phenomena to Black-Box \nAttacks Using Adversarial Samples,” arXiv, May 24, 2016, https://\ndoi.org/10.48550/arXiv.1605.07277.\n47. Ivana Kottasová, Sophie Tanno, and Heather Chen, “Pro\u0002Russian Politician Wins Slovakia’s Parliamentary Election,” CNN, \nOctober 2, 2023, https://www.cnn.com/2023/10/01/world/slovakia\n-election-pro-russia-robert-fico-win-intl-hnk/index.html.\n48. Jeongyoon Han, “New Hampshire Is Investigating a Robo\u0002call That Was Made to Sound Like Biden,” NPR, January 22, \n2024, https://www.npr.org/2024/01/22/1226129926/nh-primary\n-biden-ai-robocall.\n49. Samriddhi Sakunia, “AI and Deepfakes Played a Big Role in \nIndia’s Elections,” New Lines Magazine, July 12, 2024, https://\nnewlinesmag.com/spotlight/ai-and-deepfakes-played-a-big\n-role-in-indias-elections/.\n50. Reuters, “Fact Check: ‘Drunk’ Nancy Pelosi Video Is Manipu\u0002lated,” August 3, 2020, https://www.reuters.com/article/world/fact\n-check-drunk-nancy-pelosi-video-is-manipulated-idUSKCN24Z2B1/.\n51. Neil Perry, Megha Srivastava, Deepak Kumar, and Dan Boneh, \n“Do Users Write More Insecure Code with AI Assistants?,” arXiv, \nCornell University, December 18, 2023, https://doi.org/10.48550\n/arXiv.2211.03622.\n52. Charlotte Hill, Charlotte Allen, Tom Perkins, and Harriet \nCampbell, “Generative AI in the Courts: Getty Images v Stability \nAI,” Penningtons Manches Cooper, February 16, 2024, https://\nwww.penningtonslaw.com/news-publications/latest-news/2024\n/generative-ai-in-the-courts-getty-images-v-stability-ai.\n53. Michael M. Grynbaum and Ryan Mac, “The Times Sues \nOpenAI and Microsoft Over A.I. Use of Copyrighted Work,” New \nYork Times, December 27, 2023, https://www.nytimes.com\n/2023/12/27/business/media/new-york-times-open-ai-microsoft\n-lawsuit.html.\n54. Blake Brittain, “Music Labels Sue AI Companies Suno, Udio \nfor US Copyright Infringement,” Reuters, June 24, 2024, https://\nwww.reuters.com/technology/artificial-intelligence/music\n-labels-sue-ai-companies-suno-udio-us-copyright-infringement\n-2024-06-24/.\n55. Maja S. Svanberg, Wensu Li, Martin Fleming, Brian C. Goeh\u0002ring, and Neil C. Thompson, “Beyond AI Exposure: Which Tasks \nAre Cost-Effective to Automate With Computer Vision?,” Future \nTech, Working Paper, January 18, 2024, https://futuretech\n-site.s3.us-east-2.amazonaws.com/2024-01-18+Beyond_AI\n_Exposure.pdf.\n56. Claire Cain Miller and Courtney Cox, “In Reversal Because of \nA.I., Office Jobs Are Now More at Risk,” New York Times, August 24, \n2023, https://www.nytimes.com/2023/08/24/upshot/artificial\n-intelligence-jobs.html.\n57. Martin Neil Baily, Erik Brynjolfsson, and Anton Korinek, \n“Machines of Mind: The Case for an AI-Powered Productiv\u0002ity Boom,” Brookings Institution, May 10, 2023, https://www\n.brookings.edu/articles/machines-of-mind-the-case-for-an-ai\n-powered-productivity-boom.\n58. Pranshu Verma and Gerrit De Vynck, “ChatGPT Took Their \nJobs: Now They Walk Dogs and Fix Air Conditioners,” Washington \nPost, June 5, 2023, https://www.washingtonpost.com/technology\n/2023/06/02/ai-taking-jobs/; Challenger, Gray & Christmas, Inc., \n“Challenger Report,” May 2023, https://omscgcinc.wpengine\npowered.com/wp-content/uploads/2023/06/The-Challenger\n-Report-May23.pdf.\n59. David Autor, Caroline Chin, Anna M. Salomons, and Bryan \nSeegmiller, “New Frontiers: The Origins and Content of New \nWork, 1940–2018,” National Bureau of Economic Research, Work\u0002ing Paper 30389, August 2022, https://doi.org/10.3386/w30389.\n60. Parliament and Council Regulation 2024/1689 Artificial Intel\u0002ligence Act, art. 6-7, 2024 O.J. L 2024/1689, https://artificial\nintelligenceact.eu/section/3-1/.\n61. White House, Executive Order on the Safe, Secure, and \nTrustworthy Development and Use of Artificial Intelligence, Pres\u0002idential Actions, Briefing Room, White House, October 30, 2023, \nhttps://www.whitehouse.gov/briefing-room/presidential-actions\n/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy\n-development-and-use-of-artificial-intelligence/.\n62. Department for Science, Innovation, and Technology, “The \nBletchley Declaration by Countries Attending the AI Safety \nSummit, 1–2 November 2023,” Foreign, Commonwealth, and \nDevelopment Office, Prime Minister’s Office, November 1, 2023, \nhttps://www.gov.uk/government/publications/ai-safety-summit\n-2023-the-bletchley-declaration/the-bletchley-declaration\n-by-countries-attending-the-ai-safety-summit-1-2-november\n-2023.\n63. National Security Commission on Artificial Intelligence,\nFinal Report, March 19, 2021, https://apps.dtic.mil/sti/pdfs\n/AD1124333.pdf.\n64. C. Todd Lopez, “DOD Adopts 5 Principles of Artificial Intel\u0002ligence Ethics,” DOD News, US Department of Defense, \nCopyright © 2025 by the Board of Trustees of the Leland Stanford Junior University. All rights reserved.\n01 Artifi\nFebruary 25, 2020, https://www.defense.gov/News/News-Stories\n/article/article/2094085/dod-adopts-5-principles-of-artificial\n-intelligence-ethics.\n65. US Department of Defense, 2022 Nuclear Posture Review, \nOctober 27, 2022, 13, https://apps.dtic.mil/sti/trecms/pdf\n/AD1183539.pdf; Ministry of Defence, “Defence Artificial Intel\u0002ligence Strategy,” GOV.UK, June 15, 2022, https://www.gov\n.uk/government/publications/defence-artificial-intelligence\n-strategy/defence-artificial-intelligence-strategy; United Nations \nParties to the Treaty on Non-Proliferation of Nuclear Weapons, \nPrinciples and Responsible Practices for Nuclear Weapon States, \nNPT/CONF.2020/WP.70, July 2022, https://undocs.org/NPT\n/CONF.2020/WP.70.\n66. Ashely Deeks, “Too Much Too Soon: China, the U.S., and Auton\u0002omy in Nuclear Command and Control,” Lawfare, December 4, \n2023, https://www.lawfaremedia.org/article/too-much-too-soon\n-china-the-u.s.-and-autonomy-in-nuclear-command-and-control.\nSTANFORD EXPERT CONTRIBUTORS\nDr. Fei-Fei Li\nSETR Faculty Council, Sequoia Professor in the \nComputer Science Department, and Professor, by \ncourtesy, of Operations, Information, and Technology \nat the Graduate School of Business\nDr. Christopher Manning\nThomas M. Siebel Professor of Machine Learning, \nand Professor of Linguistics and of Computer Science\nAnka Reuel\nSETR Fellow and PhD Student in Computer Science \nCopyright © 2025 by the Board of Trustees of the Leland Stanford Junior University. All rights reserved.",
    "length": 62721,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "On the Opportunities and Risks of Foundation Models",
    "url": "https://crfm.stanford.edu/report.html",
    "text": "Stanford CRFM\n[![]] \n# On the Opportunities and Risks of Foundation Models\n> [> Download the report.\n] > **Authors**: Rishi Bommasani\\*, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,\nMichael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card,\nRodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue,\nMoussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale,\nLauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson,\nJohn Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky,\nPratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Kohd, Mark Krass,\nRanjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent,\nXiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell,\nZanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles,\nHamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech,\nEva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz,\nJack Ryan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan,\nAlex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramèr, Rose E. Wang, William Wang , Bohan Wu, Jiajun Wu,\nYuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang,\nXikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, Percy Liang\\*\n## Abstract\n> > AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data> at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore> their critically central yet incomplete character. This report provides a thorough account of the opportunities and> risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human> interaction) and technical principles (e.g., model architectures, training procedures, data, systems, security,> evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity,> misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on> standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness> across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as> the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending> widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and> what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the> critical research on foundation models will require deep interdisciplinary collaboration commensurate with their> fundamentally sociotechnical nature. ## Introduction\nTo learn more, see[this section in the report.] \nThis report investigates an emerging paradigm for building artificial intelligence (AI) systems\nbased on a general class of models which we term foundation models. A*foundation model*is any\nmodel that is trained on broad data (generally using self-supervision at scale) that can be adapted\n(e.g., fine-tuned) to a wide range of downstream tasks; current examples include BERT [Devlin et al.\n2019], GPT-3 [Brown et al. 2020], and CLIP [Radford et al. 2021]. From a technological point of view,\nfoundation models are not new —they are based on deep neural networks and self-supervised\nlearning, both of which have existed for decades. However, the sheer scale and scope of foundation\nmodels from the last few years have stretched our imagination of what is possible; for example,\nGPT-3 has 175 billion parameters and can be adapted via natural language prompts to do a passable\njob on a wide range of tasks despite not being trained explicitly to do many of those tasks [Brown\net al. 2020]. At the same time, existing foundation models have the potential to accentuate harms,\nand their characteristics are in general poorly understood. Given their impending widespread\ndeployment, they have become a topic of intense scrutiny [Bender et al. 2021].\n## Capabilities\nTo learn more, see[this section in the report.] \nFoundation models acquire various*capabilities*that can power applications.\nWe have chosen to discuss five potential capabilities:\nthe ability to process different modalities (*e.g.,*language, vision), to affect\nthe physical world (robotics), to perform reasoning, and to interact with\nhumans (interaction). Finally, we conclude with a philosophical discussion of potential limits on their capabilities.### Language\nTo learn more, see[this section in the report.] \nAuthors: Isabel Papadimitriou, Christopher D. Manning\nNLP as a field has blazed the trail for foundation models.\nWhile these models dominate standard benchmarks, there is a clear gap between the capabilities these models acquire currently and those that characterize language as a complex system for human communication and thought.\nIn response to this, we emphasize the full range of*linguistic variation*(*e.g.,*different styles, dialects, languages), which poses an opportunity and challenge given some variants are data-limited.\nFurther, child*language acquisition*is more sample efficient than the training of foundation models; we examine how signals beyond text and grounding may help to bridge this gap.\nBoth of these characteristics of language provide clear directions for future foundation models research.### Vision\nTo learn more, see[this section in the report.] \nAuthors: Shyamal Buch, Drew A. Hudson, Frieda Rong, Alex Tamkin, Xikun Zhang, Bohan Wu, Ehsan Adeli, Stefano Ermon, Ranjay Krishna, Juan Carlos Niebles, Jiajun Wu, Li Fei-Fei\nComputer vision led the adoption of deep learning in AI, demonstrating that models pretrained on large annotated datasets can transfer to numerous downstream settings.\nNow, pretraining on web-scale raw data instead of curated datasets, foundation models are on the rise in computer vision.\nThese models have shown promising results for standard tasks in the field, like image classification and object detection, and training on*multimodal and embodied*data beyond images may enable progress on significant challenges (*e.g.,*3D geometric and physical understanding, commonsense reasoning).\nWe also discuss some of the key challenges in modeling (*e.g.,*the ability to scale effectively to videos) and evaluation (*e.g.,*the measurement of higher-order capabilities) along with the applications (*e.g.,*ambient intelligence for healthcare) and societal considerations (*e.g.,*surveillance) that will determine the impact of foundation models for computer vision going forward.### Robotics\nTo learn more, see[this section in the report.] \nAuthors: Siddharth Karamcheti, Annie Chen, Suvir Mirchandani, Suraj Nair, Krishnan Srinivasan, Kyle Hsu, Jeannette Bohg, Dorsa Sadigh, Chelsea Finn\nA longstanding goal of robotics research is to develop \"generalist\" robots capable of performing myriad tasks across physically diverse environments.\nUnlike language and vision, which have led the way with foundation models both due to the abundance of raw data to train these models on and the availability of virtual applications to apply these models to, robotics faces fundamental challenges due to being anchored to the physical world.\nThe principal challenge in developing*new types of foundation models for robotics*&mdash;different in nature than their language and vision counterparts&mdash;is acquiring*sufficient data*of the*right form*that is conducive to learning: we explore how plentiful data (*e.g.,*generic videos of humans, amongst others) that is not specific to particular environments and across modalities (*e.g.,*language, vision) may help to bridge this gap.\nThese new robotic foundation models could allow for easier*task specification and learning*, ushering in new applications (*e.g.,*better robotic assistance for household tasks) and heightening the importance of*robustness and safety*(*e.g.,*formal safety evaluation).### Reasoning and search\nTo learn more, see[this section in the report.] \nAuthors: Yuhuai Wu, Frieda Rong, Hongyu Ren, Sang Michael Xie, Xuechen Li, Andy Shih, Drew A. Hudson, Omar Khattab\nReasoning and search problems such as theorem proving and program synthesis have been long-standing challenges in AI. The combinatorial search space renders traditional search-based methods intractable.\nHowever, humans are known to operate intuitively even in the most mathematical of domains,\nand indeed existing work such as AlphaGo have already shown that deep neural networks can be effective in guiding the search space.\nBut humans also transfer knowledge across tasks, facilitating\nmuch more efficient adaptation and the ability to reason more abstractly. Foundation models offer the possibility of closing this gap: their multi-purpose nature along with their strong generative and multimodal capabilities offer new leverage for controlling the combinatorial explosion inherent to search.### Interaction\nTo learn more, see[this section in the report.] \nAuthors: Joon Sung Park, Chris Donahue, Mina Lee, Siddharth Karamcheti, Dorsa Sadigh, Michael S. Bernstein\nFoundation models show clear potential to transform the developer and user experience for AI systems: foundation models lower the difficulty threshold for*prototyping and building*AI applications due to their sample efficiency in adaptation, and raise the ceiling for*novel user interaction*due to their multimodal and generative capabilities.\nThis provides a synergy we encourage going forward: developers can provide applications that better fit the*user's needs and values*, while introducing far more dynamic forms of interaction and opportunities for*feedback*.### Philosophy of understanding\nTo learn more, see[this section in the report.] \nAuthors: Christopher Potts, Thomas Icard, Eva Portelance, Dallas Card, Kaitlyn Zhou, John Etchemendy\nWhat could a foundation model come to understand about the data it is trained on? Focusing on the case of natural language, we identify different positions on the nature of understanding and explore their relevance for our central question. Our tentative conclusion is that skepticism about the capacity of future foundation models to understand natural language may be premature, especially where the models are trained on multi-modal data.\n## Applications\nTo learn more, see[this section in the report.] \nAt present, foundation model research is largely confined to computer science and AI, with the impact of foundation models and the applications they support largely being centered in the tech industry.\nMoving forward, foundation models present clear potential to transform and extend the reach of AI across many sectors beyond the tech industry, suggesting a more pervasive effect on people's lives.\nWhile there is a multitude of applications and domains to consider, we we have chosen three applications &mdash; healthcare, law, and education &mdash; because they represent foundational pillars of our society.\nFor foundation models to significantly contribute to these application domains, models will require specific capabilities as well as technical innovation to account for the unique considerations in each domain.\nFurther, since these domains are critical to societal function, applying foundation models in these domains requires engaging with deeply sociotechnical matters such as those those pertaining to data, privacy, interpretability, fairness and ethics.### Healthcare and biomedicine\nTo learn more, see[this section in the report.] \nAuthors: Michihiro Yasunaga, Jing Huang, Camilo Ruiz, Yuhui Zhang, Giray Ogut, Saahil Jain, William Wang, Yusuf Roohani, Hongyu Ren, Antoine Bosselut, Ehsan Adeli, Jure Leskovec, Russ Altman\nHealthcare tasks (*e.g.,*patient care via disease treatment) and biomedical research (*e.g.,*scientific discovery of new therapies) require expert knowledge that is limited and expensive.\nFoundation models present clear opportunities in these domains due to the*abundance of data*across*many modalities*(*e.g.,*images, text, molecules) to train foundation models, as well as the value of improved sample efficiency in adaptation due to the cost of expert time and knowledge.\nFurther, foundation models may allow for improved*interface design*for both healthcare providers and patients to interact with AI systems, and their generative capabilities suggest potential for*open-ended research problems*like drug discovery.\nSimultaneously, they come with clear risks (*e.g.,*exacerbating historical biases in medical datasets and trials).\nTo responsibly unlock this potential requires engaging deeply with the sociotechnical matters of data sources and privacy as well as model interpretability and explainability, alongside effective regulation of the use of foundation models for both healthcare and biomedicine.### Law\nTo learn more, see[this section in the report.] \nAuthors: Peter Henderson, Lucia Zheng, Jenny Hong, Neel Guha, Mark Krass, Julian Nyarko, Daniel E. Ho\nLegal applications require that attorneys read and produce long coherent\nnarratives that incorporate shifting contexts and decipher ambiguous legal standards.\nFoundation models may provide benefits in this domain:*ample data*exists in the form of legal documents and their generative capabilities are well-suited to the*many generative tasks required in law*, but significant improvements are required for foundation models to be able to reliably*reason over various sources*of information to generate*truthful*long-form documents.\nAs is the care in healthcare, the sample efficiency of adaptation for foundation models is of heightened value given the costs of expert time and knowledge in the legal domain, which may allow for the*re-allocation of expertise*towards pressing problems of justice and government service.\nThe responsible development of foundation models for law will require specific consideration of privacy, and highlights core limitations of existing foundation models that will require fundamental advances with respect to*provenance*for their behavior and*guarantees*for the factuality of their generation.### Education\nTo learn more, see[this section in the report.] \nAuthors: Ali Malik, Dorottya Demszky, Pang Wei Koh, Moussa Doumbouya, Drew A. Hudson, Allen Nie, Hamed Nilforoshan, Alex Tamkin, Emma Brunskill, Noah Goodman, Chris Piech\nEducation is a complex and subtle domain; effective teaching involves reasoning about student cognition and should reflect the learning goals of students.\nThe nature of foundation models presents promise here that has yet to be realized in the sphere of AI for education: while certain many streams of data in education are individually too limited to train foundation models, the ability to leverage relevant data from outside the domain (*e.g.,*the Internet) and make use of data across multiple modalities (*e.g.,*textbooks, mathematical formula, diagrams, video-based tutorials) jointly offers hope for foundation models that are broadly applicable to educational tasks.\nIf foundation models lead to a significant improvement in education-relevant capabilities, there is clear potential for new applications that align with the open-ended generative (*e.g.,*problem generation) and interactive (*e.g.,*feedback to teachers) aspects of foundation models; the sample efficient adaptation of foundation models suggests greater ability for*adaptive and personalized learning*.\nIn this event, renewed consideration is required of hallmarks of applying technology to education (*e.g.,*student privacy), along with certain concerns becoming more critical (*e.g.,*inequity in access to technology in education, technology-aided plagiarism).\n## Technology\nTo learn more, see[this section in the report.] \nNow we discuss the technology behind building better model architectures,\ntraining and adaptation procedures, and of course scaling up the systems.\nOne crucial but often overlooked topic is data&mdash;where does it come from and\nwhat is its composition?\nIn addition, we want foundation models to be robust to distribution shifts\nand secure against attackers.\nFinally, we wish to understand why foundation models work from both a mathematical perspective\nas well as an empirical perspective.### Modeling\nTo learn more, see[this section in the report.] \nAuthors: Drew A. Hudson, Antoine Bosselut, Alex Tamkin, Omar Khattab, Jared Quincy Davis, Jiaxuan You, Trevor Gale\nWhat structural properties give rise to a foundation model? In the modeling section, we explore the underlying architectures behind foundation models and identify 5 key attributes. First, we start by discussing*expressivity*of the computational model &mdash; to capture and assimilate real-world information, and*scalability*&mdash; to adeptly handle large quantities of high-dimensional data. These properties are successfully realized by existing architectures such as the transformer network that underpins most foundation models to date. We then proceed to attributes may be essential for the next generation of models, including:*multimodallity*&mdash; to consume, process and potentially produce content from different sources and domains,*memory*capacity &mdash; to effectively store and retrieve the acquired knowledge, and finally,*compositionality*, to foster successful generalization to novel settings and environments. We believe that realizing the full potential envisioned for foundation models will hinge on modelling advances to fulfill these desiderata.### Training\nTo learn more, see[this section in the report.] \nAuthors: Alex Tamkin\nTraining objectives mathematically specify how models should learn and acquire capabilities from their training data.\nThe current status quo for training foundation models involves modality-specific objectives (*e.g.,*masked language modeling for text and SimCLR for images) that are often chosen heuristically.\nWe envision that future training objectives for foundation models will reflect two changes:*principled selection*derived from systematic evidence and evaluation, and*domain-generality*to provide rich, scalable, and unified training signal across data sources and modalities. We also discuss important design trade-offs, including generative vs discriminative training, the choice of input data representation, and the potential of future training objectives that involve explicit representations of goals.### Adaptation\nTo learn more, see[this section in the report.] \nAuthors: Xiang Lisa Li\\*, Eric Mitchell\\*, Sang Michael Xie, Xuechen Li, Tatsunori Hashimoto\nFoundation models are intermediary assets; they are unfinished and generally should not be used directly, instead requiring adaptation for specific downstream tasks.\nThe*de facto*approach for adaptation has been fine-tuning, with recent work suggesting that lightweight fine-tuning alternatives and prompting-based methods may achieve favorable accuracy-efficiency tradeoffs.\nMoving forward, we envision a more expansive view of adaptation that goes beyond just specializing foundation models to perform the task of interest: adaptation will alleviate deficiencies of stand-alone foundation models (*e.g.,**temporal adaptation*to reflect changes over time in the world) or introduce*constraints*(*e.g.,*GDPR compliance relating to the*right to be forgotten*); this broader perspective on adaptation coincides with a need for new evaluation protocols that systematically evaluate adaptation methods while controlling for resources (*e.g.,*runtime, memory) and access requirements involved in adaptation.### Evaluation\nTo learn more, see[this section in the report.] \nAuthors: Rishi Bommasani, Kawin Ethayarajh, Omar Khattab\nEvaluation offers context to foundation models by providing a means to track progress, understand models, and document their capabilities and biases.\nFoundation models challenge the ability of standard evaluation paradigms in machine learning to achieve these goals since they are one step removed from specific tasks.\nTo envision new paradigms in evaluation that suit foundation models, we discuss (i) evaluating foundation models*directly*to measure their*inherent capabilities*and inform how foundation models are trained, (ii) evaluating task-specific models by*controlling for adaptation resources and access*, and (iii) broader*evaluation design*to provide richer context beyond measures of accuracy (*e.g.,*robustness, fairness, efficiency, environmental impact).\nReform of evaluation practices will allow for evaluation that adequately serves both the diverse goals and stakeholders involved in the foundation model paradigm.### Systems\nTo learn more, see[this section in the report.] \nAuthors: Deepak Narayanan, Trevor Gale, Keshav Santhanam, Omar Khattab, Tianyi Zhang, Matei Zaharia\nWhile the training data determines the theoretical information available for foundation models, and model architectures and training objectives determine how much of this information can be extracted, computer systems determine what is practically achievable.\nSystems are a key bottleneck for scaling in terms of data and model size, both of which appear to reliably track with improvements in capabilities.\nTo ensure that we can train the next generation of foundation models efficiently (with respect to time and cost), we will require the co-design of algorithms, models, software, and hardware.\nThis co-design is already starting to happen to in various forms, from carefully tuned parallelism strategies to new architectures such as retrieval-based and mixture-of-expert models.\nBeyond training, we consider what will be required to deploy applications on top of foundation models (*e.g.,*efficient inference).### Data\nTo learn more, see[this section in the report.] \nAuthors: Laurel Orr, Simran Arora, Karan Goel, Avanika Narayan, Michael Zhang, Christopher Ré\nData is the lifeblood of foundation models; the training data of these models largely determines what these capabilities these models can acquire.\nThe centrality of data is not unique to foundation models; recent calls for*data-centric AI*indicate the pervasive importance of managing, understanding, and documenting data used to train machine learning models.\nFor foundation models specifically, the current*modus operandi*is for training data to be selected using unspecified or unclear principles with a general lack of transparency regarding the nature of training data.\nWe believe an alternative approach is needed to re-imagine the data ecosystem surrounding foundation models: we draw upon work on data visualization and management to propose a*data hub*for foundation models.\nWe articulate how this proposal relates to many of the relevant data-centric considerations for foundation models: selection, curation, documentation, access, visualization and inspection, quality assessment, and legal regulation.### Security and privacy\nTo learn more, see[this section in the report.] \nAuthors: Florian Tramèr\\*, Rohith Kuditipudi\\*, Xuechen Li\\*\nSecurity and privacy for foundation models is largely uncharted at present.\nFundamentally, foundation models are a high-leverage*single point of failure*, making them a prime target for attack: existing work demonstrates a variety of security vulnerabilities (*e.g.,*adversarial triggers to generate undesirable outputs) or privacy risks (*e.g.,*memorization of training data) for these models.\nFurther, the generality of foundation models compounds these concerns, intensifying the risk for*function creep or dual use*(*i.e.,*use for unintended purposes).\nFor security, we view foundation models as akin to*operating systems*in traditional software systems; we discuss steps towards secure foundation models which, if achieved, would provide a strong abstraction layer to build upon for reliable ML applications.\nFor privacy, by leveraging knowledge transfer from public data, foundation models may enable more sample efficient adaptation to sensitive data distributions,*i.e.,*privacy-preserving applications may incur less degradation in accuracy when built using foundation models.### Robustness to distribution shifts\nTo learn more, see[this section in the report.] \nAuthors: Sang Michael Xie, Ananya Kumar, Rohan Taori, Tony Lee, Shiori Sagawa, Pang Wei Koh, Tatsunori Hashimoto\nA major limitation of standard machine learning is that it produces models that are not robust to*distribution shifts*, where the training distribution does not match the test distribution (for the downstream task).\nExisting work shows that adapting a foundation model trained on a broad range of unlabeled data improves the robustness of adapted models across a wide variety of shifts.\nThis opens a new set of promising directions for improving training and adaptation of foundation models for robustness.\nHowever, we do not believe that foundation models are a panacea for robustness&mdash;challenges such as extrapolation across time and spurious correlations are not likely to be fully addressed.### AI safety and alignment\nTo learn more, see[this section in the report.] \nAuthors: Alex Tamkin, Geoff Keeling, Jack Ryan, Sydney von Arx\nEnsuring foundation models are reliable, robust, and interpretable is increasingly important when considering the potential real-world applications of these models.\nIn addition to critical and immediate considerations, we also consider the relationship between foundation models and larger-scale risks, hazards, and harms that have the potential for increased relevance as model capabilities continue to advance.\nFor example, we consider the importance of*aligning*foundation models such that they are not deployed with*misspecified goals or values*. We also discuss the relevance of*forecasting the emergent behaviors*of foundation models (*e.g.,*the ability to deceive or plan strategically), which may complicate attempts to adapt them to particular tasks, and may require new approaches for interpretability or evaluation.### Theory\nTo learn more, see[this section in the report.] \nAuthors: Aditi Raghunathan, Sang Michael Xie, Ananya Kumar, Niladri Chatterji, Rohan Taori, Tatsunori Hashimoto, Tengyu Ma\nLearning theory provides a broad foundation for the variety of contexts encountered in applied machine learning; theory offers both understanding, principles, and guarantees to complement empirical findings.\nAt present, the study of foundation models is largely empirical: the theory of standard supervised learning, while relatively mature, is inadequate to fully explain foundation models.\nSpecifically, the discrepancy between the training phase and the adaptation phase within the foundation model regime pinpoints the insufficiency of existing theory, since these phases correspond to (potentially) completely different tasks and data distributions.\nNevertheless, we endeavor that advances in theory to address this discrepancy, even in simple, limited settings, will provide useful insights.### Interpretability\nTo learn more, see[this section in the report.] \nAuthors: John Hewitt\\*, Armin W. Thomas\\*, Pratyusha Kalluri, Rodrigo Castellon, Christopher D. Manning\nInterpretability provides clarity to foundation models: the opacity of the deep neural networks that underpin foundation models, alongside the expected ubiquity of foundation models, heightens the need to understand these models and their capabilities.\nInterpretability methods at present generally are designed for interpreting and explaining the behavior of task-specific models; the nature of foundation models (*i.e.,*the wide array of tasks these models are beneficial for and the unexpected emergent properties they acquire) introduces new challenges for interpretability research.\nTo frame the discussion of interpretability for foundation models, we propose the*one model-many models*paradigm, which aims to determine the extent to which the*one model*(the foundation model) and its*many models*(its adapted derivatives) share decision-making building blocks.\nIn addition to interpreting the decision-making components involved, we further discuss*explainability*in the context of foundation models (*e.g.,*the validity of*post hoc*explanations generated by models) as well as the*mechanisms*that drive model behavior (which may clarify the extent to which understanding foundation models can extend to understanding their adapted derivatives).\nGiven the critical role we ascribe interpretability in the study of foundation models, we conclude with an assessment of the societal impact of interpretability and non-interpretability.\n## Society\nTo learn more, see[this section in the report.] \nWe believe the rapid development of foundation models, adapted and\ndeployed to various applications, will have wide-ranging consequences on the\nhealth of societies. What makes these models so exciting and also so troubling\nis their task agnosticity. Societal impact is easier (but still non-trivial)\nto understand and reason about when we talk about specific systems deployed to\nusers, but how can we take into account the societal impact of all\npossible systems and use cases when developing foundation models?### Inequity and fairness\nTo learn more, see[this section in the report.] \nAuthors: Rishi Bommasani, Fereshte Khani, Esin Durmus, Faisal Ladhak, Dan Jurafsky\nIn many contexts, machine learning has been shown to contribute to, and potentially amplify, societal inequity.\nFoundation models may extend this trend,*i.e.,*furthering the unjust treatment of people who have been historically discriminated against.\nHowever, understanding the relationship between inequity and foundation models requires reckoning with the abstraction of foundation models; foundation models are intermediary assets that are adapted for applications that impact users.\nTherefore, we delineate*intrinsic biases*,*i.e.,*properties in foundation models that portend harm, and*extrinsic harms*,*i.e.,*harms arising in the context of specific applications built using foundation models.\nWe taxonomize various sources (*e.g.,*training data, lack of diversity among foundation model developers, the broader sociotechnical context) that give rise to these biases and harms, emphasizing the importance, and technical difficulty, of*source tracing*to understand ethical and legal responsibility.\nWe do not view unfairness as inevitable in the foundation model paradigm: to address unfair outcomes that arise from foundation models, we dually consider*proactive interventions*(*e.g.,*technical methods like counterfactual data augmentation) and*reactive recourse*(*e.g.,*mechanisms for feedback propagation and attribution of moral/legal responsibility).### Misuse\nTo learn more, see[this section in the report.] \nAuthors: Antoine Bosselut\\*, Shelby Grossman\\*, Ben Newman\nWe define foundation model misuse as the use of foundation models as they are technically intended (*e.g.,*to generate language or video), but with the goal of causing societal harm (*e.g.,*to generate disinformation, to develop deepfakes for harassment).\nWe argue that advances in foundation models will result in higher-quality machine-generated content that will be easier to create and personalize for misuse purposes.\nFor example, disinformation actors may use them to quickly generate collections of articles targeted across different demographic groups (*e.g.,*nationality, political party, religion, etc.).\nWhile these new capabilities may limit existing human detection methods for harmful content (*e.g.,*tracking similar text across different sources), foundation models may themselves provide promising potential as automated misuse detectors.### Environment\nTo learn more, see[this section in the report.] \nAuthors: Peter Henderson, Lauren Gillespie, Dan Jurafsky\nFoundation models are the byproducts of computationally expensive training regimes, with the existing trajectory favoring even more intensive models; the energy required for this training coincides with the release of more carbon into the atmosphere and the degradation of the environment.\nAt present, current discussion centers these enormous single-time training costs and the potential to amortize these costs across repeated use.\nWe seek to clarify these discussions by identifying assumptions that shape the calculus of environmental impact for foundation models.\nFurther, we envision that the ecosystem surrounding foundation models requires a multi-faceted approach: (i) more*compute-efficient*models, hardware, and energy grids all may mitigate the carbon burden of these models, (ii) environmental cost should be a clear factor that informs how foundation models are evaluated, such that foundation models can be more comprehensively juxtaposed with more environment-friendly baselines, and (iii) the cost-benefit analysis surrounding environmental impact necessitates greater*documentation and measurement*across the community.### Legality\nTo learn more, see[this section in the report.] \nAuthors: Neel Guha, Peter Henderson, Lucia Zheng, Mark Krass, Daniel E. Ho\nFoundation models rest on tenuous legal footings at present; how the law bears on both the development and use of these models is largely unclear.\nLegal and regulatory frameworks for foundation models specifically, alongside those for AI technology more generally, will be needed to influence, constrain, and even foster practices in research, development, and deployment.\nCentering on the legal landscape of the United States, where existing consideration of algorithmic tools remains broadly uncertain, we highlight the pertinent issues of*liability*for model predictions and*protections*from model behavior.\nWith respect to both issues, we describe how legal standards will need to be advanced to address these given the intermediary status of foundation models (as opposed to that of user-facing task-specific models).### Economics\nTo learn more, see[this section in the report.] \nAuthors: Zanele Munyikwa, Mina Lee, Erik Brynjolfsson\nFoundation models are likely to have substantial economic impact due to their novel capabilities and potential applications in a wide variety of industries and occupations.\nWe consider the implications of the development and use of foundation models for the future of the US and global economy with a focus on productivity, wage inequality, and concentration of ownership.### Ethics of scale\nTo learn more, see[this section in the report.] \nAuthors: Kathleen Creel, Dallas Card, Rose E. Wang, Isabelle Levent, Alex Tamkin, Armin W. Thomas, Lauren Gillespie, Rishi Bommasani, Rob Reich\nIn addition to running the risk of increasing inequity, as discussed in the section on fairness, the widespread adoption of foundation models poses other ethical, political and social concerns. We discuss ethical issues related to the scale of application of foundation models, such as homogenization and the concentration of power, as well as the norms and release strategies appropriate to address them.\n## Conclusion\nTo learn more, see[this section in the report.] \nIn this report, we have endeavored to comprehensively discuss many of the most critical aspects of foundation models, ranging from their technical underpinnings to their societal consequences.\nIn this way, we acknowledge the unusual approach taken: we have attempted to clarify the nature of a paradigm that may only have just begun, rather than waiting for more to unfold or the dust to settle.\nTherefore, much still remains unclear in spite of our efforts and we reiterate that this is just the beginning of a paradigm shift: foundation models have only just begun to transform the way AI systems are built and deployed in the world.\nMoving forward, we view this document as serving an important role in orienting and framing dialogue on these models and this new paradigm in AI.\nThat said, to ensure the responsible development and deployment of these models on durable foundations, we envision collaboration between different sectors, institutions, and disciplines from the onset to be especially critical.\n## Acknowledgements\nWe would like to thank the following people for their valuable feedback:\nMohit Bansal, Boaz Barak, Yoshua Bengio, Stella Biderman, Su Lin Blodgett, Sam Bowman, Collin Burns, Nicholas Carlini, David Chalmers, Jack Clark, Jeff Dean, Jesse Dodge, Jarred Dunnmon, Gabe Dupre, Jason Eisner, Iason Gabriel, Dan Hendrycks, Avery Hill, Yacine Jernite, Gabbrielle Johnson, Sarah Kreps, Jay McClelland, Preetum Nakkiran, Julian Nyarko, Fernando Pereira, Vinodkumar Prabhakaran, Colin Raffel, Marten van Schijndel, Ludwig Schmidt, Yoav Shoham, Madalsa Singh, Megha Srivastava, Jacob Steinhardt, Emma Strubell, Qian Yang, Luke Zettlemoyer, and Ruiqi Zhong.\nIn addition, we would like to especially thank Vanessa Parli for helping to organize this effort.\n## Citation Guidelines\nTo cite the entire report, please use the BibTeX entry provided below.\nTo cite an individual section of the report, please reference the section number.\nFor example, for the ethics section, cite as (Bommasani et al., 2021, §5.6) or (Bommasani et al., 2021, §5.6: Ethics).\nThis can be achieved through the command \\\\citep[][§5.6]{Bommasani2021FoundationModels} in LaTeX.\n@article{Bommasani2021FoundationModels,\ntitle={On the Opportunities and Risks of Foundation Models},\nauthor={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and S. Buch and Dallas Card and Rodrigo Castellon and Niladri S. Chatterji and Annie S. Chen and Kathleen A. Creel and Jared Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren E. Gillespie and Karan Goel and Noah D. Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas F. Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and O. Khattab and Pang Wei Koh and Mark S. Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir P. Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Benjamin Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and J. F. Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Robert Reich and Hongyu Ren and Frieda Rong and Yusuf H. Roohani and Camilo Ruiz and Jack Ryan and Christopher R'e and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishna Parasuram Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tram{\\\\`e}r and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei A. Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},\njournal={ArXiv},\nyear={2021},\nurl={https://crfm.stanford.edu/assets/report.pdf}\n}",
    "length": 40080,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "",
    "url": "https://hai.stanford.edu/assets/files/hai-policy-brief-labeling-ai-generated-content.pdf",
    "text": "Key Takeaways\nIn response to the rapidly \nimproving ability of AI tools \nto create persuasive content, \npolicymakers are increasingly \ncalling for labels on AI-generated \ncontent—but little research has \nmeasured whether adding a label \nimpacts the persuasiveness of the \nunderlying messages.\nWe surveyed how more than 1,500 \npeople perceive AI-generated \npolicy messages when told the \ncontent had been created by an \nexpert AI model, a human policy \nexpert, or told nothing about its \nauthorship.\nAdding the label changed \npeople’s perceptions of whether \nthe author was AI or human but \ndid not significantly change the \npersuasiveness of the content \nitself, regardless of the policy \ndomain (e.g., allowing colleges to \npay student athletes) or participant \ndemographics (e.g., political party).\nPolicy proposals requiring AI \ncontent labels may enhance \ntransparency, but their inability to \naffect persuasiveness highlights \nthe need for complementary \nsafeguards (e.g., media literacy \neducation) and ongoing research \ninto how AI disclosure policies \nshape the information ecosystem. \nPolicy Brief\nHAI Policy & Society\nJuly 2025\nLabeling AI-Generated \nContent May Not Change \nIts Persuasiveness\nIsabel O. Gallegos, Chen Shani, Weiyan Shi, Federico Bianchi,\nIzzy Gainsburg, Dan Jurafsky, and Robb Willer\nGENERATIVE AI TOOLS CAN NOW PRODUCE PERSUASIVE CONTENT AT \nPREVIOUSLY UNPRECEDENTED SCALE AND SPEED. There are many ways \nin which these tools can be used for positive impact in the world. But \nthe emergence of persuasive, AI-generated content also makes possible \nmany negative uses — such as influence operations, misinformation \ncampaigns, and other kinds of deception — particularly in political \ncontexts. These risks are compounded by a key issue: People struggle to \ndistinguish AI-generated content from content written by humans, which \nhelps influence campaigns and misinformation thrive.\nThese risks have led policymakers to call for authorship labels on AI-made \ncontent. In the European Union, for instance, the AI Act requires that entities \ndeploying AI-generated or -manipulated content label it as such. In the \nUnited States, the AI Labeling Act and the AI Disclosure Act, both introduced \nin Congress in 2023 but not passed, would have implemented similar rules. \nPolicymakers’ calls for labels on AI-made content raise a key question: Will a \nlabel change how much the content influences people’s political and public \npolicy views?\n1\n2\nPolicy Brief \nLabeling AI-Generated Content May \nNot Change Its Persuasiveness\nIn our paper “Labeling Messages as AI-Generated \nDoes Not Reduce Their Persuasive Effects,” we \nsurveyed a diverse group of Americans to investigate \nhow adding authorship labels did or did not affect \nhow they perceive AI-written policy appeals. Across \nfour less-polarized public policy topics, we found no \nsignificant difference between people’s support for \na policy argument when told the argument had been \ngenerated by an expert AI model, a human policy \nexpert, or told nothing about its authorship. The labels \nalso had no significant effects on people’s judgments \nof the content’s accuracy or people’s intentions to \nshare the policy argument with others.\nPolicymakers’ calls for labels \non AI-made content raise a key \nquestion: Will a label change \nhow much the content influences \npeople’s political and public \npolicy views?\nPolicymakers should continue studying and debating \neffective AI disclosure policies, including how AI \ncontent labels may empower users to make more \ninformed decisions about content consumption. Yet, \nwhile labels may enhance transparency, our work \nsuggests that on their own, they may be insufficient \nin addressing the challenges posed by AI-generated \ninformation. Policymakers need to conduct further \nresearch and explore alternative interventions, \nincluding media literacy education and deamplification \nof AI-generated content.\nIntroduction\nPrevious work has explored the persuasiveness of \nAI-generated content without AI labels; perceptions \nof the credibility, reliability, or quality of labeled\ninformation; and the effect of different labels for AI\u0002generated images on viewers’ beliefs. There remains \na gap when it comes to research on the impact of \nattaching labels to AI-generated text on the content’s \npersuasiveness. We therefore chose to investigate how \nAI labels affect persuasiveness of messages on public \npolicy issues. \nThere is good reason to assume that AI labels make \npeople more skeptical of the underlying content. \nFor instance, prior research has found that people \ngenerally prefer human content over AI content in \nnews, public health messaging, donation solicitation, \nand social media content because they perceive the \nhuman content to be more trustworthy. On the other \nhand, AI labels could trigger people’s perceptions of \ntechnological expertise and sophistication, making \nthem trust the AI-generated information more than if it \nwere written by a human.\nOur study tested both these hypotheses through a \nrandomized experiment with 1,500 U.S. participants. \nWe compared participants’ responses to AI-generated \npolicy messages when labeled as AI-created versus \nhuman-authored versus unlabeled across four public \npolicy domains: geoengineering, drug importation, \ncollege athlete salaries, and social media platform \nliability. We selected these four domains from \nthe persuasion dataset as they are neither widely \n3\nPolicy Brief \nLabeling AI-Generated Content May \nNot Change Its Persuasiveness\ndiscussed nor highly polarizing policy issues — and \nthus less likely to clash with people’s well-established \nor deeply held views. \nAfter surveying participants’ prior knowledge about, \nsupport for, and confidence in their beliefs about \none of the four public policy topics, we showed them \nan AI-generated argument about that policy area. \nWe used OpenAI’s GPT-4o to generate the policy \nmessages, manually editing the text only to correct \nfactual errors. Examples of article titles include:\n• “Geoengineering poses too many risks \nand should not be considered.”\n• “Drug importation jeopardizes safety controls \nand the domestic pharma industry.”\n• “College athletes should be paid salaries.”\nAll participants were shown the same message \nrelated to their assigned policy issue area. But they \nwere variably told, at random, either that the message \nhad been written by an expert AI model or a human \npolicy expert, or they were given no details regarding \nauthorship. Participants then indicated their levels of \nsupport for the policies, confidence in their support, \ntheir judgments of how accurate the message was, \nand their intentions to share it with others. They were \nalso asked about their perceptions of the message’s \nsource. We collected demographic data (e.g., political \nparty affiliation, education level, age) so we could test \nhow background characteristics impact the effects of \nAI labels.\nThere remains a gap \nwhen it comes to research on \nthe impact of attaching labels \nto AI-generated text on the \ncontent’s persuasiveness.\nResearch Outcomes\nWe found that adding labels changes people’s \nperceptions of whether the content was authored \nby AI or a human — suggesting that labels improve \ntransparency — but there was no evidence that these \nlabels significantly change the persuasiveness of the \ncontent. \nOf the participants who were told the message had \nbeen AI-written, 94.6% believed it, and of those told \nthe message had been human-written, 89.3% believed \nit. Of those shown no label, 39% thought it was \nhuman-written, 31% thought it was AI-generated, and \nthe remaining 30% were unsure.\nThe policy-related messages themselves were \npersuasive, in that they increased people’s support for \nthe policy as compared to their baseline support, but \nparticipants’ support for policies did not significantly \ndiffer when messages were labeled as human- versus \nAI-made — or when there was no label at all. This \ntrend persisted for the other three variables we \nmeasured: Across people’s confidence in their policy \n4\nPolicy Brief \nLabeling AI-Generated Content May \nNot Change Its Persuasiveness\nsupport, judgments of how accurate the message was, \nand intentions to share it with others, there was also no \nmeaningful difference between the label conditions. \nThese results may be attributable to the type of content \nwe showed participants and the fact that we indicated \nthat the content had been generated by an expert \nAI model. Because the advocacy messages were \nevidence-based and logical, and built on expert-based \npersuasion techniques, the chances that the information \nwill be viewed skeptically are inherently lower.\nEven for different subgroups of participants, our \nlabels did not significantly impact the persuasiveness \nof political messages. We tested whether political \nparty identity, prior knowledge about the topic, prior \nexperience with AI tools, education level, and age \ninfluence the labels’ impact on policy support. While \nparticipants on the whole were not affected by AI \nlabels, we did find that older individuals were less \npersuaded by content labeled as being AI-generated \ncompared to content labeled as having been authored \nby a human. Otherwise, though, the labels had no \neffects in any subgroups of interest.\nWe found that adding labels changes \npeople’s perceptions of whether \nthe content was authored by AI \nor a human — suggesting that \nlabels improve transparency — but \nthere was no evidence that these \nlabels significantly change the \npersuasiveness of the content. \nPolicy Discussion\nThe rapid proliferation of AI-made content continues \nto provoke important questions about how we should \nmitigate negative uses and whether we must provide \nconsumers of such content with more transparency \n— including through authorship labels. However, \npolicy proposals that rely on AI labels alone to address \nthe challenges of AI-generated information may be \ninsufficient.\nBased on our findings, telling a reader that text is \nAI-composed is unlikely to substantially affect its \npersuasiveness — meaning, for example, that placing \nan “AI-authored” label on a misleading but well-written \npolicy argument is unlikely to mitigate the message’s \nharm. However, it is important to note that our study \ndid not provide participants with variable information \nquality and veracity, which means we did not measure \nhow labels reduce persuasiveness for one individual \nviewing different policy arguments. Testing AI labels \nwith other kinds of information and in field settings \nmay lead to different results. Further research is \nneeded to better understand how AI disclosure policies \nshape trust and interactions within the information \necosystem, so that policymakers can make more \ninformed decisions about possible interventions and \ntheir impacts.\nPolicymakers should also consider that our findings \nare influenced by the current state of LLM capabilities \nand their public perceptions. In the future, the style \nand ability of AI-generated persuasive communication \ncould shift, as well as pervasive narratives around the \ntrustworthiness of AI, potentially making AI labels more \nor less persuasive for readers. \n5\nPolicy Brief \nLabeling AI-Generated Content May \nNot Change Its Persuasiveness\nGoing forward, the impact of AI labels on \npersuasiveness will likely change as regulation that \nmandates disclosure develops — alongside people’s \nacceptance of or aversion to AI in general. These \npotential shifts, coupled with our study’s limitations \n— such as focusing on Americans, considering only \ntwo types of content labels, and not accounting \nfor the other heuristics people may use to interpret \ninformation (beyond its source) — underscore the need \nfor additional research as policymakers continue to \ndebate AI content labels. Crucially, these issues also \nhighlight the importance of implementing additional \ninterventions to curb the spread of AI-generated \nmaterial, such as media literacy education, which aims \nto equip people with the tools to discern AI-generated \nwriting from human-authored writing as well as the \nvalidity of the information itself.\nPolicymakers should continue to consider the potential \npositive impacts of AI content label regulations and \nstandards. These include bringing more transparency \nto online content, helping social media users make \ninformed decisions about content consumption, and \ncurbing the spread of AI-generated material in general. \nBut if the policy goal is to decrease the persuasive \neffect of AI-generated messages, labels may not be the \ncomplete solution.\nFurther research is needed to \nbetter understand how \nAI disclosure policies shape \ntrust and interactions within the \ninformation ecosystem.\n6\nIzzy Gainsburg is a research scholar at \nStanford University’s Center on Philanthropy \nand Civil Society.\nIsabel O. Gallegos is a PhD student in computer \nscience at the Stanford School of Engineering \nand a JD candidate at Stanford Law School.\nFederico Bianchi is a senior ML scientist \nat TogetherAI.\nChen Shani is a postdoctoral researcher \nat Stanford University.\nWeiyan Shi is an assistant professor at \nNortheastern University, jointly appointed by \nthe College of Engineering and Khoury College \nof Computer Sciences.\nReference: The original article is \naccessible at Isabel O. Gallegos, Chen \nShani et al., “Labeling Messages as \nAI-Generated Does Not Reduce Their \nPersuasive Effects,” arxiv.org, April 14, \n2025, https://arxiv.org/abs/2504.09865.\nStanford University’s Institute for Human\u0002Centered Artificial Intelligence (HAI)\napplies rigorous analysis and research \nto pressing policy questions on artificial \nintelligence. A pillar of HAI is to inform \npolicymakers, industry leaders, and civil \nsociety by disseminating scholarship to \na wide audience. HAI is a nonpartisan \nresearch institute, representing a range of \nvoices. The views expressed in this policy \nbrief reflect the views of the authors. \nFor further information, please contact \nHAI-Policy@stanford.edu. \nDan Jurafsky is the Reynolds Professor in \nHumanities, a professor of linguistics, and \na professor of computer science at \nStanford University.\nRobb Willer is a professor of sociology, \npsychology (by courtesy), and organizational \nbehavior at the Graduate School of Business \n(by courtesy) at Stanford University, director of \nthe Politics and Social Change Lab, and \nco-director of Stanford’s Center on Philanthropy \nand Civil Society.\nStanford HAI: 353 Jane Stanford Way, Stanford CA 94305-5008 \nT 650.725.4537 F 650.123.4567 E HAI-Policy@stanford.edu hai.stanford.edu",
    "length": 14638,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Fundraising Policy",
    "url": "https://hai.stanford.edu/fundraising-policy",
    "text": "[Skip to main content] [Skip to secondary navigation] \n\n[Stanford University(link is external)] \n\nPage Content\n\n# Fundraising Policy\n\nThe Stanford Human-Centered Artificial Intelligence Institute (Stanford HAI) was established to advance AI research, education, policy, and practice to improve the human condition.\n\nThe Institute aims to conduct research on fundamental and applied topics; convene stakeholders from academia, government, civil society, and industry to address critical technical and societal challenges; and educate students and leaders across all sectors.\n\nHAI is part of Stanford University, a private, not-for-profit institution.  We seek and accept philanthropic donations to help fulfill our mission. In doing so, we follow all university guidelines.  This document provides additional detail and context regarding HAI’s fundraising practices.\n\n* * *\n\n## HAI Donor Policy\n\nSources of funding may include government agencies, philanthropic foundations or trusts,  individuals, or companies that wish to support our mission of advancing AI technology to serve humanity.\n\nWe do not accept donations from, or endorse, any political organizations or candidates.\n\nAccepting a donation is not a signal of Stanford HAI’s endorsement of an organization or individual, or their point of view.\n\nGifts are only accepted from named entities and individuals known to the University. HAI always discloses corporate donations, including membership in affiliate programs. Under certain circumstances, and with university approval, we may agree not to disclose the name of an individual donor who wishes to remain anonymous, as is permitted by U.S. law. Both the identity of the individual and the reason for desiring anonymity must be disclosed to and approved by both the Institute and the university.\n\nStanford employees are not allowed to accept unsolicited personal gifts exceeding $50 in value, solicited gifts in any amount, or special favors from organizations or individuals with which the university does, or may, conduct business.\n\n* * *\n\n## Use of Funds\n\nFunding is used in accordance with the terms of the gift to support general operations and staff; fellows, graduate students and postdoctoral fellows; undergraduate research, summer internships, and postgraduate fellowships; fundamental and applied research; data sets; compute resources; reports and position papers; and public events.\n\n* * *\n\n## Academic Independence\n\nNo donations or conditions on donations will be accepted that might compromise the independence, accuracy, or autonomy of our work, or restrain the views expressed by researchers at HAI.\n\nDonors cannot dictate research topics pursued by HAI researchers, though the terms of a gift may restrict its use to an area of research or a specific research project. Donors cannot direct our operational activities, including decisions related to employment, partnerships, or events. HAI does not allow donors or research sponsors to control permission to publish research results. If research employs a sponsor’s proprietary or confidential data, the sponsor may be given the opportunity to review results for the sole purpose of flagging inadvertent disclosures.\n\nStanford HAI will only do research that can be published openly and whose results can be made available to everyone equally; results cannot be proprietary to any sponsoring entity. In general, if our research generates patentable knowledge, Stanford and the faculty member own the patent, not the sponsor. Stanford does not accept classified government research.\n\nAll research published by members of Stanford HAI will disclose sources of funding and potential conflicts of interest. A list of HAI corporate, institutional, and individual donors will be published annually. If any donors have requested anonymity, this will be noted.\n\n* * *\n\n## Contact Us\n\nFor questions, please contact [HAI-Institute@stanford.edu].",
    "length": 3914,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Letter to NCMEC about AI-CSAM Report Statistics",
    "url": "https://cyberlaw.stanford.edu/letter-to-ncmec-about-ai-csam-report-statistics/",
    "text": "Letter to NCMEC about AI-CSAM Report Statistics\n[![Stanford CIS]] \nThe Center for Internet and Society at Stanford Law School is a leader in the study of the law and policy around the Internet and other emerging technologies.\n****Menu\n[Subscribe] **\n# Letter to NCMEC about AI-CSAM Report Statistics\nBy[Riana Pfefferkorn] onJanuary 29, 2026\nToday, Bloomberg issued a[jaw-dropping report] about the hundreds of thousands of[CyberTipline] reports with a generative AI component that Amazon filed to the[National Center for Missing and Exploited Children] (NCMEC), which operates the tipline, in the first half of 2025. Ever since that number was first disclosed in the[New York Times] last July, I had been wondering what it meant.\nThe standardized CyberTipline reporting form, which is used to report child sex abuse material (CSAM) to NCMEC, contains a checkbox labeled \"Generative AI.\" But what does it mean when an electronic service provider (ESP), such as a social media platform, cloud storage provider, or AI company, checks that box? It varies, as my colleagues and I found in the[research report] we published last May about AI-generated CSAM (AI-CSAM), based on interviews with respondents from AI companies, other ESPs, and NCMEC.\nThe NYT article came out shortly after our research report. It revealed that NCMEC had received \"485,000 reports of A.I.-related CSAM\" from ESPs in the first six months of 2025, of which a whopping 380,000 were from Amazon. (For comparison, in all of 2024, NCMEC had received a total of 67,000 AI-related reports.) I knew from my research that this didn't necessarily indicate 380,000 pieces of AI-generated CSAM, but the NYT article didn't specify what that number*did*mean. Neither did a[blog post] from NCMEC itself a couple months later about what it called the \"alarming increase\" in generative AI-related reports in the first part of 2025.\nSo I wondered and speculated: What the heck was going on at Amazon?!\nI never imagined the truth that Bloomberg[reported] today: Every single one of Amazon's hundreds of thousands of reports flagged*known*CSAM –CSAM that Amazon had discovered by automatically scanning the training data for its AI models to detect hash matches to a database of known CSAM of real victims. According to Amazon, \"None of its reports submitted to NCMEC were of AI-generated material.\" Zero. Out of 380,000. That is, of the nearly half-million reports to NCMEC in the first half of 2025 that had that \"Generative AI\" box checked, at least 78% did not involve any AI-generated CSAM at all.\nThat is*not*the impression you would get from reading that New York Times article, which stated, right in the headline, \"A.I.-Generated Images of Child Sexual Abuse Are Flooding the Internet.\" And so a misunderstanding was born –one that's been perpetuated for months, until two Bloomberg reporters dug into that number and found out the truth.\nFinally, I had my answer. And it so completely flabbergasted me that I wrote a very long email to my contacts at NCMEC, which I'm sharing here.\nToday's bombshell[report] from Bloomberg demonstrates that NCMEC urgently needs to make changes to the CyberTipline reporting form so that there's not just one checkbox that says \"Generative AI.\" This news shows definitively how ambiguous the meaning of checking that checkbox can be, as NCMEC told us when Shelby and I were researching our[AI-CSAM report] last year. What did it mean for Amazon to check that box 380,000 times in H1 2025? Nobody knew (outside of Amazon and maybe NCMEC), until this news article came out.\nConsequently, for 6 months —ever since NCMEC released its H1 2025 numbers in the[New York Times] last July —the public has been under the impression that those 380,000 CyberTips from Amazon represented hundreds of thousands of pieces*of**AI-generated CSAM.*The NYT article's very first words are \"A new flood of child sexual abuse material*created by*artificial intelligence,\" and, later, it calls NCMEC's 485,000 number \"an explosion of*A.I.-produced*child sexual abuse material.\" NCMEC's[blog post] last fall was more carefully worded than the NYT piece, yet nevertheless did not give any breakdown of what the numbers meant or at least explain that a report marked \"Generative AI\" can mean multiple things.\nFor half a year, \"Massive Spike In AI-Generated CSAM\" isthe framing I've seen whenever news reportsmention those H1 2025 numbers. Even the[press release] for a Senate bill about safeguarding AI models from being tainted with CSAM stated, \"According to the National Center for Missing &amp;&amp; Exploited Children,*AI-generated material*has proliferated at an alarming rate in the past year,\" citingthe NYT article.\nNow we find out from Bloomberg that*zero*of Amazon's reports involved AI-generated material;*all 380,000*were hash hits to known CSAM. And we have Fallon [McNulty, executive director of the CyberTipline] confirming to Bloomberg that \"with the exception of Amazon, the AI-related reports [NCMEC] received last year came in 'really, really small volumes.'\"\nThat is an absolutely mindboggling**misunderstanding for everyone —the general public, lawmakers, researchers like me, etc. —to labor under for so long. If Bloomberg hadn't dug into Amazon's numbers, it's not clear to me when, if ever, that misimpression would have been corrected.\nNobody benefits from being so egregiously misinformed. It isn't a basis for sound policymaking (or an accurate assessment of NCMEC's resource needs) if the true volume of AI-generated CSAM being reported is a mere fraction of what Congress and other regulators believe it is. It isn't good for Amazonif people mistakenly think the company's AI products are uniquely prone to generating CSAM compared with other options on the market (such as OpenAI, with its distant-second 75,000 reports during the same time period,[per NYT]). That impression also disserves users trying to pick safe, responsible AI tools to use; in actuality, per today's revelations about training data vetting, Amazon is indeed trying to safeguard its models against CSAM. I can certainly think of at least one other AI company that's been in the news a lot lately that seems to be acting far more carelessly.\nI understand that you fault Amazon for swamping you with non-actionable reports, and I hope that publicly naming and shaming Amazon in Bloomberg will help un-stick whatever frustrating détente you must have encountered in the private discussions I assume you've had with them about their sudden flood of reports; maybe this bad press will induce them to reform their reporting practices. (If Amazon's spokesperson had graciously deigned to go on the record like Fallon did, I would have included them on this email, which you can feel free to forward to them if you like.)\nNevertheless. Even if Amazon's reporting practices leave something to be desired, NCMEC controls the design of the reporting form. The[API documentation] says that the Generative AI file annotation means \"The file contains content that is believed to be Generative Artificial Intelligence.\" But NCMEC has known for at least a year (we interviewed NCMEC in early February 2025) that in practice, ESPs mean a range of things when they check that box. It is up to the ESP to clarify by giving additional information in the free text entry box —which they may not do when filing380,000 (presumably largely automated) reports in 6 months.\nWhen the meaning of checking a single checkbox is so ambiguous that absent additional information, reports of**known CSAM found in AI training data**are facially indistinguishable from reports of new AI-generated material (or of text-only prompts seeking CSAM, or of attempts to upload known CSAM as part of a prompt, etc.), and that ambiguity leads to a months-long massive public misunderstanding about the scale of the AI-CSAM problem, then it is clear that**the CyberTipline reporting form itself needs to change**— not just how one particular ESP fills it out.\nA more granular, nuanced design for reporting generative AI issues might have let Amazon (and in turn NCMEC) provide a clearer picture from the start, which could have preempted months of misunderstanding and changed the course of public discussions about how to regulate AI for child safety.\nIt is up to NCMEC not only to take affirmative steps to correct the widespread confusion about the scale of AI-generated CSAM being reported to the CyberTipline, but also to update the reporting form to avoid similar misunderstandings in the future by making it easy and straightforward for ESPs to submit a clear, unambiguous account of the specific role played by AI in any given report.\nThat is easier said than done, to be sure. But after today's staggering Bloomberg news, it can no longer be avoided. Indeed, having seen for myself how responsive NCMEC is to emergent trends, I can only assume that reforms to the \"Generative AI\" component of the CyberTipline reporting form are already underway. I look forward to seeing the result, and I am happy to be of assistance however I can.\n**UPDATE 1/30/26:**I got a prompt response from one of my NCMEC contacts, which made four main points: (1) NCMEC is indeed in an early stage of a multi-year tech modernization process for the CyberTipline API and reporting systems to “support more granular and clearer reporting,” but since (2) it’s almost entirely up to ESPs what information they choose to report ([true!]), that means (3) no matter how robust the API, the reports are only as good as the info ESPs elect to provide, and (4) all those Amazon reports included minimal data, not even the file in question or the hash value, much less other contextual information about where or how Amazon detected the matching file. (Big[Steamed Ham] vibes, with Amazon as Principal Skinner.) “They choose not to give detail,” NCMEC’s email to me said. (Note that the[CyberTipline reporting API] also doesn’t have a specific field for entering a PhotoDNA match ID, perceptual hash, etc.; hopefully that’s also part of the update roadmap, along with more granular AI options!)\nThe email also noted the Bloomberg article’s statement that “Amazon believes it over-reported these cases to NCMEC to avoid accidentally missing something,” quoting an Amazon spokesperson as saying, “We intentionally use an over-inclusive threshold for scanning, which yields a high percentage of false positives.” This false-positives revelation didn’t really make it into my original email, but it’s also astonishing: Not only did zero of those 380,000 Amazon reports involve AI-generated CSAM, it seems many of them didn’t involve CSAM at all. (When scanning files for CSAM, false matches can happen when the match threshold is set sufficiently low; at such high volume, Amazon’s reports were likely automated, without human review to catch false positives.) NCMEC said they only learned about this false-positives issue last week and are very frustrated by it.\nAmazon certainly deserves a lot of the heat here. There’s no question about that. NCMEC absolutely should improve the design of the CyberTipline reporting form, but Amazon also shouldn’t knowingly flood NCMEC with hundreds of thousands of reports that are useless at best and false positives at worst.\nStill, why wasn’t it clear way, way sooner what was going on? Here’s all Amazon needed to say, back when they began their training data vetting effort: “The high volume of automated reports we’ve been submitting lately are all from scanning AI training data for hash matches to known CSAM, with an over-inclusive threshold that we picked in the interest of caution but that’s probably meant plenty of false positives. To our knowledge, nothing we’ve reported is AI-generated material.” This is basic information that seems like it should have been known or knowable by Amazon, and thus shareable with NCMEC, from the very start. So*why did it take so long*for that information to come out, and why did it take a Bloomberg investigation to do it?\nIn all of[2024], Amazon AI Services (which has its own reporting flow to NCMEC and is broken out in NCMEC’s numbers from Amazon and Amazon Photos) filed 30,759 CyberTipline reports total. That is, on average, NCMEC went from receiving 84 reports per day to over 2,000 per day. Did NCMEC ever call up Amazon to ask*why*Amazon was suddenly submitting 25x more reports, and what those reports meant? Surely they must have had some talks about this, right? I said as much as the end of my TrustCon talk, which was attended by at least two NCMEC personnel, shortly after the NYT article came out last July: Amazon’s generative AI-related report numbers made it clearly an outlier, but as a big mainstream company, hopefully they’d be willing to come to the table with NCMEC to discuss whether its products were generating AI-CSAM and whether its CyberTipline reporting practices were feasible.\nIt seems like what we’ve got here is failure to communicate. But what kind of failure? Did NCMEC never ask Amazon what was up, despite suddenly getting flooded with reports that were apparently maddeningly thin on detail? Did NCMEC instead just assume —along with everyone else, from journalists to senators —that if the reports came from Amazon AI Services, they must be a massive spike in AI-CSAM (when in fact zero of it was AI-CSAM)? That seems unlikely. So, did NCMEC try to press Amazon? Did Amazon simply stonewall NCMEC’s inquiries? If they did, why would they tell a couple of journalists what they wouldn’t tell NCMEC itself?\nFrankly, neither organization comes out smelling of roses here. Amazon looks irresponsible and heedless of the burden it’s been putting on NCMEC, and it’s embarrassing for NCMEC to have to admit, after so much hullabaloo since July, that there never was a massive increase in AI-CSAM. In reality,**nearly 80% of all \"Generative AI\" CyberTipline reports to NCMEC in H1 2025 involved no AI-generated CSAM at all.**But that’s also the silver lining: Amazon’s AI models apparently aren’t a CSAM generation machine after all, and in fact AI-generated CSAM reports are very low-volume. That’s**really good news.**\nI hope that this Bloomberg exposé will cause both Amazon and NCMEC to wipe the egg off their face, take a long look at what’s not working —with their respective internal processes, and with the dynamic between them —and start making some positive changes.\n[] •\n## [] \n**",
    "length": 14411,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "",
    "url": "https://reglab.stanford.edu/publications-topics/policy/",
    "text": "# Policy\n\n[Policy] \n\n### [Considerations for governing open foundation models] \n\nScience\n\nOctober 2024\n\n[Policy] \n\n### [Assessing the Implementation of Federal AI Leadership and Compliance Mandates] \n\nHAI White Paper\n\nJanuary 2025\n\n[Policy] \n\n### [AI for Scaling Legal Reform: Mapping and Redacting Racial Covenants in Santa Clara County] \n\nJournal of Legal Analysis\n\nOctober 2024\n\n[Compliance], [Policy] \n\n### [Remote sensing and computer vision for marine aquaculture] \n\nScience Advances\n\nOctober 2024\n\n[Policy] \n\n### [Potential for allocative harm in an environmental justice data tool] \n\nNature Machine Intelligence\n\nDecember 2023\n\n[Policy] \n\n### [Governing by Assignment] \n\nPenn Law Review\n\nFebruary 2024\n\n[Methods], [Policy] \n\n### [Not (Officially) in My Backyard: Characterizing Informal Accessory Dwelling Units and Informing Housing Policy With Remote Sensing] \n\nJournal of the American Planning Association\n\nJune 2024\n\n[Methods], [Policy] \n\n### [On the Societal Impact of Open Foundation Models] \n\nFebruary 2024\n\n[Policy] \n\n### [Estimating and Implementing Conventional Fairness Metrics With Probabilistic Protected Features] \n\nConference on Secure and Trustworthy Machine Learning\n\nOctober 2023\n\n[Policy] \n\n### [How Redundant are Redundant Encodings? Blindness in the Wild and Racial Disparity when Race is Unobserved] \n\nACM FAccT\n\nJune 2023\n\n[Policy] \n\n### [The AI Regulatory Alignment Problem] \n\nHAI Policy Brief\n\nNovember 2023\n\n[Policy] \n\n### [Talent exchanges for state governments] \n\nSIEPR Policy Brief\n\nSeptember 2023\n\n[Policy] \n\n### [The Privacy-Bias Tradeoff: Data Minimization and Racial Disparity Assessments in U.S. Government] \n\nFAccT\n\nJune 2023\n\n[Policy] \n\n### [The Bureaucratic Challenge to AI Governance: An Empirical Assessment of Implementation at U.S. Federal Agencies] \n\nAIES\n\nAugust 2023\n\n[Policy] \n\n### [Integrating social services with disease investigation: A randomized trial of COVID-19 high-touch contact tracing] \n\nPLOS ONE\n\nMay 2023\n\n[Policy] \n\n### [Measuring and Mitigating Racial Disparities in Tax Audits] \n\nWorking Paper\n\nJanuary 2023\n\n[Policy] \n\n### [Implementation Challenges to Three Pillars of America’s AI Strategy] \n\nHAI - RegLab White Paper\n\nDecember 2022\n\n[Policy] \n\n### [Entropy Regularization for Population Estimation] \n\nACM AAAI (2023)\n\nAugust 2022\n\n[Policy] \n\n### [Automated vs. manual case investigation and contact tracing for pandemic surveillance: Evidence from a stepped wedge cluster randomized trial] \n\nLancet: eClinicalMedicine\n\nJanuary 2023\n\n[Policy] \n\n### [How to Build Academic-Public Health Partnerships: The Stanford – Santa Clara County Experience with COVID-19 Response] \n\nChapter for Brookings' Build Me the Evidence, (Tamar Bauer ed)\n\nJuly 2022\n\n[Policy] \n\n### [Algorithmic Fairness and Vertical Equity: Income Fairness with IRS Tax Audit Models] \n\nACM FAccT 1479\n\nJune 2022\n\n[Policy] \n\n### [Executive Control of Agency Adjudication: Capacity, Selection and Precedential Rulemaking] \n\nJournal of Law, Economics, and Organization (2024, Forthcoming)\n\nApril 2021\n\n[Policy] \n\n### [Integrating Reward Maximization and Population Estimation: Sequential Decision-Making for Internal Revenue Service Audit Selection] \n\nACM AAAI (2023)\n\nApril 2022\n\n[Policy] \n\n### [Outsider Oversight: Designing a Third Party Audit Ecosystem for AI Governance] \n\nACM AI, Ethics & Society\n\nAugust 2022\n\n[Policy] \n\n### [Designing Accountable Health Care Algorithms: Lessons from Covid-19 Contact Tracing] \n\nNEJM Catalyst\n\nMarch 2022\n\n[Policy] \n\n### [Science Translation During the COVID-19 Pandemic: An Academic-Public Health Partnership to Assess Capacity Limits in California] \n\nAJPH\n\nFebruary 2022\n\n[Policy] \n\n### [A Language-Matching Model to Improve Equity and Efficiency of COVID-19 Contact Tracing] \n\n118 PNAS e2109443118\n\nSeptember 2021\n\n[Policy] \n\n### [Building a National AI Research Resource: A Blueprint for the National Research Cloud] \n\nHAI White Paper\n\nOctober 2021\n\n[Compliance], [Policy] \n\n### [Improving the Reliability of Food Safety Disclosure: Restaurant Grading in Seattle and King County, Washington] \n\nJournal of Environmental Health, 84, 2, 30-37\n\nSeptember 2019\n\n[Policy] \n\n### [Evaluation of Allocation Schemes of COVID-19 Testing Resources in a Community-Based Door-to-Door Testing Program] \n\nJAMA Health Forum , 2, 8, e212260\n\nAugust 2021\n\n[Methods], [Policy] \n\n### [On the Opportunities and Risks of Foundation Models] \n\narXiv\n\nAugust 2021\n\n[Policy] \n\n### [How medical AI devices are evaluated: limitations and recommendations from an analysis of FDA approvals] \n\nNature Medicine\n\nApril 2021\n\n[Compliance], [Policy] \n\n### [The Distributive Effects of Risk Prediction in Environmental Compliance: Algorithmic Design, Environmental Justice, and Public Policy] \n\nACM FAccT 2021\n\nMarch 2021\n\n[Policy] \n\n### [How US Law Will Evaluate Artificial Intelligence for Covid-19] \n\nBMJ\n\nMarch 2021\n\n[Policy] \n\n### [Leveraging Administrative Data for Bias Audits: Assessing Disparate Coverage with Mobility Data for COVID-19 Policy] \n\nACM FAccT 2021\n\nMarch 2021\n\n[Policy] \n\n### [Evaluating Facial Recognition Technology: A Protocol for Performance Assessment in New Domains] \n\nStanford's Institute for Human-centered Artificial Intelligence White Paper\n\nNovember 2020\n\n[Policy] \n\n### [Affirmative Algorithms: The Legal Grounds for Fairness as Awareness] \n\nU. Chicago Law Review Online\n\nOctober 2020\n\n[Policy] \n\n### [AI’s Promise and Peril for the U.S. Government] \n\nHAI Policy Briefs\n\nSeptember 2020\n\n[Policy] \n\n### [Designing Information Disclosure] \n\nAdministrative and Regulatory Law News\n\nDecember 2012\n\n[Policy] \n\n### [Improve Restaurant Report Cards] \n\nNew York Times\n\nMarch 2012\n\n[Policy] \n\n### [Artificially Intelligent Government: A Review and Agenda] \n\nBig Data Law (Roland Vogl, ed., 2021)\n\nMarch 2020\n\n[Policy] \n\n### [Algorithmic Accountability in the Administrative State] \n\nYale Journal on Regulation\n\nAugust 2020\n\n[Policy] \n\n### [Fudging the Nudge: Information Disclosure and Restaurant Grading] \n\nThe Yale Law Journal\n\nDecember 2012\n\n[Policy] \n\n### [What To Do About Artificially Intelligent Government] \n\nThe Hill\n\nFebruary 2020\n\n[Policy] \n\n### [Government by Algorithm: Artificial Intelligence in Federal Administrative Agencies] \n\nReport to the Administrative Conference of the United States\n\nFebruary 2020\n\n[Policy] \n\n### [Did Restaurant Hygiene Grading in Los Angeles Immediately Reduce Foodborne Illness by 20% Across All of Southern California? A Response to Jin & Leslie] \n\nSIEPR Working Paper\n\nNovember 2019\n\n[Methods], [Compliance], [Policy] \n\n### [Making Street-Level Bureaucracy Work: Safer Food in Seattle and King County] \n\nEvidence Works: Cases Where Evidence Meaningfully Informed Policy\n\nJanuary 2019\n\n[Methods], [Compliance], [Policy] \n\n### [Feasible Policy Evaluation by Design: A Randomized Synthetic Stepped-Wedge Trial in King County] \n\nJune 2020\n\n[Policy] \n\n### [When Algorithms Import Private Bias into Public Enforcement: The Promise and Limitations of Statistical Debiasing Solutions] \n\nJournal of Institutional and Theoretical Economics\n\nJanuary 2019\n\n[Policy] \n\n### [New Evidence on Information Disclosure through Restaurant Hygiene Grading] \n\nAmerican Economic Journal: Economic Policy\n\nNovember 2019\n\n[Policy] \n\n### [Equity in the Bureaucracy] \n\nUC Irvine Law Review\n\nJune 2019\n\n### featured projects\n\n[Compliance] \n\n[Tax Enforcement] \n\n[See All projects]",
    "length": 7363,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Policy & Guidance",
    "url": "https://teachingcommons.stanford.edu/policy-guidance",
    "text": "Policy &amp; Guidance | Teaching Commons\n[Skip to main content] [Skip to secondary navigation] \n[Stanford University(link is external)] \n[\nStanford\nTeaching Commons\n] \nSearch this siteSubmit Search\n![Pathways at the Stanford quad] \n## Policy &amp; Guidance\nHere you will find links and guidance related to various campus policies.\n# Policy &amp; Guidance\nMain content start\n![] \n## Handling classroom disruption\nStanford University\nGuidance and links to policy to help instructors handle disruptions to instructional activities in the classroom.\n[Campus disruptions policy statement] \n[Guidance for instructors on handling classroom disruptions] \n![Student with mask] \n## Emergencies, disasters, and other disruptions\nTeaching Commons\nFind out the latest policies and guidance related to public health emergencies, natural disasters, current events, and other disruptions to teaching and learning.\n[Resources for emergencies and disruptions] \n![Stanford basketball players in a huddle] \n## Student-Athletes Policy and Guidance\nAthletic Academic Resource Center\nGuidance on and links to universitypolicy information, andcontacts for questions about student-athlete academic matters.\n[Guidance on student-athlete matters] \n![Veterans Ritchie Garcia Jr. and Destiny Goddu work together during a small group activity during class.] \n## Disability accommodations\nOffice of Accessible Education\nComprehensive information about policies related to academic accommodations for students with disabilities is provided by the Office of Accessible Education (OAE).\n[OAE faculty &amp; staff resources] \n![Wallenberg teaching setup] \n## Hybrid instruction\nTeaching Commons\nLearn about how the university defines hybrid and distance education, and recommendations for instructors interested in offering a hybrid course.\n[What is a hybrid course?] \n![] \n## Academic integrity\nOffice of Community Standards\nInformation about policies relating to academic integrity, the Honor Code, Fundamental Standard, and the student accountability process is provided by the Office of Community Standards (OCS).\n[Policies &amp; guidance from OCS] \n![In a lecture hall, students are sitting with their laptops out and listening to the speaker. ] \n## Recording courses and lectures\nTeaching Commons\nGuidance for recording lectures and course meetings.\n[Guidance on recording classes] \n![] \n## Generative AI tools\nOffice of Community Standards\nThe Board on Judicial Affairs (BJA) has been asked to address the Honor Code implications of generative AI tools such as ChatGPT, Bard, DALL-E, and Stable Diffusion.\n[Generative AI Policy Guidance] \n![] \n## Copyright and fair use for educational purposes\nStanford Libraries\nThis guidance about getting permission to use copyrighted works for academic purposes is provided by Stanford University Libraries.\n[Copyright &amp; Fair Use site] \n![Woman sitting in a secluded working area] \n## Educational records privacy\nUniversity Privacy Office\nPolicies and guidelines regarding privacy for educational records related to FERPA.\n[University Privacy Office] \n![Students dancing] \n## Academic Policies\nStanford Bulletin\nThe Stanford Bulletin is a central source for policies –both academic and non-academic –that are applicable to all Stanford students. Includes standard course meeting patterns, policy regarding academic grievances, exam policies, leaves of absence, and so on.\n[Academic Policies] \n![Molly Antopol is a recent Wallace Stegner Fellow in fiction and current Draper Lecturer in Creative Writing. Her debut story collection is The UnAmericans.] \n## New to Stanford FAQ\nTeaching Commons\nAn FAQ for faculty, lecturers, and instructors who are new to teaching at Stanford.\n[New to teaching at Stanford] \n![] \n## Faculty and Teaching Staff Handbooks\nStanford University\nThe handbooks provide policies governing faculty and teaching staff.\n[Faculty Handbook] \n[Academic Teaching Staff Handbook] \n[School of Medicine Faculty Handbook] \n![] \n## Administrative Guide\nStanford University\nThe Administrative Guide is Stanford's collection of guidelines that govern workplace interactions, approaches, procedures, and processes.\n[Administrative Guide] \n![Students work on laptops at the library tables at the Robin Li and Melissa Ma Science Library in the Sapp Center for Science Teaching and Learning.] \n## Graduate Academic Policies and Procedures\nGraduate Education\nThe Graduate Academic Policies and Procedures handbook (the GAP) is a compilation of university policies and other information related to the academic progress of Stanford graduate students.\n[Graduate Academic Policies] \n![Decorative image: electronics computer board] \n## Technology tools and academic integrity\nTeaching Commons\nDiscussion about technology tools for plagiarism detection and online proctoring.\n[Guidance on technology tools and academic integrity] \n![] \n## Postdoctoral Scholars\nOffice of Postdoctoral Affairs\nThese policies govern postdoctoral scholars, who are trainees in residence pursuing advanced studies beyond the doctoral level in preparation for an independent career.\n[Postdoctoral Scholar Policy] \nBack to Top",
    "length": 5117,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "A Path for Science‑ and Evidence‑based AI Policy | Stanford Law School",
    "url": "https://law.stanford.edu/publications/a-path-for-science%E2%80%91-and-evidence%E2%80%91based-ai-policy/",
    "text": "A Path for Science‑ and Evidence‑based AI Policy - Blog Postings - Stanford Law School\nlogo-footerlogo-fulllogo-stanford-universitylogomenu-closemenuClose IconIcon with an X to denote closing.Play IconPlay icon in a circular border.\n[Skip to main content] \n![Sls logo] \n# A Path for Science‑ and Evidence‑based AI Policy\n### Abstract\nAI is a powerful technology that carries both benefits and risks. We wish to promote innovation to ensure its potential benefits are responsibly realized and widely shared, while simultaneously ensuring that current and potential societal risks are mitigated. To address the growing societal impact of AI, many jurisdictions are pursuing policymaking. The AI research and policy community lacks consensus on the evidence base relevant for effective policymaking, as has been seen with the debates over California’s Safe and Secure Innovation for Frontier Artificial Intelligence Models Act (California’s SB-1047). Points of contention include disagreement about what risks should be prioritized, if or when they will materialize, and who should be responsible for addressing these risks.\n### Details\nAuthor(s):\n* Rishi Bommasani\n* Sanjeev Arora\n* Yejin Choi\n* Sanmi Koyejo\n* [Daniel E. Ho] \n* Dan Jurafsky\n* Hima Lakkaraju\n* Arvind Narayanan\n* Alondra Nelson\n* Emma Pierson\n* Joelle Pineau\n* Gaël Varoquaux\n* Suresh Venkatasubramanian\n* Ion Stoica\n* Percy Liang\n* Dawn Song\nPublish Date:2024Format:Blog PostingsCitation(s):\n* Rishi Bommasani, Sanjeev Arora, Yejin Choi, Daniel E. Ho, Dan Jurafsky, Sanmi Koyejo, Hima Lakkaraju, Fei‑Fei Li, Arvind Narayanan, Alondra Nelson, Emma Pierson, Joelle Pineau, Gaël Varoquaux, Suresh Venkatasubramanian, Ion Stoica, Percy Liang &amp; Dawn Song,*A Path for Science‑ and Evidence‑based AI Policy*,Understanding AI Safety(2024), https://understanding-ai-safety.org/.\n### Other Publications By\n[Daniel E. Ho] \n**Back to the Top",
    "length": 1899,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Human Rights in the AI Supply Chain - Journal Article - Stanford Law School",
    "url": "https://law.stanford.edu/publications/human-rights-in-the-ai-supply-chain/",
    "text": "Human Rights in the AI Supply Chain - Journal Article - Stanford Law School\nlogo-footerlogo-fulllogo-stanford-universitylogomenu-closemenuClose IconIcon with an X to denote closing.Play IconPlay icon in a circular border.\n[Skip to main content] \n![Sls logo] \n# Human Rights in the AI Supply Chain\n* December 31, 2025\n* By\n* Jasper D.C. Johnston\n* Pages108-151\n* [**Share on Twitter] \n* [**Share on Facebook] \n### Abstract\nArtificial intelligence (AI) has taken the world by storm since the launch of ChatGPT in November 2022, with some heralding it as the most significant technology of the century. Counterbalancing excitement for AI’s revolutionary potential, experts from fields as diverse as computer science, sociology, and global health are increasingly expressing their concerns regarding the serious risks AI may pose. While much of this attention focuses on downstream harms associated with AI’s use, comparatively less scrutiny has been given to human rights violations and environmental harms arising from the upstream processes and materials necessary for AI models’ functioning. This Note delves into these upstream harms and, drawing on the concept of the AI supply chain, assesses the ability of existing supply chain due diligence (SCDD) laws to regulate AI companies. Analyzing over a dozen enacted and pending laws from around the world, it argues that while some existing SCDD legislation applies to AI companies, the global legal landscape contains notable gaps that may enable human rights violations to remain unaddressed. The Note concludes with a discussion of proposed solutions and their limitations. Among these, lawmakers should amend or enact legislation to more clearly regulate AI supply chains, while AI companies should proactively self-regulate.\n### Details\nPublisher:Stanford UniversityStanford, CaliforniaCitation(s):\n* Jasper D.C. Johnston,*Human Rights in the AI Supply Chain*, 29 Stan. Tech. L. Rev. 108 (2025).Related Organization(s):\n* [Stanford Technology Law Review] \nAttachment(s):[**Download Now] \n**Back to the Top",
    "length": 2060,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Let Your Cre-AI-tivity Loose on the AI Playground",
    "url": "https://uit.stanford.edu/news/let-your-cre-ai-tivity-loose-ai-playground",
    "text": "[Skip to main content] \n\n# Let Your Cre-AI-tivity Loose on the AI Playground\n\nStanford’s AI Playground offers a unique way to explore AI models and tools like ChatGPT, DALL-E, and more\n\nSeptember 16, 2024\n\n× **Update:** As of Oct. 1, all Stanford faculty, staff, students, postdocs, and visiting scholars are able to access the AI Playground pilot.\n\nAs the world of AI tools and applications continues to expand, you may be wondering, “How can I start exploring AI tools?”\n\nUniversity IT (UIT) is excited to share the new [Stanford AI Playground] pilot. This service gives Stanford faculty and staff an easy way to experiment with and compare some of the top AI technologies.\n\nBased on feedback we receive from faculty and staff during this pilot phase, we will consider making the playground available to students later in the fall. Please note: The pilot duration could change or conclude based on the feedback we receive.\n\n## What is the AI Playground?\n\nThe AI Playground is a new platform built on open-source technology that provides access to various large language models (LLMs). The AI Playground is a place to learn and experiment.  The AI Playground is not approved for high-risk data.  Be sure to adhere to Stanford’s [information security and privacy policies] when using the Playground.\n\n## What can you do with the AI Playground?\n\nThe AI Playground is a Stanford-hosted, user-friendly platform to explore and experiment. The playground provides OpenAI’s ChatGPT, Google Gemini, Anthropic Claude, Meta’s Llama, DALL-E-3, Wolfram for computation and mathematics, Azure Assistants for data and coding analysis, and more.\n\n## Why use the AI Playground, and how does it work?\n\nThe AI Playground provides Stanford faculty and staff a unique environment to learn more about how leading AI technologies work. You can compare different commercially-available LLMs, without having to sign up for each one separately. In addition, you can move information between LLMs from different vendors. By hosting these tools on one platform, the AI Playground allows you to explore AI technologies in one place.\n\nThe AI Playground doesn’t offer every feature available directly from a vendor, and you may consider paying or subscribing to an AI tool in certain cases (for example, if you want AI integrated into another tool, such as Microsoft 365 Copilot).\n\nHowever, new features are being added to the playground on a regular basis, and we encourage feedback from our Stanford community.\n\n### Quick start\n\nTo start, visit [aiplayground.stanford.edu] and log in with your Stanford credentials. On your first visit, you may see a one-time user settings screen to personalize your experience.\n\nYou can begin right away by typing a prompt (or using the microphone feature to speak your request) into the message field near the bottom center of your screen.\n\n### Choose activities\n\nIf you’re not quite sure where to start, here are a few types of tasks you might try:\n\n- Summarize large amounts of information\n- Discover insights and trends in information\n- Make recommendations, such as professional development and career planning\n- Improve existing documents and artifacts\n- Skills assessment\n\nAnother fun approach: Try asking the LLM how to best structure a prompt around the question you are trying to answer and let the LLM tool guide you in your approach.\n\nThen, to go deeper, you can explore the options at the top of the page, which allow you to adjust settings and switch among various LLMs. For a detailed guide of the many options in the AI Playground, visit the [AI Playground Quick Start page].\n\n## What’s next, and what should I look out for?\n\nThe AI Playground is continually evolving, and we anticipate adding features regularly. Recently, UIT added LLM plugins for image generation, web scraping, and AI-assisted Google searches. Some upcoming features include additional AI Assistants for Stanford-specific content and improved analysis capabilities.\n\nAs the AI Playground is in its pilot phase, you might experience some quirky behavior or occasional outages during evening updates. Please remember the playground is not currently approved for high-risk data.\n\n## Additional information and providing feedback\n\n- UIT’s [GenAI Prompt Guide] and [GenAI Use Cases for Experimenting] pages provide more specific guidance.\n- To ask general questions and brainstorm with peers, join the Slack channel #ai-playground.\n- For technical questions or support, send a [Help request].\n- Share your feedback with the project team by using the [feedback form].\n\nThis image was generated with the prompt, “Create an image to use with an article about unlocking creativity with ai tools” using the Stanford AI Playground with the DALL-E-3 plugin. Keep in mind that this image is uniquely generated by the LLM based on real-time context and other factors. Using the same prompt, you might receive a different result.\n\n[Share Feedback] \n\n_DISCLAIMER: UIT News is accurate on the publication date. We do not update information in past news items. We do make every effort to keep our service information pages up-to-date. Please search our service pages at [uit.stanford.edu/search]._\n\n## What to read next:\n\n[Information Security] \n\n## Smartsheet High-Risk Data Deadlines: What You Need to Know\n\n[Learn more about Smartsheet High-Risk Data Deadlines: What You Need to Know] \n\n[Service Spotlights] \n\n## Now Available: Google Gemini, NotebookLM, and Microsoft Copilot Chat\n\n[Learn more about Now Available: Google Gemini, NotebookLM, and Microsoft Copilot Chat] \n\n[Service Spotlights] \n\n## Easier, More Reliable Printing—Thanks to You\n\n[Learn more about Easier, More Reliable Printing—Thanks to You] \n\n[Go to Newsroom] \n\n- [Stanford Home] \n- [Maps & Directions] \n- [Search Stanford] \n- [Emergency Info] \n\n- [Terms of Use] \n- [Privacy] \n- [Copyright] \n- [Trademarks] \n- [Non-Discrimination] \n- [Accessibility] \n\n©CopyrightStanford University.\nStanford,\nCalifornia94305.",
    "length": 5957,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Policy Publications | Stanford HAI",
    "url": "https://hai.stanford.edu/policy/publications",
    "text": "Policy Publications | Stanford HAI\n### Stay Up To Date\nGet the latest news, advances in research, policy work, and education program updates from HAI in your inbox weekly.\n[\n**Sign Up**For Latest News\n] \n[] \n* [] \n* [] \n* [] \n* [] \n* [] \n* [] \n###### Navigate\n* [\nAbout\n] \n* [\nEvents\n] \n* [\nCareers\n] \n* [\nSearch\n] \n###### Participate\n* [\nGet Involved\n] \n* [\nSupport HAI\n] \n* [\nContact Us\n] \n[Skip to content] \n[] \n[] \n* [] \n* [] \n* [] \n* [] \n* [] \n* [] \n[] \n# Policy Publications\nShow Results\nBy Publication Type\nResponse to Request\nPolicy Brief\nTestimony\nIssue Brief\nWhite Paper\nExplainer\nBy Topic\nArts, Humanities\nAutomation\nCommunications, Media\nComputer Vision\nDemocracy\nDesign, Human-Computer Interaction\nEconomy, Markets\nEducation, Skills\nEnergy, Environment\nEthics, Equity, Inclusion\nFinance, Business\nFoundation Models\nGenerative AI\nGovernment, Public Administration\nHealthcare\nHuman Reasoning\nIndustry, Innovation\nInternational Affairs, International Security, International Development\nLaw Enforcement and Justice\nMachine Learning\nNatural Language Processing\nPrivacy, Safety, Security\nRegulation, Policy, Governance\nRobotics\nSciences (Social, Health, Biological, Physical)\nWorkforce, Labor",
    "length": 1200,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Artificial Intelligence Teaching Guide",
    "url": "https://teachingcommons.stanford.edu/teaching-guides/artificial-intelligence-teaching-guide",
    "text": "Artificial Intelligence Teaching Guide | Teaching Commons\n[Skip to main content] [Skip to secondary navigation] \n[Stanford University(link is external)] \n[\nStanford\nTeaching Commons\n] \nSearch this siteSubmit Search\n# Artificial Intelligence Teaching Guide\n## Main navigation\n[Skip Secondary Navigation] \nMain content start\nArtificial intelligence technology has become increasingly sophisticated and readily available. We believe that educators can contribute to how this important technology is understood and used. We invite you to engage thoughtfully and attentively with this teaching guide as a way to learn about and positively influence the dialogue around artificial intelligence in education.\n## For instructors and teaching teams\nWe offer this guide to all instructors and teaching teams approaching the topic of generative AI tools in education, whether for the first time or as part of your ongoing engagement with the topic, in response to practical concerns that we heard from instructors like yourself. You don't need to be an expert or have prior experience with generative AI to use this resource, though you should have some understanding of or experience with teaching and learning in higher education contexts. We intend this guide to apply to any disciplinary area or teaching modality and to help you structure the work of integrating AI tools into your teaching practice.\n## Goals and scope of this guide\nThe goal of this guide is to help you make informed and intentional decisions as you navigate AI technology in your teaching practice and courses. We cannot comprehensively address the complex topic of artificial intelligence in any short guide. Many campus service providers, such as University Information Technology (UIT), Stanford Accelerator for Learning, and the Institute for Human-Centered Artificial Intelligence (HAI), have developed excellent resources that offer insight into AI in terms of technical aspects, innovative new tools, societal impacts, AI research, and so on. We have chosen to focus on the practical and pedagogical aspects of AI tools in the classroom. We will focus on generative AI chatbots in particular, but you may find the content here can also apply to other generative AI tools, such as image, media, or code generators.\n## Multiple instructional modules for flexibility\nEach page of this guide contains one instructional module, including content, practice tasks, and assessment activities. We suggest that you complete the activities and suggested readings in each section as a self-directed online lesson. We designed each module as a discrete and complete lesson that you can finish in a relatively short amount of time. You can work through the modules in any order. We encourage you to engage fully with each module, completing the recommended activities to reinforce your learning.\nEach module has specific goals and objectives:\n1. **Understanding AI literacy**—Examine a framework that identifies and organizes skills and knowledge useful for navigating generative AI.\n2. **Defining AI and chatbots**—Define common concepts and explain how AI tools work\n3. **Exploring the pedagogical uses of AI chatbots**—Explore educational use cases, describe risks, and access and practice using chatbots.\n4. **Analyzing the implications for your course**—Describe campus AI policy guidance, evaluate, and analyze your course\n5. **Creating your course policy on AI**—Draft a course policy on AI use for your syllabus\n6. **Integrating AI into assignments**—Examine ways to integrate AI tools into assignments and activities used to assess student learning## Complete the modules with your colleagues\nLearning together with others can deepen the learning experience. We encourage you to organize your colleagues to complete these modules together.\n## Facilitate an AI workshop based on these modules\nStanford's Center for Teaching and Learning has also developed[do-it-yourself workshop kits] inspired by these modules. The kits expand upon the topics and strategies covered in these modules. Each workshop kit typically contains a resource list, sample agenda, promotional materials, slide presentation, facilitator's notes, key strategies, and an evaluation tool to assess learning and gather feedback.\n* [Exploring Pedagogic Uses of AI Workshop Kit] \n* [Analyzing the Implications of AI Workshop Kit] \n* [Creating an AI Course Policy Workshop Kit] \n* [Integrating AI into Assignments Workshop Kit] ## Complete the self-paced Canvas resource on critical AI literacy\nStanford's Center for Teaching and Learning has also developed a[Critical AI Literacy for Instructors] self-paced, professional development resource available in Canvas. It is designed for instructors new to generative AI (genAI) technology and aims to provide foundational knowledge and skills to critically and effectively navigate genAI in teaching and learning contexts.\n## How AI was used in the creation of this guide\nWe did not copy and paste any language generated by AI chatbots into this guide. We used AI chatbots, primarily ChatGPT, to generate feedback on the clarity and structure of some of the writing and to clean up some text formatting. We used ChatGPT and other chatbots more extensively in the development of the module \"Exploring the pedagogical uses of AI chatbots\" to mimic how we thought instructors and students might use them in a course and to better understand the pedagogical potential and challenges of such tools.\n## Authors and acknowledgments\nWe are a team of support staff from different parts of the Office of the Vice Provost for Undergraduate Education (VPUE). Our team created the guide in the summer of 2023 through the collective effort of dedicated colleagues from across the university. We want to thank the following people who contributed to this resource.\n* Kenji Ikemoto (Center for Teaching and Learning) for leading the project team and being the primary author of this guide.\n* Marvin Diogenes (Program for Writing and Rhetoric) for brainstorming, editorial feedback, and sage advice on the meaning of language.\n* Sarah Pickett and Kritika Yegnashankaran (CTL) for their pedagogical expertise and teacherly support.\n* Carlos Seligo (CTL), Josh Weiss (GSE), and Andy Saltarelli (VPSA) for being thought partners and providing the initial inspiration for this guide.\n* Laura Otero (California State University Monterey Bay) for sharing her exemplary AI in education Canvas course.\n* John Mitchell (Computer Science) and Glenn Fajardo (d.school) for leading the[Seminar on Generative AI and Education].\n* Merve Tekgurler (History) and Patrick Young (Computer Science) for sharing their teaching experience with us.\n* Anyone who recommended an article, had a random hallway conversation, or cheered us on.\nYou may adapt, remix, or enhance these modules for your own needs. This guide is licensed under[Creative Commons BY-NC-SA 4.0] (attribution, non-commercial, share-alike) and should be attributed to Stanford Teaching Commons. If you have any questions, contact us at[TeachingCommons@stanford.edu].\nThank you!\n## Understanding AI literacy\nPreview of the first module\nAn AI literacy framework that identifies and organizes skills and knowledge useful for navigating generative AI in education.\n[Go to the first module] \nBack to Top",
    "length": 7306,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "Strategies for Assigning AI Use",
    "url": "https://ctl.stanford.edu/strategies-assigning-ai-use",
    "text": "Strategies for Assigning AI Use | Center for Teaching and Learning\n[Skip to main content] [Skip to secondary navigation] \n[Stanford University(link is external)] \n[\nStanford\nCenter forTeaching and Learning\n] \nSearch this siteSubmit Search\n![AI-Assigned] \n# Strategies for Assigning AI Use\nMain content start\nWhile the[AIMES Library of Examples] allows you to filter and explore real course policies, assignments, and other teaching artifacts that actively assign AI use, this page provides a brief overview of strategies. AI use in higher education is a rapidly evolving domain with new research emerging frequently. We do not yet have an extensive body of evidence to draw from, but the strategies included here are consistent with broader educational research findings, as well as having been found useful by instructors at Stanford. Please consider them “emerging practices” rather than “best practices.”\n## General Approaches to Assigning AI Use\n### Increase transparency and motivation:\nShare with students why you are asking them to use AI in this course or assignment. What skills do you want them to develop? How will those skills serve them in their studies, life, and work?\nRevisit these reasons throughout the course–for example, in the context of specific course learning goals, class activities, assignments, and assessments. Check in with students about how they are experiencing assigned AI use in their learning.\nGuide students on how to cite AI use. Some instructors require a disclosure with every assignment; it can also include metacognition about how and why the students approached their work on the assignment and what kinds of tools were most and least helpful.\nNormalize cognitive effort and struggle, which are crucial for learning, as well as places where AI may assist students in deep learning, and how to tell the difference between uses of AI that support their learning and uses of AI that shortchange their learning.\nEmphasize students’ progress on how effectively they use AI through individual feedback and discussion with the whole class (e.g., feedback on prompting, documentation, refinement and verification of outputs).\n### Emphasize alignment and process:\nEnsure that course policies are clear and aligned with the goals and reasons you have for students to use AI in the course.\nRequire that students show their process with using AI in the course, e.g., save full histories of prompts and outputs, how and why they chose specific generative AI tools (if given a choice).\nFor major assignments such as projects, long papers, and capstones, include several stages that provide scaffolding and feedback on the way to the completed product. Feedback and discussion along the way can increase students’ engagement and accountability for the development of their own ideas, ensure accountability for their work and appropriate disclosure and citation of sources and AI use, and lead to strong final products. Logs or journals that accompany major projects and are reviewed with the instructor or TA periodically can also help.\n### Model ways of working safely and ethically with AI:\nGuide students to go to the Stanford AI Playground. Discuss the risks of sharing sensitive data or information. Open up a conversation about ethical use of AI in ways that tap into disciplinary perspectives represented in the course.\nConsider whether an alternative assignment or approach may be necessary in cases where a student has a strong ethical objection to using AI tools.\nBring some of the important intellectual and academic work into class, where you can provide guidance and modeling of productive strategies with AI.\nDiscuss and model examples of prompting and fact-checking AI results in ways that are specific and appropriate to your discipline, course, and assignment.\n### Design assignments and assessments that demonstrate learning:\nFollow up on assignments in which students use AI tools, e.g., by asking students to explain their reasoning, discuss an example from their work, explain a technique they used, and articulate their approach to working with AI tools.\nArticulate criteria and create a rubric that illustrates the important AI-related skills you will evaluate in students’ final work.\n## Further Reading\nThe following publications highlight recent discussion about incorporating AI use in higher education courses. This list is not a comprehensive bibliography.\n* AI Pedagogy Project (N.D.).*Assignments*.****[**https://aipedagogy.org/assignments/**] \n* Bowen, J. A. &amp; Watson, C. E. (2024).*Teaching with AI: a Practical Guide to a New Era of Human Learning*. Johns Hopkins University Press.[Stanford University Libraries Ebook] (login required).\n* Dungo, C. A. B., Beltran, Z. L. E., Declaro, B. C., Dela-Cruz, J. J. C., &amp; Viray, R. U. (2025). Students’ level of awareness on the environmental implications of generative AI.*Journal of Education in Science, Environment and Health*,*11(2),*93-107.[doi.org/10.55549/jeseh.777] \n* Perkins, Furze, Roe &amp; MacVaugh (2024).[The Al Assessment Scale] (includes several peer-reviewed publications using the scale).\n* Yang, T., Cheon, J., Cho, MH. et al. Undergraduate students’ perspectives of generative AI ethics.*Int J Educ Technol High Educ 22*, 35 (2025).[doi.org/10.1186/s41239-025-00533-1] ## **Last updated**\n**Date**\nBack to Top",
    "length": 5342,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "How persuasive is AI-generated propaganda?",
    "url": "https://cyber.fsi.stanford.edu/publication/how-persuasive-ai-generated-propaganda",
    "text": "# How persuasive is AI-generated propaganda?\n\n# How persuasive is AI-generated propaganda?\n\nCan large language models, a form of artificial intelligence (AI), generate persuasive propaganda?\n\n- [Shelby Grossman],\n- [Josh A. Goldstein],\n- [Alex Stamos],\n- Michael Tomz,\n- Jason Chao,\n- Michael Tomz,\n- Jason Chao\n\nFebruary 20, 2024\n\n[Download] \n\n[All Cyber Publications]",
    "length": 369,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  },
  {
    "query": "site:stanford.edu Stanford University generative AI policy",
    "title": "The Stanford Emerging Technology Review 2025",
    "url": "https://setr.stanford.edu/sites/default/files/2025-01/SETR2025_web-240128.pdf",
    "text": "\nTHE STANFORD EMERGING \nTECHNOLOGY REVIEW 2025\nA Report on Ten Key Technologies and Their Policy Implications\nCO-CHAIRS\nCondoleezza Rice\nJohn B. Taylor\nJennifer Widom\nAmy Zegart\nDIRECTOR AND EDITOR IN CHIEF\nHerbert S. Lin\nStanford University\nStanford, California\nMANAGING EDITOR\nMartin Giles\nCONTENTS\nFOREWORD 4\nEXECUTIVE SUMMARY 10\nINTRODUCTION 18\nThe Role of Science and Technology in \nAdvancing National Interests 18 \nPolicy for Science and Technology 19\nTen Science and Technology Fields 19\n01 Artificial Intelligence 21\n02 Biotechnology and Synthetic \nBiology 39\n03 Cryptography 53\n04 Lasers 65\n05 Materials Science 77\n06 Neuroscience 91\n07 Robotics 103\n08 Semiconductors 115\n09 Space 127\n10 Sustainable Energy Technologies 141\n11 Crosscutting Themes and \nCommonalities 156\nKey Observations About How Technologies \nEvolve over Time 157\nCommon Innovation Enablers and \nInhibitors 164\n12 Technology Applications by \nPolicy Area 174\nEconomic Growth 174\nNational Security 176\nEnvironmental and Energy \nSustainability 177\nHealth and Medicine 179\nCivil Society 180\nCONCLUSION 182\nLEADERSHIP 184\nACKNOWLEDGMENTS 188\n4\ngovernment offices are likely to set trajectories for \nthe United States and the world for years to come. \nNow more than ever, understanding the landscape \nof discovery and how to harness technology to \nforge a better future requires working across sectors, \nfields, and generations. Universities like Stanford \nhave a vital role to play in this effort. In 2023, we \nlaunched the Stanford Emerging Technology Review \n(SETR), the first-ever collaboration between Stanford \nUniversity’s School of Engineering and the Hoover \nInstitution. Our goal is ambitious: transforming tech\u0002nology education for decision makers in both the \npublic and private sectors so that the United States \ncan seize opportunities, mitigate risks, and ensure \nthe American innovation ecosystem continues to \nthrive. \nThis is our latest report surveying the state of ten \nkey emerging technologies and their implications. It \nharnesses the expertise of leading faculty in science \nand engineering fields, economics, international \nrelations, and history to identify key technological \ndevelopments, assess potential implications, and \nhighlight what policymakers should know. \nThis report is our flagship product, but it is just one \nelement of our continuous technology education \ncampaign for policymakers that now involves nearly \none hundred Stanford scholars across forty depart\u0002ments and research institutes. In the past year, SETR \nexperts have briefed senior leaders across the US \ngovernment—in Congress and in the White House, \nCommerce Department, Defense Department, and \nUS intelligence community. We have organized and \nparticipated in fifteen Stanford programs, including \nmultiday AI and biotechnology boot camps for con\u0002gressional staff; SETR roundtables for national media \nand officials from European partners and allies; and \nIn every era, technological discoveries bring both \npromise and risk. Rarely, however, has the world \nexperienced technological change at the speed and \nscale we see today. From nanomaterials that are fifty \nthousand times smaller than the width of a human \nhair to commercial satellites and other private\u0002sector technologies deployed in outer space, break\u0002throughs are rapidly reshaping markets, societies, \nand geopolitics. What’s more, US technology policy \nisn’t the unique province of government like it used \nto be. Instead, inventors and investors are making \ndecisions with enormous policy consequences, even \nif they may not always realize it. Artificial intelligence \n(AI) algorithms are imbued with policy choices about \nwhich outcomes are desired and which are not. \nNearly every new technology, from bioengineer\u0002ing new medicines to building underwater research \ndrones, has both commercial and military applica\u0002tions. Private-sector investment, too, simultaneously \ngenerates both national advantages and vulnerabil\u0002ities by developing new capabilities, supply chains, \nand dependencies and by pursuing commercial \nopportunities that may not serve long-term national \ninterests.\nWhile engineers and executives need to better \nunderstand the policy world, government lead\u0002ers need to better understand the engineering \nand business worlds. Otherwise, public policies \nintended to protect against societal harms may end \nup accelerating them, and efforts to align innova\u0002tion with the national interest could end up harming \nthat interest by dampening America’s innovation \nleadership and the geopolitical advantages that \ncome with it.\nIn these complex times, the only certainties are \nthat uncertainty is rampant and the stakes are high: \nDecisions made today in boardrooms, labs, and \nFOREWORD\nSTANFORD EMERGING TECHNOLOGY REVIEW\n5\n2. Academia’s role in American innovation is \nessential—and at risk.\nThe US innovation ecosystem rests on three pillars: \nthe government, the private sector, and the acad\u0002emy. Success requires robust research and develop\u0002ment (R&D) in all three. But they are not the same, \nand evidence increasingly suggests that universities’ \nrole as the engines of innovation is at a growing risk.\nUniversities, along with the US National Laboratories, \nare the only institutions that conduct research on the \nfrontiers of knowledge without regard for poten\u0002tial profit or foreseeable commercial application. \nThis kind of research is called basic or fundamental \nresearch. It takes years, sometimes decades, to bear \nfruit. But without it, future commercial innovations \nwould not be possible. Radar, the Global Positioning \nSystem (GPS), and the internet all stemmed from \nbasic research done in universities. So did the recent \n“overnight success” of the COVID-19 mRNA vac\u0002cines, which relied on decades of university research \nthat discovered mRNA could activate and block \nprotein cells and figured out how to deliver mRNA \nto human cells to provoke an immune response. \nSimilarly, the cryptographic algorithms protecting \ndata on the internet today would not have been \npossible without decades of academic research in \npure math. And many of the advances in AI, from \nChatGPT to image recognition, build on pioneering \nwork done in university computer science depart\u0002ments that also trained legions of students who have \ngone on to found, fund, and lead many of today’s \nmost important tech companies. In many ways and \nin nearly every field, America’s innovation supply \nchain starts with research universities.\nYet evidence suggests that the engine of inno\u0002vation in US research universities is not running \nworkshops convening leaders across sectors in semi\u0002conductors, space technology, and bioengineering. \nAnd we are just getting started.\nOur efforts are guided by three observations:\n1. America’s global innovation leadership \nmatters.\nAmerican innovation leadership is not just impor\u0002tant for the nation’s economy and security. It is the \nlinchpin for maintaining a dynamic global technol\u0002ogy innovation ecosystem and securing its benefits. \nInternational scientific collaboration has long been \npivotal to fostering global peace, progress, and \nprosperity, even in times of intense geopolitical \ncompetition. During the Cold War, American and \nSoviet nuclear scientists and policymakers worked \ntogether to reduce the risk of accidental nuclear \nwar through arms control agreements and safety \nmeasures. Today, China’s rise poses many new chal\u0002lenges. Yet maintaining a robust global ecosystem \nof scientific cooperation remains essential—and it \ndoes not happen by magic. It takes work, leader\u0002ship, and a fundamental commitment to freedom \nto sustain the openness essential for scientific dis\u0002covery. Freedom is the fertile soil of innovation, \nand it takes many forms: the freedom to criticize a \ngovernment; to admit failure in a research program \nas a step toward future progress; to share findings \nopenly with others; to collaborate across geograph\u0002ical and technical borders with reciprocal access \nto talent, knowledge, and resources; and to work \nwithout fear of repression or persecution. In short, it \nmatters whether the innovation ecosystem is led by \ndemocracies or autocracies. The United States has \nits flaws and challenges, but this country remains the \nbest guarantor of scientific freedom in the world. \nF\n6 STANFORD EMERGING TECHNOLOGY REVIEW\nToday, only a handful of the world’s largest compa\u0002nies have both the talent and the enormous com\u0002pute power necessary for developing sophisticated \nlarge language models (LLMs) like ChatGPT. No uni\u0002versity comes close. In 2024, for example, Princeton \nUniversity announced that it would use endowment \nfunds to purchase 300 advanced Nvidia chips to \nuse for research, costing about $9 million, while \nMeta announced plans to purchase 350,000 of the \nsame chips by year’s end, at an estimated cost of \n$10 billion.7\nThese trends have several concerning implications.8\nA very significant one is that research in the field is \nlikely to be skewed to applications driven by com\u0002mercial rather than public interests. The ability for \nuniversities—or anyone outside of the leading AI \ncompanies—to conduct independent analysis of the \nweaknesses, risks, and vulnerabilities of AI (espe\u0002cially LLMs recently in the news) will become more \nimportant and simultaneously more difficult. Further, \nthe more that industry offers unparalleled talent con\u0002centrations, computing power, training data, and the \nmost sophisticated models, the more likely it is that \nfuture generations of the best AI minds will continue \nto flock there (see figure  F.1)—potentially eroding \nthe nation’s ability to conduct broad-ranging foun\u0002dational research in the field.\n3. The view from Stanford is unique, important—\nand needed now more than ever.\nStanford University has a unique vantage point when \nit comes to technological innovation. It is not an \naccident that Silicon Valley surrounds Stanford; tech\u0002nology developed at Stanford in the 1930s served \nas the foundation for the pioneering companies \nlike Varian Associates and Hewlett-Packard that first \nshaped industry in the region. Since then, the univer\u0002sity has continued to fuel that innovation ecosystem. \nStanford faculty, researchers, and former students \nhave founded Alphabet, Cisco Systems, Instagram, \nLinkedIn, Nvidia, Sun Microsystems, Yahoo!, and \nmany other companies, together generating more \nannual revenues than most of the world’s economies. \nas well as it could, posing long-term risks to the \nnation. In 2024, for the first time, the number of \nChinese contributions surpassed those of the \nUnited States in the closely watched Nature Index, \nwhich tracks eighty-two of the world’s premier \nscience journals.1 Funding trends are also head\u0002ing in the wrong direction. The US government is \nthe only funder capable of making large and risky \ninvestments in the basic science conducted at uni\u0002versities (as well as at national laboratories) that is \nessential for future applications. Yet federal R&D \nfunding has plummeted in percentage terms since \nthe 1960s, from 1.86 percent of GDP in 1964 to just \n0.66 percent of GDP in 2016.2 The Creating Helpful \nIncentives to Produce Semiconductors (CHIPS) and \nScience Act of 2022 was supposed to turn the tide \nby dramatically raising funding for basic research, \nbut major increases were subsequently scrapped \nin budget negotiations. The United States still \nfunds more basic research than China does, but \nChinese investment is rising six times faster—and \nis expected to overtake US spending within a \ndecade.3\nAlthough private-sector investment in technol\u0002ogy companies and associated university research \nhas increased substantially, it is not a substitute \nfor federal funding, which supports university R&D \ndirected at national and public issues, not commer\u0002cial viability.4\nTo be sure, the rising dominance of private indus\u0002try in innovation brings significant benefits. But it is \nalso generating serious and more hidden risks to the \nhealth of the entire American innovation ecosystem. \nTechnology and talent are migrating from academia \nto the private sector, accelerating the development \nof commercial products while eroding the founda\u0002tion for the future. We are already reaching a tip\u0002ping point in AI. In 2022, more than 70 percent of \nstudents who received PhDs in artificial intelligence \nat US universities took industry jobs, leaving fewer \nfaculty to teach the next generation.5 As the bipar\u0002tisan National Security Commission on Artificial \nIntelligence put it, “Talent follows talent.”6\nFor\ndriving Google; to optogenetics, a technique pio\u0002neered in 2005 that uses light to control neurons, \nenabling precise studies of brain function. In this \nmoment of rapid technological change, we must do \neven more to connect emerging technologies with \npolicy. We are proud and excited to highlight this \ncollaboration between Stanford’s Hoover Institution \nand the School of Engineering to bring policy analy\u0002sis, social science, science, medicine, and engineer\u0002ing together in new ways.\nToday, technology policy and education efforts are \noften led by policy experts with limited technolog\u0002ical expertise. The Stanford Emerging Technology \nReview flips the script, enlisting ten of the brightest \nscientific and engineering minds at the university to \nshare their knowledge of their respective fields by \nworking alongside social scientists to translate their \nwork to nonexpert audiences. We start with science \nand technology, not policy. And we go from there \nto emphasize the important interaction between sci\u0002ence and all aspects of policy.\nStart-ups take flight in our dorm rooms, classrooms, \nlaboratories, and kitchens. Technological innovation \nis lived every day and up close on our campus—with \nall its benefits and downsides. This ecosystem and its \nculture, ideas, and perspectives often seem a world \napart from the needs and norms of Washington, DC. \nBridging the divide between the locus of American \npolicy and the heart of American technological inno\u0002vation has never been more important. \nStanford has a rich history of policy engagement, \nwith individuals who serve at the highest levels of \ngovernment as well as institutional initiatives that \nbring together policymakers and researchers to \ntackle the world’s toughest policy problems. And as \nStanford’s School of Engineering celebrates its one \nhundredth anniversary in 2025, we are reminded of \nthe profound impact that generations of Stanford \nfaculty, students, and staff have had through their \ndiscoveries—from the klystron, a microwave ampli\u0002fier developed in the 1930s that enabled radar and \nearly satellite communications; to the algorithms \nEmployment of new AI PhDs (% of total) in the United States and Canada by sector, 2010–22\nIndustry Academia\nNew AI PhD graduates (% of total)\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022\n70%\n60%\n50%\n40%\n30%\n20%\n10%\n0%\nGovernment\n70.71%\n19.95%\n0.76%\nSource: Adapted from Nestor Maslej, Loredana Fattorini, Raymond Perrault, et al., The AI Index 2024 Annual Report, AI \nIndex Steering Committee, Institute for Human-Centered AI, Stanford University, Stanford, CA, April 2024. Data from CRA \nTaulbee Survey, 2023\nFIGURE F.1  Most new AI PhDs hired in North America are flocking to industry \n8 STANFORD EMERGING TECHNOLOGY REVIEW\ndiscussion. The themes include broad trends, like the \ntendency for technological breakthroughs to come \nin fits, starts, and lengthy plateaus that are extremely \ndifficult even for leaders in those fields to predict. (AI \nleaders have experienced several so-called AI winters \nover decades as well as moments of profound and \nsudden progress like the 2022 release of ChatGPT.) \nThey include enduring and widespread technolog\u0002ical challenges like cybersecurity. And they include \ncognitive blind spots like frontier bias—the natural \nbut mistaken assumption that the only transforma\u0002tional technologies sit on the frontiers of a field. \nFor each of the ten technology chapters, reviews \nof the field were led by world-renowned tenured \nStanford faculty members who also delivered sem\u0002inars with other faculty discussants within and out\u0002side their areas of expertise. (SETR contributors and \ntheir fields are listed at the end of each chapter.) \nThe SETR team also involved more than a dozen \npostdoctoral scholars and undergraduate research \nassistants who interviewed faculty across Stanford \nand drafted background materials. \nEach technology chapter begins with an overview of \nthe basics—the major technical subfields, concepts, \nand terms needed to understand how a technol\u0002ogy works and could affect society. Next, we out\u0002line important developments and advances in the \nfield. Finally, each chapter concludes by offering an \nover-the-horizon outlook that covers the most cru\u0002cial considerations for policymakers over the next \nfew years—including technical as well as policy, \nlegal, and regulatory issues. The report ends with a \nchapter that looks across the ten technologies, offer\u0002ing analysis of implications for economic growth, \nnational security, environmental and energy sustain\u0002ability, human health, and civil society.\nThree points bear highlighting. First, we offer no spe\u0002cific policy recommendations in this report. That is \nby design. Washington is littered with reports offering \npolicy recommendations that were long forgotten, \novertaken by events, or both. Opinions are plentiful. \nExpert insights based on leading research are not.\nHow to Use This Report: \nOne Primer, Ten Major \nTechnology Areas\nThis report is intended to be a one-stop-shopping \nprimer that covers developments and implications in \nten major emerging technology areas: AI, biotech\u0002nology and synthetic biology, cryptography, lasers, \nmaterials science, neuroscience, robotics, semicon\u0002ductors, space, and sustainable energy technologies. \nThe list is broad by design, and it includes fields \nthat are widely regarded as pivotal to shaping soci\u0002ety, economics, and geopolitics today and into the \nfuture. \nThat said, the ten major technology areas covered in \nthis report are nowhere near an exhaustive catalogue \nof technology research areas at Stanford. And the list \nmay change year to year—not because a particular \ntechnology sputtered or because we got it wrong, \nbut because categorizing technologies is inherently \ndynamic. Limiting this report to ten areas imposes \ndiscipline on what we cover and how deeply we go. \nWe seek to highlight relationships among technolo\u0002gies in ways that may not be obvious: Quantum com\u0002puting, for example, is an important field but does \nnot have its own chapter. Instead, it is covered within \nthe semiconductor chapter because we wanted to \nemphasize that even if quantum breakthroughs are \nrealized, they will not address many important com\u0002puting needs and challenges. Of note, nine of the \nten technology chapters appearing in this edition \nare on the same subjects as in our previous report. \nIn this report, we have combined nuclear energy and \nsustainable energy technologies into a single chap\u0002ter and added a chapter on lasers.\nMany of the most important issues cut across tech\u0002nological fields. We have expanded our previous \nreport’s crosscutting themes chapter to highlight four\u0002teen of these themes and offer more examples and \nFor\nWe aim to provide a reference resource that is both \ntimeless and timely, an annual state-of-the-art guide \nthat can inform successive generations of policymak\u0002ers about how to think about evolving technological \nfields and their implications. Individual SETR faculty \nmay well have views about what should be done. \nSome of us engage in policy writing and advising. \nBut the mission of this collective report is informing, \nnot advocating. We encourage readers interested in \nlearning more about specific fields and policy ideas \nto contact our team at SETReview2025@stanford.edu. \nSecond, SETR offers a view from Stanford, not the\nview from Stanford. There is no single view of any\u0002thing in a university. Faculty involved in this report \nmay not agree with everything in it. Their colleagues \nwould probably offer a different lay of the technology \nlandscape with varying assessments about impor\u0002tant developments and over-the-horizon issues. The \nreport is intended to reflect the best collective judg\u0002ment about the state of these ten fields—guided by \nleading experts in them.\nThird, this report is intended to be the introductory \nproduct that translates a broad swatch of tech\u0002nological research for nontechnical readers. Other \nSETR offerings provide deeper dives into specific \ntechnological areas that should be of interest for \nsubject-matter experts. \nEnsuring continued American leadership in science \nand technology is essential, and it’s a team effort. \nWe hope this edition of the Stanford Emerging \nTechnology Review continues to spark meaning\u0002ful dialogue, better policy, and lasting impact. The \npromise of emerging technology is boundless if we \nhave the foresight to understand it and the fortitude \nto embrace the challenges.\nCondoleezza Rice\nJohn B. Taylor\nJennifer Widom\nAmy Zegart\nCo-chairs, Stanford Emerging Technology Review\nNOTES\n1. Simon Baker, “China Overtakes United States on Contribution \nto Research in Nature Index,” Nature, May 19, 2023, https://doi\n.org/10.1038/d41586-023-01705-7.\n2. Council on Foreign Relations, Innovation and National Security: \nKeeping Our Edge, Independent Task Force Report No. 77, James \nManyika and William H. McRaven, chairs, 2019, 10, https://www\n.cfr.org/keeping-our-edge/pdf/TFR_Innovation_Strategy.pdf.\n3. OECD, “OECD Data Explorer: Gross Domestic Expenditure \non R&D by Sector of Performance and Type of R&D,” OECD.org, \n2024, https://data-viewer.oecd.org/?chartId=b08256f3-6000-42a5\n-ad81-b1ab5bd8ac6a. Analysis was conducted by comparing two \nmethods of measuring annual increases of US and China spending \non basic research and projecting rates into future.\n4. Council on Foreign Relations, Innovation, 21.\n5. Nestor Maslej, Loredana Fattorini, Raymond Perrault, et al., \nThe AI Index 2024 Annual Report, AI Index Steering Commit\u0002tee, Institute for Human-Centered AI, Stanford University, Stan\u0002ford, CA, April 2024, https://aiindex.stanford.edu/wp-content\n/uploads/2024/05/HAI_AI-Index-Report-2024.pdf.\n6. Eric Schmidt and Robert Work, NSCAI Interim Report, National \nSecurity Commission on Artificial Intelligence, 2020, 30, https://apps\n.dtic.mil/sti/pdfs/AD1112059.pdf.\n7. Poornima Apte, “Princeton Invests in New 300-GPU Cluster for \nAcademic AI Research,” AI at Princeton, March 15, 2024, https://\nai.princeton.edu/news/2024/princeton-invests-new-300-gpu\n-cluster-academic-ai-research; Michael Kan, “Zuckerberg’s Meta Is \nSpending Billions to Buy 350,000 Nvidia H100 GPUs,” PCMAG, Jan\u0002uary 18, 2024, https://www.pcmag.com/news/zuckerbergs-meta-is\n-spending-billions-to-buy-350000-nvidia-h100-gpus; Kif Leswing, \n“Nvidia’s Latest AI Chip Will Cost More than $30,000, CEO Says,” \nCNBC, March 19, 2024, https://www.cnbc.com/2024/03/19/nvidias\n-blackwell-ai-chip-will-cost-more-than-30000-ceo-says.html.\n8. Roman Jurowetzki, Daniel Hain, Juan Mateos-Garcia, and Kon\u0002stantinos Stathoulopoulos, “The Privatization of AI Research (-ers): \nCauses and Potential Consequences—From University-Industry \nInteraction to Public Research Brain-Drain?,” arXiv, 2021, https://\narxiv.org/ftp/arxiv/papers/2102/2102.01648.pdf.\n10\nUS government departments. However, SETR focus \ntechnologies are likely to change over time, not \nbecause we were incorrect, but because science and \ntechnology never sleep, the borders between fields \nare porous, and different people categorize similar \nresearch in different ways. \nReport Design\nThis report is organized principally by technology, \nwith each area covered in a standalone chapter that \ngives an overview of the field, highlights key devel\u0002opments, and offers an over-the-horizon view of \nimportant technological and policy considerations. \nAlthough these chapters can be read individually, \none of the most important and unusual hallmarks of \nthis moment is convergence: Emerging technologies \nare intersecting and interacting in a host of ways, \nwith important implications for policy. We examine \nthese broader dynamics in chapters  11 and 12. In \nchapter  11, we describe a number of themes and \ncommonalities that cut across many of the technolo\u0002gies we describe earlier in the report. In chapter 12, \nwe consolidate technological developments across \nall ten areas and discuss how they apply to five \npolicy domains: economic growth, national security, \nenvironmental and energy sustainability, health and \nmedicine, and civil society.\nThree tensions run throughout and are worth keep\u0002ing in mind: \n1. Timeliness and timelessness Each chapter \nseeks to strike a balance between covering recent \ndevelopments in science and in the headlines and \nproviding essential knowledge about how a field \nEmerging technologies have never been more \nimportant or difficult to understand. Breakthrough \nadvances seem to be everywhere, from ChatGPT \nto the COVID-19 mRNA vaccines to constellations \nof cheap commercial shoebox-size satellites that \ncan track events on Earth in near–real time. This is \na pivotal technological moment offering both tre\u0002mendous promise and unprecedented challenges. \nPolicymakers need better expert resources to help \nthem understand the burgeoning and complex array \nof technological developments—more easily and \nmore continuously. \nThe Stanford Emerging Technology Review is \ndesigned to meet this need, offering an easy-to\u0002use reference tool that harnesses the expertise of \nStanford University’s leading science and engineer\u0002ing faculty in ten major technological areas.\nSETR 2025 FOCUS TECHNOLOGIES\nArtificial Intelligence\nBiotechnology and Synthetic Biology\nCryptography\nLasers\nMaterials Science\nNeuroscience\nRobotics\nSemiconductors\nSpace\nSustainable Energy Technologies\nThese particular fields were chosen for this report \nbecause they leverage areas of deep expertise \nat Stanford and cover many critical and emerging \ntechnologies identified by the Office of Science and \nTechnology Policy in the White House and other \nEXECUTIVE SUMMARY\nSTANFORD EMERGING TECHNOLOGY REVIEW\n11\nlearning, interacting, problem-solving, and even \nexercising creativity. In the past year, the main \nAI-related headlines have been the rise of large \nlanguage models (LLMs) like GPT-4, on which some \nversions of the chatbot ChatGPT are based, and the \nrecognition of AI’s significance through the award\u0002ing of two Nobel Prizes in physics and chemistry for \nAI-related work. \nKEY CHAPTER TAKEAWAYS\n° Artificial intelligence (AI) is a foundational tech\u0002nology that is supercharging other scientific fields \nand, like electricity and the internet, has the \npotential to transform societies, economies, and \npolitics worldwide.\n° Despite rapid progress in the past several years, \neven the most advanced AI still has many failure \nmodes that are unpredictable, not widely appre\u0002ciated, not easily fixed, not explainable, and \ncapable of leading to unintended consequences.\n° Mandatory governance regimes for AI, even \nthose to stave off catastrophic risks, will face stiff \nopposition from AI researchers and companies, \nbut voluntary regimes calling for self-governance \nare more likely to gain support. \nBiotechnology and Synthetic Biology\nBiotechnology is the use of cellular and biomo\u0002lecular processes to develop products or services. \nSynthetic biology is a subset of biotechnology that \ninvolves using engineering tools to modify or create \nbiological functions—like creating a bacterium that \ncan glow in the presence of explosives. Synthetic \nbiology is what created the COVID-19 mRNA vac\u0002cine in record time—although it relied on decades \nworks, what is important within it, and what chal\u0002lenges lie ahead.\n2. Technical depth and breadth This report inten\u0002tionally skews toward breadth, offering a 30,000-\nfoot view of a vast technological landscape in one \ncompendium. Readers should consider it an intro\u0002ductory course. Other products and/or educational \ntools will be released in the months ahead that will \noffer additional insights into each field.\n3. Technical and nontechnical aspects of inno\u0002vation We start with the science but do not end \nwith the science. Technological breakthroughs are \nnecessary but not sufficient conditions for successful \ninnovation. Economic, political, and societal factors \nplay enormous and often hidden roles. Johannes \nGutenberg invented the printing press in 1452, but it \ntook more than 150 years before the Dutch invented \nthe first successful newspapers—not because they \nperfected the mechanics of movable type, but \nbecause they decided to use less paper, making \nnewspapers sustainably profitable for the first time.1\nEach chapter in this report was written with an eye \ntoward highlighting important economic, political, \npolicy, legal, and societal factors likely to impede, \nshape, or accelerate progress.\nTechnologies and Takeaways \nat a Glance\nArtificial Intelligence \nArtificial intelligence (AI) is a computer’s ability \nto perform some of the functions associated with \nthe human brain, including perceiving, reasoning, \nExec\n12 STANFORD EMERGING TECHNOLOGY REVIEW\nprovenance of information, identity management, \nsupply chain management, and cryptocurrencies.\nKEY CHAPTER TAKEAWAYS\n° Cryptography is essential for protecting infor\u0002mation, but alone it cannot secure cyberspace \nagainst all threats.\n° Cryptography is the enabling technology of \nblockchain, which is the enabling technology of \ncryptocurrencies.\n° Central bank digital currencies (CBDCs) are a \nparticular type of cryptography-based digital \ncurrency supported by states and one that could \nenhance financial inclusion. Although the United \nStates lags some countries in experimenting \nwith a CBDC, it may benefit from a cautious, \nwell-timed approach by learning from other \nnations’ efforts.\nLasers\nImprovements in laser technology since its inven\u0002tion in 1960 have enabled light to be manipulated \nand used in previously unimaginable ways. It is \nnow applied so broadly that it can be considered \nan enabling technology—one whose existence \nand characteristics enable other applications that \nwould not be feasible and/or affordable without \nit. New lasers are being developed by researchers \nand companies across a wider range of light wave\u0002lengths, which will make the devices even more \nuseful.\nKEY CHAPTER TAKEAWAYS\n° Laser technology has become essential for a wide \nrange of applications, including communications, \nhigh-end chip production, defense, manufactur\u0002ing, and medicine.\n° Because advances in laser technology tend to \noccur in the context of specific applications, laser \nof earlier research. Just as rockets enabled humans \nto overcome the constraints of gravity to explore the \nuniverse, synthetic biology is enabling humans to \novercome the constraints of lineage to develop new \nliving organisms.\nKEY CHAPTER TAKEAWAYS\n° Biotechnology is poised to emerge as a general\u0002purpose technology by which anything bioen\u0002gineers learn to encode in DNA can be grown \nwhenever and wherever needed—essentially \nenabling the production of a wide range of prod\u0002ucts through biological processes across multiple \nsectors. \n° The US government is still working to grasp the \nscale of this bio-opportunity and has relied too \nheavily on private-sector investment to support \nthe foundational technology innovation needed \nto unlock and sustain progress. \n° Biotechnology is one of the most important areas \nof technological competition between the United \nStates and China, and China is investing consid\u0002erably more resources. Lacking equivalent efforts \ndomestically, the United States runs the risk of \nSputnik-like strategic surprises in biotechnology.\nCryptography\nThe word cryptography originates from Greek words \nthat mean “secret writing.” In ancient times, cryp\u0002tography involved the use of ciphers and secret \ncodes. Today, it relies on sophisticated mathemat\u0002ical models to protect data from being altered or \naccessed inappropriately. Cryptography is often \ninvisible, but it is essential for most internet activi\u0002ties, such as messaging, e-commerce, and banking. \nIn recent years, a type of cryptographic technol\u0002ogy called blockchain—which records transac\u0002tions in distributed ledgers in the computing cloud \nthat cannot be altered retroactively without being \ndetected—has been used for a variety of applica\u0002tions, including time-stamping and ensuring the \nExecuti\ndevelopment to degeneration in later years. The \nbrain is perhaps the least understood and yet most \nimportant organ in the human body. Three major \nresearch subfields of neuroscience are neuroengi\u0002neering (e.g., brain-machine interfaces), neurohealth \n(e.g., brain degeneration and aging), and neurodis\u0002covery (e.g., the science of addiction). \nKEY CHAPTER TAKEAWAYS\n° Popular interest in neuroscience vastly exceeds \nthe actual current scientific understanding of \nthe brain, giving rise to overhyped claims in the \npublic domain that revolutionary advances are \njust around the corner. \n° Advances in human genetics and experimental \nneuroscience, along with computing and neuro\u0002science theory, have led to some progress in sev\u0002eral areas, including understanding and treating \naddiction and neurodegenerative diseases and \ndesigning brain-machine interfaces for restoring \nvision. \n° American leadership is essential for establishing \nand upholding global norms about ethics and \nhuman subjects research in neuroscience, but \nthis leadership is slipping with decreased strate\u0002gic planning and increased foreign investments \nin the field.\nRobotics\nRobotics is an integrative field that draws on advances \nin multiple technologies rather than a single disci\u0002pline. The question “What is a robot?” is harder to \nanswer than it appears. At a minimum, the emerg\u0002ing consensus among researchers is that a robot is a \nphysical entity that has ways of sensing itself and the \nworld around it and can create physical effects on \nthat world. Robots are already used across a range of \nsectors in a variety of ways—including assembly-line \nmanufacturing, space exploration, autonomous vehi\u0002cles, tele-operated surgery, military reconnaissance, \nand disaster assistance. \ntechnology research and development is widely \ndispersed among different types of laboratories \nand facilities. \n° Broad investment in next-generation lasers holds \nthe potential to improve progress in nuclear \nfusion energy technology, weapons develop\u0002ment, and quantum communication.\nMaterials Science\nMaterials science studies the structure and proper\u0002ties of materials—from those visible to the naked \neye to microscopic features—and how they can be \nengineered to change performance. Contributions \nto the field have led to better semiconductors, \n“smart bandages” with integrated sensors and sim\u0002ulators to accelerate healing, more easily recycla\u0002ble plastics, and more energy-efficient solar cells. \nMaterials science has also been key to the devel\u0002opment of additive manufacturing, often known as \n3-D printing.\nKEY CHAPTER TAKEAWAYS\n° Materials science is a foundational technology \nthat underlies advances in many other fields, \nincluding robotics, space, energy, and synthetic \nbiology. \n° Materials science will exploit artificial intelligence \nas another promising tool to predict new mate\u0002rials with new properties and identify novel uses \nfor known materials.\n° Future progress in materials science requires new \nfunding mechanisms to more effectively tran\u0002sition from innovation to implementation and \naccess to more computational power. \nNeuroscience\nNeuroscience is the study of the human brain and \nthe nervous system—its structure, function, healthy \nand diseased states, and life cycle from embryonic \n14 STANFORD EMERGING TECHNOLOGY REVIEW\nKEY CHAPTER TAKEAWAYS\n° The growing demand for artificial intelligence and \nmachine learning is driving innovations in chip \nfabrication that are essential for enhancing com\u0002putational power and managing energy efficiency.\n° Advances in memory technologies and high\u0002bandwidth interconnects, including photonic \nlinks, are critical for meeting the increasing data \nneeds of modern applications.\n° Even if quantum computing advancements are \nrealized, the United States will still need compre\u0002hensive innovation across the technology stack \nto continue to scale the power of information \ntechnology.\nSpace\nSpace technologies include any technology devel\u0002oped to conduct or support activities approximately \nsixty miles or more beyond Earth’s atmosphere. A \nsingle space mission is a system of systems—including \neverything from the spacecraft itself to propulsion, \ndata storage and processing, electrical power gener\u0002ation and distribution, thermal control to ensure that \ncomponents are within their operational and sur\u0002vival limits, and ground stations. While in the past, \nspace was the exclusive province of government spy \nsatellites and discovery missions, the number and \ncapabilities of commercial satellites have increased \ndramatically in recent years. Today, around ten thou\u0002sand working satellites, many no larger than a loaf \nof bread, circle the planet. Some operate in constel\u0002lations that can revisit the same location multiple \ntimes a day and offer image resolutions so sharp \nthey can identify different car models driving on \na road. \nKEY CHAPTER TAKEAWAYS\n° A burgeoning “NewSpace” economy driven by \nprivate innovation and investment is transforming \nKEY CHAPTER TAKEAWAYS\n° Future robots may be useful for improving the \nUS manufacturing base, reducing supply chain \nvulnerabilities, delivering eldercare, enhancing \nfood production, tackling the housing shortage, \nimproving energy sustainability, and performing \nalmost any task involving physical presence.\n° Progress in artificial intelligence holds the poten\u0002tial to advance robotics significantly but also raises \nethical concerns that are essential to address, \nincluding the privacy of data used to train robots, \ndata bias that could lead to physical harm by \nrobots, and other safety issues.\n° Achieving the full potential of robots will require \na major push from the federal government and \nthe private sector to improve robotics adoption \nand research across the nation.\nSemiconductors\nSemiconductors, or chips, are crucial and ubiqui\u0002tous components used in everything from refrig\u0002erators and toys to smartphones, cars, computers, \nand fighter jets. Chip production involves two \ndistinct steps: (1) design, which requires talented \nengineers to design complex integrated circuits \ninvolving millions of components; and (2) fabri\u0002cation, which is the task of manufacturing chips in \nlarge, specially designed factories called “fabs.” \nBecause fabs involve highly specialized equipment \nand facilities, they can cost billions of dollars. US \ncompanies still play a leading role in semiconduc\u0002tor design, but US semiconductor-manufacturing \ncapacity has plummeted, leaving the country heav\u0002ily dependent on foreign chips, most notably from \nTaiwan. The Creating Helpful Incentives to Produce \nSemiconductors (CHIPS) and Science Act of 2022 \nwas intended to help the US semiconductor indus\u0002try regain a foothold in fabrication, but progress will \ntake years, if not decades. \nExecuti\nvehicles, and transitioning residential and com\u0002mercial heating and industrial energy. \n° In the long term, technologies for decarboniz\u0002ing buses and long-haul trucks, decarbonizing \ncarbon-intensive industries, and reducing green\u0002house gases from refrigerants and agriculture will \nplay key roles in a net-zero, emissions-free energy \ninfrastructure.\nImportant Crosscutting \nThemes\nChapter 11 discusses fourteen themes that cut across \nthe technological areas. We split these themes into \ntwo categories.\nCategory 1: Key Observations About How \nTechnologies Evolve over Time\n1. The Goldilocks challenge: moving too quickly, \nmoving too slowly. Innovation that emerges too \nfast threatens to disrupt the status quo around which \nmany national, organizational, and personal inter\u0002ests have coalesced. It is also more likely to lead to \nunintended consequences and give short shrift to \nsecurity, safety, ethics, and geopolitics. Innovation \nthat moves too slowly increases the likelihood that a \nnation will lose the technical, economic, and national \nsecurity advantages that often accrue to first movers \nin a field.\n2. There is a trend toward increasing access to \nnew technologies worldwide. Even innovations \nthat are US born are unlikely to remain in the exclu\u0002sive control of American actors for long periods.\n3. The synergies between different technologies \nare large and growing. Advances in one tech\u0002nology domain often support advances in other \ntechnologies.\nspace launch, vehicles, communications, and key \nspace actors in a domain that has until now been \ndominated by superpower governments.\n° Space is a finite planetary resource. Because of \ndramatic increases in satellites, debris, and geo\u0002political space competition, new technologies \nand new international policy frameworks will be \nneeded to prevent and manage international \nconflict in space and ensure responsible steward\u0002ship of this global commons.\n° A race to establish a permanent human presence \non the Moon is underway, with serious concerns \nthat, despite Outer Space Treaty prohibitions \nagainst it, the first nation to reach the Moon may \nbe in a strong position to prevent others from \nestablishing their own lunar presences.\nSustainable Energy Technologies\nThis vital strategic resource for nations typically \ninvolves generation, transmission, and storage. \nIn recent years it has also come to include carbon \ncapture and carbon’s removal from the atmosphere. \nEnergy mix and innovation are key to efforts to \naddress climate change. Success will also depend \non tackling challenges such as decentralizing and \nmodernizing electricity grids and achieving greater \nnational consensus about energy goals to enable \nstrategic and effective R&D programs and funding.\nKEY CHAPTER TAKEAWAYS\n° Although many clean energy technologies are \nnow available and increasingly affordable, scaling \nthem to a meaningful degree and building the \nmassive infrastructure needed to deploy them \nwill take decades.\n° The largest impact on reducing emissions in the \nnear to medium term will come from building a \nno- to very-low-emission electricity grid, elec\u0002trifying passenger cars and small commercial \n16 STANFORD EMERGING TECHNOLOGY REVIEW\nCategory 2: Common Innovation Enablers \nand Inhibitors\n1. Ideas and human talent play a central role in \nscientific discovery and cannot be manufactured \nat will. They must be either domestically nurtured \nor imported from abroad. Today, both paths for \ngenerating ideas and human talent face serious and \nrising challenges.\n2. A policy bias toward science or technology at \nthe frontiers of knowledge tends to overestimate \nthe benefits accruing from such advances, at least \nin the short term. Many technologies with transfor\u0002mational potential are not necessarily on the techni\u0002cal frontier, and frontier bias carries with it the risk of \noverlooking older technologies that can be used in \nnovel and impactful ways.\n3. Good public policy anticipates wide variations \nin perspectives on any given technology. When \neveryone in a decision-making organization shares \nsimilar perspectives on technology—creating ana\u0002lytical blind spots and potentially groupthink, or \nunwarranted conformity in beliefs—the risks associ\u0002ated with innovation can be underestimated.\n4. US universities play a pivotal role in the inno\u0002vation ecosystem that is increasingly at risk.\nAlthough the US government frequently talks about \nthe importance of public-private partnerships in \nemerging technology, universities also play a pivotal \nand often underappreciated role. They are the only \norganizations with the mission of pursuing high-risk \nresearch that may not pay off commercially for a \nlong time, if ever. That high-risk focus has yielded \nhigh-benefit payoffs in a wide range of fields.\n5. Sustaining American innovation requires long\u0002term government R&D. Investments with clear \nstrategies and sustained priorities—not the increas\u0002ingly common wild swings in research funding from \nyear to year—are crucial. \n4. The path from research to application is often \nnot linear. Many believe that technological break\u0002throughs arise from a step-by-step linear progres\u0002sion where basic research leads to applied research, \nwhich then leads to development and prototyping \nand finally to a marketable product. Yet innovation \noften does not work this way. Many scientific devel\u0002opments enhance understanding but never advance \nto the marketplace. Many marketable products \nemerge in a nonlinear fashion, after many rounds of \nfeedback between phases. Other products emerge \nonly when several different technologies reach some \nlevel of maturity.\n5. The speed of change is hard even for leading \nresearchers to anticipate. Technology often pro\u0002gresses in fits and starts, with long periods of incre\u0002mental results followed by sudden breakthroughs.\n6. Nontechnical factors often determine whether \nnew technologies succeed or fail. Adoption of \nnovel technologies hinges on economic viability and \nsocietal acceptability, not just scientific proof of con\u0002cept and engineering feasibility.\n7. The US government is no longer the primary \ndriver of technological innovation or funder of \nresearch and development. Historically, technolog\u0002ical advances (including semiconductors, the inter\u0002net, and jet engines) were funded and advocated \nfor by the US government. Today, private-sector \nR&D investment is playing a much larger role, rais\u0002ing important concerns about how to ensure that US \nnational interests are properly taken into account \nand that basic science—which is an important foun\u0002dation for future innovation—remains strong. \n8. Technological innovation occurs in both democ\u0002racies and autocracies, but different regime \ntypes enjoy different advantages and challenges.\nDemocracies provide greater freedom for explo\u0002ration, while authoritarian regimes can direct sus\u0002tained funding and focus toward the technologies \nthey believe are most important.\nExecuti\n6. Cybersecurity is an enduring concern for every \naspect of emerging technology research. State \nand nonstate actors will continue to threaten the \nconfidentiality, integrity, and availability of informa\u0002tion that is crucial for emerging technology R&D.\nFinally, each of the ten technology fields covered \nin this report bears on five policy areas that are of \ninterest to policymakers: economic growth, national \nsecurity, environmental and energy sustainability, \nhealth and medicine, and civil society. Chapter  12\nidentifies applications and consequences of each \nfield as they apply to these policy areas.\nNOTES\n1. Andrew Pettegree and Arthur der Weduwen, The Bookshop of \nthe World: Making and Trading Books in the Dutch Golden Age \n(New Haven, CT: Yale University Press, 2019), 70–72.\n18\ndiseases, energy supply and demand, and sustain\u0002able development.3\nS&T is one important battleground for seeking \nadvantage in geopolitical competition, as advances \nin scientific and technical fields can contribute to \nnational interests, including a stronger national \nsecurity posture, greater national pride and self\u0002confidence, economic influence, and diplomatic \nleverage. But four other points about S&T are equally \nimportant:\n° Advances in S&T must be leveraged alongside \nstrong public policy if those advances are to \nserve the national interest. Coupling advanced \ntechnology with poor policy to influence that \ntechnology rarely ends well. \n° Advantages gained from S&T advances are tran\u0002sient in the long run. Attempting to restrict the \ntransfer of scientific and technical knowledge \nto other nations may delay its spread, but the \nfirst successful demonstration of a technological \nadvance on the part of the United States is often \nthe impetus for other nations to launch their own \nefforts to catch up.\n° Internationally, S&T is not always a zero-sum \ngame, as advances originating in one nation \noften benefit others. For example, the internet \nand what most people know simply as GPS navi\u0002gation are US-born innovations whose uses have \nspread around the world—and the United States \nitself has gained from that spread.\n° International competition does not occur only \nwith adversaries. Our allies and partners also \ncompete in the S&T space, developing technol\u0002ogy or deploying policy that can leave the United \nStates at a disadvantage.\nThe Role of Science and \nTechnology in Advancing \nNational Interests\nVannevar Bush, an engineer and policymaker who \noversaw the development of the Manhattan Project, \nwas the nation’s first presidential science advisor. In \n1945, he wrote, “Advances in science when put to \npractical use mean more jobs, higher wages, shorter \nhours, more abundant crops, more leisure for rec\u0002reation, for study, for learning how to live without \nthe deadening drudgery which has been the burden \nof the common man for ages past. .  .  . Advances \nin science will also bring higher standards of living, \nwill lead to the prevention or cure of diseases, \nwill promote conservation of our limited national \nresources, and will assure means of defense against \naggression.”1\nScience and technology (S&T) remain essential to \nour national interests. Advances in S&T are closely \ntied to national needs in transportation, agricul\u0002ture, communication, energy, education, environ\u0002ment, health, and defense—as well as to millions \nof American jobs. S&T also underpins and drives \nmany strategic objectives in foreign policy, such \nas reducing the proliferation of weapons of mass \ndestruction, strengthening relationships with allies \nand partners, improving humanitarian assistance, \nand promoting growth in developing and transi\u0002tional economies.2\n Research and development in \nS&T fields such as information technology, biotech\u0002nology, materials science, and lasers will impact \nboth “hard power” issues—defense, arms control, \nnonproliferation—and “soft power” concerns, \nsuch as climate change, infectious and chronic \nINTRODUCTION\nSTANFORD EMERGING TECHNOLOGY REVIEW\n19\nacademia and think tanks, as well as discussions \nwith science and engineering colleagues at Stanford \nUniversity and other research universities. We do not \nclaim that any one of these ten is more important \nthan the others, and the discussion below addresses \nsubjects in alphabetical order. Indeed, one of the \nunexpected aspects of this technological moment \nis convergence: New technologies are intersecting, \noverlapping, and driving each other in all sorts of \nways—some obvious, some more hidden.\nThe description of each field is divided into three \nparts. The first part is an overview of the field. The \nsecond part addresses noteworthy key develop\u0002ments in the domain that are relevant to under\u0002standing the field from a policy perspective. The last \npart, providing an over-the-horizon perspective, \nis itself subdivided into three sections: the poten\u0002tial impact of the field in the future (i.e., the field’s \npotential over-the-horizon impact); the likely chal\u0002lenges facing innovation and implementation; and \nrelevant policy, legal, and regulatory issues.\nPolicy for Science and \nTechnology\nPolicymakers have a wide variety of tools to influence \nthe conduct of S&T research and development. Many \nof these are obvious, such as research funding, tax \nincentives to firms, intellectual property rights, export \ncontrols, classification authority,4 regulation, public \nprocurement, funding and other aid to strategic sec\u0002tors, as well as labor force training and education. \nOn the other hand, policy need not be directed \nat S&T to have a meaningful impact. For example, \nimmigration policy is not primarily directed at the S&T \nworkforce, but it can have profound effects on the \ntalent available to academic and industry research. \nPolicy oriented in one direction attracts talent to \nthe United States, while policy oriented in another \ndiminishes such talent. Or consider the national \neconomic environment. Stable fiscal and monetary \npolicies make it easier for private-sector decision \nmakers to plan and invest for the long term—a criti\u0002cal consideration when many S&T advances must be \nnurtured along an extended path from conception \nto maturity. \nTen Science and \nTechnology Fields\nChapters 1 through 10 describe in more detail ten \nS&T fields important to the national agenda. Our \nselection of these fields was driven by several fac\u0002tors: inclusion on common lists of key technologies \ndeveloped by government, the private sector, and \nNOTES\n1. Vannevar Bush, Science: The Endless Frontier (Washington, \nDC: US Government Printing Office, 1945), https://www.nsf.gov\n/od/lpa/nsf50/vbush1945.htm. \n2. National Research Council, The Pervasive Role of Science, \nTechnology, and Health in Foreign Policy: Imperatives for the \nDepartment of State (Washington, DC: National Academies Press, \n1999), https://doi.org/10.17226/9688.\n3. National Intelligence Council, Global Trends 2015: A Dia\u0002logue About the Future with Nongovernment Officials, Decem\u0002ber 2000, https://www.dni.gov/files/documents/Global%20Trends\n_2015%20Report.pdf; National Intelligence Council, Mapping \nthe Global Future: Report of the National Intelligence Council’s \n2020 Project, December 2004, https://www.dni.gov/files/documents\n/Global%20Trends_Mapping%20the%20Global%20Future\n%202020%20Project.pdf.\n4. Under some circumstances (such as when federal funding is \ninvolved), the US government may have the authority to classify \nresearch even if that research was performed without access to \nclassified information.\n\n21\nOverview\nArtificial intelligence (AI), a term coined by computer \nscientist and Stanford professor John McCarthy in \n1955, was originally defined as “the science and \nengineering of making intelligent machines.” In \nturn, intelligence might be defined as the ability to \nlearn and perform suitable techniques to solve prob\u0002lems and achieve goals, appropriate to the context \nin an uncertain, ever-varying world.1 AI could be said \nto refer to a computer’s ability to display this type of \nintelligence.\nThe emphasis today in AI is on machines that can \nlearn as well as humans can learn, or at least some\u0002what comparably so. However, because machines \nare not limited by the constraints of human biology, \nAI systems may be able to run at much higher speeds \nand digest larger volumes and types of information \nthan are possible with human capabilities.\nKEY TAKEAWAYS\n° Artificial intelligence (AI) is a foundational tech\u0002nology that is supercharging other scientific fields \nand, like electricity and the internet, has the \npotential to transform societies, economies, and \npolitics worldwide.\n° Despite rapid progress in the past several years, \neven the most advanced AI still has many failure \nmodes that are unpredictable, not widely appre\u0002ciated, not easily fixed, not explainable, and \ncapable of leading to unintended consequences.\n° Mandatory governance regimes for AI, even \nthose to stave off catastrophic risks, will face stiff \nopposition from AI researchers and companies, \nbut voluntary regimes calling for self-governance \nare more likely to gain support. \nARTIFICIAL INTELLIGENCE\n01\n22 STANFORD EMERGING TECHNOLOGY REVIEW\nproductivity growth by 1.5 percent over a ten-year \nperiod if it is adopted widely.10 Private funding for \ngenerative AI start-ups surged to $25.2 billion in \n2023, a nearly ninefold increase from 2022, and \naccounted for around a quarter of all private invest\u0002ments related to AI in 2023.11\nThe question of what subfields are considered part \nof AI is a matter of ongoing debate, and the bound\u0002aries between these fields are often fluid. Some of \nthe core subfields are the following:\n° Computer vision, enabling machines to recog\u0002nize and understand visual information from the \nworld, convert it into digital data, and make deci\u0002sions based on these data\n° Machine learning (ML), enabling computers \nto perform tasks without explicit instructions, \noften by generalizing from patterns in data. This \nincludes deep learning that relies on multilayered \nartificial neural networks—which process infor\u0002mation in a way inspired by the human brain—to \nmodel and understand complex relationships \nwithin data. \n° Natural language processing, equipping machines \nwith capabilities to understand, interpret, and \nproduce spoken words and written texts\nMost of today’s AI is based on ML, though it draws \non other subfields as well. ML requires data and \ncomputing power—often called compute12—and \nmuch of today’s AI research requires access to these \non an enormous scale.\nIn October 2024, the Royal Swedish Academy of \nSciences awarded the Nobel Prize in Physics for \n2024 to John Hopfield and Geoffrey Hinton for \ntheir work in applying tools and concepts from sta\u0002tistical mechanics to develop “foundational discov\u0002eries and inventions that enable machine learning \nwith artificial neural networks”13 (further discussed \nbelow). Underscoring the importance of AI-based \ntechniques in advancing science, it also awarded the \nToday, AI promises to be a fundamental enabler of \ntechnological advancement in many fields, arguably \nof comparable importance to electricity in an earlier \nera or the internet in more recent years. The science \nof computing, worldwide availability of networks, \nand civilization-scale data—all that collectively \nunderlies the AI of today and tomorrow—are poised \nto have similar impact on technological progress in \nthe future. Moreover, the users of AI will not be lim\u0002ited to those with specialized training; instead, the \naverage person on the street will increasingly inter\u0002act directly with sophisticated AI applications for a \nmultitude of everyday activities.\nThe global AI market was worth $196.63 billion in \n2023, with North America receiving 30.9 percent of \ntotal AI revenues.2 The Stanford Institute for Human\u0002Centered Artificial Intelligence (HAI) AI Index 2024 \nAnnual Report found that private investment in all AI \nstart-ups totaled $95.99 billion in 2023, marking the \nsecond consecutive year of decline since a record \nhigh of over $120 billion in 2021.3 Amid a 42 per\u0002cent fall in overall global venture funding across all \nsectors in 2023,4 AI start-ups raised $42.5 billion in \nventure capital that year, marking only a 10 percent \ndecrease5 from 2022.6\nMany tech companies are significantly ramping up \ninvestments in AI infrastructure, such as larger and \nmore powerful computing clusters to meet the grow\u0002ing demand for AI capabilities. Companies such as \nAmazon and Meta have begun revamping their data \ncenters,7 and BlackRock, Microsoft, and the technol\u0002ogy investor MGX, which is backed by the United Arab \nEmirates, announced in September 2024 the new \nGlobal AI Infrastructure Investment Partnership fund, \nwhich seeks to raise $30 billion in private equity cap\u0002ital to finance data centers and other projects that \nspan the AI infrastructure ecosystem.8 The fund may \nultimately invest up to $100 billion over time.9\nOne estimate forecasts that generative AI—which \ncan create novel text, images, and audio output and \nis discussed in more detail later in this chapter—\ncould raise global GDP by $7  trillion and raise \n01 Artifi\nhardware components were likely also needed—\nsuggests the overall hardware costs for GPT-4 were \nat least a few hundred million dollars. And the chips \nunderlying this hardware are specialty chips often \nfabricated offshore.16 (Chapter 8 on semiconductors \ndiscusses this point at greater length.)\nLastly, AI models consume a lot of energy. Consider \nfirst the training phase: One estimate of the elec\u0002tricity required to train a foundation model such \nas GPT-4 pegs the figure at about fifty  million \nkilowatt-hours (kWh).17 The average American house\u0002hold uses about 11,000 kWh per year, meaning the \nenergy needed to train GPT-4 was approximately \nthe same as that used by 4,500 average homes \nin a year. Paying for this energy adds significant \ncost, even before a single person actually uses a \nmodel.\nThen, once a model is up and running, the cost of \nenergy used to power queries can add up fast. This \nis known as the inference phase. For ChatGPT, \nthe energy used per query is around 0.002 of a \nkilowatt-hour, or 2 watt-hours.18 (For comparison, \na single Google search requires about 0.3 watt\u0002hours,19 and an alkaline AAA battery contains about \n2 watt-hours of energy.) Given hundreds of millions \nof queries per day, the operating energy require\u0002ment of ChatGPT might be a few hundred thousand \nkilowatt-hours per day, at a cost of several tens of \nthousands of dollars. \nAI can automate a wide range of tasks. But it also \nhas particular promise in augmenting human capa\u0002bilities and further enabling people to do what they \nare best at doing.20 AI systems can work alongside \nhumans, complementing and assisting their work \nrather than replacing them. Some present-day \nexamples are discussed below.\nHealthcare\n° Medical diagnostics An AI system that can pre\u0002dict and detect the onset of strokes qualified for \nMedicare reimbursement in 2020.21\nNobel Prize in Chemistry for 2024 to Demis Hassabis \nand John M. Jumper for AI-based protein structure \nprediction,14 an important and long-standing prob\u0002lem in biology and chemistry involving the predic\u0002tion of the three-dimensional shape a protein would \nassume given only the DNA sequence associated \nwith it.\nMachine learning also requires large amounts of \ndata from which it can learn. These data can take \nvarious forms, including text, images, videos, sensor \nreadings, and more. Learning from these data is \ncalled training the AI model.\nThe quality and quantity of data play a crucial role \nin determining the performance and capabilities \nof AI systems. Without sufficient and high-quality \ndata, models may generate inaccurate or biased \noutcomes. (Roughly speaking, a traditional ML \nmodel is developed to solve a particular problem—\ndifferent problems call for different models; for prob\u0002lems sufficiently different from each other, entirely \nnew models need to be developed. Foundation \nmodels, discussed below, break this tradition to \nsome extent.) Research continues on how to train \nsystems incrementally, starting from existing models \nand using a much smaller amount of specially curated \ndata to refine those models’ performance for special\u0002ized purposes.\nFor a sense of scale, estimates of the data required \nto train GPT-4, OpenAI’s large language model \n(LLM) released in March 2023 and the base on which \nsome versions of ChatGPT were built, suggest that \nits training database consisted of the textual equiv\u0002alent of around 100 million books, or about 10 tril\u0002lion words, drawn from billions of web pages and \nscanned books. (LLMs are discussed further below.) \nThe hardware requirements for computing power \nare also substantial. The costs to compute the training \nof GPT-4, for example, were enormous. Reports indi\u0002cate that the training took about twenty-five thousand \nNvidia A100 GPU deep-learning chips—at a cost \nof $10,000 each—running for about one hundred \ndays.15 Doing the math—and noting that other \n24 STANFORD EMERGING TECHNOLOGY REVIEW\n° Autonomous trucking Multiple companies col\u0002laborated in a consortium that arranged for trucks \ncarrying tires to drive autonomously for over \nfifty thousand long-haul trucking miles in the \nperiod from January to August 2024.28 If this and \nother demonstrations continue to be successful, \nit is possible that long-haul drives—the most \nboring and time-consuming aspect of a truck \ndriver’s job—can be automated; at the same time, \naspects of such jobs requiring human-centered \ninteractions, including navigating the first miles \nout of the factory and the last miles of delivering \ngoods to customers, could be retained.\nLaw\n° Legal transcription AI enables the real-time \ntranscription of legal proceedings and client \nmeetings with reasonably high accuracy, and \nsome of these services are free of charge.29\n° Legal review AI-based systems can reduce \nthe time lawyers spend on contract review by as \nmuch as 60  percent. Further, such systems can \nenable lawyers to search case databases more \nrapidly than online human searches—and even \nwrite case summaries.30\nKey Developments\nFoundation Models\nFoundation models dominated the conversation \nabout AI in both 2023 and 2024. These models \nare large-scale systems trained on vast amounts \nof diverse data that can handle a variety of tasks.31\nThey often contain billions or trillions of parame\u0002ters,32 and their massive size allows them to capture \nmore complex patterns and relationships. Trained \non these datasets, foundation models can develop \nbroad capabilities33 and are thus sometimes called \ngeneral-purpose models. They excel at transfer \n° Drug discovery An AI-enabled search iden\u0002tified a compound that inhibits the growth of a \nbacterium responsible for many drug-resistant \ninfections, such as pneumonia and meningitis, by \nsifting through a library of seven thousand poten\u0002tial drug compounds for an appropriate chemical \nstructure.22\n° Patient safety Smart AI sensors and cameras \ncan improve patient safety in intensive care units, \noperating rooms, and even at home by improv\u0002ing healthcare providers’ and caregivers’ ability \nto monitor and react to patient health develop\u0002ments, including falls and injuries.23\n° Robotic assistants Mobile robots using AI can \ncarry out healthcare-related tasks such as making \nspecialized deliveries, disinfecting hospital wards, \nand assisting physical therapists, thus supporting \nnurses and enabling them to spend more time \nhaving face-to-face human interactions.24\nAgriculture\n° Production optimization AI-enabled computer \nvision helps some salmon farmers pick out fish \nthat are the right size to keep, thus off-loading \nthe labor-intensive task of sorting them.25\n° Crop management Some farmers are using AI \nto detect and destroy weeds in a targeted manner, \nsignificantly decreasing environmental harm by \nusing herbicides only on undesired vegetation \nrather than entire fields, in some cases reducing \nherbicide use by as much as 90 percent.26\nLogistics and Transportation\n° Resource allocation AI enables some commer\u0002cial shipping companies to predict ship arrivals \nfive days into the future with high accuracy, thus \nallowing real-time allocations of personnel and \nschedule adjustments.27\n01 Artifi\nmedical advice, and they outscore the median \nhuman performance on clinical examination in obstet\u0002rics and gynecology,34 on standardized tests of \ndivergent thinking,35 and on other standardized tests \nsuch as the LSAT, sections of the GRE, and various \nAP exams.36 However, models do not necessarily \nexcel at the actual tasks or skills that these tests are \ntrying to capture and, as discussed below, still pro\u0002duce errors and fail in all sorts of other ways, many \nof them unexpected.\nWell-known closed-source LLMs include OpenAI’s \nGPT models (e.g., GPT-3, GPT-3.5, and GPT-4), \nAnthropic’s Claude, and Google’s Gemini. Well\u0002known open-source LLMs include Meta’s Llama, Big\u0002Science’s BLOOM, EleutherAI’s GPT-J, and Google’s \nBERT and T5.\nSpecialized foundation models have also been \ndeveloped in other modalities such as audio, video, \nand images:\n° Foundation models for images are able to gen\u0002erate new images based on a user’s text input. \nNovel methods for handling images, combined \nwith using very large collections of pictures and \ntext for training, have led to models that can turn \nwritten descriptions into images that are quickly \nbecoming comparable to—and sometimes indis\u0002tinguishable from—real-life photographs and \nartwork created by humans. Examples include \nOpenAI’s DALL-E 3, the open-source Stable \nDiffusion, Google’s Imagen, Adobe Firefly, and \nMeta’s Make-A-Scene.\n° An example of a foundation model for audio \nis UniAudio, which handles all audio types and \nemploys predictive algorithms to generate \nhigh-quality speech, sound, and music, sur\u0002passing leading methods in tasks such as text \nto speech, speech enhancement, and voice \nconversion.\n° Foundation models in video such as Meta’s Emu \nVideo represent a significant advancement in \nlearning—applying knowledge learned in one con\u0002text to another—making them more flexible and effi\u0002cient than traditional task-specific models. A single \nfoundation model is often fine-tuned for various \ntasks, reducing the need to train separate models \nfrom scratch. \nThese models are generally classified as closed \nsource or open source. A closed-source model is \na proprietary one developed and maintained by \na specific organization, usually a for-profit com\u0002pany, with its source code, data, and architecture \nkept confidential. Access to these models is typi\u0002cally restricted through technically enforced usage \npermissions, such as application programming \ninterfaces, allowing the developers to control the \nmodel’s distribution, usage, and updates. By con\u0002trast, an open-source model is one whose code, \ndata, and underlying architecture are publicly acces\u0002sible, allowing anyone to use, modify, and distribute \nit freely. \nThe most familiar type of foundation model is an \nLLM—a system trained on very large volumes of \ntextual content. LLMs are an example of generative \nAI, a type of AI that can produce new material based \non how it has been trained and the inputs it is given. \nModels trained on text can generate new text based \non a statistical analysis that makes predictions about \nwhat other words are likely to be found immediately \nafter the occurrence of certain words. \nThese models do not think or feel like humans do, \neven though their responses may make it seem like \nthey do. Instead, LLMs use statistical analysis based \non training data. For example, because the word \nsequence “thank you” is far more likely to occur than \n“thank zebras,” a person’s query to an LLM asking it \nto draft a thank-you note to a colleague is unlikely to \ngenerate the response “thank zebras.”\nThese models generate linguistic output surpris\u0002ingly similar to that of humans across a wide range \nof subjects. For example, LLMs can generate useful \ncomputer code, poetry, legal case summaries, and \n26 STANFORD EMERGING TECHNOLOGY REVIEW\nto various formats and learner types, improving \nengagement and comprehension. When integrated \nwith virtual and augmented reality, it can create \nimmersive, highly realistic training environments that \nare particularly valuable in fields like healthcare. The \nadvent of multimodal AI is also set to further trans\u0002form human-computer interactions, enabling more \nintuitive communication and expanding the range of \ntasks that AI systems can handle. \nEmbodied AI\nEmbodied AI involves integrating AI systems into \nrobots or other physical devices. This approach aims \nto bridge the gap between the digital and physical \nrealms. Embodied AI has the potential to enhance \nrobotic capabilities and expand the range of inter\u0002actions robots have with the physical world. These \nrobot-plus-AI systems could potentially address \nknowledge tasks, physical tasks, or combinations \nof both. (This topic is explored further in chapter 7\non robotics.) As research progresses in AI autonomy \nand reasoning, embodied AI systems may be able to \nhandle increasingly complex tasks with greater inde\u0002pendence. This could lead to applications in various \nfields such as logistics and domestic assistance.\nvideo generation. Emu first generates an image \nfrom text input and then creates a video based \non both the text and the generated image. Emu \nVideo has demonstrated superior performance \nover previous state-of-the-art methods in terms \nof image quality, faithfulness to text instructions, \nand evaluations from humans. \nMultimodal Models\nAI systems that incorporate multiple modalities—text, \nimages, and sound—within single models are \nbecoming increasingly popular. This multimodal \napproach, shown in figure 1.1, aims to create more \nhumanlike experiences by leveraging various senses \nsuch as sight, speech, and hearing to mirror how \nhumans interact with the world. \nMultimodal AI systems have diverse applications \nacross sectors. They can enhance accessibility for \npeople with disabilities through real-time transcrip\u0002tion, sign language translation, and detailed image \ndescriptions. They can also eliminate language \nbarriers via cost-effective, near-real-time transla\u0002tion services. In education, multimodal AI can sup\u0002port personalized learning by adapting content \nSingle-modal AI model\nInput Process Output\nText AI Text\nMultimodal AI model\nProcess\nInput Output\nText\nImage AI\nSound\nText\nImages\nSounds\nFIGURE 1.1  Multimodal AI systems can transform one type of input into a different type of output\n01 Artifi\nA National AI Research Resource\nLLMs such as GPT-4, Claude, Gemini, and Llama \ncan be developed only by large companies with the \nresources to build and operate very large data and \ncompute centers. For a sense of scale, Princeton \nUniversity announced in March 2024 that it would dip \ninto its endowment to purchase 300 advanced Nvidia \nchips to use for research at a total estimated cost \nof about $9 million.40 By contrast, Meta announced \nat the start of 2024 that it intended to purchase \n350,000 such chips by the end of the year41—over \none thousand times as many chips as Princeton and \nwith a likely price tag of nearly $10 billion. \nTraditionally, academics and others in civil society \nhave undertaken research to understand the poten\u0002tial societal ramifications of AI, but with large com\u0002panies controlling access to these AI systems, they \ncan no longer do so independently. In July 2023, a \nbipartisan bill (S.2714, the CREATE AI Act of 2023)42\nwas proposed to establish the National Artificial \nIntelligence Research Resource (NAIRR) as a shared \nnational research infrastructure that would provide \ncivil society researchers greater access to the com\u0002plex resources, data, and tools needed to support \nresearch on safe and trustworthy AI. The bill’s text did \nnot mention funding levels, but the final NAIRR task \nforce report, released in January 2023, indicated that \nNAIRR should be funded at a level of $2.6 billion over \nits initial six-year span.43 In January 2024, the National \nScience Foundation established the NAIRR pilot to \nestablish proof of concept for the full-scale NAIRR. \nExistential Concerns About AI\nLLMs have generated considerable attention because \nof their apparent sophistication. Indeed, their capa\u0002bilities have led some to suggest that they are the \ninitial sparks of artificial general intelligence (AGI).37\nAGI is AI that is capable of performing any intellec\u0002tual task that a human can perform, including learn\u0002ing. But, according to this argument, because an \nelectronic AGI would run on electronic circuits rather \nthan biological ones, it is likely to learn much faster \nthan biological human intelligences—rapidly out\u0002stripping their capabilities.\nThe belief in some quarters that AGI will soon be \nachieved has led to substantial debate about its \nrisks. Scholars have continued to argue over the past \nyear about whether current models present initial \nsparks of AGI,38 although there hasn’t been substan\u0002tial evidence presented that proves they possess \nsuch capabilities.\nOthers suggest that focusing on low-probability \ndoomsday scenarios distracts from the real and imme\u0002diate risks AI poses today.39 Instead, society should \nbe prioritizing efforts to address the harms that AI \nsystems are already causing, like biased decision\u0002making, hallucinations (error-ridden responses that \nappear to provide accurate information), and job \ndisplacement. Those who support this view argue \nthat these problems are the ones on which govern\u0002ments and regulators should be concentrating their \nefforts.\nThe advent of multimodal AI is . . . set to further \ntransform human-computer interactions, enabling \nmore intuitive communication and expanding the \nrange of tasks that AI systems can handle.\n28 STANFORD EMERGING TECHNOLOGY REVIEW\nalways relevant, but in certain cases, such as medical \ndecision-making, they may be critical so that users \ncan have confidence in an AI system’s output.\nBias and fairness Because ML models are trained \non existing datasets, they are likely to encode any \nbiases present in these datasets. (Bias should be \nunderstood here as a property of the data that is \ncommonly regarded as societally undesirable.) For \nexample, if a facial recognition system is primarily \ntrained on images of individuals from one ethnic \ngroup, its accuracy at identifying people from other \nethnic groups may be reduced.45 Use of such a system \ncould well lead to disproportionate singling out of \nindividuals in those other groups. To the extent that \nthese datasets reflect historical approaches, they will \nalso reflect the biases embedded in that history, and \nan ML model based on such datasets will also reflect \nthese biases. \nVulnerability to spoofing It is possible to tweak \ndata inputs to fool many AI models into drawing \nfalse conclusions. For example, in figure 1.2, chang\u0002ing a small number of pixels in a visual image of a \ntraffic stop sign can lead to its being classified as \nAs a point of comparison to the fledgling NAIRR \neffort, investments from high-tech companies for AI \nexceeded $27 billion in 2023 alone.44\nOver the Horizon\nImpact of New AI Technologies\nPotential positive impacts of new AI technologies \nare most likely to be seen in the applications they \nenable for societal use, as described in detail above. \nOn the other hand, no technology is an unalloyed \ngood. Potential negative impacts from AI will likely \nemerge from known problems with current state-of\u0002the-art AI and from technical advances in the future. \nSome of the known issues with today’s leading AI \nmodels include the following:\nExplainability This is the ability to explain the rea\u0002soning behind—and describe the data underlying—\nan AI system’s conclusions. Today’s AI is largely \nincapable of explaining the basis on which it arrives \nat any particular conclusion. Explanations are not \nChange a few\npixels\nNew AI classi\u001fcation:\n“yield”\nOriginal AI classi\u001fcation:\n”stop”\nSource: Derived from figure 1 in Fabio Carrara, Fabrizio Falchi, Giuseppe Amato, Rudy Becarelli, and Roberto \nCaldelli, “Detecting Adversarial Inputs by Looking in the Black Box,” in “Transparency in Algorithmic Decision \nMaking,” special issue, ERCIM News 116 (January 2019): 16–19.\nFIGURE 1.2  Changing a few pixels can fool AI into thinking a picture of a stop \nsign is a picture of a yield sign\n01 Artifi\nelection by buying votes.47 In January 2024, voters in \nNew Hampshire received robocalls that used a voice \nsounding like President Biden’s telling them not to \nvote in the state’s presidential primary.48 In elections \nin India in early 2024, deepfake videos were used \nto depict deceased politicians as though they were \nstill alive (see figure 1.3).49 All of these deepfakes are \nmuch more sophisticated than attempts such as the \n“dumbfake” video of Representative Nancy Pelosi \n(D-CA) that involved merely slowing down an exist\u0002ing video of her to make her look drunk.50\nPrivacy Many LLMs are trained on data found \non the internet rather indiscriminately, and such \ndata may include personal information of individu\u0002als. When incorporated into LLMs, this information \ncould be publicly disclosed more often.\nOvertrust and overreliance If AI systems become \ncommonplace in society, their novelty will inevitably \ndiminish for users. The level of trust in computer out\u0002puts often increases with familiarity. But skepticism \nabout answers received from a system is essential if \none is to challenge the correctness of these outputs. \nAs trust in AI grows, reducing skepticism, there’s a \na yield sign, even though this fuzzing of the image \nis invisible to the naked eye. That example seems \ninnocuous, but as AI models are used increasingly \nin applications from medical treatment to intelli\u0002gence and military operations, the potential harms \ncould be substantial. It is also possible that an attack \ntargeting one AI model could work against other \nmodels performing the same task—a phenomenon \nknown as transferability. One study reports that as \noften as 80 percent of the time, transferability allows \nattackers to create an attack on a surrogate model \nand then apply it to their intended target, too.46\nData poisoning An attacker manipulating the \ndataset used to train an ML model can damage its \nperformance and even create predictable errors.\nDeepfakes AI provides the capability for gener\u0002ating highly realistic but entirely inauthentic audio \nand video imagery. This has obvious implications for \nevidence presented in courtrooms and for efforts to \nmanipulate political contests. In September 2023, just \nbefore elections took place in Slovakia, a deepfake \naudio was posted to Facebook in which a candidate \nwas heard discussing with a journalist how to rig the \nPhoto from the late Indian Congress leader\nH. Vasanthakumar‘s funeral in 2020\nScreenshot from a deepfake video of\nH. Vasanthakumar endorsing his son‘s\nparliamentary candidacy in 2024\nSource: (Left) PTI Photo / R. Senthil Kumar; (right) “H Vasantha Kumar,” posted April 16, 2024, by Vasanth TV, YouTube, https://\nwww.youtube.com/watch?v=98_K-Ag7p2M\nFIGURE 1.3  Deepfake videos of deceased Indian politicians speaking as if they were alive were used \nin India’s 2024 elections\n30 STANFORD EMERGING TECHNOLOGY REVIEW\nand metadata in building and offering the prod\u0002ucts Stable Diffusion (an application that generates \nimages from text) and DreamStudio (the app that \nserves as a user interface to Stable Diffusion).52 In \nlate 2023, the New York Times sued OpenAI and \nMicrosoft over their alleged use of millions of arti\u0002cles published by the Times to train the companies’ \nLLMs.53 In June 2024, music labels Sony Music, \nUniversal Music Group, and Warner Records sued \nAI start-ups Suno and Udio for copyright infringe\u0002ment, alleging that the companies had trained their \nmusic-generation systems on protected content.54\nAI researchers are cognizant of issues such as these, \nand in many cases work has been done—or is being \ndone—to develop corrective measures. However, \nin most cases, these defenses don’t apply very well \nto instances beyond the specific problems that they \nwere designed to solve.\nChallenges of Innovation and \nImplementation \nThe primary challenge of bringing AI innovation into \noperation is risk management. It is often said that \nAI, and especially ML, brings a new conceptual par\u0002adigm for how systems can exploit information to \ngain advantage, relying on pattern recognition in the \nbroadest sense rather than on explicit understand\u0002ing of situations that are likely to occur. Because \nthere have been significant recent advances in AI, \nthe people who would make decisions to deploy \nAI-based systems may not have a good under\u0002standing of the risks that could accompany such \ndeployment.\nConsider, for example, AI as an important approach \nfor improving the effectiveness of military oper\u0002ations. Despite broad agreement by the military \nservices and the US Department of Defense (DOD) \nthat AI would be of great benefit, the actual inte\u0002gration of AI-enabled capabilities into military forces \nhas proceeded at a slow pace. Certainly, it is well \nunderstood that technical risks of underperformance \nand error in new technologies take time to mitigate. \nhigher risk that errors, mishaps, and unforeseen inci\u0002dents will be overlooked. One recent experiment \nshowed that developers with access to an AI-based \ncoding assistant wrote code that was significantly less \nsecure than those without an AI-based assistant—\neven though the former were more likely to believe \nthey had written secure code.51\nHallucinations As noted earlier, AI hallucina\u0002tions refer to situations where an AI model gen\u0002erates results or answers that are plausible but do \nnot correspond to reality. In other words, models \ncan simply make things up, but human users will \nnot be aware they have done this. The results are \nplausible because they are constructed based on \nstatistical patterns that the model has learned to \nrecognize from its training data. But they may not \ncorrespond to reality because the model does not \nhave an understanding of the real world. For exam\u0002ple, in September 2024, a Stanford professor asked \nan AI model to name ten publications she had writ\u0002ten. The AI responded with five correct publications \nand five that she had never actually written—but the \nAI results included titles and summaries that made \nthem seem real. When she told the model that “the \nlast two entries are hallucinations,” it simply pro\u0002vided two new results that were also hallucinations.\nOut-of-distribution inputs All ML systems must \nbe trained on a large volume of data. If the inputs \nsubsequently given to a system are substantially \ndifferent from the training data—a situation known \nas being out-of-distribution—the system may draw \nconclusions that are more unreliable than if the \ninputs were similar to the training data. \nCopyright violations Some AI-based models have \nbeen trained on large volumes of data found online. \nThese data have generally been used without the \nconsent or permission of their owners, thereby raising \nimportant questions about appropriately compen\u0002sating and acknowledging those owners. For exam\u0002ple, in January 2023, Getty Images sued Stability AI \nin an English court for infringing on the copyrights of \nmillions of photographs, their associated captions, \n01 Artifi\nthe humans it replaces.58 In at least some cases, \ncompanies are deciding that the cost savings of \neliminating human workers outweigh the draw\u0002backs of mediocre AI performance.\n° Training displaced workers to be more compet\u0002itive in an AI-enabled economy does not solve \nthe problem if new jobs are not available. The \nnature and extent of new roles resulting from \nwidespread AI deployment are not clear at this \npoint, although historically the introduction of \nnew technologies has not resulted in a long-term \nnet loss of jobs.59\nGOVERNANCE AND REGULATION OF AI\nGovernments around the world have been increas\u0002ingly focused on establishing regulations and guide\u0002lines for AI. Research on foundational AI technologies \nis difficult to regulate across international bound\u0002aries even among like-minded nations, especially \nwhen other nations have strong incentives to carry \non regardless of actions taken by US policymakers. It \nis even more difficult, and may well be impossible, to \nreach agreement between nations that regard each \nother as strategic competitors and adversaries. The \nsame applies to voluntary restrictions on research \nby companies concerned about competition from \nless constrained foreign rivals. Regulation of specific \napplications of AI may be more easily implemented, \nin part because of existing regulatory frameworks in \ndomains such as healthcare, finance, and law. \nThe most ambitious attempt to regulate AI came \ninto force in August 2024 with the European Union’s \nAI Act. This forbids certain applications of AI, such \nas individual predictive policing based solely on a \nperson’s data profile or tracking of their emotional \nstate in the workplace and educational institutions, \nunless for medical or safety reasons.60 Additionally, \nit imposes a number of requirements on what the \nAI Act calls “high-risk” systems. (The legislation pro\u0002vides a very technical definition of such systems, but \ngenerally they include those that could pose a sig\u0002nificant risk to health, safety, or fundamental rights.) \nBut another important reason for the slow pace is \nthat the DOD acquisition system has largely been \ndesigned to minimize the likelihood of program\u0002matic failure, fraud, unfairness, waste, and abuse—in \nshort, to minimize risk. It is therefore not surprising \nthat the incentives at every level of the bureaucracy \nare aligned in that manner. For new approaches like \nAI to take root, a greater degree of programmatic \nrisk acceptance may be necessary, especially in light \nof the possibility that other nations could adopt the \ntechnology faster, achieving military advantages \nover US forces.\nPolicy, Legal, and Regulatory Issues\nTHE FUTURE OF WORK\nSome researchers expect that, within the next five \nto ten years, more and more workers will have AI \nadded to their workflows to enhance productivity \nor will even be replaced by AI systems, which may \ncause significant disruptions to the job market in the \nnear future.55 LLMs have already demonstrated how \nthey can be used in a wide variety of fields, includ\u0002ing law, customer support, coding, and journalism. \nThese demonstrations have led to concerns that \nthe impact of AI on employment will be substan\u0002tial, especially on jobs that involve knowledge work. \nHowever, uncertainty abounds. What and how many \npresent-day jobs will disappear? Which tasks could \nbest be handled by AI? And what new jobs might be \ncreated by the technology today and in the future?\nSome broad outlines and trends are clear: \n° Individuals whose jobs entail routine white-collar \nwork may be more affected than those whose \njobs require physical labor; some will experience \npainful shifts in the short term.56\n° AI is helping some workers to increase their pro\u0002ductivity and job satisfaction.57 At the same time, \nother workers are already losing their jobs as AI \ndemonstrates adequate competence for business \noperations, despite potentially underperforming \n32 STANFORD EMERGING TECHNOLOGY REVIEW\nin September 2024. The act sought to hold the cre\u0002ators of advanced AI models liable in civil court for \ncausing catastrophic harms unless they had taken \ncertain advance measures to forestall such an out\u0002come. Opposition to the bill was based on con\u0002cerns about a technologically deficient definition of \nadvanced AI models, the burden that the bill would \nplace on small start-ups and academia, and the \nunfairness of holding model developers responsible \nfor harmful applications that others build using the \ndevelopers’ models.\nOther important developments regarding AI gov\u0002ernance include the AI Safety Summit, held on \nNovember 1–2, 2023, at Bletchley Park in the United \nKingdom,62 which issued the Bletchley Declaration, \nand the AI Seoul Summit of May 2024. In the Bletchley \nDeclaration, the European Union and twenty-eight \nnations collectively endorsed international coopera\u0002tion to manage risks associated with highly capable \ngeneral-purpose AI models. Signatories commit\u0002ted to ensuring that AI systems are developed and \ndeployed safely and responsibly. The summit also \nled to the establishment of the United Kingdom’s \nAI Safety Institute and the US Artificial Intelligence \nSafety Institute, located within the National Institute \nof Standards and Technology.\nThe Seoul Declaration from the AI Seoul Summit \n2024 built on the Bletchley Declaration to acknowl\u0002edge the importance of interoperability between \nnational AI governance frameworks to maximize \nbenefits and minimize risks from advanced AI sys\u0002tems. In addition, sixteen major AI organizations \nThese requirements address data quality, documen\u0002tation and traceability, transparency and explainabil\u0002ity, human oversight, accuracy, cybersecurity, and \nrobustness.\nIn the United States, the president’s Executive Order \non the Safe, Secure, and Trustworthy Development \nand Use of Artificial Intelligence was issued on \nOctober 30, 2023.61 The order addressed actions to \nadvance AI safety and security; privacy; equity and \ncivil rights; consumer, patient, student, and worker \ninterests; the promotion of innovation and com\u0002petition, as well as American leadership; and gov\u0002ernment use of AI. Of particular note is the order’s \nrequirement that developers of advanced AI systems \nposing a serious risk to national security, national eco\u0002nomic security, or national public health and safety \ninform the US government when training them and \nshare with it all results from internal safety testing \nconducted by red teams. (A red team is a team of \nexperts that attempts to subvert or break the system \nit is asked to test. It then reports its findings to the \nowner of the system so that the owner can take cor\u0002rective action.) The order also requires government \nactions to develop guidance to help protect against \nthe use of AI to develop biological threats and to \nadvance the use of AI to protect against cyberse\u0002curity threats, to help detect AI-generated content, \nand to authenticate official content.\nAt the state level in the United States, an attempt \nto pass an AI regulatory bill in California (SB 1047, \nthe Safe and Secure Innovation for Frontier Artificial \nIntelligence Models Act) was vetoed by the governor \nThe resources needed to train GPT-4 far exceed those \navailable through grants or any other sources to any \nreasonably sized group of the top US research universities.\n01 Artifi\nof weapons or in making decisions about the use \nof deadly force. Notably, in late 2023, press reports \nindicated that President Biden and Chinese President \nXi Jinping had considered entering into a dialogue \nabout AI in nuclear command and control, but such \nan arrangement was never formalized.66\nTALENT\nThe United States is eating its seed corn with respect \nto the AI talent pool. As noted in the Foreword, fac\u0002ulty at Stanford and other universities report that the \nnumber of students studying in AI who are joining \nthe industry, particularly start-ups, is increasing at \nthe expense of those pursuing academic careers and \ncontributing to foundational AI research. \nMany factors are contributing to this trend. One \nis that industry careers come with compensation \npackages that far outstrip those offered by aca\u0002demia. Academic researchers must also obtain \nfunding to pay for research equipment, computing \ncapability, and personnel like staff scientists, tech\u0002nicians, and programmers. This involves searching \nfor government grants, which are typically small \ncompared to what large companies might be will\u0002ing to invest in their own researchers. Consider, for \nexample, that the resources needed to build and \ntrain GPT-4 far exceed those available through \ngrants or any other sources to any reasonably sized \ngroup of the top US research universities, let alone \nany single university.\nIndustry often makes decisions more rapidly than \ngovernment grant makers and imposes fewer reg\u0002ulations on the conduct of research. Large com\u0002panies are at an advantage because they have \nresearch-supporting infrastructure in place, such as \ncompute facilities and data warehouses.\nOne important consequence is that academic access \nto research infrastructure is limited, so US-based stu\u0002dents are unable to train on state-of-the-art systems—\nat least this is the case if their universities do not \nhave access to the facilities of the corporate sector. \nagreed on the Frontier AI Safety Commitments, a \nset of voluntary guidelines regarding the publication \nof safety frameworks for frontier AI models and the \nsetting of thresholds for intolerable risks, among \nother things. \nNATIONAL SECURITY\nAI is expected to have a profound impact on mil\u0002itaries worldwide.63 Weapons systems, command \nand control, logistics, acquisition, and training will \nall seek to leverage multiple AI technologies to \noperate more effectively and efficiently, at lower \ncost and with less risk to friendly forces. Trying \nto overcome decades of institutional inertia, the \nDOD is dedicating billions of dollars to institutional \nreforms and research advances aimed at integrat\u0002ing AI into its warfighting and war preparation \nstrategies. Senior military officials recognize that \nfailure to adapt to the emerging opportunities and \nchallenges presented by AI would pose significant \nnational security risks, particularly considering that \nboth Russia and China are heavily investing in AI \ncapabilities.\nIn adopting a set of guiding principles that address \nresponsibility, equity, traceability, reliability, and \ngovernability in and for AI,64 the DOD has taken \nan important first step in meeting its obligation to \nproceed ethically with the development of AI capa\u0002bilities; eventually, these principles will have to be \noperationalized in specific use cases. An additional \nimportant concern, subsumed under these principles \nbut worth calling out, is determining where the use \nof AI may or may not be appropriate—for example, \nwhether AI is appropriate in nuclear command and \ncontrol. The United States, the United Kingdom, and \nFrance have made explicit commitments to maintain \nhuman control over nuclear weapons.65\nMeanwhile, other countries are also adopting AI, \nand nations such as Russia and China are unlikely \nto make the same operational and ethical decisions \nas Western countries about the appropriate roles \nof AI vis-à-vis humans in controlling the operation \n34 STANFORD EMERGING TECHNOLOGY REVIEW\nFigure 1.4 shows that most notable ML systems are \nnow released by industry, while very few are released \nby academic institutions.\nAt the same time, China’s efforts to recruit top sci\u0002entific talent offer further temptations for scientists \nto leave the United States. These efforts are often \ntargeted toward ethnic Chinese in the US—ranging \nfrom well-established researchers to those just fin\u0002ishing graduate degrees—and offer recruitment \npackages that promise benefits comparable to those \navailable from private industry, such as high salaries, \nlavish research funding, and apparent freedom from \nbureaucracy. \nAll of these factors are leading to an AI “brain drain” \nthat does not favor the US research enterprise. 2003 2004 2005 2006 2007 2008 2009 2010\n2011201220132014201520162017201820192020202120222023\n50\n40\n30\n20\n10\n0\nNumber of notable machine-learning models by sector, 2003–23 Number of notable machine-learning models\nIndustry Industry-academia collaboration Academia\n51\n21\n15\nSource: Adapted from Nestor Maslej, Loredana Fattorini, Raymond Perrault, et al., The AI Index 2024 Annual Report, AI Index Steering Committee, \nInstitute for Human-Centered AI, Stanford University, Stanford, CA, April 2024. Data from Epoch, 2023\nFIGURE 1.4  Most notable machine-learning models are now released by industry\nNOTES\n1. Christopher Manning, “Artificial Intelligence Definitions,” Insti\u0002tute for Human-Centered AI, Stanford University, September 2020, \nhttps://hai.stanford.edu/sites/default/files/2020-09/AI-Definitions\n-HAI.pdf.\n2. Grand View Research, “Artificial Intelligence Market Size, \nShare, Growth Report 2024–2030,” accessed September 23, 2024, \nhttps://www.grandviewresearch.com/industry-analysis/artificial\n-intelligence-ai-market.\n3. Nestor Maslej, Loredana Fattorini, Raymond Perrault, et al., \nThe AI Index 2024 Annual Report, AI Index Steering Committee, \nInstitute for Human-Centered AI, Stanford University, Stanford, CA, \nApril 2024, https://aiindex.stanford.edu/wp-content/uploads/2024\n/05/HAI_AI-Index-Report-2024.pdf.\n4. CB Insights, “State of Venture 2023 Report,” February 1, 2024, \nhttps://www.cbinsights.com/research/report/venture-trends-2023/.\n5. Reputable sources disagree as to whether global venture fund\u0002ing for AI start-ups increased or decreased in 2023. We have fol\u0002lowed the majority view.\n6. CB Insights, “State of AI 2023 Report,” February 1, 2024, https://\nwww.cbinsights.com/research/report/ai-trends-2023/.\n7. Karen Weise, “In Race to Build A.I., Tech Plans a Big Plumbing \nUpgrade,” New York Times, April 27, 2024, https://www.nytimes\n.com/2024/04/27/technology/ai-big-tech-spending.html.\n01 Artifi\n8. Jack Pitcher and Connor Hart, “BlackRock, Microsoft, Others \nForm AI and Energy Infrastructure Investment Partnership,” \nWall Street Journal, September 17, 2024, https://www.wsj.com\n/tech/ai/blackrock-global-infrastructure-partners-microsoft\n-mgx-launch-ai-partnership-1d00e09f.\n9. BlackRock, “BlackRock, Global Infrastructure Partners, Micro\u0002soft, and MGX Launch New AI Partnership to Invest in Data Cen\u0002ters and Supporting Power Infrastructure,” September 17, 2024, \nhttps://ir.blackrock.com/news-and-events/press-releases/press\n-releases-details/2024/BlackRock-Global-Infrastructure-Partners\n-Microsoft-and-MGX-Launch-New-AI-Partnership-to-Invest-in\n-Data-Centers-and-Supporting-Power-Infrastructure/default.aspx.\n10. Goldman Sachs, “Generative AI Could Raise Global GDP by \n7%,” April 5, 2023, https://www.goldmansachs.com/intelligence\n/pages/generative-ai-could-raise-global-gdp-by-7-percent.html.\n11. Maslej et al., The AI Index 2024 Report.\n12. Jafar Alzubi, Anand Nayyar, and Akshi Kumar, “Machine \nLearning from Theory to Algorithms: An Overview,” Journal of \nPhysics: Conference Series 1142, Second National Conference \non Computational Intelligence (December 2018), https://doi\n.org/10.1088/1742-6596/1142/1/012012.\n13. The Nobel Prize in Physics 2024, “Summary,” The Nobel Prize, \nOctober 12, 2024, https://www.nobelprize.org/prizes/physics\n/2024/summary/.\n14. The Nobel Prize in Chemistry 2024, “Summary,” The Nobel Prize, \nOctober 12, 2024, https://www.nobelprize.org/prizes/chemistry\n/2024/summary/.\n15. Kif Leswing, “Meet the $10,000 Nvidia Chip Powering the \nRace for A.I.,” CNBC, February 23, 2023, https://www.cnbc\n.com/2023/02/23/nvidias-a100-is-the-10000-chip-powering-the\n-race-for-ai-.html; Kasper Groes Albin Ludvigsen, “The Carbon \nFootprint of GPT-4,” Medium, July 18, 2023, https://towardsdata\nscience.com/the-carbon-footprint-of-gpt-4-d6c676eb21ae.\n16. Darian Woods and Adrian Ma, “The Semiconductor Founding \nFather,” December 21, 2021, in The Indicator from Planet Money, \npodcast produced by NPR, MP3 audio, 10:14, https://www.npr\n.org/transcripts/1066548023.\n17. Ludvigsen, “The Carbon Footprint.”\n18. Kasper Groes Albin Ludvigsen, “ChatGPT’s Electricity Con\u0002sumption,” Medium, July 12, 2023, https://towardsdatascience\n.com/chatgpts-electricity-consumption-7873483feac4. Different \nsources provide somewhat different numbers for the energy cost \nper query, but they all are in the range of a few watt-hours.\n19. EPRI, “Powering Intelligence: Analyzing Artificial Intelligence \nand Data Center Energy Consumption,” Technology Innovation, \naccessed September 23, 2023, https://www.epri.com/research\n/products/3002028905.\n20. Hope Reese, “A Human-Centered Approach to the AI \nRevolution,” Institute for Human-Centered AI, Stanford Uni\u0002versity, October 17, 2022, https://hai.stanford.edu/news/human\n-centered-approach-ai-revolution.\n21. Viz.ai, “Viz.ai Receives New Technology Add-on Payment \n(NTAP) Renewal for Stroke AI Software from CMS,” August 4, \n2021, https://www.viz.ai/news/ntap-renewal-for-stroke-software.\n22. Gary Liu, Denise B. Catacutan, Khushi Rathod, et al., “Deep \nLearning-Guided Discovery of an Antibiotic Targeting Acineto\u0002bacter baumannii,” Nature Chemical Biology (2023), https://doi\n.org/10.1038/s41589-023-01349-8.\n23. Albert Haque, Arnold Milstein, and Fei-Fei Li, “Illuminating \nthe Dark Spaces of Healthcare with Ambient Intelligence,” Nature\n585 (2020): 193–202, https://doi.org/10.1038/s41586-020-2669-y.\n24. Khari Johnson, “Hospital Robots Are Helping Combat a Wave \nof Nurse Burnout,” Wired, April 19, 2022, https://www.wired.com\n/story/moxi-hospital-robot-nurse-burnout-health-care.\n25. Fish Site, “Innovasea Launches AI-Powered Biomass Camera \nfor Salmon,” August 17, 2023, https://thefishsite.com/articles\n/innovasea-launches-ai-powered-biomass-camera-for-salmon.\n26. Itransition, “Machine Learning in Agriculture: Use Cases and \nApplications,” February 1, 2023, https://www.itransition.com/\nmachine-learning/agriculture.\n27. GateHouse Maritime, “Vessel Tracking Giving Full Journey \nVisibility,” accessed August 15, 2023, https://gatehousemaritime\n.com/data-services/vessel-tracking.\n28. Kodiak, “J.B. Hunt, Bridgestone and Kodiak Surpass 50,000 \nAutonomous Long-Haul Trucking Miles in Delivery Collabo\u0002ration,” August 7, 2024, https://kodiak.ai/news/jb-hunt-and\n-kodiak-collaborate.\n29. JD Supra, “Artificial Intelligence in Law: How AI Can Reshape \nthe Legal Industry,” September 12, 2023, https://www.jdsupra\n.com/legalnews/artificial-intelligence-in-law-how-ai-8475732.\n30. Steve Lohr, “A.I. Is Doing Legal Work. But It Won’t Replace Law\u0002yers, Yet,” New York Times, March 19, 2017, https://www.nytimes\n.com/2017/03/19/technology/lawyers-artificial-intelligence.html.\n31. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, et al., “On \nthe Opportunities and Risks of Foundation Models,” arXiv, Stan\u0002ford University, July 12, 2022, https://doi.org/10.48550/arXiv\n.2108.07258. \n32. Parameters are like the building blocks of knowledge that \nmake up an AI system’s understanding. You can think of them as \ntiny bits of information the AI uses to make sense of data and gen\u0002erate responses. When we say AI models have billions or trillions \nof parameters, it means they have an enormous number of these \ninformation pieces to work with. This allows them to understand \nand generate more sophisticated and nuanced content.\n33. Bommasani et al., “On the Opportunities and Risks.”\n34. Sarah W. Li, Matthew W. Kemp, Susan J. S. Logan, Sebastian \nE. Illanes, and Mahesh A. Choolani, “ChatGPT Outscored Human \nCandidates in a Virtual Objective Structured Clinical Examination \nin Obstetrics and Gynecology,” American Journal of Obstetrics & \nGynecology 229, no. 2 (August 2023): 172.E1-172.E12, https://\ndoi.org/10.1016/j.ajog.2023.04.020.\n35. Kent F. Hubert, Kim N. Awa, and Darya L. Zabelina, “The Cur\u0002rent State of Artificial Intelligence Generative Language Models \nIs More Creative Than Humans on Divergent Thinking Tasks,” \nScientific Reports 14, no. 3440 (February 2024), https://doi\n.org/10.1038/s41598-024-53303-w.\n36. Josh Achiam, Steven Adler, Sandhini Agarwal, et al., “GPT-4 \nTechnical Report,” arXiv, March 4, 2024, https://doi.org/10.48550\n/arXiv.2303.08774. \n37. Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, et \nal., “Sparks of Artificial General Intelligence: Early Experiments \nwith GPT-4,” arXiv, Cornell University, April 13, 2023, https://doi\n.org/10.48550/arXiv.2303.12712.\n38. For an argument that they are, see Bubeck et al., “Sparks of \nArtificial General Intelligence”; for an argument that they are not, \nsee Thomas Macaulay, “Meta’s AI Chief: LLMs Will Never Reach \n36 STANFORD EMERGING TECHNOLOGY REVIEW\nHuman-Level Intelligence,” The Next Web, April 10, 2024, https://\nthenextweb.com/news/meta-yann-lecun-ai-behind-human\n-intelligence.\n39. ”Stop Talking about Tomorrow’s AI Doomsday When AI Poses \nRisks Today,” editorial, Nature 618 (June 2023): 885–86, https://\nwww.nature.com/articles/d41586-023-02094-7.\n40. Poornima Apte, “Princeton Invests in New 300-GPU Clus\u0002ter for Academic AI Research,” AI at Princeton, Princeton Uni\u0002versity, March 15, 2024, https://ai.princeton.edu/news/2024\n/princeton-invests-new-300-gpu-cluster-academic-ai-research.\n41. Tae Kim, “Mark Zuckerberg Says Meta Will Own Billions \nWorth of Nvidia H100 GPUs by Year End,” Barrons, January 19, \n2024, https://www.barrons.com/articles/meta-stock-price-nvidia\n-zuckerberg-b0632fed.\n42. The bill’s full name is Creating Resources for Every American to \nExperiment with Artificial Intelligence Act of 2023. Congress.gov, \n“S.2714 – 118th Congress (2023–2024): CREATE AI Act of 2023,” \nJuly 27, 2023, https://www.congress.gov/bill/118th-congress/senate\n-bill/2714. \n43. White House, National Artificial Intelligence Research Resource \nTask Force Releases Final Report, Office of Science and Tech\u0002nology Policy, News & Updates, Press Releases, https://www\n.whitehouse.gov/ostp/news-updates/2023/01/24/national\n-artificial-intelligence-research-resource-task-force-releases\n-final-report/.\n44. Karen Kwok, “AI Firms Lead Quest for Intelligent Business \nModel,” Reuters, December 12, 2023, https://www.reuters.com\n/breakingviews/ai-firms-lead-quest-intelligent-business-model\n-2023-12-12/.\n45. Joy Buolamwini and Timnit Gebru, “Gender Shades: Intersec\u0002tional Accuracy Disparities in Commercial Gender Classification,” \nProceedings of Machine Learning Research 81, Conference on Fair\u0002ness, Accountability, and Transparency (February 2018): 1–15, https://\nwww.media.mit.edu/publications/gender-shades-intersectional\n-accuracy-disparities-in-commercial-gender-classification.\n46. Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow, \n“Transferability in Machine Learning: From Phenomena to Black-Box \nAttacks Using Adversarial Samples,” arXiv, May 24, 2016, https://\ndoi.org/10.48550/arXiv.1605.07277.\n47. Ivana Kottasová, Sophie Tanno, and Heather Chen, “Pro\u0002Russian Politician Wins Slovakia’s Parliamentary Election,” CNN, \nOctober 2, 2023, https://www.cnn.com/2023/10/01/world/slovakia\n-election-pro-russia-robert-fico-win-intl-hnk/index.html.\n48. Jeongyoon Han, “New Hampshire Is Investigating a Robo\u0002call That Was Made to Sound Like Biden,” NPR, January 22, \n2024, https://www.npr.org/2024/01/22/1226129926/nh-primary\n-biden-ai-robocall.\n49. Samriddhi Sakunia, “AI and Deepfakes Played a Big Role in \nIndia’s Elections,” New Lines Magazine, July 12, 2024, https://\nnewlinesmag.com/spotlight/ai-and-deepfakes-played-a-big\n-role-in-indias-elections/.\n50. Reuters, “Fact Check: ‘Drunk’ Nancy Pelosi Video Is Manipu\u0002lated,” August 3, 2020, https://www.reuters.com/article/world/fact\n-check-drunk-nancy-pelosi-video-is-manipulated-idUSKCN24Z2B1/.\n51. Neil Perry, Megha Srivastava, Deepak Kumar, and Dan Boneh, \n“Do Users Write More Insecure Code with AI Assistants?,” arXiv, \nCornell University, December 18, 2023, https://doi.org/10.48550\n/arXiv.2211.03622.\n52. Charlotte Hill, Charlotte Allen, Tom Perkins, and Harriet \nCampbell, “Generative AI in the Courts: Getty Images v Stability \nAI,” Penningtons Manches Cooper, February 16, 2024, https://\nwww.penningtonslaw.com/news-publications/latest-news/2024\n/generative-ai-in-the-courts-getty-images-v-stability-ai.\n53. Michael M. Grynbaum and Ryan Mac, “The Times Sues \nOpenAI and Microsoft Over A.I. Use of Copyrighted Work,” New \nYork Times, December 27, 2023, https://www.nytimes.com\n/2023/12/27/business/media/new-york-times-open-ai-microsoft\n-lawsuit.html.\n54. Blake Brittain, “Music Labels Sue AI Companies Suno, Udio \nfor US Copyright Infringement,” Reuters, June 24, 2024, https://\nwww.reuters.com/technology/artificial-intelligence/music\n-labels-sue-ai-companies-suno-udio-us-copyright-infringement\n-2024-06-24/.\n55. Maja S. Svanberg, Wensu Li, Martin Fleming, Brian C. Goeh\u0002ring, and Neil C. Thompson, “Beyond AI Exposure: Which Tasks \nAre Cost-Effective to Automate With Computer Vision?,” Future \nTech, Working Paper, January 18, 2024, https://futuretech\n-site.s3.us-east-2.amazonaws.com/2024-01-18+Beyond_AI\n_Exposure.pdf.\n56. Claire Cain Miller and Courtney Cox, “In Reversal Because of \nA.I., Office Jobs Are Now More at Risk,” New York Times, August 24, \n2023, https://www.nytimes.com/2023/08/24/upshot/artificial\n-intelligence-jobs.html.\n57. Martin Neil Baily, Erik Brynjolfsson, and Anton Korinek, \n“Machines of Mind: The Case for an AI-Powered Productiv\u0002ity Boom,” Brookings Institution, May 10, 2023, https://www\n.brookings.edu/articles/machines-of-mind-the-case-for-an-ai\n-powered-productivity-boom.\n58. Pranshu Verma and Gerrit De Vynck, “ChatGPT Took Their \nJobs: Now They Walk Dogs and Fix Air Conditioners,” Washington \nPost, June 5, 2023, https://www.washingtonpost.com/technology\n/2023/06/02/ai-taking-jobs/; Challenger, Gray & Christmas, Inc., \n“Challenger Report,” May 2023, https://omscgcinc.wpengine\npowered.com/wp-content/uploads/2023/06/The-Challenger\n-Report-May23.pdf.\n59. David Autor, Caroline Chin, Anna M. Salomons, and Bryan \nSeegmiller, “New Frontiers: The Origins and Content of New \nWork, 1940–2018,” National Bureau of Economic Research, Work\u0002ing Paper 30389, August 2022, https://doi.org/10.3386/w30389.\n60. Parliament and Council Regulation 2024/1689 Artificial Intel\u0002ligence Act, art. 6-7, 2024 O.J. L 2024/1689, https://artificial\nintelligenceact.eu/section/3-1/.\n61. White House, Executive Order on the Safe, Secure, and \nTrustworthy Development and Use of Artificial Intelligence, Pres\u0002idential Actions, Briefing Room, White House, October 30, 2023, \nhttps://www.whitehouse.gov/briefing-room/presidential-actions\n/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy\n-development-and-use-of-artificial-intelligence/.\n62. Department for Science, Innovation, and Technology, “The \nBletchley Declaration by Countries Attending the AI Safety \nSummit, 1–2 November 2023,” Foreign, Commonwealth, and \nDevelopment Office, Prime Minister’s Office, November 1, 2023, \nhttps://www.gov.uk/government/publications/ai-safety-summit\n-2023-the-bletchley-declaration/the-bletchley-declaration\n-by-countries-attending-the-ai-safety-summit-1-2-november\n-2023.\n63. National Security Commission on Artificial Intelligence, \nFinal Report, March 19, 2021, https://apps.dtic.mil/sti/pdfs\n/AD1124333.pdf.\n64. C. Todd Lopez, “DOD Adopts 5 Principles of Artificial Intel\u0002ligence Ethics,” DOD News, US Department of Defense, \n01 Artifi\nFebruary 25, 2020, https://www.defense.gov/News/News-Stories\n/article/article/2094085/dod-adopts-5-principles-of-artificial\n-intelligence-ethics.\n65. US Department of Defense, 2022 Nuclear Posture Review, \nOctober 27, 2022, 13, https://apps.dtic.mil/sti/trecms/pdf\n/AD1183539.pdf; Ministry of Defence, “Defence Artificial Intel\u0002ligence Strategy,” GOV.UK, June 15, 2022, https://www.gov\n.uk/government/publications/defence-artificial-intelligence\n-strategy/defence-artificial-intelligence-strategy; United Nations \nParties to the Treaty on Non-Proliferation of Nuclear Weapons, \nPrinciples and Responsible Practices for Nuclear Weapon States, \nNPT/CONF.2020/WP.70, July 2022, https://undocs.org/NPT\n/CONF.2020/WP.70.\n66. Ashely Deeks, “Too Much Too Soon: China, the U.S., and Auton\u0002omy in Nuclear Command and Control,” Lawfare, December 4, \n2023, https://www.lawfaremedia.org/article/too-much-too-soon\n-china-the-u.s.-and-autonomy-in-nuclear-command-and-control.\nSTANFORD EXPERT CONTRIBUTORS\nDr. Fei-Fei Li\nSETR Faculty Council, Sequoia Professor in the \nComputer Science Department, and Professor, by \ncourtesy, of Operations, Information, and Technology \nat the Graduate School of Business\nDr. Christopher Manning\nThomas M. Siebel Professor of Machine Learning, \nand Professor of Linguistics and of Computer Science\nAnka Reuel\nSETR Fellow and PhD Student in Computer Science \n\n39\nOverview\nBiotechnology involves using living systems and \norganisms to develop or make products and solve \nproblems. First-generation biotechnology arose over \nmillennia and involved the domestication and selec\u0002tive breeding of plants and animals1\n for agriculture, \nfood production, companionship, and other purpos\u0002es.2\n Second-generation biotechnology was launched \na half century ago with the invention of recombinant \nDNA3 and has since encompassed techniques such \nas genetic engineering, polymerase chain reac\u0002tion (PCR), high-throughput DNA sequencing, and \nCRISPR gene-editing technology.4 Both breeding \nand editing approaches continue to advance, cre\u0002ating and using ever better tools for sculpting5\n and \nediting6 living systems. \nBiotechnology products and services realized through \nbreeding and editing are already widely deployed. A \n2020 National Academies of Sciences, Engineering, \nand Medicine report valued the US bioeconomy at \nKEY TAKEAWAYS\n° Biotechnology is poised to emerge as a general\u0002purpose technology by which anything bioen\u0002gineers learn to encode in DNA can be grown \nwhenever and wherever needed—essentially \nenabling the production of a wide range of prod\u0002ucts through biological processes across multiple \nsectors.\n° The US government is still working to grasp the \nscale of this bio-opportunity and has relied too \nheavily on private-sector investment to support \nthe foundational technology innovation needed \nto unlock and sustain progress.\n° Biotechnology is one of the most important areas \nof technological competition between the United \nStates and China, and China is investing consid\u0002erably more resources. Lacking equivalent efforts \ndomestically, the United States runs the risk of \nSputnik-like strategic surprises in biotechnology.\nBIOTECHNOLOGY AND \nSYNTHETIC BIOLOGY\n02\n40 STANFORD EMERGING TECHNOLOGY REVIEW\ntechnology over the past several decades. Fifty \nyears ago, computers were mostly industrial, dis\u0002connected, and centralized.13 The emergence of \npersonal computers, packet-switching networks,14\nand programming languages that made comput\u0002ing accessible and fun15 changed how information \nscience and technologies developed and led to \ndecentralized access to computing and information \nat unprecedented speed and scale.16 Biology could \nexperience the same transformation within the \nnext two decades—manufacturing processes could \nmove from being largely invisible to being obvious \nand apparent to people as they begin to manipulate \nsome of these workflows for themselves.\nKey Developments\nDistributed Biomanufacturing\nThe significance of distributed biomanufacturing lies \nin its flexibility, both in location and timing. Because \nthe apparatus for a fermentation process can be \nestablished wherever there is access to sugar and \nelectricity, a production site can be set up almost \nanywhere. The timing aspect is equally transfor\u0002mative: By removing the need to grow feedstocks, \nbiomanufacturers can swiftly respond to sudden \ndemands, such as a rapid outbreak of disease requir\u0002ing specific medications. This adaptability not only \nenhances efficiency but also revolutionizes how we \napproach manufacturing, making it far more respon\u0002sive to urgent needs than traditional methods.\nIn an important demonstration illustrating that dis\u0002tributed biomanufacturing is not a mirage, the \nsynthetic biology company Antheia reported in \nearly 2024 that it had completed validation of a \nfermentation-based process for brewing thebaine, \na key starting material used in treating opioid \noverdoses with Narcan.17 The company partnered \nwith Olon, an Italian contract manufacturing orga\u0002nization. Antheia’s bioengineered yeast strain was \nsent to Olon’s large-scale fermentation facility in \naround 5 percent of GDP, or more than $950 billion \nannually.7 Existing applications involve primarily agri\u0002culture, medicines, and industrial materials.8\n A 2020 \nMcKinsey & Company report noted that hundreds of \nbiotechnology projects were under development and \nestimated that the resulting products could add $2 to \n$4 trillion in annual economic impact within the next \ntwo decades.9 This projected doubling of the bioeco\u0002nomy’s contribution to worldwide GDP every seven \nyears would match biotechnology’s economic track \nrecord.10 The McKinsey report concluded that, ulti\u0002mately, biomanufacturing could account for around \n60 percent of the global economy’s physical inputs.11\nBiology, as a natural manufacturing process, is \nremarkably distributed and localized. For example, \nleaves on trees do not come from factories or central \nfacilities; rather, they grow on trees themselves—all \nover the place. Yet, outside of agriculture, biotech\u0002nology has until now been largely practiced and \ncommercialized in a capital-intensive, industrialized, \nand centralized context.12 This contrast between \nbiology as a naturally distributed platform and \nindustrialized biomanufacturing processes suggests \nthat biotechnology may be ripe for new modes of \npractice and products.\nNotably, synthetic biology continues to emerge as \nan important new approach within biotechnology. \nSynthetic biology combines principles from biology, \nengineering, and computer science to modify living \nsystems and construct new ones by developing \nnovel biological functions, such as custom metabolic \nor genetic networks, novel amino acids and pro\u0002teins, and even entire cells. These new functions are \nperformed through the construction of engineered \nbiological parts that can be reused by humans when \nappropriate, thereby reducing the need for each \nproject to start from scratch. Synthetic biology thus \nhelps us to create more complex, biologically based \nsystems, including those with functions that do not \nexist in nature. \nIn thinking about biotechnology’s potential, it is \ninstructive to consider the evolution of information \n02 Bitechno\nwhich in turn underlie different cellular behaviors and \nfunctions.\nDNA sequencing (i.e., reading of DNA) and synthe\u0002sis (i.e., writing of DNA) are two foundational tech\u0002nologies underlying synthetic biology.22 Sequencers \nare machines that determine the precise order of \nnucleotides in a DNA molecule, effectively convert\u0002ing genetic information from a physical to a digital \nformat. Synthesizers generate user-specified digital \nsequences of A’s, C’s, T’s, and G’s, creating physi\u0002cal genetic material from scratch that encodes the \nuser-specified sequence, thus effectively transform\u0002ing bits into atoms. If DNA reading and writing tools \ncould themselves be distributed, anyone with an \ninternet connection could upload and download \napplication-specific DNA programs that direct dis\u0002tributed biomanufacturing processes powered by \nlocally available energy and supplied by locally avail\u0002able materials.\nIn the 1990s, public funding for sequencing the \nhuman genome jump-started advances in DNA\u0002sequencing tools by creating significant demand for \nreading DNA.23 Private capital and entrepreneurs \nquickly responded.24 The Human Genome Project \n(HGP) favored development of DNA sequencers that \ncould read billions of bases of DNA as cheaply as \npossible, resulting in large-format DNA sequencers \nthat were organized in centralized DNA-sequencing \nfactories.25 A complementary approach to DNA \nsequencing has since matured that allows for individ\u0002ual DNA molecules to be sequenced via tiny pores, \nor nanopores, in ultra-thin membranes.26 UK-based \nOxford Nanopore Technologies has exploited this \napproach to market small-format, portable DNA \nsequencers that can be used with laptop computers, \nallowing DNA sequencing to become a distributed \ntechnology (see figure 2.1).27\nThe market for DNA synthesis has developed organ\u0002ically over the past forty-five years.28 So far, there has \nbeen no equivalent to the HGP that has resulted in \nsignificant public funding from Western governments \nfor improving the technology of DNA synthesis.29\nItaly. Working together, they repeatedly brewed \n116,000-liter batches of bioengineered yeast, with \neach batch making broth containing a metric ton of \nthebaine—roughly enough for one hundred million \nNarcan doses.18 This demonstration highlights the \npotential for on-demand production of critical phar\u0002maceuticals, potentially revolutionizing drug supply \nchains and improving access to essential medicines.\nIn 2022, Chinese researcher Chenwang Tang and \ncolleagues noted more generally how synthetic \nbiology allows the rewiring of biological systems to \nsupport portable, on-site, and on-demand manufac\u0002turing of biomolecules.19 In 2024, as one of many \npioneering examples, Stanford researchers reported \non-demand bioproduction of sensors enabling \npoint-of-care health monitoring and detection of \nenvironmental hazards aboard the International \nSpace Station.20 They had already realized many sim\u0002ilar demonstrations of distributed biomanufacturing \non Earth, ranging from biotechnology educational \nkits to the production of conjugate vaccines used to \nstimulate stronger immune responses.21\nThese are just a few examples demonstrating how \nbiotechnology can be used to make valuable prod\u0002ucts and services locally. Viewed from a traditional \nperspective, what’s happening is a sort of molecular \ngardening: The energy and material inputs needed \nto make the biotechnology products are supplied \nlocally, but the process differs from conventional gar\u0002dening in that the genetic instructions for what the \nbiology should do or make are being programmed \nby bioengineers. To fully unlock the power of distrib\u0002uted biomanufacturing, it must also become possi\u0002ble to make the physical DNA used to encode the \ngenetic programs locally. \nDistributed DNA Reading and Writing\nDNA is physical material that encodes biological \nfunctions in natural living systems. It is often repre\u0002sented abstractly by its four constituent bases (A, C, \nT, and G), also known as nucleotides. Unique order\u0002ings of these bases encode different biomolecules, \n42 STANFORD EMERGING TECHNOLOGY REVIEW\ninvestments may not be sufficient to make the tech\u0002nology real in a practical sense. \nMeanwhile, researchers in China have had the \nresources to advance gene and genome synthesis. \nFor example, the first synthetic plant chromosome \nwas reported by Chinese researcher Yuling Jiao \nand colleagues in January 2024.34 More broadly, \nresearchers in China published nearly 350 papers \nthat ranked among the top 10 percent most-cited \npapers on synthetic biology in 2023, compared to \n41 such papers in the United States (see figure 2.2). \nBiology as a General-Purpose Technology\nBiotechnology is currently used to make med\u0002icines, foods, and a relatively narrow range of \nsustainable materials. However, as noted earlier, \nanything whose biosynthesis engineers can learn \nto encode in DNA could be grown using biology. \nImprovements in DNA synthesis in Western coun\u0002tries have been sporadic and dependent primarily \non private capital.30 Commercially available gene\u0002length DNA-synthesis services in the United States \nhave improved only modestly in the past six years.31\nToday, most DNA synthesis is carried out via central\u0002ized factories.32 Customers order DNA online and \nreceive it via express shipping; it typically takes these \nfactories from days to weeks to make the DNA mol\u0002ecules themselves. \nA new generation of companies is pursuing novel \napproaches to building DNA—most notably enzy\u0002matic DNA synthesis, which uses enzymes and sim\u0002pler chemical inputs to build DNA.33 These new \napproaches support hardware and reagent formats \nthat could potentially enable fast, reliable, and \ndistributed DNA synthesis. However, the creation \nof widely distributed DNA printers is not receiv\u0002ing significant public support, and existing private \nSource: Oxford Nanopore Technologies, 2024\nFIGURE 2.1  Portable DNA sequencers enable biotechnology to become \nmore distributed\n02 Bitechno\nan approach was demonstrated in 2024 when the \nHoover Institution Library & Archives partnered with \nTwist Bioscience to encode a digital copy of the tele\u0002gram from President Hoover founding his namesake \ninstitution within synthetic DNA contained in a tiny \nampule (see figure 2.3). Made in this way, the DNA \nserves as a data storage medium whose digital con\u0002tents must be recovered via DNA sequencing. \nThe ultimate goal of SRC’s road map is to enable \nbottom-up construction of microprocessors. To fully \nrealize this goal might cost $100 billion in foundational \nresearch investment, smartly managed over twenty \nyears, and as yet there is no such coordinated effort \nunderway. However, the novel notion of growing com\u0002puters already challenges many framing assumptions \nand realities underlying contemporary geopolitics.40\nConcerns about computer manufacturing and supply \nchains presume that making computers is hard. What \nif making them becomes as easy as growing zucchini? \nExamples from nature highlight the potential here: \nSome bacteria are capable of growing arrays of tiny \nmagnets,35 while select sea sponges grow glass fil\u0002aments with a refraction index—which determines \nthe speed at which light travels through a medium—\nsimilar to that of human-made fiber-optic cables.36\nThese bio-made magnets and filaments are created \nunder ambient conditions through naturally sustainable \nprocesses and are often more robust than traditionally \nmanufactured alternatives. These and other examples \nhave inspired calls for biology to be recognized as a \ngeneral-purpose technology that, with appropriate \nvision and leadership, could become the foundation of \na much more resilient manufacturing base.37\nAs an example of the vision that’s needed, in 2018, \nthe Semiconductor Research Corporation (SRC) \noutlined an ambitious twenty-year synthetic biol\u0002ogy road map.38 SRC’s first proposed step was to \ndevelop DNA as a data storage medium.39 Such Number of highly cited papers published\n0\n50\n100\n150\n200\n250\n300\n350\n400\n20032004200520062007200820092010201120122013201420152016201720182019202020212022\n2023\nUnited States China Germany United Kingdom South Korea\nSource: Adapted from Australian Strategic Policy Institute, Critical Technology Tracker, based on “Appendix 2: Detailed \nMethodology,” in Jennifer Wong-Leung, Stephan Robin, and Danielle Cave, ASPI’s Two-Decade Critical Technology Tracker, \nAugust 2024\nFIGURE 2.2  China is outpacing the United States in publishing highly cited research papers on \nsynthetic biology\n44 STANFORD EMERGING TECHNOLOGY REVIEW\ncarefully contained within steel tanks or constrained \nby doctors’ prescriptions. However, recent devel\u0002opments in consumer access to these products \nand applications suggest that this will not always \nremain the case. A US company called Light Bio, for \nexample, now sells petunia plants bioengineered to \nemit light (see figure 2.4).44 Light Bio’s offering rep\u0002resents one of the first successful launches of a live \nconsumer biologic, enabling anyone in the United \nStates to source and keep a bioengineered organ\u0002ism for personal use. \nIn 2024, UK-based Norfolk Plant Sciences first made \navailable to US consumers seeds for its purple \ntomato, a kind of tomato bioengineered to pro\u0002duce high levels of antioxidants thought to help \nprevent cancer (see figure  2.5).45 Stanford faculty \nbought seeds, and soon bioengineered tomatoes \nwere growing in gardens across campus. Indeed, \nthese tomatoes are available for consumer purchase \nA more long-standing example of biology as an \nincreasingly general-purpose technology with poten\u0002tial geopolitical impacts can be found in the 2011 US \nNavy program, Application of Synthetic Biological \nTechniques for Energetic Materials.41 This program \nbegan exploring the ability to brew propellants and \nexplosives through a process akin to how Antheia \nand Olon partnered to brew medicines—an ability \nthat could enable any nation anywhere to create \nmore resilient supply chains for key military materials. \nA distributed and resilient biomanufacturing network \ncould, for example, help NATO members meet their \nArticle 3 obligations related to supply chain resil\u0002ience.42 A bio-based approach to brewing fuels could \nalso help meet climate and sustainability goals.43\nPervasive and Embedded Biotechnologies \nMost modern biotechnology products and applica\u0002tions are presumed to be destined for deployments \nFIGURE 2.3  DNA is used as a storage medium for a digital copy of Herbert Hoover’s \ntelegram founding his namesake institution\n02 Bitechno\nand around us—and be made available through \nestablished and far-reaching consumer channels. \nBiological Large Language Models \nIn the 2023 edition of The Stanford Emerging \nTechnology Review, we discussed how research\u0002ers had developed and deployed methods that \nare based on artificial intelligence (AI) to predict \nthe three-dimensional structures of over 200 mil\u0002lion natural proteins,51 an accomplishment recently \nrecognized via the 2024 Nobel Prize in Chemistry.52\nAnybody with a laptop can now take a DNA \nsequence encoding a protein and quickly estimate \nits expected shape. The shape of a protein helps \ndetermine its placement and function in a living \nsystem. The ability to rapidly generate predicted \nshapes helps bioengineers modify existing proteins \nand design new ones from scratch. However, the \nwork of modifying an existing protein sequence and \ndesigning a new protein still requires direct human \ngenius and labor.\nin a number of grocery stores in the American \nSoutheast.46\nAnother category of pervasive and potentially \nconsumer-facing biotechnology involves bioen\u0002gineering the bacteria that live on our skin and \ninside our bodies, as well as within the environment \naround us. For example, in 2023, Stanford research\u0002ers pioneered the bioengineering of skin microbes \nto combat skin cancer.47 They have since expanded \nsuch work to enable the eliciting of more broadly \nantigen-specific T cells, which target and eliminate \ncells infected with viruses and bacteria, as well as \ncancerous cells. T cells also play a role in providing \nlong-term immunological memory.48 In addition, \nresearchers have identified specific odorants pro\u0002duced by human-skin microbes whose production \ncould be modulated to reduce mosquito bites49 and \nhave also developed methods for bioengineering \nmicrobes to improve gut health.50\nAs these examples suggest, twenty-first-century bio\u0002technologies may increasingly be deployed in, on, \nSource: Light Bio Inc.\nFIGURE 2.4  Light Bio’s petunias are bioengineered to emit light\n46 STANFORD EMERGING TECHNOLOGY REVIEW\ngenome language models will continue to emerge \nand develop throughout the 2020s. The greatest \nbottleneck will likely be the limited capacity avail\u0002able to build and test the biological sequences gen\u0002erated by the models. Any adult English speaker can \nquickly read a passage of LLM-generated English \ntext and evaluate its purpose and quality. For now, \nonly living systems themselves can ultimately inter\u0002pret and establish whether the function and perfor\u0002mance of a bioLLM-generated design actually works \nas expected. The ability to operate platforms that \nscale high-throughput testing of bioLLM designs is \na significant advantage in inventing, improving, and \noffering world-leading foundation models in biology \nand biotechnology.\nOver the Horizon\nRoutinization of Cellular-Scale \nEngineering\nThere is no natural cell on Earth that is fully under\u0002stood. Even for well-studied model organisms \nlike E. coli, there remain genes with unknown or \nincompletely understood functions, highlighting \nthe complexity of cellular systems. The microbes \nthat have been subject to the most intense study \nstill require more than seventy genes whose func\u0002tions no researcher understands.56 Each gene \nencodes some unknown life-essential mechanism. \nOur collective ignorance means that all bioengi\u0002neering workflows remain Edisonian at the cellular \nscale—we are tinkering and testing. Bioengineering \nstudents are taught the mantra “design, build, test, \nlearn,”57 where the test portion implies a very large \namount of empirical lab work to understand basic \nphenomenology. By contrast, the routinization of \nbioengineering workflows at the cellular scale suffi\u0002cient to realize “design, build, work” workflows—a \nhallmark of all other modern technologies that \nimplies doing a relatively small amount of empirical \nwork primarily to validate the analysis underlying \nthe construction of a biological artifact—remains \nAdvances in AI may change that. In 2024, new \nlarge language models (LLMs) have emerged that \nare trained on natural DNA, RNA, and protein \nsequences. For context, ChatGPT and similar LLMs, \nwhen trained on sequences of letters and words \nfrom composing human languages like English, can \ngenerate meaningful new human-readable text. In \nsimilar fashion, biological LLMs (bioLLMs), trained on \nvast datasets of biological sequences, can generate \nnovel sequences with potential biological functions, \naccelerating the design process in fields like protein \nengineering and synthetic biology. For example, in \nearly 2024 Stanford researchers reported develop\u0002ing and using a general protein language model to \nquickly design better virus-neutralizing antibodies \ntargeting Ebola and SARS-CoV-2.53 Unlike wide\u0002spread speculative concerns about the destabiliz\u0002ing potential of the use of AI in biotechnology,54\nactual known work in the field seems to instead have \ndirectly contributed to public health and biosecurity.\nAs a second example, researchers at Stanford \nreleased a genomic foundation model named Evo \nthat performs prediction and generation tasks across \nDNA, RNA, and proteins.55 (Foundation models are \ndiscussed in chapter 1 on artificial intelligence.) \nThey then used Evo to help design synthetic gene\u0002editing systems. DNA, RNA, protein, gene, and \nSource: Norfolk Plant Sciences\nFIGURE 2.5  Norfolk Plant Sciences has bioengineered \na purple tomato\n02 Bitechno\nRecent thinking, however, suggests that electricity \ncould be used to fix carbon directly from the air to create \norganic molecules that could be fed to microbes—a \nprocess that may come to be known as electrobio\u0002synthesis or, more simply, “eBio”—and that doing so \ncould be an order of magnitude more efficient from a \nland-use perspective than traditional agriculture.64\nIn other words, the idea is to engineer a parallel \ncarbon cycle that starts with air and electricity, per\u0002haps generated via solar panels, to create organic \nmolecules that can power bioproduction processes. \nFor example, in August 2024, Stanford researchers \nreported the creation of a system that combines \nelectrochemistry with biological processes that do \nnot use cells to transform simple carbon compounds \ninto a key organic molecule called acetyl-CoA, which \nis present in all living things and acts as a building \nblock for other molecules within cells.65\nAlthough eBio is still a very immature technology, \nits potential significance and impacts are hard to \noverstate. For example, surplus power from large\u0002scale renewable energy generation could be used \nto directly produce biomolecules such as proteins \nand cellulose without requiring massive conven\u0002tional battery banks to store energy that cannot be \nused immediately. The development of eBio could \nalso enable bioproduction in places where soils are \npoor, water is scarce, or climate and weather are too \nuncertain. And it could raise the ceiling on how much \nhumanity could make in partnership with biology. \nWe would be constrained only by how much energy \nwe can generate for such purposes. This approach \ncould significantly reduce the land and water require\u0002ments for biomass production, potentially alleviating \nfringe foundational research. Consequently, such \nbioengineering workflows remain in their earliest \nstages.58\nNevertheless, because cells are the fundamental unit \nof life, researchers59 and start-ups60 across the United \nStates, Europe, Japan, and China are scrambling to \nlearn how to build fully understandable cells from \nscratch. The ability to construct life for the first time, \nwithout being restricted to any terrestrial lineage, \nis akin to launching to orbit the first artificial satel\u0002lite. Just as rockets allow us to ascend Earth’s gravity \nwell, giving us access to the privilege, perspective, \nand power of space, the ability to transcend the \nconstraints of Earth’s existing life-forms61—organ\u0002isms constrained by lineage and the requirements \nof reproduction and evolvability—will unlock the \nnext level of biotechnologies, providing a powerful \nperch from which to access everything that biology \ncan become. \nA first organized and professional attempt to con\u0002struct life from scratch will likely cost $100 mil\u0002lion. The Institute of Synthetic Biology (ISB) at the \nShenzhen Institute of Advanced Technology in \nChina is one organization where such an effort could \nnow be carried out rapidly. The ISB hosted a global \nsummit on coordination of synthetic-cell building in \nOctober 2024.62 Lacking equivalent efforts domesti\u0002cally, the United States is risking a Sputnik-like bio\u0002technology surprise.63\nElectrobiosynthesis \nCarbon is central to life. Currently, we rely on photo\u0002synthesis for production of organic carbon molecules. \nThe ability to construct life for the first time, without \nbeing restricted to any terrestrial lineage, is akin to \nlaunching to orbit the first artificial satellite.\n48 STANFORD EMERGING TECHNOLOGY REVIEW\nlocal food chains or natural species have long been \na concern. Moreover, as the science and technology \nof synthetic biology becomes increasingly available \nto state and nonstate entities, there are legitimate \nconcerns that malicious actors will create organisms \nharmful to people and the environment.69\nEthical considerations Different religious traditions \nmay have different stances toward life and whether \nthe engineering of new life-forms violates any of their \nbasic precepts. Often classified as potential non\u0002physical impacts, the effects on biotechnology when \nconsidering these religious concerns are sometimes \ndifficult to predict in advance. In the words of a Wilson \nCenter report on this topic, such concerns involve \n“the possibility of harm to deeply held (if sometimes \nhard to articulate) views about what is right or good, \nincluding . . . the appropriate relationship of humans \nto themselves and the natural world.”70\nThe United States and other nations are working \nhard to develop, advance, and refine strategies for \nbiotechnology, biomanufacturing innovation, bio\u0002security, and the bioeconomy overall. For example, \nthe United States’ National Security Commission on \nEmerging Biotechnology continues its work.71 The \ncongressionally mandated Department of Defense \nTask Force on Emerging Biotechnologies and \nNational Security is also underway.72 Both efforts \nare expected to produce substantial reports and \nproducts throughout 2025, complementing activ\u0002ities ongoing within the executive office of the \npresident, including work as ordered by Executive \nOrder 14081 on biotechnology and by Executive \nOrder 14110 on artificial intelligence. Internationally, \nthe Organisation for Economic Co-operation and \nDevelopment’s Global Forum on Technology selected \nsynthetic biology as one of three key initial technol\u0002ogies to focus on, with work now well underway.73\nThe World Economic Forum has also renewed its \nGlobal Futures Council on Synthetic Biology, which \ncontinues its work.74\nOne overall challenge for policymakers—and the \nbiotechnology community—is to preserve and \nadvance the very significant public benefits of \nresearch into biosciences and biotechnology while \npressure on agricultural resources and offering a \nmore sustainable path for biomanufacturing. \nChallenges of Innovation and \nImplementation\nMany first-generation synthetic biology companies \ncontinue to struggle.66 Billions of dollars of private cap\u0002ital have been lost in biotechnology investments made \nwith the best of intentions in the United States alone \nover the past two decades. One perspective is that \nthese early big bets were simply too early.67 The hope \nis that smaller and scrappy next-generation efforts \nwill find their way to success. However, an immediate \nshort-term issue is that many sources of private capital \nfunding to support these next-generation commercial \nefforts are now shut off for synthetic biology, adding \nheadwinds to the general challenges of obtaining \ncapital that young, innovative businesses face. \nAnother perspective is that America has relied too \nheavily on the private sector to invent, advance, \nand deploy emerging biotechnologies. The biotech \nequivalent of the publicly funded tooling and infra\u0002structure development in the early days of US stra\u0002tegic computing and networking programs is today \npursued only via private investment and commercial \nplatforms. Because private investors expect these \nfoundational tools and platforms to quickly generate \nand sustain revenue growth to justify further funding, \nbusinesses developing them often fail repeatedly.\nBreaking this cycle will require smart and sustained \npublic investments in foundational bioengineering \nresearch, from tools for measuring, modeling, and \nmaking biology to public-benefit research platforms. \nThe National Science Foundation’s August 2024 \ninvestment in five academic biofoundries may be \none small step forward in this respect.68\nPolicy, Legal, and Regulatory Issues\nSafety and national security concerns New organ\u0002isms not found in nature raise concerns about how \nthey will interact with natural and human environ\u0002ments. For instance, bioengineered organisms that \nescape into the environment and possibly disrupt \n02 Bitechno\nminimizing the real and perceived risks associated \nwith potential misuse of the resulting knowledge \nand capacities. For example, in response to the con\u0002cern about the escape of harmful bioengineered \norganisms into the environment, synthetic biology \nitself offers the possibility of bioengineering organ\u0002isms from scratch that are incapable of escaping \nor evolving.75 But it is a matter of policy to ensure \nthat necessary safeguards are included in projects \nintended to create new organisms.\nIn short, policymakers will have to be aware of—and \nable to navigate—issues and aspects of emerging \nbiotechnologies, such as the ones included in this \nsection, if they are to help guide the development of \nthe field and the increasing diversity of the biotech\u0002nologies that emerge from it.\nNOTES\n1. Jared Diamond, “Evolution, Consequences, and Future of Plant \nand Animal Domestication,” Nature 418 (August 2020): 700–707, \nhttps://doi.org/10.1038/nature01019. \n2. Freeman Dyson, “Our Biotech Future,” New York Review of \nBooks 54, no. 12 (July 2007): 4–7, https://www.nybooks.com/articles\n/2007/07/19/our-biotech-future/.\n3. Tim Beardsley, “Biotechnology: Cohen-Boyer Patent Finally \nConfirmed,” Nature 311, no. 5981 (September 1984): 3, https://\ndoi.org/10.1038/311003a0. \n4. Irina Gostimskaya, “CRISPR-Cas9: A History of Its Discovery \nand Ethical Considerations of Its Use in Genome Editing,” Bio\u0002chemistry (Moscow) 87, no. 8 (August 2022): 777–78, https://doi\n.org/10.1134/S0006297922080090. CRISPR is an acronym stand\u0002ing for clustered regularly interspaced short palindromic repeats.\n5. Frances H. Arnold, “Innovation by Evolution: Bringing New \nChemistry to Life,” December 8, 2018, Stockholm University, \nStockholm, Sweden, PDF, https://www.nobelprize.org/prizes\n/chemistry/2018/arnold/lecture/.\n6. “Genetic Scissors: A Tool for Rewriting the Code of Life,” Press \nRelease, The Nobel Prize, October 7, 2020, https://www.nobelprize\n.org/prizes/chemistry/2020/press-release/.\n7. National Academies of Sciences, Engineering, and Medicine, \nSafeguarding the Bioeconomy (Washington, DC: National Acade\u0002mies Press, 2020), 73, https://doi.org/10.17226/25525.\n8. Planetary Technologies, “Bioeconomy Dashboard,” last modified \n2023, https://www.planetarytech.earth/bioeconomy-dashboard-1.\n9. Michael Chui, Matthias Evers, James Manyika, et al., “The Bio \nRevolution: Innovations Transforming Economies, Societies, and \nOur Lives,” McKinsey & Company, May 13, 2020, https://www\n.mckinsey.com/industries/life-sciences/our-insights/the\n-bio-revolution-innovations-transforming-economies-societies\n-and-our-lives.\n10. Robert Carlson, “Estimating the Biotech Sector’s Contribution \nto the US Economy,” Nature Biotechnology 34, no. 3 (March 2016): \n247–55, https://doi.org/10.1038/nbt.3491. \n11. Chui et al., “The Bio Revolution.”\n12. National Research Council, Industrialization of Biology: A \nRoadmap to Accelerate the Advanced Manufacturing of Chemi\u0002cals (Washington, DC: National Academies Press, 2015), https://\ndoi.org/10.17226/19001. \n13. “IBM Mainframe,” Wikipedia, last modified July 20, 2024, \nhttps://en.wikipedia.org/wiki/IBM_mainframe.\n14. Vinton G. Cerf and Robert E. Kahn, “A Protocol for Packet \nNetwork Intercommunication,” IEEE Transactions on Commu\u0002nications 22, no. 5 (May 1974): 637–48, https://doi.org/10.1109\n/TCOM.1974.1092259. \n15. Whole Earth Epilog (Baltimore, MD: Penguin Books, 1974), \nhttps://archive.org/details/wholeearthepilog00unse.\n16. “History of Personal Computers,” Wikipedia, last modified \nAugust 19, 2024, https://en.wikipedia.org/wiki/History_of_personal\n_computers.\n17. Antheia, “Antheia Completes Successful Product Validation,” \nJanuary 8, 2024, https://antheia.bio/antheia-completes-successful\n-product-validation/.\n18. Antheia, “U.S. Secretary of State Antony Blinken Visits Antheia \nto Discuss Biotechnology Innovation,” May 30, 2024, https://\nantheia.bio/u-s-secretary-of-state-antony-j-blinken-visits-antheia\n-to-discuss-biotechnology-innovation/.\n19. Chenwang Tang, Lin Wang, Lei Zang, et al., “On-Demand \nBiomanufacturing Through Synthetic Biology Approach,” Mate\u0002rials Today Bio 18, no. 100518 (February 2023), https://doi.org\n/10.1016/j.mtbio.2022.100518. \n20. Selin Kocalar, Bess M. Miller, Ally Huang, et al., “Validation \nof Cell-Free Protein Synthesis Aboard the International Space \nStation,” ACS Synthetic Biology 13, no. 3 (March 2024): 942–50, \nhttps://doi.org/10.1021/acssynbio.3c00733. \n21. Jessica C. Stark, Thapakorn Jaroentomeechai, Tyler D. \nMoeller, et al., “On-Demand Biomanufacturing of Protective \nConjugate Vaccines,” Science Advances 7, no. 6 (February 2021): \neabe9444, https://doi.org/10.1126/sciadv.abe9444; and Jessica \nC. Stark, Ally Huang, Peter Q. Nguyen, et al., “BioBitsTM Bright: A \nFluorescent Synthetic Biology Education Kit,” Science Advances 4, \nno. 8 (August 2018): eaat5107, https://doi.org/10.1126/sciadv\n.aat5107.\n22. National Security Commission on Emerging Biotechnology, \n“DNA: Reading, Writing, and Editing,” February 2024, https://\nwww.biotech.senate.gov/press-releases/dna-reading-writing-and\n-editing/.\n23. National Institutes of Health, “The Human Genome Project,” \nNational Human Genome Research Institute, last modified May 14, \n2024, https://www.genome.gov/human-genome-project.\n24. Kristen Philipkoski, “Celera Wins Genome Race,” Wired, April 6, \n2000, https://www.wired.com/2000/04/celera-wins-genome-race/.\n25. James M. Heather and Benjamin Chain, “The Sequence \nof Sequencers: The History of Sequencing DNA,” Genomics\n107, no. 1 (January 2016): 1–8, https://doi.org/10.1016/j.ygeno\n.2015.11.003. \n26. Davied Deamer, Mark Akeson, and Daniel Branton, “Three \nDecades of Nanopore Sequencing,” Nature Biotechnology 34 \n(May 2016): 518–24, https://doi.org/10.1038/nbt.3423. \n27. Oxford Nanopore Technologies, “Company History,” accessed \nSeptember 3, 2024, https://nanoporetech.com/about/history.\n28. Marvin H. Caruthers, “The Chemical Synthesis of DNA/\nRNA: Our Gift to Science,” Journal of Biological Chemistry 288, \n50 STANFORD EMERGING TECHNOLOGY REVIEW\nJuly 18, 2024, https://www.nato.int/cps/en/natohq/topics\n_91048.htm.\n44. Light Bio, “Light Bio,” accessed September 3, 2024, https://\nwww.light.bio.\n45. Sasa Woodruff, “Gardeners Can Now Grow a Genetically Mod\u0002ified Purple Tomato Made with Snapdragon DNA,” NPR, February \n6, 2024, https://www.npr.org/sections/health-shots/2024/02/06\n/1228868005/purple-tomato-gmo-gardeners.\n46. Norfolk Healthy Produce, “Empress Limited Edition Tomato,” \naccessed September 3, 2024, https://www.norfolkhealthyproduce\n.com.\n47. Hadley Leggett, “Researchers Use Skin-Colonizing Bacteria \nto Create a Topical Cancer Therapy in Mice,” Stanford Medicine, \nApril 12, 2023, https://med.stanford.edu/news/all-news/2023/04\n/cancer-bacteria.html.\n48. Michael A. Fischbach, Kazuki Nagashima, Yiyin E. Chen, et \nal., “Bacteria-engineered to Elicit Antigen-Specific T Cells,” US \nPatent 2024/0024380 A1, filed December 22, 2021, and issued \nJanuary 25, 2024. \n49. Iliano V. Coutinho-Abreu, Omid Jamshidi, Robyn Raban, et al., \n“Identification of Human Skin Microbiome Odorants That Manip\u0002ulate Mosquito Landing Behavior,” Scientific Reports 14, no. 1631 \n(January 2024), https://doi.org/10.1038/s41598-023-50182-5. \n50. Purna C Kashyap, Michael Fischbach, and Brianna B. Williams, \n“Methods and Materials for Using Ruminococcus gnavus or Clos\u0002tridium sporogenes to Treat Gastrointestinal Disorders,” US Patent \n11,878,002 B2, filed April 8, 2021, and issued January 23, 2024. \n51. Herbert S. Lin, ed., “Biotechnology and Synthetic Biology,” \nin The Stanford Emerging Technology Review 2023 (Stanford, CA: \nHoover Institution Press, 2023): 33–43, https://setr.stanford.edu/; \nEwen Callaway, “‘The Entire Protein Universe’: AI Predicts Shape \nof Nearly Every Known Protein,” Nature, July 29, 2023, https://\nwww.nature.com/articles/d41586-022-02083-2. \n52. The Nobel Prize in Chemistry 2024, “Summary,” The Nobel Prize, \nOctober 12, 2024, https://www.nobelprize.org/prizes/chemistry\n/2024/summary/.\n53. Brian L. Hie, Varun R. Shanker, Duo Xu, et al., “Efficient \nEvolution of Human Antibodies from General Protein Language \nModels,” Nature Biotechnology 42 (April 2024): 275–83, https://\ndoi.org/10.1038/s41587-023-01763-2. \n54. Doni Bloomfield, Jaspreet Pannu, Alex W. Zhu, et al., “AI and \nBiosecurity: The Need for Governance,” Science 385, no. 6711 \n(August 2024): 831–33, https://doi.org/10.1126/science.adq1977. \n55. Eric Nguyen, Michael Poli, Matthew G. Durrant, et al., \n“Sequence Modeling and Design from Molecular to Genome \nScale with Evo,” bioRxiv, March 6, 2024, https://doi.org/10.1101\n/2024.02.27.582234. \n56. John I. Glass, Chuck Merryman, Kim S. Wise, et al., “Mini\u0002mal Cells—Real and Imagined,” Cold Spring Harbor Perspectives \nin Biology 9, no. 12 (December 2017): a023861, https://doi.org\n/10.1101/cshperspect.a023861. \n57. Shohei Kitano, Ciai Lin, Jee Loon Foo, et al., “Synthetic Biol\u0002ogy: Learning the Way Toward High-Precision Biological Design,” \nPLOS Biology 21, no. 4 (April 2023): e3002116, https://doi.org\n/10.1371/journal.pbio.3002116. \n58. Akshay J. Maheshwari, Jonathan Calles, Sean K. Waterton, et al., \n“Engineering tRNA Abundances for Synthetic Cellular Systems,” \nNature Communications 14, no. 4594 (July 2023), https://doi\n.org/10.1038/s41467-023-40199-9. A useful description of the con\u0002trast between “design-build-test-learn” and “design-build-work” \ncan be found at https://centuryofbio.com/p/design-build-work.\nno. 2 (December 2012): 1420–24, https://doi.org/10.1074/jbc\n.X112.442855. \n29. Ryan Cross, “At GP-write, Scientists Take First Steps on \nWay to Synthetic Human Genome,” Chemical & Engineering \nNews, May 14, 2018, https://cen.acs.org/biological-chemistry\n/synthetic-biology/GP-write-scientists-take-first/96/i20.\n30. Business Wire, “Twist Bioscience Expands Gene Offering \nwith Long Gene Fragments up to 5.0kb,” August 8, 2024, https://\nwww.businesswire.com/news/home/20240808863612/en/Twist\n-Bioscience-Expands-Gene-Offering-With-Long-Gene-Fragments\n-up-to-5.0kb.\n31. Planetary Technologies, “Bioeconomy Dashboard.” \n32. Business Wire, “Integrated DNA Technologies Invests in New \nU.S. Synthetic Biology Manufacturing Facility,” May 28, 2024, \nhttps://www.businesswire.com/news/home/20240528727951/en\n/Integrated-DNA-Technologies-Invests-in-New-U.S.-Synthetic\n-Biology-Manufacturing-Facility.\n33. MaryAnn Labant, “Enzymatic DNA Synthesis: Shorter Waits, \nLonger Strands,” Genetic Engineering & Biotechnology News, \nJuly 1, 2024, https://www.genengnews.com/topics/omics/enzymatic\n-dna-synthesis-shorter-waits-longer-strands.\n34. Lian Ge-Chen, Tianlong Lan, Shuo Zhang, et al., “A Designer \nSynthetic Chromosome Fragment Functions in Moss,” Nature \nPlants 10 (January 2024): 228–39, https://doi.org/10.1038/s41477\n-023-01595-7. \n35. Pranami Goswami, Kuang He, Jinhua Li, et al., “Magnetotactic \nBacteria and Magnetofossils: Ecology, Evolution, and Environmen\u0002tal Implications,” NPJ Biofilms and Microbiomes 8, no. 43 (June \n2022), https://doi.org/10.1038/s41522-022-00304-0. \n36. Sarah Graham, “Sea Sponge Inspires Better Fiber-Optic \nCables,” Scientific American, August 21, 2003, https://www\n.scientificamerican.com/article/sea-sponge-inspires-bette/.\n37. Abigail Kukura, PJ Maykish, David Lin, et al., National Action \nPlan for U.S. Leadership in Biotechnology, Special Competitive \nStudies Project, April 12, 2023, 1, https://www.scsp.ai/wp-content\n/uploads/2023/04/National-Action-Plan-for-U.S.-Leadership\n-in-Biotechnology.pdf.\n38. Semiconductor Research Corporation, “SemiSynBio Consortium \nand Roadmap Development,” accessed September 3, 2024, https://\nwww.src.org/program/grc/semisynbio/semisynbio-consortium\n-roadmap.\n39. DNA Data Storage Alliance, “DNA Data Storage Alliance,” \naccessed September 3, 2024, https://dnastoragealliance.org.\n40. Larry Diamond, James O. Ellis, Jr., and Orville Schell, eds., Sil\u0002icon Triangle: The United States, Taiwan, China, and Global Semi\u0002conductor Security (Stanford, CA: Hoover Institution Press, 2023), \nhttps://www.hoover.org/research/silicon-triangle-book.\n41. Strategic Environmental Research and Development Pro\u0002gram, “Synthetic Biological Techniques for Energetic Mate\u0002rials,” Environmental Security Technology Certification Pro\u0002gram, accessed September 3, 2024, https://serdp-estcp.mil\n/newsitems/details/ac878993-2005-4182-948a-f63c95668499\n/synthetic-biological-techniques-for-energetic-materials.\n42. NATO Allied Command Transformation, “Resilience in NATO,” \nDecember 15, 2023, https://www.act.nato.int/article/resilience\n-in-nato/.\n43. Tore Brinck, ed., Green Energetic Materials (Chichester, West \nSussex, United Kingdom: John Wiley & Sons, 2014), https://doi\n.org/10.1002/9781118676448; North Atlantic Treaty Organiza\u0002tion, “Environment, Climate Change and Security,” last modified \n02 Bitechno\n59. Build-A-Cell, “Build-A-Cell,” accessed September 3, 2024, \nhttps://www.buildacell.org; BaSyC, “BaSyC,” accessed Septem\u0002ber 3, 2024, https://www.basyc.nl. \n60. B.Next, “B.Next,” accessed September 3, 2024, https://\nbnext.bio.\n61. Drew Endy, “Upwelling,” Original Syn, Blog, October 28, \n2020, https://blog.originalsyn.bio/2020/10/upwelling.html.\n62. Sznews, “Six Countries Sign MOU on Synthetic Biology,” \nGeneral Office of Guangming District People’s Government, last \nmodified April 7, 2024, https://www.szgm.gov.cn/english/news\n/latestnews/content/post_11232084.html.\n63. Defense Advanced Research Projects Agency, “The Sputnik \nSurprise,” US Department of Defense, accessed September 3, \n2024, https://www.darpa.mil/about-us/timeline/creation-of-darpa.\n64. Dorian Leger, “Photovoltaic-driven Microbial Protein Production \nCan Use Land and Sunlight More Efficiently than Conven\u0002tional Crops,” Proceedings of the National Academy of Sciences \nUSA 118, no. 26 (June 2021): e2015025118, https://doi.org/10.1073\n/pnas.2015025118 and Emiliano Bellini, “Solar-Powered Large \nScale Microbial Food Production,” pv Magazine, August 3, 2021, \nhttps://www.pv-magazine.com/2021/08/03/solar-powered-large\n-scale-microbial-food-production.\n65. Grant M. Landwehr, Bastian Vogeli, Cong Tian, et al., “A \nSynthetic Cell-Free Pathway for Biocatalytic Upgrading of \nOne-Carbon Substrates,” bioRxiv, August 8, 2024, https://doi.org\n/10.1101/2024.08.08.607227. \n66. Reuters, “Biotech Firm Amyris Files for Bankruptcy in US,” \nAugust 10, 2023, https://www.reuters.com/business/biotech-firm\n-amyris-files-bankruptcy-us-2023-08-10; Amy Feldman and Angel \nAu-Yeung, “The Inside Story of How SoftBank-Backed Zymergen \nImploded Four Months after Its $3 Billion IPO,” Forbes, October 13, \n2021, https://www.forbes.com/sites/amyfeldman/2021/10/13/the\n-inside-story-of-how-softbank-backed-zymergen-imploded-four\n-months-after-its-3-billion-ipo.\n67. Robert F. Service, “Synthetic Biology, Once Hailed as a \nMoneymaker, Meets Tough Times,” Science, August 22, 2024, \nhttps://www.science.org/content/article/synthetic-biology-once\n-hailed-moneymaker-meets-tough-times.\n68. US National Science Foundation, “NSF Invests in BioFoundries \nto Drive Advances Across Science and Engineering,” August 28, \n2024, https://new.nsf.gov/news/nsf-invests-biofoundries-drive\n-advances-across-science.\n69. For example, polio, horsepox, SARS-CoV-2, and the Spanish \nflu virus have been synthesized from scratch in laboratories. See, \nrespectively, Jeronimo Cello, Aniko V. Paul, and Eckard Wimmer, \n“Chemical Synthesis of Poliovirus cDNA: Generation of Infectious \nVirus in the Absence of Natural Template,” Science 297,1016–18 \n(2002), https://www.science.org/doi/10.1126/science.1072266; \nRyan S. Noyce, Seth Lederman, and David H. Evans, “Construction \nof an Infectious Horsepox Virus Vaccine from Chemically Synthesized \nDNA Fragments,” PLOS ONE 13,1 e0188453, January 19, 2018, \nhttps://journals.plos.org/plosone/article?id=10.1371/journal\n.pone.0188453; Tran Thi Nhu Thao, Fabien Labroussaa, Nadine \nEbert, et al., “Rapid Reconstruction of SARS-CoV-2 Using a Synthetic \nGenomics Platform,” Nature 582, 561–65 (2020), https://www.nature\n.com/articles/s41586-020-2294-9; Terrence M. Tumpey, Christopher \nF. Basler, Patricia V. Aguilar, et al., “Characterization of the \nReconstructed 1918 Spanish Influenza Pandemic Virus,” Science\n310, 77–80 (2005), https://www.science.org/doi/10.1126/science\n.1119392.\n70. Erik Parens, Josephine Johnston, and Jacob Moses, “Ethical \nIssues in Synthetic Biology: An Overview of the Debates,” Woodrow \nWilson International Center for Scholars, June 2009, https://\nwww.wilsoncenter.org/sites/default/files/media/documents\n/publication/synbio3.pdf.\n71. National Security Commission on Emerging Biotechnology, \n“Home,” accessed September 3, 2024, https://www.biotech\n.senate.gov.\n72. The Under Secretary of Defense, “Terms of Reference—\nDefense Science Board Study on Emerging Biotechnologies and \nNational Security,” Memorandum, US Department of Defense,\nhttps://dsb.cto.mil/wp-content/uploads/TORs/TOR_DSB_\nEmergingBiotechnologiesandNationalSecurity.pdf.\n73. Organisation for Economic Co-Operation and Development, \n“Global Forum on Technology,” accessed September 3, 2024, \nhttps://www.oecd.org/en/networks/global-forum-on-technology\n.html; OECD Science, Technology and Innovation, “What Is Syn\u0002thetic Biology? OECD Global Forum on Technology,” YouTube, \nDecember 13, 2023, video, 1:36, https://youtu.be/-0OJU_l-pLA. \n74. World Economic Forum, “Global Future Council on the Future \nof Synthetic Biology,” accessed September 3, 2024, https://www\n.weforum.org/communities/gfc-on-synthetic-biology/.\n75. Jonathan Calles, Isaac Justice, Detravious Brinkley, et al., \n“Fail-Safe Genetic Codes Designed to Intrinsically Contain Engi\u0002neered Organisms,” Nucleic Acids Research 47, no. 19 (Novem\u0002ber 2019): 10439–51, https://doi.org/10.1093/nar/gkz745; Akos \nNyerges, Svenja Vinke, Regan Flynn, et al., “A Swapped Genetic \nCode Prevents Viral Infections and Gene Transfer,” Nature 615 \n(2023): 720–27, https://doi.org/10.1038/s41586-023-05824-z.\nSTANFORD EXPERT CONTRIBUTORS\nDr. Drew Endy\nSETR Faculty Council and Associate Professor of \nBioengineering\nDr. Michael Jewett\nProfessor of Bioengineering\nDr. Jenn Brophy\nAssistant Professor of Bioengineering\nDr. Brian Hie\nAssistant Professor of Chemical Engineering\nDr. Artem Trotsyuk\nSETR Fellow and Postdoctoral Scholar in Biomedical \nEthics\n\n53\nOverview\nThe word cryptography originates from Greek words \nthat mean “secret writing.” In ancient times, cryp\u0002tography involved the use of ciphers and secret \ncodes. Today it relies on sophisticated mathemat\u0002ics to protect data from being altered or accessed \ninappropriately.1 We are typically unaware that many \nof our day-to-day interactions with computers and \nthe internet involve cryptography, from securing our \nonline shopping to protecting our cell phone calls.\nCryptography is often invisible, but it is essential \nfor most internet activities such as messaging, \ne-commerce, banking, or even simple internet \nbrowsing. Yet cryptography alone will never be \nenough to ensure the confidentiality, integrity, or \navailability of information. Inherent vulnerabilities \nin the software code that underpins all our internet\u0002connected devices and the strong incentives for bad \nactors—from criminals to nation states—to engage \nKEY TAKEAWAYS\n° Cryptography is essential for protecting infor\u0002mation, but alone it cannot secure cyberspace \nagainst all threats.\n° Cryptography is the enabling technology of \nblockchain, which is the enabling technology of \ncryptocurrencies.\n° Central bank digital currencies (CBDCs) are a \nparticular type of cryptography-based digital \ncurrency supported by states and one that could \nenhance financial inclusion. Although the United \nStates lags some countries in experimenting with \na CBDC, it may benefit from a cautious, well\u0002timed approach by learning from other nations’ \nefforts.\nCRYPTOGRAPHY\n03\n54 STANFORD EMERGING TECHNOLOGY REVIEW\nIn this scenario, both Drew and Taylor must share \na secret piece of information, namely N. N is the \ncryptographic key, which in general is a string of \ndigits needed both to encrypt and to decrypt the \nmessage. Drew and Taylor must also know that the \nalgorithm is the shift cipher. If Ellen somehow learns \nboth of those facts, she can decrypt the message \nas well. This type of encryption algorithm—of which \nthe shift cipher is an example—is known as sym\u0002metric cryptography, or secret-key cryptography. It \nrequires a secure key distribution, which is a method \nof distributing secret keys to all parties who should \nhave them—but preventing those who shouldn’t \nfrom obtaining them.\nSymmetric key cryptography proved to be cumber\u0002some because parties wishing to communicate \nsecurely must connect physically to share the cryp\u0002tographic key before such a communication can take \nplace. Imagine how awkward phone communica\u0002tions would be if you had to meet every telephone \npartner in person before talking to that party.\nIn the 1970s, Stanford professor Martin Hellman and \nWhitfield Diffie codeveloped a technique known as \nasymmetric cryptography, or public-key cryptogra\u0002phy. Public-key cryptography relies on a public key \nfor encrypting messages that is freely available to \neveryone, which means it can be widely distributed \neven over insecure channels. However, decrypting a \nmessage requires a private key that is held only by \nthe authorized party (see figure 3.1).2 Although it is \ntheoretically possible to derive a private key from \na public key, that process (if well designed) would \nin cyberattacks that exploit human and technical vul\u0002nerabilities help to explain why cybersecurity will be \nan ongoing challenge.\nCryptography Basics: Public Keys, Private \nKeys, and Hashes\nHere’s an example: Drew has a private message \nintended only for Taylor. To keep it confidential, she \nscrambles (encrypts) the message using an encryp\u0002tion algorithm and transmits the scrambled mes\u0002sage to Taylor as ciphertext. When Taylor receives \nthe ciphertext, he unscrambles (decrypts) it to reveal \nwhat it originally said. This piece of decrypted text is \nknown as the plaintext. Along comes Ellen, a third\u0002party eavesdropper who wants to see the plaintext, \nso she must use any means at her disposal to break \nthe cryptographically provided protection.\nAn example of an encryption algorithm is the shift \ncipher. Each letter in the plaintext is replaced by a \nletter that is some fixed number N of positions later \nin the alphabet. For example, if N = 2, Drew substi\u0002tutes an A in the plaintext with a C in ciphertext, B \nin plaintext with D in ciphertext, and so on. If N = 3, \nthen Drew substitutes A in plaintext with D in cipher\u0002text. To decrypt the ciphertext, Taylor must know that \nDrew is using the shift cipher and must also know the \nvalue of N so that he can invert it. For example, know\u0002ing that N = 2, he knows to write down A when he \nsees C in the ciphertext. (Note that modern encryp\u0002tion algorithms are more sophisticated and secure \nthan what has been presented here; they are also \nharder to explain.)\nCryptography is often invisible, but it is essential for \nmost internet activities such as messaging, e-commerce, \nbanking, or even simple internet browsing. \n03 Cryp\noriginator of the message is who he or she claims \nto be. \nTo illustrate, Alice (the sender) first computes the \nhash value of her message. Next, she encrypts the \nhash value with her private key, a process analo\u0002gous to signing a document, generating a digital \nsignature of the message’s hash.3 Alice then sends \nthe message and its digital signature to Bob (the \nreceiver).\nUpon receipt of the message, Bob can recover the \nhash value for the message that Alice purportedly \nsent and compare that value to his own compu\u0002tation of the hash value. If these match, Bob can \nbe assured that the message has not been altered \nin transmission and also that Alice was the party \nwho sent it, since only Alice could have used her \nprivate key to create a digital signature of the \nmessage’s hash.\ntake much too long for practical purposes (it would \ntake longer than the age of the universe). It is this \nessential property that is placed at risk by quantum \ncomputing, as discussed below.\nThe mathematics of cryptography also underlie \nthe creation of secure hashes. A hash is designed \nto accept a message of any length and compute \na unique fixed-length string of numbers—called \nthe hash value—corresponding to that message. \nHashes have two key properties. First, it is extremely \ndifficult to find another message that results in the \nsame string of numbers. Second, if all you have is \nthe string of numbers, it is infeasible to recover the \noriginal message. \nUsing a secure hash function, the sender can use \npublic-key cryptography to provide assurances of \nintegrity—information that cannot be tampered \nwith or altered in any way—and identity, in that the \nServers\nmi34fpe9501qk\nEncrypted email to Taylor\nDrew Taylor\nPublic Keys\nmi34fpe9501qk\nmi34fpe9501qk\nDrew’s \nprivate \nkey\nTaylor’s \nprivate \nkey\nSend \nme $\nSend \nme $\nTaylor uses their\nprivate key to \ndecrypt the \nmessage\nDrew uses \nTaylor’s\npublic key \nto encrypt\nTaylor’s\npublic key\nDrew’s \npublic key\nFIGURE 3.1  How public-key cryptography works\n56 STANFORD EMERGING TECHNOLOGY REVIEW\nApplications that run on a blockchain are called \nsmart contracts. These are computer programs that \nare always available and whose execution cannot be \nreversed—once a smart contract processes an incom\u0002ing request, that processing cannot be rolled back. \nSmart contracts can be used to implement financial \ninstruments, to record ownership of digital assets, \nand to create marketplaces where people can buy \nand sell assets. Smart contracts are composable—\none smart contract can use another—thus creating \na vibrant ecosystem of innovation where one proj\u0002ect can make use of a service developed by another \nproject. Once deployed, they are available forever, \nrunning whenever someone interacts with them. By \ncontrast, cloud computing applications are inherently \ntransient—as soon as the application developer \nstops paying the cloud fees, the cloud provider kills \nthe application. \nKey Developments\nA Host of Blockchain Applications\nBlockchain technology was developed decades ago \nbut has recently been used for a variety of applica\u0002tions. All those listed below have been implemented \nin some form and are operational today, though per\u0002haps not on particularly large scales.\nTime-stamping and data provenance Because \ndata written to a blockchain cannot be modified or \nremoved, blockchains provide a good mechanism \nfor data provenance and time-stamping. An artist or \nan author who creates a new work of art can post a \nhash of the work to the chain, thereby proving the \ntime at which the object was created. If later some\u0002one else claims authorship of the creation, the artist \ncan point to the chain to prove its provenance.\nIdentity management A blockchain stores all \nthe data from a person’s important documents—\n diplomas, healthcare and financial records, tax \nreturns, birth certificate—in encrypted form. These \nMessages can also be digitally time-stamped. A \nknown authoritative time and date server—such as \nthe Internet Time Service, operated by the National \nInstitute of Standards and Technology—accepts a \nmessage, appends the current date and time, and then \nprovides a digital signature for the stamped message.\nBlockchain \nBlockchain is a technology that enables multiple par\u0002ties to coordinate when there is no central trusted \nparty. This often comes up in financial settings. A \nblockchain records transactions so that they cannot \nbe altered retroactively without detection. Because \nthe entire blockchain can be distributed over thou\u0002sands of computers, it is always accessible; anyone \ncan deploy an application for it, and no one can \nprevent any such deployment. Moreover, anyone \ncan interact with this application, and no one can \nprevent such an interaction. Finally, data cannot be \nerased. Later transactions may indicate that correc\u0002tions are necessary, but the original data remain.\nA blockchain can be visualized as a chain of blocks \nwhere each block contains a single transaction and \na cryptographic hash of the previous block. This cre\u0002ates a chain in which every block except the first is \nlinked to the previous block. As more transactions \noccur, the blockchain gets longer because more \nblocks are added to the chain.\nThe distributed nature of blockchain also increases \nsecurity. A new transaction is broadcast to every \nparty in the network, each of which has a replica \nof the entire blockchain (see figure 3.2). Each party \ntries to validate the new transaction. It could happen \nthat these replicas may not be fully synchronized; \nsome might have received the new transaction while \nothers have not. To ensure that all replicas are iden\u0002tical, blockchains have mechanisms for coming to \nconsensus on the correct information. Ethereum, for \nexample, accepts transactions that have been vali\u0002dated by two-thirds of the participants. Blockchains \nare designed with economic incentives for replicas \nto behave honestly. \n03 Cryp\nA wants to send money \nto B\nThe transaction is represented \nonline as a “block”\nThe block is broadcast to \nevery party in the network\nThose in the network \ncon\u0006rm the validity of \nthe transaction\nThe block then can be added \nto the chain, which provides \nan indelible and transparent \nrecord of transactions\nThe money moves from\nA to B\nB\nA\nFIGURE 3.2  How a blockchain manages transactions\nSupply chain management Blockchain can pro\u0002vide a transparent and secure way to track the move\u0002ment of goods and their origin and quantity. This \ncan be particularly valuable for high-value industries, \nsuch as the diamond industry; industries with signif\u0002icant counterfeit issues, such as luxury goods (see \nfigure  3.3); or industries where the true source of \ngoods is important, such as organic or vegan food. \nBlockchain can greatly simplify the job of forensic \naccountants trying to trace transactions.\nTransactional records Many kinds of transactional \nrecords can be stored on a blockchain, thereby \nstreamlining the process of buying and selling items \nby reducing fraud, increasing transparency, cutting \noriginal records are saved digitally, signed by their \noriginal providers, and, when made available through \nthe blockchain, provided with provenance and \ntime-stamping. Blockchain also facilitates selective \nrevelation: Upon request, the person can authorize \nrelease of data only to the minimal extent neces\u0002sary to satisfy the request. For example, people can \nprove that their age is above some legal minimum, \nlike twenty-one, but not have to reveal their date of \nbirth. A woman can allow a healthcare researcher \nto look at her records for specific data—for example, \nwhether she has ever had an abortion—without \nrevealing her name. Applications of blockchain for \nidentity management, such as SpruceID, are already \nbeing deployed.4\n58 STANFORD EMERGING TECHNOLOGY REVIEW\nunder the jurisdiction of the Securities and Exchange \nCommission.\nSecure Computation \nThe field of cryptography has also expanded in scope \nto include secure computation, a well-established \nsubfield that enables multiple parties to contribute \ninputs to a function that they jointly compute in such \na way that the specific inputs from each party are \nkept secret from the others. Secure computation \nenables data privacy during computation, ensuring \nthat no party learns more information about the \nother parties’ inputs than what can be inferred from \nthe result alone. Secure computation also allows \nusers to prove they possess knowledge of a state\u0002ment without having to disclose the actual content \nof that statement.\nTo illustrate secure computation, consider the prob\u0002lem of determining the collective wealth of three \npeople while keeping the individual wealth of each \nperson secret. Alice chooses a large random number \nand in secret adds her wealth to that number. Alice \nthen gives the sum to Bob privately, who adds his \nwealth secretly to the number received from Alice. \nBob secretly passes the total to Charlie, who does \nthe same computation and then passes the result \nto Alice. Alice then in secret subtracts her original \nrandom number from the number received from \nCharlie and reveals the result to everyone else. That \nrevealed number is the sum of each party’s wealth \nbut at no time does anyone learn of anyone else’s \nwealth.5\nThis example is oversimplified but is offered to \nsuggest how computation on secret data might be \naccomplished. The example is not exactly how a \nreal-world secure computation works (in fact, there \nis a subtle flaw in the procedure described); true \nsecure computation protocols use more complex \nmathematics to defend against malicious behavior \nand to guarantee the privacy of each person’s input \nduring the computation process.\npaperwork, and generally making the process more \nefficient. \nCryptocurrencies Cryptocurrencies are digital \ninstruments that many people use as a medium \nof exchange. Well-known ones include Bitcoin, \nEthereum, Avalanche, and Polygon, each of which has \nits own unique features and applications. Because \nthey are not issued by any central authority, they are \nnot subject to the same national regulatory regimes \nthat govern traditional currencies (i.e., so-called \nfiat currencies). Cryptocurrencies use a blockchain \nstructure to ensure the integrity and immutabil\u0002ity of transaction data, making it resistant to fraud \nand counterfeiting and reducing its susceptibility to \ngovernment interference or manipulation. Contrary \nto a common belief, cryptocurrencies can, but do \nnot have to, support private or secret transactions—\nindeed, the most popular cryptocurrencies deliber\u0002ately do not hide the details of their transactions. \nThose who transact in cryptocurrencies often wish to \nexchange their instruments for fiat currency (e.g., real \ndollars) and generally use a cryptocurrency exchange \nto do so. In the United States, such exchanges are \nregulated financial institutions and are presently \nSource: Shutterstock / TY Lim\nFIGURE 3.3  Blockchain helps tackle counterfeiting \nin the luxury goods industry\n03 Cryp\nTo do so, Paul has Vivian write something on a piece \nof paper without showing it to him. Together, they \nput the paper into the safe and spin the combination \nlock. Vivian now challenges Paul to say what is on the \npaper. Paul responds by asking Vivian to turn around \n(so that Vivian cannot see Paul) and then enters the \ncombination of the safe, opens it, looks at the paper \nand returns it to the safe, and closes it. When Vivian \nturns around, Paul tells her what was on the paper. \nPaul has thus shown Vivian that he knows the com\u0002bination without revealing to Vivian anything about \nthe combination. \nIn practice, of course, zero-knowledge proofs are \nmore complex, yet they already have seen real\u0002world implementations:\nBanking A buyer may wish to prove to a seller \nthe possession of sufficient funds for a transaction \nwithout revealing the exact amount of those funds. \nThis capability has been implemented in the Zcash \ncryptocurrency.7\nProvenance for digital images Cameras can pro\u0002vide a digital signature for every photo, capturing \nan image and information about the time, date, \nand location. But such photos can then be digitally \ncropped, resized, or converted from color to black\u0002and-white. Zero-knowledge proofs have been imple\u0002mented in the standards of the Coalition for Content \nProvenance and Authenticity to ensure that the \noriginal photo was properly signed and that only \npermissible edits were made to the original without \nhaving to trust the editing software that was used.8\nCooperative tracking and verification of numbers \nof tactical nuclear warheads A zero-knowledge \nproof methodology has been developed to cooper\u0002atively provide updates on the movement and status \nchanges of warheads in accordance with a political \nagreement to do so without revealing other sensitive \ninformation. This approach has not yet been imple\u0002mented in any real arms control agreement, but its \nfeasibility has been demonstrated in principle.9\nIn addition, the example is somewhat artificial \ncompared to more realistic examples (with more \ncomplex mathematics) such as tallying vote counts \nor bidding in an auction. For example, at an auc\u0002tion, three bidders each have a secret bid in mind, \nand the goal could be to determine which bid is \nthe highest without publicly revealing information \nabout the other bids. \nApplications of secure computation allow data ana\u0002lytics to be performed on aggregated data without \ndisclosing the data associated with any individual \nelement of the dataset. Banks can detect fraud with\u0002out violating the privacy of individual customers. A \ngroup of workers can calculate their average salary \nwithout revealing their colleagues’ personal pay. A \nStanford system called Prio allows for a network of \nconnected computers to work together to compute \nstatistics, with clients holding their individual data \nprivately.6 This was deployed, for example, on \nmobile phones during the COVID-19 pandemic \nto calculate how many people were exposed to \nCOVID-19 in aggregate, without learning who was \nexposed. \nZero-Knowledge Proofs\nA zero-knowledge proof is a cryptographic method \nthat allows Paul (the prover) to prove to Vivian (the \nverifier) that Paul knows a specific piece of informa\u0002tion without revealing to Vivian any details about \nthat information. The term zero knowledge indi\u0002cates that Vivian gains zero new knowledge about \nthe information in question, apart from the fact that \nwhat Paul is saying is true.\nConsider a simplified example that demonstrates \nthe logic: two people dealing with a locked safe. \nLet’s say Paul wants to prove to Vivian that he knows \nthe combination to the safe, but he doesn’t want \nto reveal the combination to Vivian. With a zero\u0002knowledge proof, Paul can convince Vivian that he \nknows the combination without exposing the com\u0002bination itself.\n60 STANFORD EMERGING TECHNOLOGY REVIEW\nto share problem sets and understand the potential \nbenefits that cryptographically enabled techniques \nand approaches could provide. \nResearch is funded by both the US government and \nprivate industry, but funding from the US govern\u0002ment is subject to many requirements that increase \nthe difficulty of proposal submission manyfold (as \nmuch as by a factor of sixty). Thus, research faculty \noften prefer arrangements with the private sector, \nwhich tend to be much simpler. On the other hand, \nonly the US government is able to fund research that \nmay not pay off for many years (as in the case of \nquantum computing). \nPolicy, Legal, and Regulatory Issues\nAs a rule, public policy considerations are applica\u0002tion specific; there has been no push to regulate \nbasic research in cryptography for several decades. \nEXCEPTIONAL ACCESS\nExceptional access regulations would require com\u0002munications carriers and technology vendors to \nprovide US law enforcement agencies access to \nencrypted information (both data storage and \ncommunications) under specific legal conditions. \nOpponents of exceptional access argue that imple\u0002menting this capability inevitably weakens the secu\u0002rity afforded by encryption to everyone. Supporters \nof exceptional access do not debate this technical \nassessment: It is true that exceptional access, by \ndefinition, weakens encryption. However, they argue \nthat even if lower security is the result of implement\u0002ing exceptional access, that price is worth the bene\u0002fits to law enforcement.10\nCRYPTOCURRENCY REGULATORY CONCERNS\nParticularly considering the 2023 FTX trading scan\u0002dal, in which the FTX cryptocurrency exchange went \nbankrupt and founder Sam Bankman-Fried was \nsubsequently convicted of fraud, many have ques\u0002tioned the extent to which cryptocurrencies should \nOver the Horizon \nImpact of Cryptography \nThe applications described above suggest a broad \nrange of possibilities for cryptographically enabled \ndata management services. Whether we will see \ntheir widespread deployment depends on compli\u0002cated decisions about economic feasibility, costs, \nregulations, and ease of use.\nMisaligned incentives can affect how fast innovations \nare deployed. Some of the applications described \nabove provide significant benefits for the parties \nwhose data can be better protected and kept more \nprivate. But existing companies, having built their \nbusiness models on legacy systems that ingest all \ntheir customers’ data, have no incentive to change \ntheir practices. They are the ones who would have \nto pay for these privacy-protecting capabilities, yet \nthey would not benefit from their adoption.\nA second point is that widespread deployment \nwill require confidence that proposed innovations \nwill work as advertised. That is, would-be users of \nthese innovations must have confidence in them. \nBut concepts such as secure computation and \nzero-knowledge proofs are math heavy and coun\u0002terintuitive to most people. Expecting policymak\u0002ers, consumers, and regulators to place their trust in \nthese applications will be challenging. \nChallenges of Innovation and \nImplementation\nAlthough cryptography is fundamentally a math\u0002ematical discipline, it requires both human talent \nand substantial computing resources to examine the \nefficiency of new techniques, write software that is \ncomputationally expensive such as zero-knowledge \nprovers, and conduct comprehensive scans of the \ninternet. Progress also relies on interdisciplinary cen\u0002ters that bring together faculty from different fields \n03 Cryp\nThe lack of a regulatory framework for cryptocurrency \naffects many American users, consumers, and investors \nwho are often confused about the basic workings of \ncryptocurrencies and their markets.\nto build quantum computers that are capable of \nthis, but under the May 2022 National Security \nMemorandum 10, Promoting US Leadership in \nQuantum Computing While Mitigating Risks to \nVulnerable Cryptographic Systems, the US gov\u0002ernment has initiated the transition to quantum\u0002resistant public-key algorithms. Many experts in the \nfield expect quantum-resistant algorithms will be \nwidely available by the time quantum computing \ncomes online. \nAt the intersection of quantum computing and cryp\u0002tography are two important issues. The first is that \nsupport for the transition to a quantum-resistant \nencryption environment should continue with urgency \nand focus. \nA second issue is that messages protected by \npre-quantum cryptography will be vulnerable in a \npost-quantum world. If those messages had been \nsaved by adversaries (likely in the case of parties like \nRussia), those bad actors will be able to read a host \nof old messages. Containing secrets from the past, \nthey may reveal embarrassments and dangers with \npotentially detrimental policy implications.12\nCENTRAL BANK DIGITAL CURRENCIES AND THE \nEROSION OF US FINANCIAL INFLUENCE\nA central bank digital currency (CBDC) is a type of \ncryptography-based digital currency issued and reg\u0002ulated by a country’s central bank, with legal tender \nstatus and value equivalent to the country’s tradi\u0002tional currency—that is, digital assets backed by \nbe exchangeable for national currency and whether \nthey are better regulated as investment instruments \nor as currency. The lack of a regulatory framework \nfor cryptocurrency affects many American users, \nconsumers, and investors who are often confused \nabout the basic workings of cryptocurrencies and \ntheir markets. It may also prevent entrepreneurs \nfrom implementing their ideas in the United States \nor inadvertently incentivize them to move offshore.\nENERGY CONSUMPTION\nBitcoin, an older and today the dominant crypto\u0002currency, consumes an enormous amount of \nenergy; Bitcoin mining uses more energy than the \nNetherlands.11 For this reason, newer blockchains—\nnotably Ethereum—are designed to use far less \nenergy, and today Ethereum’s annual energy use is \nless than a 10,000th of YouTube’s annual consump\u0002tion. But Ethereum’s market capitalization is less \nthan half that of Bitcoin, and it remains to be seen \nwhether any less energy-intensive cryptocurrency \nwill displace the latter. \nQUANTUM COMPUTING AND CRYPTOGRAPHY\nCurrent public-key cryptography is based on the \nextraordinarily long times (times comparable to the \nage of the universe) required with today’s computers \nto derive a private key from its public-key counter\u0002part. When realized, quantum computing (discussed \nmore fully in chapter  8 on semiconductors) will \npose a significant threat to today’s public-key algo\u0002rithms. Experts disagree on how long it will take \n62 STANFORD EMERGING TECHNOLOGY REVIEW\ncentral banks. A CBDC can be designed with any \nnumber of the functional characteristics of crypto\u0002currencies and thus can be regarded as a national \ncryptocurrency. However, a CBDC could be imple\u0002mented in a centralized manner to improve perfor\u0002mance and efficiency instead of using distributed \nblockchain technology.\nAn important benefit of a CBDC is the marriage of \nconvenience and lower costs of digital transactions—\nby cutting out intermediaries—and the regulatory \noversight of traditional banking. In 2021, nearly \nsix  million Americans had no access to a bank \naccount. Lower transaction costs would improve \nfinancial inclusion and enable many more people \nto have access to a well-regulated financial system. \nThose lower costs would also apply to cross-border \ntransactions, therefore reducing the costs of interna\u0002tional commerce. \nThe United States is considering issuing its own \nCBDC.13 Although the dollar is the currency most \nused in cross-border transactions, the development \nof CBDCs by others could reduce global depen\u0002dence on the US currency and on a financial infra\u0002structure largely controlled today by the United \nStates (e.g., the Society for Worldwide Interbank \nFinancial Telecommunication, or SWIFT, which is \nused by banks and other institutions to send secure \nmessages to each other about financial transactions). \nThis could significantly undermine the effectiveness \nof US economic sanctions and other financial tools. \nToday, more than ninety nations are researching, \npiloting, or deploying CBDCs, with several already \ntesting cross-border transactions. China is the first \nmajor country to deploy a CBDC, the digital yuan, \nwidely within its own economy.14 America may lag \nChina and some other countries, but it could bene\u0002fit from a cautious, well-timed approach by learning \nfrom earlier adopters’ experiences.\nNOTES\n1. National Institute of Standards and Technology, “Cryptogra\u0002phy,” US Department of Commerce, accessed August 15, 2023, \nhttps://www.nist.gov/cryptography.\n2. Whitfield Diffie and Martin Hellman, “New Directions in Cryp\u0002tography,” IEEE Transactions on Information Theory IT-22, no. 6 \n(November 1976): 644–54.\n3. In this context, encrypting the hash value simply means running \nthe encryption algorithm using as the input key a string of numbers \nthat just happen to be Alice’s private key. In most cases involving \npublic-key cryptography, the private key is used only for decryp\u0002tion purposes, but nothing stops a user from using it in other ways.\n4. SpruceID, “SpruceID,” Spruce Systems, accessed October 13, \n2024, https://spruceid.com/.\n5. This example is inspired by Keyless Technologies, “A Beginner’s \nGuide to Secure Multiparty Computation,” Medium, February 22, \n2020, https://medium.com/@keylesstech/a-beginners-guide-to\n-secure-multiparty-computation-dc3fb9365458.\n6. See Stanford University, “Prio,” accessed September 25, 2023, \nhttps://crypto.stanford.edu/prio.\n7. Zcash, “What Are Zero-Knowledge Proofs?,” accessed August 30, \n2023, https://z.cash/learn/what-are-zero-knowledge-proofs.\n8. Trisha Datta and Dan Boneh, “Using ZK Proofs to Fight Disin\u0002formation,” Medium, September 29, 2009, https://medium.com\n/@boneh/using-zk-proofs-to-fight-disinformation-17e7d57fe52f.\n9. Miles A. Pomper, William Alberque, Marshall L. Brown Jr., et al., \nOP55: Everything Counts: Building a Control Regime for Nonstra\u0002tegic Nuclear Warheads in Europe, CNS Occasional Paper Series, \nJames Martin Center for Nonproliferation Studies, May 10, 2022, \nhttps://nonproliferation.org/op55-everything-counts-building\n-a-control-regime-for-nonstrategic-nuclear-warheads-in-europe.\n10. Office of Public Affairs, “Attorney General William P. Barr \nDelivers Keynote Address at the International Conference \non Cybersecurity,” US Department of Justice, July 23, 2019, \nhttps://www.justice.gov/opa/speech/attorney-general-william-p\n-barr-delivers-keynote-address-international-conference-cyber.\n11. Digiconomist, “Bitcoin Energy Consumption Index,” accessed \nSeptember 16, 2023, https://digiconomist.net/bitcoin-energy\n-consumption.\n12. Herbert Lin, “A Retrospective Post-Quantum Policy Problem,” \nLawfare, September 14, 2022, https://www.lawfaremedia.org\n/article/retrospective-post-quantum-policy-problem.\n13. Board of Governors of the Federal Reserve System, “Central \nBank Digital Currency (CBDC): Frequently Asked Questions,” \naccessed August 15, 2023, https://www.federalreserve.gov/cbdc\n-faqs.htm.\n14. Darrell Duffie and Elizabeth Economy, eds., Digital Curren\u0002cies: The US, China, and the World at a Crossroads (Stanford, CA: \nHoover Institution, 2022), https://www.hoover.org/sites/default\n/files/research/docs/duffie-economy_digitalcurrencies_web\n_revised.pdf.\n03 Cryp\nSTANFORD EXPERT CONTRIBUTORS\nDr. Dan Boneh\nSETR Faculty Council and Professor of Computer \nScience and of Electrical Engineering\nDr. David Tse\nThomas Kailath and Guanghan Xu Professor of \nEngineering\nNeil Perry\nSETR Fellow and PhD Student in Computer Science\n\n65\nOverview\nImprovements in laser technology since its invention \nin 1960 have allowed light to be manipulated and \nused in previously unimaginable ways. Lasers now \nunderpin a huge range of scientific and industrial \napplications. Already, lasers with ever higher energy \nand power are being developed across a wider \nrange of wavelengths and with pulse lengths that \ncan illuminate many details of what is happening \nvery rapidly at an atomic and molecular level.\nA laser—an acronym derived from “light amplifi\u0002cation by stimulated emission of radiation”—is a \nlight source with three important characteristics. \nFirst, its light is monochromatic (i.e., single color), \nmeaning the light is highly concentrated around a \ncentral wavelength, with very little emitted at other \nwavelengths. Monochromatic light enhances data \ntransmissions by minimizing chromatic aberration, \nwhich occurs when a lens can’t focus different colors \nKEY TAKEAWAYS\n° Laser technology has become essential for a wide \nrange of applications, including communications, \nhigh-end chip production, defense, manufactur\u0002ing, and medicine. \n° Because advances in laser technology tend to \noccur in the context of specific applications, laser \ntechnology research and development is widely \ndispersed among different types of laboratories \nand facilities. \n° Broad investment in next-generation lasers holds \nthe potential to improve progress in nuclear \nfusion energy technology, weapons develop\u0002ment, and quantum communication.\nLASERS\n04\n66 STANFORD EMERGING TECHNOLOGY REVIEW\nmore of these elements and is generally measured \nwith respect to five technical characteristics (figures \nof merit) of the beam: \nPeak power Generating the brightest possible \nlaser pulses—which equates to the greatest possi\u0002ble power—for very short times. The 2018 Nobel \nPrize in Physics was awarded for the development \nof the chirped pulse amplification technique, a high\u0002power, short-duration approach for producing laser \npulses that significantly outperformed prior peak \npower achievements. Peak powers in state-of-the\u0002art lasers can now reach levels that damage the \nlaser itself. Because of this, to reach even higher \npower levels, scientists have used multiple beams \nfrom multiple lasers focused on a target. In 2024, \none laser delivered a peak power of 10 petawatts (or \n1016 watts) with a pulse time (duration) of 24 femto\u0002seconds (a femtosecond is 10–15 seconds).1\n (For com\u0002parison, total global electrical generation capacity \ntoday is about 9 terawatts, or about 1/1,000th of the \npeak power of a 10-petawatt laser.)\nEnergy Delivering as much energy as possible in \na beam. A high-energy laser beam generally deliv\u0002ers its energy on timescales of a few nanoseconds, \nor around a million times longer than the lasers \nof light on a single point. Monochromatic lasers are \nalso essential in scientific and medical applications \nthat need specific wavelengths for controlled inter\u0002actions with materials or tissues. \nSecond, a laser is directional, which means its energy \ncan be concentrated into a small spot, significantly \nincreasing intensity and making lasers useful for appli\u0002cations that require precision and high energy density, \nsuch as cutting, welding, and surgical procedures. \nThird, laser light is coherent, which means that the \nlight waves it uses are in phase with each other—that \nis, they repeatedly reach the same peak or trough at \nthe same point in time and space. This property is \nimportant for holography, interferometry (the mea\u0002surement of light sources), and optical sensing, \nwhere precise phase information is needed to create \naccurate and detailed images or measurements. \nThere are many ways to produce laser light. Lasers \ntypically involve a power source (a pump), a gain \nmedium (a material within which the energy supplied \nby the pump is turned into laser light), and a reso\u0002nator that encloses the gain medium within which \nlaser light is produced (see figure  4.1). Progress \nin laser technology depends on advancing one or \nA power source (pump)\nGain medium\nLaser beam\nResonator\nFIGURE 4.1  Typical components of a laser\n04 Lase\nused across a far wider range of the electromagnetic \nspectrum. Being able to use a wider range of wave\u0002lengths enables the investigation and manipulation \nof matter under a wider variety of conditions.\nThe engineering characteristics of lasers are also \nan important aspect of how fast the technology \nadvances. For example, different configurations of \npower sources, resonators, and gain mediums can \nresult in lasers of different sizes, weight, reliability, \ncost, and other key features. Moreover, some appli\u0002cations require mechanisms that can steer beams \nin particular directions. Addressing these and other \nengineering issues helps take lasers from labs to the \ncommercial world, where many non-research appli\u0002cations make important use of them.\nFor example, researchers have miniaturized a titanium\u0002sapphire laser by polishing and etching a bulk \ntitanium-sapphire crystal to a nanoscale-thick layer \non a silicon dioxide support.5 They then patterned a \ncircular waveguide into the titanium-sapphire layer. \nThe intensity of the generated light is increased over \nthe length of the waveguide. The miniaturized laser \nis several orders of magnitude smaller and signifi\u0002cantly less expensive than existing titanium-sapphire \nlasers, which are currently the best ones for a variety \nof applications including quantum optics, spectro\u0002scopy, and biomedical research. \nKey Developments\nThe basic operating principles and physics of lasers \nare generally well understood. What stands out in \nreviewing key developments in laser technology is \nthe wide variety of applications to which the tech\u0002nology is relevant. Below is a list of some examples \nof important applications.\nMedicine\nLasers in medicine have historically been used to \nablate, cut out, or vaporize tissue or to clot bodily \ndiscussed above. The highest energy lasers today are \nfound at the National Ignition Facility of the Lawrence \nLivermore National Laboratory (LLNL). These deliver \nbeam energies as high as 2.2 megajoules and have \nbeen used to drive controlled nuclear fusion reac\u0002tions at the lab that produced a net energy gain.2\nAverage power Reliably delivering high power \nand energy at elevated repetition rates. Many laser \napplications need pulses whose quality is consis\u0002tent and that are delivered frequently and reliably. \nAn important technical challenge is managing heat \nbuildup in the resonator, which can limit the number \nof pulses a laser can produce over a given time. \nToday, high-average-power lasers—ones rated in \nexcess of 300 kilowatts—use active liquid or gas \ncooling in what is called a distributed gain laser \narchitecture.3 Some efforts to manage possible laser \ndamage involve development of improved material \nproduction techniques that can, for instance, reduce \nerosion of the coating in lasers’ optical systems. \nOther solutions involve the use of lasers based on \ngaseous media, which are inherently less prone to \ndamage.\nPulse length Generating shorter laser pulses. \nOver the course of lasers’ development, squeezing \nenergy into ever shorter pulse lengths has been the \nprimary way of generating beams with higher power. \nIn addition, short pulse lengths can be used like a \nstrobe light to observe rapid motion. For example, \nthe generation of attosecond pulses (10–18 seconds) \nwas recognized with the Nobel Prize in Physics in \n2023.4 These pulses are shorter than the timescale \nof electronic motion within atoms, enabling atomic \nprocesses to be observed with the electrons effec\u0002tively frozen in place. \nWavelength Delivering laser-like pulses at more \nfrequencies. Historically, the term laser is generally \nused to refer to devices operated near to optical \nwavelengths, distinguishing them from the micro\u0002wave “masers” that preceded them. Today, how\u0002ever, pulses of radiation with the same properties \nthat make lasers so useful can be generated and \n68 STANFORD EMERGING TECHNOLOGY REVIEW\nit causes only minimal damage to the surrounding \ntissue.\nLasers may come to play an important role in certain \ncancer treatments. Specifically, some recent cancer \nresearch has discovered that charged particle beams \ndelivered at extremely high dose rates to cancerous \ntissue may have unique benefits. A very high dose \nof proton beam radiation delivered to a cancerous \ntumor over a very short time will kill it while signifi\u0002cantly reducing collateral damage to surrounding \ntissue compared to current approaches. The pro\u0002duction of such proton beams was driven by a laser \nwhose operating characteristics could be very tightly \ncontrolled, leading to a beam precisely tailored for \nthe tumor in question.10\nMilitary Applications\nLasers as weapons could serve a variety of ground\u0002based missions,11 including attacking satellites and \nproviding short-range air defense to counter drones, \nrockets, artillery, and mortar rounds. In these roles, \nlasers have several advantages over conventional \nmunitions—in particular, lower cost per shot and \npotentially more rounds in their magazines (assum\u0002ing their power supplies are not exhausted). But they \nhave certain disadvantages as well—most importantly, \nrain, fog, and some other atmospheric conditions \npotentially limit their range and beam quality. \nBecause a laser beam traveling through the atmo\u0002sphere loses energy as the range to the target \nincreases, laser weapons need high-power beams \nto damage distant targets. These two conflict\u0002ing requirements can be resolved with a laser that \ndelivers a beam with a very long pulse length. This \nmeans the beam must dwell on its target for the \nentire duration of the pulse—and if the target moves \nduring that time, the weapon must have a pointing \nmechanism that keeps the beam on target for a few \nseconds. Another way to resolve these requirements \nis to select a wavelength for laser operation that is \nnot strongly absorbed by the atmosphere. But since \nsome degree of absorption will occur in any event, \nfluids.6 For example, a robot-guided laser has been \nused to perform bone surgery.7 Traditional tools like \nsaws, drills, and burs can cause mechanical and ther\u0002mal damage to bone and tissue and are also limited \nto simple cuts. In contrast, lasers offer more precise, \ncleaner cuts with less damage to surrounding tissue, \nand they can handle complex trajectories, especially \nwhen guided by a computer-controlled robot arm \nfor fully automated surgery. This technology not only \nenhances accuracy but also reduces recovery time \nfor patients. \nA well-known example is laser eye surgery, where \nultrashort laser pulses are used to remove small \namounts of corneal tissue with great precision, thus \nreshaping the cornea to improve how light is focused \nonto the retina (see figure  4.2). Interestingly, this \ntechnique, popularly known as LASIK, was inspired \nby a laser eye injury in a research lab.8\nLasers can also be used to destroy subsurface tumors \nwith minimal thermal damage to surrounding healthy \ntissue. Researchers have demonstrated the use of a \nfocused laser beam from an ultrashort-pulse diode \nlaser source.9 The beam is intense enough to destroy \na tumor but focused enough and short enough that \nSource: Shutterstock / Terelyuk\nFIGURE 4.2  Laser eye surgery uses laser pulses to \nremove corneal tissue\n04 Lase\nand the hardware required is smaller and lighter. The \nprimary technical challenge they face is the issue of \nbeam alignment: Because the beams need to be \nvery narrow, aligning the sending laser and the laser \nreceiver properly is hard if they are far apart. \nLasers are well suited for space-to-space commu\u0002nications, where there is very little to interfere with \nthe beam (see figure  4.3). Starlink—the space\u0002based internet service provider wholly owned and \noperated by SpaceX—uses laser communications to \ntransfer data at high speeds directly between satel\u0002lites in low Earth orbit (LEO) without going through \nground stations.14 In December 2023, NASA suc\u0002cessfully demonstrated its first two-way laser com\u0002munication link between the International Space \nStation in LEO and a geostationary satellite.15 Efforts \nare also underway to adapt laser communications \nfor ground/air/sea-to-space applications, which will \nmean overcoming challenges posed by atmospheric \ninterference with data-carrying laser beams.\nthe tension between these requirements can only be \nreduced and not eliminated.\nIn general, laser weapons require a laser to supply \nthe necessary beam, a power supply (typically \nan electrical battery source or chemicals that are \nmixed to produce energy), and a way of tracking \na target and directing the beam to remain trained \non it while it is in motion so enough energy can be \ndelivered to destroy or disable the target. (When \nthe mission is to disable things like sensors that a \ntarget may be carrying, such as cameras on a recon\u0002naissance satellite, the power required is much \nlower than if the target’s entire physical structure \nmust be destroyed.)\nProgress in laser weapons involves making them \nsmaller and lighter, more rugged for an operational \nmilitary environment, more powerful, and more effi\u0002cient in their conversion of energy in the magazine \nto shots fired. Auxiliary technologies such as those \nfor beam tracking and target sensing must also work \nin concert with the lasers themselves. \nCommunications\nLasers play a key role in communications by transmit\u0002ting data through fiber-optic cables, which make up \nthe bulk of the infrastructure behind the internet. As \ndemand for information transfer grows, approaches \nthat raise data transfer rates are increasingly impor\u0002tant. Recent results have shown that data can be sent \nthrough fiber-optic cables using much shorter laser \npulses without a loss of transfer fidelity, potentially \nlowering power requirements significantly.12\nLasers can transmit data over long ranges and are \neven being used to enable satellites in orbit to com\u0002municate with one another.13 Compared to traditional \nradio transmission systems, laser communications \nallow for data-transfer rates that are 10 to 100 times \nfaster than radio. They are also more secure than radio \nsystems because they have directed narrower beam \nwidths that make them harder to intercept. Laser com\u0002munications systems are also more energy efficient, \nSource: General Atomics Electromagnetic Systems\nFIGURE 4.3  Lasers are well suited for space-to\u0002space communications\n70 STANFORD EMERGING TECHNOLOGY REVIEW\npotentially rendering space activities and satellite \noperations in certain orbital ranges difficult or impos\u0002sible for many future generations. Technologies for \ndebris removal may become important in the future, \nand lasers could be used for this purpose. \nSpecifically, NASA is supporting a project to research \na network of lasers mounted on space platforms.17\nThese lasers are supposed to deflect debris of vari\u0002ous sizes through ablation, which involves an intense \nlaser pulse vaporizing surface material on an object. \nThe material is ejected away from it, altering the \nobject’s momentum. If the impulse of that ejec\u0002tion is properly oriented, the object’s speed can be \nreduced, and eventually it will deorbit and burn up \nin the atmosphere on reentry.\nImaging\nAt short wavelengths, pulses from an X-ray free\u0002electron laser (XFEL) can penetrate through mate\u0002rials to image structures and measure a material’s \nphysical properties. The current Linac Coherent Light \nSource (LCLS)-II High-Energy upgrade to the XFEL \nat the SLAC National Accelerator Laboratory will push \nthe maximum energy that it can reach even higher, \nallowing heavier and denser materials to be probed.18\nXFELs are particularly useful for imaging where the \nshorter wavelengths of X-rays allow better spatial res\u0002olution compared to visible light—an example is the \ncoherent X-ray imaging end station of SLAC’s LCLS.19\nIn addition, XFELs can emit very short pulses, which \nhelps them excel at tracking changes over very short \ntime periods. Previous results have allowed new pro\u0002teins to be imaged and have enabled researchers \nto observe phase transitions of quantum materials in \nreal time20 or observe materials under extreme con\u0002ditions of pressure, such as those in the center of \nthe sun. The approach has also shown how biomol\u0002ecules move in real time, and an extended research \neffort has followed the complex series of reactions \nthat occur throughout the process of photosynthe\u0002sis, with implications for future photovoltaic cells and \nother devices that seek to harness solar power.21\nAdditive Manufacturing\nLasers are useful for additive manufacturing (also \nknown as 3-D printing), enabling precise and efficient \ncreation of complex structures through various tech\u0002niques. For example, in stereolithography an ultravio\u0002let laser is used to cure a photosensitive resin layer by \nlayer. The laser selectively turns on and off, curing the \nlayer with the appropriate structure. The next layer \nis treated similarly until the artifact is fully formed. \nAnother method, selective laser sintering, uses a laser \nto harden (sinter) a layer of powder, such as nylon or \nmetal. These laser-based techniques can be adapted \nfor various materials, making them suitable for rapid \nprototyping and other manufacturing applications.\nParticle Traps / Quantum Computing\nLasers can be used to create the coldest tempera\u0002tures achieved on Earth—significantly colder than \nthe void of interstellar space. The record low tem\u0002perature is around a few millionths of a degree \nkelvin from absolute zero (about minus 273 degrees \nCelsius) for small material samples, with parallel \nwork focusing on cooling larger samples, such as \nthe mirrors at the Laser Interferometer Gravitational\u0002Wave Observatory, in order to reduce thermal noise \nin the system that interferes with the detection of \ngravitational waves from space.\nLaser-cooled atoms demonstrate measurable quan\u0002tum behavior and hence are one of the approaches \nbeing pursued to work with quantum bits, or qubits, \nin labs.16 (Qubits are the building blocks for quan\u0002tum computers, which are discussed in chapter 8\non semiconductors.) By focusing laser beams into a \nvery small space, scientists can trap atoms and other \nparticles and manipulate them into quantum states \nto produce qubits using yet more lasers. \nOrbital Debris Removal\nChapter 9 on space describes the Kessler syndrome, \na scenario in which the density of objects both large \nand small in LEO becomes so high that collisions \nbetween some of them create a cascade of debris, \n04 Lase\nThe benefits of short pulses also extend to surgery, \nas described above, and to dentistry, where the \nwavelength can be chosen to reduce the risk of dam\u0002aging soft tissue. Both approaches are now being \ncombined with robotics to automate treatments.23\nChip Fabrication\nFor a long time, the mass production of chips with \nstructures smaller than 100 nanometers relied on \nthe availability of high-average-power lasers that \ncan produce light for lithography purposes, which \ninvolves transferring circuit patterns onto silicon \nwafers. In recent years, new processes have pushed \naverage power requirements even higher, reflected \nin both peak power and the rate at which laser pulses \ncan be generated. These new processes entail evap\u0002orating tin droplets with lasers to generate a plasma, \nwhich is then stimulated to produce extreme ultra\u0002violet (EUV) light to project a mask that carries circuit \nMaterials Processing\nLasers are now used for a wide range of applica\u0002tions in materials processing, including laser cutting \nof precise shapes (see figure  4.4), laser drilling of \nmicron-scale holes, and laser peening—deliberately \ndeforming surfaces—to add stress to materials. \nUltrashort pulse lasers enable material to be ablated \nprecisely with minimal damage to surrounding \nareas—a process useful both in manufacturing and \nin surgery. This process, sometimes called cold \nablation, works by vaporizing material faster than \nheat can spread through it. However, to prevent \noverheating, each spot must be processed slowly, \nwhich limits overall throughput. To address this \nchallenge, a beam from a powerful laser is split into \nsmaller beams, which can work on multiple areas \nsimultaneously.22\nThe limited collateral damage it causes means laser \nprocessing can be used even on biological samples. \nFIGURE 4.4  A laser is used to cut precise shapes in a metal form\nSource: Shutterstock / Pixel B\n72 STANFORD EMERGING TECHNOLOGY REVIEW\na much higher energy efficiency than is possible with \ncurrent facilities.\nIn conjunction with operating the world’s most pow\u0002erful XFEL, SLAC is developing a major laser facil\u0002ity that will house a petawatt peak power laser and \nincrease the beam energy of another of its lasers to \nhundreds of joules.24 An important feature of this \nfacility will be its ability to achieve highly symmetri\u0002cal compressions of fusion targets, which will enable \nmuch more accurate measurements of implosion \nphenomena, both spatially and temporally, and sup\u0002port more precise modeling techniques.\nOver the Horizon\nImpact of Laser Technologies\nAs described earlier in this chapter, lasers are criti\u0002cal components across a wide range of applications, \nincluding communications, high-end chip produc\u0002tion, defense, manufacturing, and medicine. This \nrange is so broad that lasers could fairly be regarded \nas an enabling technology—that is, a technology \nwhose existence and characteristics enable appli\u0002cations that would not otherwise be feasible or \naffordable. \nImproving key laser figures of merit—peak power, \nenergy, average power, pulse length, and wavelength—\nis a primary focus of extensive laser research. A \nrecent Basic Research Needs report from the US \nDepartment of Energy’s Office of Science empha\u0002sized that progress in all these areas is crucial for \nfuture scientific advances and new applications.25\nThe report highlighted that progress requires novel \napproaches and techniques. It also noted the \nimportance of additional engineering advances that \naddress the limitations of current inefficient laser \narchitectures and easily damaged optical compo\u0002nents, calling for advancements in laser architec\u0002tures, gain mediums, components, and control \ntechniques. \npatterns onto wafers. Since the process is not very \nefficient—a few hundred kilowatts to operate the \nlaser generates only a few hundred watts of EUV \nlight—power demands for state-of-the-art foundries \nhave already grown dramatically and keep rising.\nProducing structures smaller than 2 nanometers on \nvery high-end chips relies completely on this tech\u0002nology. These chips are critical for applications that \nrequire high processing power, extremely energy\u0002efficient operation, and miniaturization—requirements \nthat characterize many systems of economic and \nnational security importance. Although the capabil\u0002ity originated from laser research programs in the \nUnited States, the chips are now being produced by \na number of companies around the world, many of \nwhich are outside the United States. \nNuclear Fusion\nFusion occurs when two light atomic nuclei (usually \ndeuterium and tritium, both isotopes of hydrogen) \ncollide to form a heavier nucleus, releasing a large \namount of energy in doing so. As an energy source, \nfusion energy is still in the research and develop\u0002ment stage, as described in chapter 10 on sustain\u0002able energy technologies.\nToday, the central issue in research on fusion for pro\u0002ducing energy is the confinement problem—how to \nconfine the fuel for long enough to ensure “ignition” \nof the fusion reaction. One approach to solving this \nproblem is magnetic confinement fusion, which uses \npowerful magnets to contain and control a super\u0002heated plasma of deuterium and tritium. A second \nis inertial confinement fusion, which calls for rapidly \ncompressing a deuterium-tritium fuel pellet using \nlasers to ignite the fusion reaction. \nFor inertial fusion energy to become commercially \nviable, high-energy, high-repetition-rate laser beams \nare needed to drive the samples to the extreme \nstates required. The necessary lasers must deliver \nhigh energy beams at the relevant wavelength with\u0002out the risk of damaging their components and with \n04 Lase\nexpansive and promising, or does it appear that it \nhas plateaued?\nCost-effectiveness Are lasers really the best way \nto support a given application? Given total life-cycle \ncosts, are there more cost-effective ways of perform\u0002ing the same missions?\nAdequacy of the industrial base To what extent \nis the present industrial base capable of producing \nlaser systems and components in necessary quanti\u0002ties? What resources are needed, if any, to develop \nits capacity for procuring a given laser-based system?\nDual-use considerations As laser technology ad -\nvances, what are the implications, if any, for con\u0002trolling dual-use laser technologies that have both \nmilitary and civilian applications?\nEnvironmental and safety concerns What, if any, \nare the environmental and safety concerns raised by \nthe deployment of a given laser-based system? How \nshould such concerns be addressed?\nFor illustrative purposes, consider how some of these \nquestions might play out in two specific contexts.\nLasers as a defense against ballistic missiles The \nproblem of using lasers for intercepting ballistic mis\u0002siles is primarily characterized by the distance at which \nsuch intercepts must occur. Today, short-range rocket \nintercepts appear to be possible,26 but longer-range \nChallenges of Innovation and \nImplementation\nFor lasers, the challenges of innovation and imple\u0002mentation are addressed in a highly distributed \nfashion—that is, across a multitude of laboratories \nand facilities. The reason is that progress in laser \ntechnology seems to be highly dependent on the \nspecific application that requires a laser. An improve\u0002ment in laser technology useful for application A \nmay not be particularly useful for application B. For \nexample, improvements in the average power of \nlasers used at the National Ignition Facility at LLNL \nwill be of little value to lasers used in space-to-space \ncommunications. However, developments in beam \npointing and alignment technology in a laser com\u0002munications context may be helpful for laser weap\u0002ons development, as some of the same problems \narise with both the latter and the former.\nPolicy, Legal, and Regulatory Issues\nGiven that lasers are an enabling technology for \nmany applications, public policy issues tend not to \narise for lasers per se. Rather, they arise in the sectoral, \nsocietal, or policy context of a particular application. \nThese issues could include the following:\nTechnological maturity Is laser technology at a state \nof maturity to support a given application? What are \nthe alternatives to using lasers for that application? \nIs the growth path for a particular laser technology \nLasers could fairly be regarded as an enabling \ntechnology — that is, a technology whose existence \nand characteristics enable applications that \nwould not otherwise be feasible or affordable. \n74 STANFORD EMERGING TECHNOLOGY REVIEW\nintercepts are not, at least not with ground-based \nsystems for most feasible laser technologies. Against \nshort-range rockets, lasers have an economic advan\u0002tage over missile interceptors, costing only a few dol\u0002lars per laser shot as opposed to tens or hundreds \nof thousands of dollars per missile interceptor. Some \ntechnology usable for laser weapons has important \ncivilian use—one example is deformable mirrors that \ncan be used to enhance the quality of laser beams \npropagating through the atmosphere. When chem\u0002ical lasers were contemplated for military use, envi\u0002ronmental considerations were one negative aspect, \nas the lasers’ exhaust was toxic.\nLasers for surgery Key concerns here include safety \nand cost-effectiveness. Safety guidelines for health\u0002care are constantly being updated and refined. For \ninstance, in 2022 the American National Standards \nInstitute released a new standard for the safe use of \nlasers that includes an updated section on maximum \npermissible medical-related exposures in terms of \nilluminance, or the amount of light allowed to fall \non a given surface area.27 In terms of cost, while \nsome lasers for highly specific applications can be \nvery expensive, others that can be used for multiple \napplications are much cheaper. For example, some \nexcimer lasers, which emit short pulses of high\u0002energy light and are used for medical procedures, \nsuch as LASIK and treating eczema, as well as in \nmanufacturing, are available for under $100,000.\nNOTES\n1. Thales Group, “New World Record for Thales Lasers at ELI\u0002NP,” April 20, 2023, https://www.thalesgroup.com/en/worldwide\n-market-specific-solutions/lasers/news/new-world-record-thales\n-lasers-eli-np.\n2. Lawrence Livermore National Laboratory, “LLNL’s National \nIgnition Facility Delivers Record Laser Energy,” US Department of \nEnergy, November 16, 2023, https://www.llnl.gov/article/50616\n/llnls-national-ignition-facility-delivers-record-laser-energy.\n3. Michael D. Perry, Paul S. Banks, Jason Zweiback, et al., “Laser \nContaining a Distributed Gain Medium,” US Patent 6,937,629 B2, \nfiled November 21, 2002, and issued August 30, 2005.\n4. The Nobel Prize in Physics 2023, “Press Release,” The Nobel \nPrize, October 3, 2023, https://www.nobelprize.org/prizes/physics\n/2023/press-release/.\n5. Joshua Yang, Kasper Van Gasse, Daniil M. Lukin, et al., “Tita\u0002nium:sapphire-on-insulator Integrated Lasers and Amplifiers,” \nNature 630 (June 2024): 853–59, https://doi.org/10.1038/s41586\n-024-07457-2; Nathan J. Szymanski, Bernardus Rendy, Yuxing Fei, \net al., “An Autonomous Laboratory for the Accelerated Synthesis \nof Novel Materials,” Nature 624 (November 2023): 86–91, https://\ndoi.org/10.1038/s41586-023-06734-w.\n6. Bundesamt für Strahlenschutz, “Laser Applications,” last modi\u0002fied March 14, 2024, https://www.bfs.de/EN/topics/opt/application\n-medicine-wellness/laser-applications/laser-applications_node\n.html.\n7. Matthias Ureel, Marcello Augello, Daniel Holzinger, et al., “Cold \nAblation Robot-Guided Laser Osteotome (CARLO®): From Bench \nto Bedside,” Journal of Clinical Medicine 10, no. 3 (January 2021): \n450, https://doi.org/10.3390/jcm10030450.\n8. Jason Bates, “Invention to Impact: The Story of LASIK Eye \nSurgery,” Science Matters, US National Science Foundation, \nMarch 15, 2024, https://new.nsf.gov/science-matters/invention\n-impact-story-lasik-eye-surgery.\n9. Amir Yousef Sajjadi, Kunal Mitra, and Michael Grace, “Ablation \nof Subsurface Tumors Using an Ultra-Short Pulse Laser,” Optics \nand Lasers in Engineering 49, no. 3 (March 2011): 451–56, https://\ndoi.org/10.1016/j.optlaseng.2010.11.020.\n10. Florian Kroll, Florian-Emanuel Brack, Constantin Bernert, et \nal., “Tumour Irradiation in Mice with a Laser-Accelerated Proton \nBeam,” Nature Physics 18 (March 2022): 316–22, https://doi\n.org/10.1038/s41567-022-01520-3.\n11. Congressional Research Service, Department of Defense \nDirected Energy Weapons: Background and Issues for Congress \n(R46925), July 11, 2024, https://crsreports.congress.gov/product\n/details?prodcode=R46925. \n12. Hannah Weisman, “ASU Research Team Develops Break\u0002through Ultrashort Laser Pulse Technology,” ASU News, Arizona \nState University, June 16, 2023, https://news.asu.edu/20230616\n-asu-research-team-develops-breakthrough-ultrashort-laser-pulse\n-technology. \n13. Owais Ali, “How is Laser Communication Used in Space?,” \nAZo Optics, September 13, 2023, https://www.azooptics.com\n/Article.aspx?ArticleID=2474. \n14. Evelyn Janeidy Arevalo, “SpaceX Reveals Operation of Over \n8,000 ‘Space Lasers’ Across Starlink Satellite Constellation—\nEnabling Faster Internet,” Tesmanian, September 26, 2023, https://\nwww.tesmanian.com/blogs/tesmanian-blog/starlink-video.\n15. Andrew Jones, “Pew! Pew! Pew! NASA’s 1st Successful Two\u0002way Laser Experiment Is a Giant Leap for Moon and Mars Communi\u0002cations,” Space.com, Future US, Inc., December 19, 2023, https://\nwww.space.com/nasa-laser-communication-1st-two-way-link-iss.\n16. Dmitry Solenov and Dmitry Mozyrsky, “Cold Atom Qubits,” \nJournal of Computational and Theoretical Nanoscience 8, no. 3 \n(March 1, 2011): 481–89, https://doi.org/10.1166/jctn.2011.1713.\n17. TechPort, “Rapid Response Debris Removal Using Recon\u0002figurable Space-Based Laser Networks,” National Aeronautics \nand Space Administration, accessed October 13, 2023, https://\ntechport.nasa.gov/view/156377; Micaela Morrissette, “WVU Engi\u0002neer Developing Laser System to Defend Space Assets from \nDebris in Earth’s Orbit,” WVU Today, West Virginia University, \nOctober 4, 2023, https://wvutoday.wvu.edu/stories/2023/10/04\n/wvu-engineer-developing-laser-system-to-defend-space-assets\n-from-debris-in-earth-s-orbit.\n04 Lase\nSTANFORD EXPERT CONTRIBUTORS\nDr. Siegfried Glenzer\nSETR Faculty Council, Professor of Photon \nScience, and Professor, by courtesy, of Mechanical \nEngineering\nDr. Norbert Holtkamp\nSETR Advisory Board and Professor of Particle \nPhysics and Astrophysics and of Photon Science\nDr. Eric Galtier\nLead Scientist at the SLAC National Accelerator \nLaboratory\nDr. Nicholas Hartley\nSETR Fellow and Associate Scientist at the SLAC \nNational Accelerator Laboratory\n18. Linac Coherent Light Source, “LCLS-II-HE,” SLAC National \nAccelerator Laboratory, accessed October 13, 2023, https://lcls\n.slac.stanford.edu/lcls-ii-he.\n19. Mengning Liang, Garth J. Williams, Marc Messerschmidt, et al., \n“The Coherent X-Ray Imaging Instrument at the Linac Coherent \nLight Source,” Journal of Synchrotron Radiation 22, no. 3 (May 2015): \n514–19, https://doi.org/10.1107/S160057751500449X.\n20. Jack Griffiths, Ana F. Suzana, Longlong Wu, et al., “Resolving \nLength-Scale-Dependent Transient Disorder Through an Ultrafast \nPhase Transition,” Nature Materials 23 (June 2024): 1041–47, \nhttps://doi.org/10.1038/s41563-024-01927-8.\n21. Asmit Bhowmick, Rana Hussein, Isabel Bogacz, et al., “Struc\u0002tural Evidence for Intermediates during O2\n Formation in Photo\u0002system II,” Nature 617 (May 2023): 629–36, https://doi.org/10.1038\n/s41586-023-06038-z.\n22. Oskar Hoffman, Jochen Stollenwerk, and Peter Loosen, \n“Design of Multi-Beam Optics for High Throughput Parallel Pro\u0002cessing,” Journal of Laser Applications 32, no. 1 (February 2020): \n012005, https://doi.org/10.2351/1.5125778.\n23. Ureel et al., “Cold Ablation Robot-Guided Laser Osteotome.”\n24. Linac Coherent Light Source, “MEC-U,” SLAC National Accel\u0002erator Laboratory, accessed October 13, 2023, https://lcls.slac\n.stanford.edu/mec-u.\n25. Basic Research Needs Workshop on Laser Technology, Report \nof the Basic Research Needs Workshop on Laser Technology: \n2023 Report, Office of Science, US Department of Energy, Jan\u0002uary 5, 2024, https://science.osti.gov/-/media/ardap/pdf/2024\n/Laser-Technology-Workshop-Report_20240105_final.pdf.\n26. Mikayla Easley, “Israeli-Made High-Energy Laser Makes \nDebut,” National Defense, March 21, 2023, https://www\n.nationaldefensemagazine.org/articles/2023/3/21/israeli-made\n-high-energy-laser-makes-debut.\n27. ANSI Webstore, “ANSI Z136.1-2022: American National \nStandard for Safe Use of Lasers,” American National Standards \nInstitute, accessed October 13, 2024, https://webstore.ansi.org\n/standards/lia/ansiz1362022; Brad Kelechava, “ANSI Z136.1-2022: \nSafe Use of Lasers,” ANAB Blog (blog), American National Stan\u0002dards Institute, March 13, 2023, https://blog.ansi.org/ansi-z136\n-1-2022-safe-use-of-lasers/. \n\n77\nOverview\nFrom semiconductors in computer chips to plas\u0002tics in everyday objects, materials are everywhere. \nKnowing how to synthesize and process them, as \nwell as understanding their structure and properties, \nhas helped to shape the world around us. Materials \nscience contributes to the development of stronger, \nlighter, and more flexible materials that improve \neverything from battery electrodes to medical \nimplants and from automobiles to spacecraft.\nIt is a broad field. At Stanford University, for exam\u0002ple, faculty working on materials science research \nprograms are found in many departments, includ\u0002ing Materials Science and Engineering, Chemical \nEngineering, Electrical Engineering, Bioengineering, \nChemistry, and Physics. \nResearchers in materials science study the properties \nand behavior of materials to understand and pre\u0002dict their structure and performance under various \nKEY TAKEAWAYS\n° Materials science is a foundational technology that \nunderlies advances in many other fields, including \nrobotics, space, energy, and synthetic biology. \n° Materials science will exploit artificial intelligence \nas another promising tool to predict new mate\u0002rials with new properties and identify novel uses \nfor known materials.\n° Future progress in materials science requires new \nfunding mechanisms to more effectively tran\u0002sition from innovation to implementation and \naccess to more computational power. \nMATERIALS SCIENCE \n05\n78 STANFORD EMERGING TECHNOLOGY REVIEW\nand several other complementary methods, under\u0002lies these four areas. \nBasics of Materials Science\nAll materials are composed of atoms. The periodic \ntable of the elements (figure 5.1) lists all the known \ntypes of atoms. Certain atoms can be combined \nwith others into molecules that have vastly different \nproperties than the individual atoms involved. For \nexample, table salt consists of sodium and chlorine, \nwhich are elements. Sodium burns on contact with \nwater, and chlorine is a poisonous gas, yet the table \nsalt we consume every day is a completely different \nsubstance.\nThere are two important points to note about the \nperiodic table. First, there are a lot of elements—\nninety-two naturally occurring ones and twenty-six \nthat can be observed only in laboratory conditions. \nThat’s a lot of building blocks from which different \nconditions. Their goal is to develop new materials \nwith desirable properties and improve existing ones \nby understanding how the structure of a material \ninfluences its properties and how processing it can \nchange its structure and therefore its performance. \nThis knowledge can then be used to design new \nmaterials with desirable properties for specific uses. \nBroadly speaking, materials science and engineer\u0002ing research focuses on four major areas. The first \nis the study of the structure of materials to under\u0002stand how they are composed and organized from \natomic to macroscopic scales. The second involves \nverifying the properties of materials, such as their \nconductivity, strength, and elasticity. The third area \ncovers analysis and benchmarking of how materials \nperform in specific situations. The final one involves \nassessing how materials can be fabricated and man\u0002ufactured. Characterization of materials, or the gen\u0002eral process by which their structure and properties \nare ascertained through spectroscopic, microscopic, \nGroup\nPeriod\n1\n2\n3\n4\n5\n6\n7\n1\n1\nH\n3\nLi\n11\nNa\n19\nK\n37\nRb\n55\nCs\n87\nFr\n2\n4\nBe\n12\nMg\n20\nCa\n38\nSr\n56\nBa\n88\nRa\n*\n*\n*\n*\n*\n*\n3\n21\nSc\n39\nY\n71\nLu\n103\nLr\n57\nLa\n89\nAc\n4\n22\nTi\n40\nZr\n72\nHf\n104\nRf\n58\nCe\n90\nTh\n5\n23\nV\n41\nNb\n73\nTa\n105\nDb\n59\nPr\n91\nPa\n6\n24\nCr\n42\nMo\n74\nW\n106\nSg\n60\nNd\n92\nU\n7\n25\nMn\n43\nTc\n75\nRe\n107\nBh\n61\nPm\n93\nNp\n8\n26\nFe\n44\nRu\n76\nOs\n108\nHs\n62\nSm\n94\nPu\n9\n27\nCo\n45\nRh\n77\nIr\n109\nMt\n63\nEu\n95\nAm\n10\n28\nNi\n46\nPd\n78\nPt\n110\nDs\n64\nGd\n96\nCm\n11\n29\nCu\n47\nAg\n79\nAu\n111\nRg\n65\nTb\n97\nBk\n12\n30\nZn\n48\nCd\n80\nHg\n112\nCn\n66\nDy\n98\nCf\n13\n5\nB\n13\nAl\n31\nGa\n49\nIn\n81\nTl\n113\nNh\n67\nHo\n99\nEs\n14\n6\nC\n14\nSi\n32\nGe\n50\nSn\n82\nPb\n114\nFl\n68\nEr\n100\nFm\n15\n7\nN\n15\nP\n33\nAs\n51\nSb\n83\nBi\n115\nMc\n69\nTm\n101\nMd\n16\n8\nO\n16\nS\n34\nSe\n52\nTe\n84\nPo\n116\nLv\n70\nYb\n102\nNo\n17\n9\nF\n17\nCl\n35\nBr\n53\nI\n85\nAt\n117\nTs\n18\n2\nHe\n10\nNe\n18\nAr\n36\nKr\n54\nXe\n86\nRn\n118\nOg\nSource: Adapted from Wikimedia Commons, CC BY-SA 4.0\nFIGURE 5.1  The periodic table of the elements\n05 Materi\nof macromolecules often mean the material is more \nflexible, making many plastics possible. Research \non new macromolecular structures can be used to \ndevelop plastic materials that are easier to recycle \nor have advantageous mechanical properties while \nweighing less than metals. \nKey Developments\nSome interesting present-day applications of materi\u0002als science are discussed below.\nFlexible Electronics\nFlexible or stretchable electronics involves the cre\u0002ation of electrical devices that can bend, stretch, and \ndeform without compromising their performance. \nSuch electronics can be used as wearable, skinlike \ndevices. For example, “electronic skin,” or e-skin, \ncan conform to real skin and sense things such as \ntemperature and pressure, as well as encode \nthese into electrical signals.1 A “smart bandage” \nwith integrated sensors to monitor wound condi\u0002tions and with electrical stimulation can acceler\u0002ate the time needed to heal chronic wounds by \n25 percent.2\nRecent research has also shown the development of \nintegrated circuits on soft and flexible substrates can \ndrive a micro-LED (light-emitting diode) screen that \ncan read out a braille array ten times more sensi\u0002tively than human fingertips.3\n The latest version of \nthis device is many times smaller than previous itera\u0002tions and operates three orders of magnitude faster \nthan them. \nThis performance is achieved using a combination \nof carbon nanotubes (tubular molecules made up \nof carbon atoms), a one-dimensional conductor, \nand a flexible polymer substrate. This substrate \nencases the carbon nanotubes and maintains a con\u0002nected internal electrical circuit while bending and \nmaterials and molecules can be synthesized, and an \nastronomically large number of different compounds \nare possible. The challenge for materials science is \nto sift through this vast array of possibilities to find \nthe ones that are useful. Machine-learning (ML) \nalgorithms can accelerate this process by predicting \nthe properties of materials and identifying promising \ncandidates for synthesis, significantly reducing the \ntime and resources required for experimental test\u0002ing. (More detail on this subject is provided later in \nthis chapter.)\nThe second important point is that the elements in \nthe periodic table are lined up in a certain order. \nThose in the same column have properties that are \noften similar in key ways. This means insights devel\u0002oped through experimentation or calculation on \none element also apply, with some modifications, \nto another element above or below it in the peri\u0002odic table.\nAtoms can be arranged spatially in various ways. A \ncrystal, for example, is the result of arranging atoms \nin a periodically repeating lattice. The silicon wafer \nat the heart of the semiconductor industry is one \nsuch crystal; more precisely, it’s a slice of a single \nsilicon crystal. \nMolecules, which are composed of atoms, can, in \nturn, be linked together into structures called mac\u0002romolecules (see figure 5.2). These can occur natu\u0002rally, as is the case for proteins, DNAs, and cellulose, \nor they can be synthesized artificially and used to \ncreate things such as polymers/plastics. Long chains \nComposites\nOrganic\nMolecules\nInorganic\nAtom\nPlastics\nMacromolecules\nFIGURE 5.2  Objects of study in materials science\n80 STANFORD EMERGING TECHNOLOGY REVIEW\ndeforming. The transistors it uses are built onto this \npliable substrate and enable the device to power \nthe micro-LED display. \nCommercialization will require more work to elimi\u0002nate variations in electrical properties of the device’s \ncircuits when mounted on a moving host, as well \nas work to eliminate its susceptibility to moisture. \nStill, the research team responsible for this invention \nenvisions using it to make biocompatible probes for \nthe brain and gut that are more capable and energy \nefficient than current ones. This could pave the way \nto more complex and longer-lasting brain-machine \ninterfaces. \nAdditive Manufacturing\nOne of the most promising advances in materials pro\u0002cessing over the past fifteen years is additive man\u0002ufacturing, colloquially known as 3-D printing. The \ntechnology comes in different forms. For instance, a \nmethod known as continuous liquid interface pro\u0002duction (CLIP) uses directed ultraviolet (UV) light to \nform structures from a polymer resin (see figure 5.3).4\nA key aspect of CLIP is its use of an oxygen-permeable \nwindow placed above a UV light projector that pre\u0002vents the resin from curing in unwanted places. \nEspecially at high speeds, 3-D printing struggles with \nproducing small features. The 3-D printing process \nrequires several components, including the material \nresin, the light source, and the build platform where \nan object is printed, to perform in concert. That is \ntechnically challenging, but by printing on a tensioned \nfilm made from polyethylene terephthalate that’s fed \nthrough a CLIP printer, it’s possible to 3-D print very \nsmall particles at a pace of one million a day from a \nsingle machine.5\nNanotechnology\nNanotechnology is a large and growing subfield of \nmaterials science. Size has a profound impact on the \nproperties of a material. Figure  5.4 compares the \nlength of a water molecule (below a nanometer), a \nhuman hair (roughly 105 nanometers), and a human \neyeball (at 107 nanometers). A structure is typically \nreferred to as nanoscale if at least one of its dimen\u0002sions is in the 1-to-100-nanometer range. \nIn the past twenty years, nanoscience and nano\u0002technology have attracted enormous interest for two \nreasons. First, many significant biological organ\u0002isms, such as viruses and proteins, are nanoscale \nin size. Second, it turns out that the properties \nof nanoscale materials—including their elec\u0002tronic, optical, magnetic, thermal, and mechan\u0002ical properties—are often very different from the \nsame material in bulk form.6 Materials that are \nsmaller than about 100 nanometers in one dimen\u0002sion, two dimensions, or all dimensions are called \nnanosheets, nanowires, and nanoparticles, respec\u0002tively (see figure  5.5). Somewhat more complex \nshapes include nanotubes, which are flexible hollow \ntubes made of carbon atoms, and nanorods, which \nSource: Carbon Inc. / John Tumbleston are slightly wider, rigid structures.\nFIGURE 5.3  A CLIP-based 3-D printer created a \nminiature print of the Eiffel Tower \n05 Materi\nexample of how size affects a material’s properties \nbecause their optoelectronic properties differ from \nthose of the same bulk material. The diameter of \nquantum dots determines the color of light that they \nproduce, with larger ones emitting longer wave\u0002lengths. This enables tunable light emission based \non the desired application.\nSome current uses of the technology include the \nfollowing:\nMedical imaging Quantum dots can improve \nbiomedical imaging by, for example, acting as flu\u0002orescent markers that make it possible to selec\u0002tively label biological structures in vitro and in vivo.8\nBiocompatible nanomaterials can also be employed \nas optical probes that sense mechanical forces and \nelectrical fields in biological organisms, remov\u0002ing the need for bulky, specialized equipment and \nmaking new experiments possible.9\nSolar cells Quantum dots’ ability to absorb differ\u0002ent frequencies of light means they can potentially \ncapture more of the solar spectrum, boosting the \nperformance of solar panels.10\nSensors Quantum dots can be used in sensors for \ndetecting chemicals and biological substances.11\nAnticounterfeiting When they are embedded in \nproduct labels, quantum dots can help defend against \ncounterfeiting.12\nApart from applications related to quantum dots, \nother examples of applications of nanomaterials \ninclude the following:\nPharmaceutical delivery An injectable polymer\u0002nanoparticle hydrogel has been developed to pre\u0002cisely control the delivery of drugs, proteins, and \ncells.13 The efficacy of insulin administration can also \nbe improved through this research.14 Among other \ninnovative uses, nanoparticles can be engineered to \npermeate the blood–brain barrier, delivering drugs \nto treat neurodegenerative diseases.15\nQuantum dots—for which the Nobel Prize in \nChemistry was awarded in 2023—have garnered \npublic attention through their use in televisions. \nThey are metallic, carbonaceous, or semiconductor \nspherical nanocrystals that emit bright monochro\u0002matic light in response to excitation by a light source \nwith a higher energy, such as blue light from the \nback panel in a display.7 Quantum dots are a model \nWater \nmolecule\nHuman \neyeball\nin nanometers\nAntibody\nVirus\nBacterium\nCancer cell\nHuman hair\n108\n107\n106\n105\n10-1\n104\n103\n102\n10\n1\nH\nO\nH\nNanoscale\nFIGURE 5.4  The size of nanoscale objects\n82 STANFORD EMERGING TECHNOLOGY REVIEW\nnanowires solve this problem and deliver a tenfold \nincrease in battery capacity.19\nCatalysis Catalysts are important anywhere a \nchemical reaction must be speeded up to be useful. \nFor example, catalytic converters in cars use platinum \nand palladium catalysts to rapidly break down carbon \nmonoxide into carbon dioxide (CO2). Nanomaterials \nare particularly well suited for this role.20 This is \nbecause their high surface-to-volume ratio allows \nmany more active catalytic sites to participate in a \nreaction than would be the case for the same mate\u0002rial in bulk. Nanomaterials can also be chemically \narchitected to catalyze various reactions. Advances \nhave been made in converting CO2 to value-added \nchemicals using electrified nanoparticle catalysts and \nin employing palladium catalysts for the combustion \nof methane, which could improve the efficiency of \nelectricity generation from the gas.21 Nanocatalysts \nhave also been used to improve the rate at which \nhydrogen can be produced from water through elec\u0002trolysis.22 These approaches still face challenges, \nincluding developing catalysts that are sufficiently \nactive and stable—as well as cheap enough—to \ninexpensively produce hydrogen in large quantities.23\nBiosensing\nDetecting pathogenic bacteria usually takes a long \ntime, often involving culture-based methods or \nVaccine stabilization Nanoassemblies can be used \nto stabilize certain types of vaccines—that is, protect \ntheir components from degradation—by encapsu\u0002lating them.16 In this form, it is easier to inject a vaccine \ninto the human body and to ensure its release over \ntime inside the body in a controlled manner. This is \nespecially useful for mRNA vaccines such as the ones \ndeveloped for COVID-19. \nSmart windows Silver nanowires arranged into a \nthin film on a window become a transparent conduc\u0002tive surface. Running a current through the film can \nthen change the opacity of the window electrically.17\nTwo-dimensional (2-D) semiconductors, graphene, \ncarbon nanotubes, and nanoscale materials These \nare at the forefront of the next generation of high\u0002tech electronic devices. Active research efforts are \ndesigning new methods to integrate 2-D or carbon \nnanotube semiconductors into electronics that are \ncurrently silicon-based to improve their energy effi\u0002ciency and heat management.18 (A 2-D semiconduc\u0002tor is a semiconductor with atomic-scale thickness.)\nHigher-capacity batteries High-performance lith\u0002ium battery anodes have been developed by inte\u0002grating silicon nanowires as an anode material. When \nbulk silicon is used as an anode, it undergoes signifi\u0002cant changes in volume as a battery charges and dis\u0002charges, often leading to mechanical failure. Silicon \n0-D\nnanoparticle\nNanotechnology (at least one dimension <100 nm) >100 nm\n1-D\nnanowire\n2-D\nnanosheet Bulk\nFIGURE 5.5  Dimensionality of nanomaterials\n05 Materi\nof complex materials. Databases such as the \nMaterials Project led by Lawrence Berkeley National \nLaboratory represent a significant effort,26 but they \nstill have limitations in terms of the range of prop\u0002erties covered. To truly understand and predict the \nproperties of materials, such as their thermal con\u0002ductivity or optical characteristics, more accurate, \ncomprehensive, and tailored databases are needed. \nAmong their many applications, these could help \naccelerate the development of materials that enable \nresearchers to overcome bottlenecks in chip assem\u0002bly as semiconductors continue to be miniaturized.\nSeveral major industry efforts are already underway \nto harness artificial intelligence (AI) for materials \nexploration. Google DeepMind’s Graph Networks \nfor Materials Exploration project aims to use neural \nnetworks—ML models that process data in ways \nsimilar to human brains—to predict new materials. \nOther companies like IBM, Citrine Informatics, and \nMaterialsZone are combining materials science exper\u0002tise with data science and AI to accelerate materials \ndevelopment and optimize product design. \nIn addition to those mentioned above, some other \ncurrent applications of ML in materials science \ninclude the following:\nKnowledge discovery The technology has been \nused in materials science to examine the scientific \nliterature for hidden relationships and predict new \nmaterial properties. For example, researchers using \nan ML model were able to analyze patterns in scien\u0002tific abstracts.27 The model was trained to calculate \npolymerase chain reaction, a lab technique that repro\u0002duces DNA sequences for study. A new method has \nbeen developed that uses Raman spectroscopy—\na means of analyzing chemical structures—to rapidly \nidentify pathogenic bacteria in blood samples based \non their unique optical signatures when exposed to \nlaser light.24 This system creates nanodroplets from \nblood samples, with each droplet containing only \na few cells. Adding gold nanorods to these drop\u0002lets makes it easier to detect dangerous bacteria \nbecause the rods adhere to suspect cells and amplify \ntheir signature in a spectrographic analysis. An ML \nalgorithm can then use the results to determine if a \npathogen is present.\nThe Application of Artificial Intelligence in \nMaterials Science\nOne of the foremost challenges of materials science \nas a discipline is the vast number of possible materi\u0002als and material combinations that can be used and \nthe associated time and cost involved in synthesis \nand characterization. ML offers promising solutions \nby leveraging experimental and computational data \non the properties of materials.25 ML algorithms can \nrecognize patterns in existing data and make gen\u0002eralized predictions about new materials. While this \napproach has been successful with relatively simple \nmaterials, much remains to be done when dealing \nwith complicated ones. \nML provides a starting point for further exploration, \nbut additional data is needed to make ML-informed \nsolutions more accurate, especially in the case \nOne of the foremost challenges of materials science as \na discipline is the vast number of possible materials and \nmaterial combinations that can be used and the associated \ntime and cost involved in synthesis and characterization.\n84 STANFORD EMERGING TECHNOLOGY REVIEW\nidentify key factors for better performance.28 They dis\u0002covered that lower oxygen content in the solvent leads \nto improved battery cycling and used this insight to \ndevelop new electrolytes that improved the process. \nAutomated labs To address the challenge of lim\u0002ited experimental data, there is growing interest in \ndeveloping autonomous laboratories that can rapidly \nsynthesize and characterize materials at scale. For \nexample, the A-Lab developed by researchers at the \nUniversity of California–Berkeley is a robotic platform \nthat combines AI-guided synthesis with automated \ncharacterization to enable materials discovery (see \nfigure 5.6).29 The A-Lab system consists of several key \ncomponents:\n° A robotic arm that picks, weighs, and mixes \ndry chemical precursors, which are compounds \nneeded for use in chemical reactions\n° A sample preparation station where the precur\u0002sors are turned into a slurry\nthe probability that a material’s name would co-occur \nwith the word thermoelectric in the analyzed text. \nThe researchers then identified abstracts involving \nother materials that had words with close semantic \nrelationships to thermoelectric but that had never \nbeen studied as such. Remarkably, their model’s top \npredictions were eight times more likely to be stud\u0002ied as thermoelectrics in the succeeding five years \ncompared to randomly chosen materials. This demon\u0002strates the potential of AI to uncover latent knowledge \nin existing scientific literature and to guide future \nresearch directions.\nBattery electrolyte design Improving electrolyte \ndesign in batteries can enhance their performance, \nbut predicting and designing effective electrolytes \nis challenging because of their complexity. ML has \nbeen applied to a data-driven electrolyte design for \nlithium-metal batteries. Liquid electrolytes play an \nimportant role in determining how well these bat\u0002teries can cycle, or charge and discharge. Researchers \nused models to analyze electrolyte compositions and \nSource: © 2023 The Regents of the University of California, Lawrence Berkeley National Laboratory\nFIGURE 5.6  The A-Lab combines AI-guided synthesis with automated materials characterization\n05 Materi\nissues. Such criticisms highlight the need for con\u0002tinued validation and human oversight in AI-driven \nmaterials research. \nAlthough much remains to be done, this field \nundoubtedly has great promise for optimizing com\u0002plex material properties. For instance, in the future, \nautomated laboratories may be able to accurately \npredict new materials, balancing the need for quick, \nhigh-volume calculations with the desire for precise, \nhigh-quality real-world results. They may also be able \nto predict and create actual material samples with suf\u0002ficient accuracy to reduce the amount of human effort \nneeded to confirm the automated analysis.\nThe integration of AI and ML into materials science pres\u0002ents immense opportunities for accelerating discovery \nand innovation. By combining advanced algorithms \nwith expanded databases, automated experimenta\u0002tion, and increased computational resources, research\u0002ers aim to navigate the vast landscape of possible \nmaterials more efficiently than ever before. \nChallenges of Innovation and \nImplementation\nThe materials science research infrastructure does \nnot adequately support the transition from research \nto real-world applications at scale. Such transitions \ngenerally require construction of a small-scale pilot \nproject to demonstrate the feasibility of potential \nlarge-scale manufacturing. At this point, the tech\u0002nology is too mature to qualify for most research \nfunding—because basic science does not address \nissues related to scaling up—but not mature enough \nto be commercialized by actual companies. Neither \ngovernment nor venture capital investors are partic\u0002ularly enthusiastic about financing pilot projects, so \ndifferent forms of funding are required to bridge this \ngap between bench-scale research and company\u0002level investment. Such support could also establish \nnational rapid prototyping centers, where academic \nresearchers can find the help and tools neces\u0002sary to build prototypes and pilot plants for their \ntechnology. \n° A conveyor belt system that transports the sam\u0002ples to different stations\n° An oven to allow precursors to react, with the \nenvironment determined by an ML algorithm\n° A grinding station where the product is turned \ninto a fine powder\n° Automated characterization equipment to vali\u0002date morphology and elemental distribution\nThe A-Lab demonstration showcased the potential \nfor autonomous materials synthesis and characteriza\u0002tion. Out of 58 targeted materials, the authors claimed \n36 successful syntheses and 7 partial successes. This \nhigh-throughput approach could significantly accel\u0002erate the process of validating computational predic\u0002tions and generating new experimental data to train \nML models.\nOver the Horizon\nMachine Learning in Materials Science \nOver the horizon, there is the hope that ML-guided \napproaches will dramatically shorten the timescale for \nmaterials discovery and enable the design of mater\u0002ials optimized for specific applications. Continued \ndevelopment of both bottom-up computational \napproaches and top-down experimental data-driven \nmethods will be needed to bridge the gap between \nfundamental material parameters and real-world \ndevice performance.\nComputational approaches will need to be vali\u0002dated. For example, the A-Lab described above has \nfaced some criticism from the scientific community. \nOne critique centered on concerns about the accu\u0002racy of its characterization work and claims of new \nmaterial synthesis,30 pointing to errors in the A-Lab’s \nanalysis that included poor and incorrect fits of struc\u0002tural models to the experimental data, among other \n86 STANFORD EMERGING TECHNOLOGY REVIEW\nthe US Food and Drug Administration (FDA) created \na Nanotechnology Regulatory Science Research Plan \nin 2013.31 Today, FDA regulation and review of nano\u0002technology is governed by Executive Order 13563.32\nOutside of biomedicine, regulation of and infrastruc\u0002ture for nanomaterials research from the government \nside is based largely in agencies of the National \nNanotechnology Initiative, which include the \nDepartment of Energy, the National Cancer Institute, \nthe National Institutes of Health, the National Institute \nof Standards and Technology in the Department of \nCommerce, and the National Science Foundation.\nTOXICITY AND ENVIRONMENTAL ISSUES\nNanoparticles raise particular concerns because their \nsmall size may enable them to pass through various \nbiological borders such as cell membranes or the \nblood–brain barrier, potentially affecting biological \nsystems in harmful ways. Nanoscale particles inhaled \ninto the lungs, for example, may lodge themselves \nthere permanently, causing severe health outcomes, \nincluding pulmonary inflammation, lung cancer, and \npenetration into the brain and skin.33\nMoreover, because engineered nanoparticles are, \nby definition, new to the natural environment, they \npose unknown dangers to humans and the environ\u0002ment. These concerns include managing the risk of \nincorporating nanomaterials into products that enter \nthe environment at the end of their life cycles. As \nnanomaterials are employed in, and considered for, \nelectronic and energy products, it is paramount that \nthey safely degrade or can be recycled at the end of \na product’s life. Policy will be particularly important \nPast research processes are also ill-suited to rapid \ntransitions to real-world applications. Such pro\u0002cesses emphasize sequential steps. The standard \nprocess has been to characterize a material and then \nproceed to a simple demonstration of how it might \nbe used. Today, addressing big societal challenges \ncalls for a more scalable, system-level approach that \ninvolves extensive rapid prototyping and fast, reli\u0002able demonstrations to provide feedback on the \npotential value of specific materials and to fill in \nknowledge gaps. \nCurrent infrastructure makes this difficult. For exam\u0002ple, in collaborations with a medical school, it is often \nnecessary to bring almost-finished products to clin\u0002ical tests to validate the true impact of a new medi\u0002cal device using innovative materials. With typically \nless than a thirty-minute window to place a device \non a patient and gather data, any malfunction, such \nas a sudden equipment failure or a loose wire, can \njeopardize an entire experiment and potentially halt \nfuture patient interactions. Lab-assembled devices \nmay not meet this standard of reliability, even if they \ndo demonstrate the value of the underlying science. \nPolicy, Legal, and Regulatory Issues\nREGULATION OF PRODUCTS INCORPORATING \nNANOMATERIALS\nAs with regulation in other areas of technology, mater\u0002ials science faces concerns about the appropriate \nbalance between the need to ensure public safety \nand the imperative to innovate quickly and leap\u0002frog possible competitors. In the biomedical space, \nHistorically, the United States has led the world in \nnanotechnology, but the gap between it and China \nhas narrowed. \n05 Materi\nin shaping responsible end-of-life solutions for prod\u0002ucts incorporating them.\nFinally, end-of-life considerations that take into \naccount environmental sustainability and resource \nconservation are inherently a part of developing and \ndistributing new materials. This is especially impor\u0002tant for plastics and materials containing per- and \npolyfluoroalkyl substances (PFAS), which pose signif\u0002icant environmental and health risks. Material devel\u0002opers can incorporate recyclability into their design \nprocesses. For PFAS and other persistent chemicals, \nthe US Environmental Protection Agency strategic \nroad map of October 2021 provides guidance for \ntheir use and disposal and calls for research into safe \nalternatives and effective degradation methods.34\nFOREIGN COLLABORATION AND COMPETITION\nHistorically, the United States has led the world in nano\u0002technology, but the gap between it and China has nar\u0002rowed. Notably, in 2016, the president of the Chinese \nAcademy of Sciences openly announced Beijing’s \nambition to compete in the field of nanotechnology.35\nAs great power competition intensifies, many re\u0002searchers are concerned that fundamental research \ncould now be subject to export controls. Policy \nambiguity can inadvertently hinder innovation by \ncreating obstacles for non-US researchers wishing \nto contribute to work in America and by deterring \ninternational collaborations with allies and partners \nwho are important for advancing the field. In nano\u0002materials, for example, researchers in South Korea \nare making significant strides with biomedical appli\u0002cations and ones for consumer electronics. There \nis an urgent need for clarification of these policies, \nparticularly those delineating fundamental research \nand export-controlled research. \nINFRASTRUCTURE FOR ML-ASSISTED \nMATERIALS SCIENCE\nThe United States benefits from having some of the \nworld’s largest supercomputing resources, which are \nessential not only for ML but also for developing \nextensive databases. However, better access to \ncomputing power is essential for researchers in \nmaterials science to generate and analyze databases \neffectively. Greater access to data, including to data\u0002bases that might not always be openly available to \nacademics, is also needed. \nOne additional area where policymakers could have \na significant impact is in bridging the gap between \nthe scientific community and makers of compu\u0002tational hardware. Frequent changes in comput\u0002ing architectures can lead to a loss of productivity \nfor researchers because code must be constantly \nupdated. Improved collaboration with hardware \nmanufacturers and other providers of computing \nresources could ensure scientific needs are better \naligned with advances in computing technology, \nenhancing overall research efficiency.\nNOTES\n1. Weichen Wang, Yuanwen Jiang, Donglai Zhong, et al., “Neuro\u0002morphic Sensorimotor Loop Embodied by Monolithically Inte\u0002grated, Low-Voltage, Soft E-Skin,” Science 380, no. 6646 (2023): \n735–42, https://doi.org/10.1126/science.ade0086.\n2. Yuanwen Jiang, Artem A. Trotsyuk, Simiao Niu, et al., “Wire\u0002less, Closed-Loop, Smart Bandage with Integrated Sensors and \nStimulators for Advanced Wound Care and Accelerated Healing,” \nNature Biotechnology 41 (2023): 652–62, https://doi.org/10.1038\n/s41587-022-01528-3.\n3. Donglai Zhong, Can Wu, Yuanwen Jiang, et al., “High-Speed \nand Large-Scale Intrinsically Stretchable Integrated Circuits,” \nNature 627 (March 2024): 313–20, https://doi.org/10.1038/s41586\n-024-07096-7. \n4. John R. Tumbleston, David Shirvanyants, Nikita Ermoshkin, \net al., “Continuous Liquid Interface Production of 3D Objects,” \nScience 347, no. 6228 (March 2015): 1349–52, https://doi\n.org/10.1126/science.aaa2397; Kaiwen Hsiao, Brian J. Lee, Tim \nSamuelsen, et al., “Single-Digit-Micrometer-Resolution Contin\u0002uous Liquid Interface Production,” Science Advances 8, no. 46 \n(November 2022), https://doi.org/10.1126/sciadv.abq2846.\n5. For more on polyethylene terephthalate, see Polymershapes, \n“PET Plastic Film,” accessed October 13, 2023, https://www\n.polymershapes.com/pet-plastic-film/; Jason M. Kronenfeld, Lukas \nRother, Max A. Saccone, et al., “Roll-to-Roll, High-Resolution 3D \nPrinting of Shape-Specific Particles,” Nature 627 (March 2024): \n306–12, https://doi.org/10.1038/s41586-024-07061-4. \n6. Hui Pan and Yuan Ping Feng, “Semiconductor Nanowires \nand Nanotubes: Effects of Size and Surface-to-Volume Ratio,” \nACS Nano 2, no. 11 (November 2008): 2410–14, https://doi\n.org/10.1021/nn8004872; Anna C. Balazs, Todd Emrick, and \n88 STANFORD EMERGING TECHNOLOGY REVIEW\nThomas P. Russell, “Nanoparticle Polymer Composites: Where \nTwo Small Worlds Meet,” Science 314, no. 5802 (November 2006): \n1107–10, https://doi.org/10.1126/science.1130557.\n7. A. P. Alivisatos, “Semiconductor Clusters, Nanocrystals, and \nQuantum Dots,” Science 271, no. 5251 (February 1996): 933–37, \nhttps://doi.org/10.1126/science.271.5251.933.\n8. X. Michalet, F. F. Pinaud, L. A. Bentolila, et al., “Quantum \nDots for Live Cells, in Vivo Imaging, and Diagnostics,” Science\n307, no. 5709 (January 2005): 538–44, https://doi.org/10.1126\n/science.1104274.\n9. Randy D. Mehlenbacher, Rea Kolbl, Alice Lay, et al., “Nano\u0002materials for In Vivo Imaging of Mechanical Forces and Electrical \nFields,” Nature Reviews Materials 3, no. 17080 (2018), https://doi\n.org/10.1038/natrevmats.2017.80.\n10. Prashant V. Kamat, “Quantum Dot Solar Cells: Semiconductor \nNanocrystals as Light Harvesters,” Journal of Physical Chemistry \nC 112, no. 48 (October 2008): 18737–53, https://doi.org/10.1021\n/jp806791s; Ralph Nuzzo, Harry Atwater, Andrei Faraon, et al., \n“Light Material Interactions in Energy Conversion (Final Report),” \nOffice of Scientific and Technical Information, US Department of \nEnergy, April 1, 2019, https://doi.org/10.2172/1504275.\n11. Babatunde Ogunlade, Loza F. Tadesse, Hongquan Li, et al., \n“Predicting Tuberculosis Drug Resistance with Machine Learning\u0002Assisted Raman Spectroscopy,” arXiv, Cornell University, June  9, \n2023, https://doi.org/10.48550/arXiv.2306.05653; Fareeha Safir, \nNhat Vu, Loza F. Tadesse, et al., “Combining Acoustic Bioprinting \nwith AI-Assisted Raman Spectroscopy for High-Throughput Identi\u0002fication of Bacteria in Blood,” Nano Letters 23, no. 6 (March 2023): \n2065–73, https://doi.org/10.1021/acs.nanolett.2c03015.\n12. Yang Liu, Fei Han, Fushan Li, et al., “Inkjet-Printed Unclonable \nQuantum Dot Fluorescent Anti-Counterfeiting Labels with Artificial \nIntelligence Authentication,” Nature Communications 10, no. 2409 \n(June 2019), https://doi.org/10.1038/s41467-019-10406-7.\n13. Hector Lopez Hernandez, Abigail K. Grosskopf, Lyndsay M. \nStapleton, et al., “Non-Newtonian Polymer–Nanoparticle Hydro\u0002gels Enhance Cell Viability During Injection,” Macromolecular \nBioscience 19, no. 1 (January 2019), https://doi.org/10.1002/mabi\n.201800275.\n14. Caitlin L. Maikawa, Leslee T. Nguyen, Joseph L. Mann, et al., \n“Formulation Excipients and Their Role in Insulin Stability and \nAssociation State in Formulation,” Pharmaceutical Research 39 \n(2022): 2721–28, https://doi.org/10.1007/s11095-022-03367-y; \nJoseph L. Mann, Caitlin L. Maikawa, Anton A. A. Smith, et al., \n“An Ultrafast Insulin Formulation Enabled by High-Throughput \nScreening of Engineered Polymeric Excipients,” Science Transla\u0002tional Medicine 12, no. 550 (July 2020), https://doi.org/10.1126\n/scitranslmed.aba6676.\n15. Vladimir P. Torchilin, “Multifunctional, Stimuli-Sensitive Nano\u0002particulate Systems for Drug Delivery,” Nature Reviews Drug \nDiscovery 13 (November 2014): 813–27, https://doi.org/10.1038\n/nrd4333; Cláudia Saraiva, Catarina Praça, Raquel Ferreira, et al., \n“Nanoparticle-Mediated Brain Drug Delivery: Overcoming Blood\u0002Brain Barrier to Treat Neurodegenerative Diseases,” Journal of \nControlled Release 235 (August 2016): 34–47, https://doi.org\n/10.1016/j.jconrel.2016.05.044.\n16. Norbert Pardi, Michael J. Hogan, Frederick W. Porter, et al., \n“mRNA Vaccines—A New Era in Vaccinology,” Nature Reviews \nDrug Discovery 17 (2018): 261–79, https://doi.org/10.1038\n/nrd.2017.243.\n17. Zhiqiang Niu, Fan Cui, Elisabeth Kuttner, et al., “Synthesis of \nSilver Nanowires with Reduced Diameters Using Benzoin-Derived \nRadicals to Make Transparent Conductors with High Transparency \nand Low Haze,” Nano Letters 18, no. 8 (2018): 5329–44, https://\ndoi.org/10.1021/acs.nanolett.8b02479.\n18. Weisheng Li, Xiaoshu Gong, Zhihao Yu, et al., “Approaching \nthe Quantum Limit in Two-Dimensional Semiconductor Contacts,” \nNature 613 (2023): 274–79, https://doi.org/10.1038/s41586\n-022-05431-4; Eric Pop, “Energy Dissipation and Transport in \nNanoscale Devices,” Nano Research 3 (2010): 147–69, https://doi\n.org/10.1007/s12274-010-1019-z.\n19. Candace K. Chan, Hailin Peng, Gao Liu, et al., “High\u0002Performance Lithium Battery Anodes Using Silicon Nanowires,” \nNature Nanotechnology 3 (December 2008): 31–35, https://doi\n.org/10.1038/nnano.2007.411.\n20. U. P. M. Ashik, Anchu Viswan, Shinji Kudo, et al., “Chapter 3. \nNanomaterials as Catalysts,” in Applications of Nanomaterials: \nAdvances and Key Technologies, eds. Sneha Mohan Bhagyaraj, \nOluwatobi Samuel Oluwafemi, Nandakumar Kalarikkal, et al. \n(Cambridge, MA: Elsevier, 2018), 4582, https://doi.org/10.1016\n/B978-0-08-101971-9.00003-X.\n21. Weixin Huang, Aaron C. Johnston-Peck, Trenton Wolter, et al., \n“Steam-Created Grain Boundaries for Methane C–H Activation in \nPalladium Catalysts,” Science 373, no. 6562 (September 2021): \n1518–23, https://doi.org/10.1126/science.abj5291; Chengshuang \nZhou, Arun S. Asundi, Emmett D. Goodman, et al., “Steering CO2\nHydrogenation Toward C–C Coupling to Hydrocarbons Using \nPorous Organic Polymer/Metal Interfaces,” Proceedings of the \nNational Academy of Sciences 119, no. 7 (February 2022), https://\ndoi.org/10.1073/pnas.2114768119.\n22. Thomas F. Jaramillo, Kristina P. Jørgensen, Jacob Bonde, et al., \n“Identification of Active Edge Sites for Electrochemical H2 Evolu\u0002tion from MoS2\n Nanocatalysts,” Science 317, no. 5834 (July 2007): \n100–102, https://doi.org/10.1126/science.1141483.\n23. Zhi Wei Seh, Jakob Kibsgaard, Colin F. Dickens, et al., “Com\u0002bining Theory and Experiment in Electrocatalysis: Insights into \nMaterials Design,” Science 355, no. 6321 (January 2017), https://\ndoi.org/10.1126/science.aad4998.\n24. Safir et al., “Combining Acoustic Bioprinting.”\n25. Steven G. Louie, Yang-Hao Chan, Felipe H. da Jornada, et al., \n“Discovering and Understanding Materials through Computa\u0002tion,” Nature Materials 20 (2021): 728–35, https://doi.org/10.1038\n/s41563-021-01015-1.\n26. The Materials Project, “The Materials Project,” accessed Sep\u0002tember 18, 2024, https://next-gen.materialsproject.org.\n27. Vahe Tshitoyan, John Dagdelen, Leigh Weston, et al., “Unsu\u0002pervised Word Embeddings Capture Latent Knowledge from \nMaterials Science Literature,” Nature 571 (July 2019): 95–98, \nhttps://doi.org/10.1038/s41586-019-1335-8.\n28. Sang Cheol Kim, Solomon T. Oyakhire, Constantine Atha\u0002nitis, et al., “Data-Driven Electrolyte Design for Lithium Metal \nAnodes,” Proceedings of the National Academy of Sciences 120, \nno. 10 (February 2023): e2214357120, https://doi.org/10.1073\n/pnas.2214357120.\n29. Nathan J. Szymanski, Bernardus Rendy, Yuxing Fei, et al., “An \nAutonomous Laboratory for the Accelerated Synthesis of Novel \nMaterials,” Nature 624 (November 2023): 86–91, https://doi\n.org/10.1038/s41586-023-06734-w.\n30. Josh Leeman, Yuhan Liu, Joseph Stiles, et al., “Challenges \nin High-Throughput Inorganic Material Prediction and Autono\u0002mous Synthesis,” ChemRxiv, Preprint, submitted January 7, 2024, \nhttps://doi.org/10.26434/chemrxiv-2024-5p9j4.\n05 Materi\nSTANFORD EXPERT CONTRIBUTORS\nDr. Zhenan Bao\nSETR Faculty Council, K. K. Lee Professor of Chemical \nEngineering, and Professor, by courtesy, of Materials \nScience and Engineering and of Chemistry\nDr. Felipe Jornada\nAssistant Professor of Materials Science and \nEngineering\nDr. Andrew Spakowitz\nTang Family Foundation Chair of the Department \nof Chemical Engineering and Professor of \nChemical Engineering and of Materials Science and \nEngineering\nDr. Stefano Cestellos-Blanco\nSETR Fellow and Postdoctoral Scholar in Chemical \nEngineering\nDr. Lukas Michalek\nSETR Fellow and Postdoctoral Scholar in Chemical \nEngineering\n31. US Food and Drug Administration, “2013 Nanotechnol\u0002ogy Regulatory Science Research Plan,” last modified March 19, \n2018, https://www.fda.gov/science-research/nanotechnology\n-programs-fda/2013-nanotechnology-regulatory-science\n-research-plan.\n32. White House, “Executive Order 13563–Improving Regula\u0002tion and Regulatory Review,” Office of the Press Secretary, Jan\u0002uary 18, 2011, https://obamawhitehouse.archives.gov/the-press\n-office/2011/01/18/executive-order-13563-improving-regulation\n-and-regulatory-review.\n33. Paresh Chandra Ray, Hongtao Yu, and Peter P. Fu, \n“Toxicity and Environmental Risks of Nanomaterials: Chal\u0002lenges and Future Needs,” Journal of Environmental Sci\u0002ence and Health, Part C 27 (February 2009): 1–35, https://doi\n.org/10.1080/10590500802708267.\n34. EPA Council on PFAS, “PFAS Strategic Roadmap: EPA’s Com\u0002mitment to Action 2021–2024,” US Environmental Protection \nAgency, October 18, 2021, epa.gov/system/files/documents\n/2021-10/pfas-roadmap_final-508.pdf.\n35. Chunli Bai, “Ascent of Nanoscience in China,” Science 309, \nno. 5731 (July 2005): 61–63, https://doi.org/10.1126.\n\n91\nOverview\nNeuroscience is a multidisciplinary field of study that \nfocuses on the components, functions, and dysfunc\u0002tions of the brain and our nervous system at every \nlevel. It reaches from the earliest stages of embry\u0002onic development to dysfunctions and degeneration \nlater in life and from the individual molecules that \nshape the functions of a neuron to the study of the \ncomplex system dynamics that are our thoughts and \ndictate our behaviors. \nThe human brain consumes 20 to 25 percent of the \nbody’s energy even though it constitutes only a small \npercentage of a human’s body weight, a fact that \nunderscores its outsize importance.1 The power of \nthe human brain is what has allowed us to become \nthe dominant species on Earth without being the \nfastest, strongest, or biggest.\nThe brain is unfathomably complex, containing \napproximately 86 billion neurons2—nerve cells that \nsense the physical world, transmit information to the \nKEY TAKEAWAYS\n° Popular interest in neuroscience vastly exceeds \nthe actual current scientific understanding of \nthe brain, giving rise to overhyped claims in the \npublic domain that revolutionary advances are \njust around the corner. \n° Advances in human genetics and experimental \nneuroscience, along with computing and neuro\u0002science theory, have led to some progress in sev\u0002eral areas, including understanding and treating \naddiction and neurodegenerative diseases and \ndesigning brain-machine interfaces for restoring \nvision. \n° American leadership is essential for establishing \nand upholding global norms about ethics and \nhuman subjects research in neuroscience, but \nthis leadership is slipping with decreased strate\u0002gic planning and increased foreign investments \nin the field. \nNEUROSCIENCE\n06\n92 STANFORD EMERGING TECHNOLOGY REVIEW\npath for electricity to flow through a circuit, neural \ncircuits can be defined by the parallel and recurrent \nconnections between neurons that occur to compute \na specific function, such as deciding to move a limb \nor identifying an object visually. Neurons can also \ncommunicate with each other using hormone-like \nsignaling, which is relatively slow but longer lasting \ncompared to fast-acting electric signals. These types \nof communications underlie mood and behavior \nstates such as sleep/awake and hunger/satiety. \nComplete understanding of what each neuron is \ndoing at any given time is currently impossible. Even \nfor a mouse brain, which is much simpler than a \nhuman brain, it is still a tremendous effort to charac\u0002terize individual brain regions despite the availability \nof powerful techniques that allow us to identify activ\u0002ity in individual neurons or to noninvasively tag cells \nto respond to light signals. \nOver the past several years, however, it has become \nclear that individual neurons are almost never \nbrain, process information, and send information \nfrom the brain to other parts of the body. A single \nneuron can make thousands or tens of thousands of \nconnections to other neurons. These connections \nare called synapses (see figure 6.1).\nAll of our consciousness and behavior, from the \naction of stabbing a potato with a fork to contem\u0002plating the mysteries of the universe, is underpinned \nby which neurons connect with one another, the \nneurotransmitter/receptor pairs involved, the strength \nof the connections, and the electrical properties of \nthe neurons—as well as by how these various fea\u0002tures change over time.\nNeurons and synapses function in many ways that are \nsimilar to electrical circuits. Indeed, the exploration \nof the electrical properties of neurons came directly \nfrom the same technologies, theories, and equations \ndeveloped for harnessing electricity. Many pioneer\u0002ing neuroscientists started as electrical engineers \nand physicists. Just as electrical connectors create a \nDirection of\ninformation \u001fow\nDendrites\nCell body\nAxon\nAxon terminal\nSynapses\nFIGURE 6.1  Structure of a neuron\n06 Neur\ntechnical problems of safely implanting electrodes \nhave not been solved. \nPerhaps the most encouraging example of a \nbrain-machine interface is the recent development \nof an artificial retina. The retina is the part of the \neye that converts light into corresponding electri\u0002cal signals sent to the brain. People who have cer\u0002tain incurable retinal diseases are blind because the \nlight-detecting cells in their retinas do not work. \nTo restore sight, the Stanford artificial retina proj\u0002ect aims to take video images and use electrodes \nimplanted in the eye to simulate the electronic sig\u0002nals in a pattern that a functional retina would nor\u0002mally produce.3\nThe project involves recording spontaneous neural \nactivity to identify cell types and their normal sig\u0002nals, understanding how electrodes activate cells, \nand stimulating retinal ganglion cells—which collect \nvisual information from photoreceptors in retinas—to \nrepresent an image so that this information can be \ntransmitted by the optic nerve to the brain. Solving \nthese technical problems calls for deep knowledge \nof relevant surgical techniques as well as significant \nengineering know-how in multiple areas—including \ntranslating the scientific understanding of the stim\u0002ulation algorithm used into practical applications, \nmaking experimental recordings, and fabricating \nand packaging the electrode into the device.\nThe artificial retina project is the most mature \nbrain-machine interface to date in terms of its abil\u0002ity to “read” and “write” information. The retina, a \npart of the central nervous system, is well suited as \nan experimental environment, as its stimuli (light) is \nexperimentally controllable and can be captured by \na digital camera. It is the best-understood neural \ncircuit and the theory of its function has developed \nto the point where much of retinal processing can \nbe modeled. Compared to complex cognitive pro\u0002cesses like learning and memory—where even the \ninputs aren’t fully understood—the task of recon\u0002structing vision is more achievable, albeit still \nchallenging. \nresponsible for any given behavior or computation; \ninstead, they act in parallel, duplicating some func\u0002tions and combining to determine thoughts and \nactions. This neural redundancy makes it easier to \ninfer what is going on in the brain more broadly. \nA particular brain region can be considered like a \nmagnificent one-thousand-person choir. Just sam\u0002pling 1 percent of the singers can provide a pretty \ngood idea of the music the overall choir is producing \nat any given time. Researchers already have the abil\u0002ity to record from thousands of neurons at a time. \nThis provides useful insight into how a brain func\u0002tions, even if we don’t understand in detail what the \nother 99 percent of its neurons are doing.\nKey Developments\nThis chapter focuses on three research areas in neu\u0002roscience that show major promise for concrete \napplications: brain-machine interfaces (neuroengi\u0002neering), degeneration and aging (neurohealth), and \nthe science of addiction (neurodiscovery). \nNeuroengineering and the Development \nof Brain-Machine Interfaces\nA brain-machine interface is a device that maps \nneural impulses from the brain and translates these \nsignals to computers. The potential applications \nfor mature brain-machine interface technologies \nare wide-ranging: The augmentation of vision, other \nsenses, and physical mobility; direct mind-to-computer \ninterfacing; and computer-assisted memory recall \nand cognition are all within the theoretical realms \nof possibility. However, headlines about mind\u0002reading chip implants are exaggerated and still \nmore the realm of science fiction. Even with tremen\u0002dous interest and rapid progress in neuroscience \nand engineering, the necessary theoretical under\u0002standing of how neurocircuits work is still limited \nto only a few areas of the brain. What’s more, the \n94 STANFORD EMERGING TECHNOLOGY REVIEW\nsuch as how to safely and accurately insert probes \ninto deep-layer tissues.\nNeurohealth and Neurodegeneration\nNeurodegeneration is a major challenge as humans \nlive longer. Alzheimer’s disease is of particular con\u0002cern. In the United States alone, the annual cost of \ntreating it is projected to grow from $305 billion today \nto $1 trillion by 2050.4 Diseases like Alzheimer’s and \nParkinson’s surge in frequency with age—while just \n5 percent of 65- to 74-year-olds have Alzheimer’s, this \nrises to 33 percent for those over 85 (see figure 6.2).5\nAs modern medicine and society enable longer lifes\u0002pans, the human body and brain remain maladapted \nto maintaining nervous system function for decades \npast childbearing age.\nAlzheimer’s disease is characterized by the accumu\u0002lation of two different proteins—amyloid beta and \ntau—into toxic aggregates. Amyloid beta accumulates \nOther brain-machine interfaces are currently being \ndeveloped, though they are less mature or less ambi\u0002tious than the artificial retina project. Some of these \ndecode brain activity without controlling a neural \nsignal. For instance, one interface can translate brain \nactivity in areas controlling motor functions into sig\u0002nals that can then be sent to an artificial prosthetic \nlimb. Here, feeding high-dimensional patterns of \nrecorded neural activity into an artificial intelligence \n(AI) algorithm can make it possible to control an arti\u0002ficial limb without requiring direct control of neural \nfunctions—a form of control that remains beyond \nour current scientific understanding. \nThese demonstrations hint at the prospect of other \nbrain-machine interfaces in the future, such as \ncomputer-assisted memory recall, even if the full \nsuite of potential applications is still unclear. The \nscope and feasibility of these applications will be \ndetermined by advances in neuroscientific theory \nand in technical solutions to engineering problems \n5%\n0%\n10%\n15%\n20%\n25%\n30%\n35%\nPercentage of age group with Alzheimer’s\n65–74 75–84 85+\nAge group\nFIGURE 6.2  Alzheimer’s disease surges in frequency with age\nSource: Data from “2023 Alzheimer’s Disease Facts and Figures,” Alzheimer’s & \nDementia 19, no. 4 (April 2023): 1598–1695\n06 Neur\nforce and the billions spent on the criminal justice \nsystem and healthcare related to addiction.9 Beyond \neconomics, there are the significant emotional costs \nthat impact individuals experiencing addiction, as \nwell as their families and friends. Death also takes \nits toll: The number of opioid deaths in the United \nStates has risen from 21,000 in 2010 to 111,000 in \n2022,10 which places deaths from opioid overdoses \non the same level as those caused by diabetes and \nAlzheimer’s.11 Overdose deaths fell by 3 percent in \n2023 compared with the prior year,12 but it is not \nclear yet whether the downtick is merely a pause in \ngrowth or a fundamental turning point.\nMany of the most impactful changes for dealing with \nthe societal problems arising from addiction come \nfrom public policy interventions and societal shifts, \nsuch as raising taxes on tobacco or changing physi\u0002cians’ prescribing practices for addictive substances \nsuch as opioids (see figure 6.3). Nevertheless, neu\u0002roscience has a potentially important role to play in \naddressing addiction. For example, a nonaddictive \npainkiller drug as effective as current-generation \nopioids could be transformative.13\noutside of neurons, induces cellular stress, and in turn \nmay cause tau to build up inside the neurons. As the \nbrain regions where tau accumulates are those most \ncognitively impacted, a reasonable consensus exists \nthat tau is the more direct cause of the neural death \nresponsible for dementia. \nHowever, despite what is known about neurodegen\u0002erative diseases such as Alzheimer’s, little progress \nhas been made in producing effective treatments \nthat slow disease progression. For example, tau \nremains harder to target therapeutically, and the \nrecently approved drugs target amyloid beta. While \nthese amyloid drugs are very effective in eliminating \nthe amyloid plaques from patient brains, their effec\u0002tiveness in improving patients’ cognitive abilities \nremains questionable. \nAnother form of neurodegeneration results from \ntraumatic brain injury (TBI), which can manifest itself \nin a range of complex symptoms and pathologies.6\nTraumatic impact to brain systems can affect cog\u0002nitive and behavioral functions in ways that lead to \nlong-term and severe psychiatric conditions requir\u0002ing specialized care. This is particularly evident in the \ncurrent surge of athletic and military brain injuries \nthat exhibit predominantly psychiatric symptoms. \nA person’s past medical and psychiatric records, as \nwell as any coexisting conditions, play a vital role \nin diagnosis and treatment. TBI offers insights into \nother neuropsychiatric disorders and can pave the \nway for innovative concepts in neurodegenerative \ndisease.\nNeurodiscovery and the \nScience of Addiction\nResearchers are working to understand the neural \nbasis of addiction and of chronic pain while work\u0002ing with psychiatrists and policymakers to address \nthe opioid epidemic.7 Estimates of the economic \ncosts of that epidemic range from $100  billion to \n$1 trillion a year when the loss of potential lifetime \nearnings of overdose victims is included.8 Additional \neconomic losses occur due to depletion of the labor Source: iStock.com / Johnrob\nFIGURE 6.3  Opioids prescribed by physicians \n96 STANFORD EMERGING TECHNOLOGY REVIEW\nexperiencing lasting remission.17 SNT improves \nupon traditional TMS by using individualized brain \nscans and condensing treatment into five days. If \nthese remission rates hold, it could represent a sig\u0002nificant step forward in treating depression.\nOver the Horizon\nProgress and Prospects in Neuroscience \nThe pace of neuroscientific discovery is slow and lim\u0002ited by the biological nature and complexity of the \nnervous system. Year-over-year advances tend to be \nincremental. Researchers use simple model organ\u0002isms like fruit flies with short generation times to \nstudy fundamental questions inexpensively. But the \ncloser research gets toward human application, the \nmore complex, time-consuming, and expensive it \nbecomes. For instance, because neurodegeneration \nis a slow, progressive disease where day-to-day wors\u0002ening is minimal, clinical trials often take many years.\nMost of the economic impacts of neuroscience in \nsome way connect to the healthcare industry and its \nsearch for treatments for neurodegenerative disor\u0002ders (such as Alzheimer’s and Parkinson’s disease), \nneuropsychiatric disorders (addiction, depression, and \nschizophrenia), and neural prosthesis (brain-machine \ninterfaces to restore limb function and speech). \nIt is important to keep in mind that the brain’s \ncomplexity often prevents researchers from under\u0002standing fully why even effective treatments for neu\u0002rological conditions actually work. For example, we \nknow that drugs called selective serotonin reuptake \ninhibitors block the reabsorption of serotonin into \nneurons, but neuroscientists do not have a clear \nexplanation for why this helps treat depression. New \nneurological therapies may work, but because we \ndon’t have a good understanding of exactly why \nthey do so, fine-tuning and improving them often \ncomes down to simple trial and error. Luckily for \nmedical science, an in-depth understanding of how \nAnother approach is to leverage neuroscience to \nidentify and target brain states that reinforce addic\u0002tion or make it more likely. Consider the problem of \nrelapse in tackling addiction. Scientists have found \nthat the brain mechanisms leading to an initial opioid \naddiction differ significantly from those that trigger a \nrelapse. It turns out that opioid receptors are found in \nneural circuits related to the desire for social interac\u0002tion. Stanford neuroscientists have recently identified \na circuit that is responsible for the onset of aversion \nto social interactions during recovery.14 Such an aver\u0002sion is a significant challenge to recovery because \nsocial interactions are often key to helping an indi\u0002vidual cope with the vulnerabilities associated with \nthe recovery process. The finding suggests it may be \npossible to develop drugs that inhibit social aversion \nduring withdrawal, thereby assisting patients in seek\u0002ing help or companionship from friends, families, \nrecovery programs, and doctors. \nDepression is also a major driver of addiction and \na barrier to recovery. It involves a loss of the abil\u0002ity to feel good, which addictive drugs temporarily \ncounteract by activating the brain’s reward centers. \nHowever, over the long term, these drugs can also \ndull emotions, making normal experiences less \nrewarding and worsening patients’ overall mood. \nAddiction also impairs executive control, makes \nnormal life seem unsatisfactory, and creates a belief \nthat drug use is essential for survival. Addressing any \nfactor that contributes to depression-driven addic\u0002tion can help facilitate the recovery process.\nOne nonpharmaceutical intervention for depres\u0002sion is Stanford neuromodulation therapy (SNT).15\nSNT employs transcranial magnetic stimulation \n(TMS)—the use of magnetic fields to stimulate spe\u0002cific brain regions—on regions involved in executive \nfunctioning and emotional regulation, particularly \nthe left dorsolateral prefrontal cortex, which is \nresponsible for functions such as problem-solving \nand self-control.16 This approach aims to strengthen \nconnections between brain areas to better regulate \nnegative emotions. Initial trials have shown prom\u0002ising results, with nearly 80 percent of participants \n06 Neur\ncortex. However, if about 80 percent of neural activ\u0002ity can be represented by a small group of neurons, \nthen a single, minimally invasive probe might be suf\u0002ficient to interpret movement intentions and control \nan artificial limb. Although the remaining 20 percent \nof neural activity, which the probe wouldn’t capture, \nwould likely still be important for fine-tuning limb \nmovement, a computer could help manage these \ndetails once there’s a clear understanding of how to \ninterpret movement intentions.\nNeural redundancy is also important in neural pros\u0002theses for seizure treatment. If a probe can be \nimplanted into an area of the brain prone to seizures, \nthen it might be possible, even without a complete \nsampling of the neural population, to predict the \nstate of the relevant part of the brain and warn of \nan imminent seizure. Such a prediction could allow \nfor intervening immediately to disrupt those net\u0002work dynamics or informing the patient of imminent \ndanger. It wouldn’t be necessary to understand the \ncomplete set of neural computations to have a suffi\u0002ciently clear signal for medical intervention.\nNEUROSCIENCE AND AI\nAs understanding of the mathematics of our neural \ncomputations increases, these computational models \nmay have direct relevance to AI. In particular, \nmachine learning requires vast training datasets. By \ncontrast, humans can learn languages with a small \nfraction of the training data that AI models require \n(for more discussion of this point, please refer to \nchapter 1 on artificial intelligence). Better under\u0002standing the mathematical principles that define \nhow human brains compute may therefore improve \nAI. The melding of neuroscience theory and AI is a \ntopic of increasing interest under the umbrella of \nStanford’s Wu Tsai Neurosciences Institute.21\nChallenges of Innovation and \nImplementation\nContrasting the work on artificial retinas and the work \non the science of neurodegeneration and addiction \nillustrates the dual-pronged nature of neuroscience \na particular treatment works may not always be nec\u0002essary for therapeutic intervention. \nALZHEIMER’S DISEASE DETECTION \nAND TREATMENT\nThe potential for early detection prior to the onset of \ncognitive impairment is higher than it has ever been \nbefore. Current-generation diagnostic tools now \ninclude the ability to cheaply test for biomarkers from \nblood plasma paired with more accurate but expen\u0002sive spinal taps and positron emission topography, or \nPET, scans for toxic tau and amyloid buildup. While \nanti-amyloid drugs are controversial for treating even \nmild cases of Alzheimer’s because of their side effects \n(which include brain swelling and bleeding), a rollout \nof mass blood-plasma screening, along with con\u0002firmation using more expensive tests, might mean \nthese drugs could be applied before clinical symp\u0002toms manifest, possibly increasing their effectiveness. \nAt this point, detection is more advanced than \ntreatments. Antisense oligonucleotides (ASOs) are \nan up-and-coming class of drugs that may actively \nslow cognitive decline in patients already exhibiting \ndisease symptoms. An ASO that disrupts the pro\u0002duction of additional tau showed positive results in \nan early clinical trial for safety in early 2023.18 The \nsample size was small, but the trial showed cogni\u0002tive improvements from treatment.19 Participants \nare currently being recruited for another clinical trial \nscheduled to stretch from 2022 to 2030.20 While this \napproach suffers from the drawback of the treatment \nrequiring spinal injections, extreme adverse events \nwere mostly limited to side effects of the injections \nthemselves, rather than the brain swelling or bleed\u0002ing frequently observed with the anti-amyloid anti\u0002body drugs. \nNEUROSCIENCE-BASED PROSTHESES\nNeural redundancy has important ramifications for \nthe development of brain-controlled prostheses. \nFor example, if the goal were to develop an artifi\u0002cial limb controlled by the brain, it would be nearly \nimpossible to monitor every neuron in the motor \n98 STANFORD EMERGING TECHNOLOGY REVIEW\nactivities are critical in facilitating the integration \nof well-understood scientific theory, technical engi\u0002neering, and final application.\nPolicy, Legal, and Regulatory Issues\nDISCONNECT BETWEEN PUBLIC INTEREST \nAND CAPABILITY\nThe brain is perhaps the least understood, yet most \nimportant, organ in the human body. Demand for \nneuroscience research advances and applications—\nincluding understanding brain circuitry, developing \nnew drugs, treating diseases and disorders, and cre\u0002ating brain-machine interfaces—is expected to con\u0002tinue to grow considerably over the coming years. \nThe Society for Neuroscience’s annual meeting \ndraws close to thirty thousand attendees.22\nScience fiction and fantastical headlines fuel beliefs \nthat mind-reading technology, brains controlled \nby computers, and other dystopias are imminent. \nIn reality, work to comprehend the brain’s stagger\u0002ing complexity remains in its early stages. Most \nadvances involve incremental progress, expanding \nour theoretical foundations rather than produc\u0002ing revolutionary leaps to futuristic applications. \nThis vast gap between public expectations and \nscientific reality creates an environment ripe for \nexploitation. Impatience for solutions to pressing \napplications. They have two primary components: \na scientific one that focuses on identifying relevant \nbrain circuits and understanding how these function \nand compute, and a technical engineering one that \nfocuses on how to safely stimulate the relevant brain \ncircuits to create the desired responses. \nThere is much about the brain’s anatomy, physiol\u0002ogy, and chemistry that is still not well understood, \nand addressing the theoretical issues in neurosci\u0002ence is almost exclusively the purview of academia \nrather than industry. There are research programs \nin industry that solve basic biological questions in \nneuroscience, but these are tied to solving problems \nwith a profit motive—usually the development of \nnew drugs. \nOnce the basic science has been developed and a \nresearch area approaches an economically viable \napplication, industry does a much better job of \ndeveloping it. Consequently, helping to smooth the \nfriction of moving a project from academia to indus\u0002try is crucial to overcoming roadblocks in develop\u0002ment. Incubators and accelerators can help transition \nthe findings of basic research to application by \naiding in high-throughput screening—the use of \nautomated equipment to rapidly test samples—\nand prototyping. With viable prototypes, new com\u0002panies can be created or licenses granted to exist\u0002ing companies to produce a final product. Such \nScience fiction and fantastical headlines fuel beliefs that \nmind-reading technology, brains controlled by computers, \nand other dystopias are imminent. In reality, work to \ncomprehend the brain’s staggering complexity remains in \nits early stages.\n06 Neur\nfuture research problem as the nature of brain\u0002machine interfaces becomes more ambitious over \nthe coming decades. As government is still figuring \nout how to regulate internet forums that influence \nwhat people believe and how they feel—a prob\u0002lem that has existed for three decades—regulation \nwill likely not come fast enough to guide even the \nlater-stage promises made about brain-machine \ninterfaces. Establishing proper cultural norms at the \noutset and careful consideration of technologies is \nwarranted.\nFUNDING CUTS TO TRANSFORMATIVE \nNEUROSCIENCE\nOver the past decade, much of the work outlined \nin this chapter was funded by the Brain Research \nThrough Advancing Innovative Neurotechnologies \n(BRAIN) Initiative. Starting in 2014, this aimed to be \nthe equivalent of the Human Genome Project for the \nhuman brain. Research from the BRAIN Initiative has \nhelped neuroscience generate advances that specif\u0002ically aid in translating science to medicine. In 2024, \nhowever, the initiative’s budget was cut by 40 per\u0002cent, from $680 million to $402 million. The decline \nwas due to a combination of reduced funding from \nthe National Institutes of Health (NIH) and through \nthe 21st Century Cures Act. Funding through that \nact is expected to fall by an additional $81 million in \n2025.25 Without additional financial support through \nthe NIH, neuroscience research in the United States \nand the country’s ability to tackle some of the most \nsocietally impactful diseases will decline. \nFOREIGN COLLABORATION\nHuman expertise will continue to be the primary \ndriver of future advances in neuroscience, and suc\u0002cess will continue to depend on the United States \nbeing the best place for international scientists to \ntrain, conduct research, and use their own expertise \nto teach the next generation of scientists. Against \nthis backdrop, the apparent targeting of US scien\u0002tists with personal and professional links to China \nraises concerns,26 and the United States only loses if \nthese scientists leave and move their labs to China. \nmedical problems like dementia and mental ill\u0002ness leave many open to dubious proclamations or \npseudoscience. \nDRUG POLICY AND NEUROSCIENCE RESEARCH\nThe Controlled Substances Act governs US policy \nregarding regulation of the manufacture, impor\u0002tation, possession, use, and distribution of certain \nsubstances. Substances on Schedule 1 are drugs or \nother substances with a high potential for abuse and \nnot currently accepted for medical use in the United \nStates. No research exceptions are provided for \nSchedule 1 substances such as cannabis or MDMA \n(often known as Ecstasy or Molly), which have poten\u0002tial for medical use that might be realized through \nresearch. In May 2024, the Biden administration pro\u0002posed to reassign marijuana to Schedule 3, a schedule \nwith fewer restrictions.23 Placing drugs on Schedule 1 \nsharply constrains researchers because it becomes \ndifficult to obtain these potentially helpful substances \nfor study. This constraint also denies the public the \nbenefits that might flow from such research—such \nas better medical treatments—and potentially harms \nthe public if, for example, individual states choose to \nlegalize certain drugs without adequate research into \ntheir safety, addictiveness, and public health impacts. \nTHE IMPACT OF COGNITIVE AND BEHAVIORAL \nNEUROSCIENCE ON LAW\nCognitive and behavioral neuroscience, which stud\u0002ies the biological basis of thoughts and actions, \nhas broad implications for public policy. For exam\u0002ple, a basic aspect of criminal law is the nature and \nextent of an individual’s responsibility for a criminal \nact. Under a 2005 US Supreme Court ruling, minors \nunder eighteen years of age cannot be subject to the \ndeath penalty for crimes they committed because \nadolescent brains are not fully developed, putting \nminors at higher risk of impulsive, irrational thoughts \nand behaviors.24\nTHOUGHT IMPLANTS\nThe possibility that information can be implanted \ndirectly into a person’s consciousness is an interesting \n100 STANFORD EMERGING TECHNOLOGY REVIEW\nETHICAL FRAMEWORKS\nNeuroscience research naturally raises many ethi\u0002cal concerns that merit careful, ongoing discussion \nand monitoring. Chief among these is research \non human subjects, which is governed by several \nexisting frameworks and regulations that guide \nneuroscience studies in American academia today. \nEthical guidelines for scientific research are usually \nnational, not international. Some countries might \nallow particular types of brain research and drugs, \nwhile others might not; for example, a nation might \npermit experimentation on prisoners or on ethnic \nminorities. Managing differences in state research \nregimes will be critical to harnessing the power of \ninternational collaboration.\nNOTES\n1. Marcus E. Raichle and Debra A. Gusnard, “Appraising the \nBrain’s Energy Budget,” Proceedings of the National Academy of \nSciences 99, no. 16 (July 2002): 10237–39.\n2. Frederico A. C. Azevedo, Ludmila R. B. Carvalho, Lea T. Grin\u0002berg, et al., “Equal Numbers of Neuronal and Nonneuronal Cells \nMake the Human Brain an Isometrically Scaled-up Primate Brain,” \nJournal of Comparative Neurology 513, no. 5 (April 2009): 532–41, \nhttps://doi.org/10.1002/cne.21974. \n3. Stanford Medicine, “The Stanford Artificial Retina Project,” \naccessed August 30, 2023, https://med.stanford.edu/artificial\n-retina.html.\n4. Winston Wong, “Economic Burden of Alzheimer Disease and \nManaged Care Considerations,” American Journal of Managed \nCare 26, no. 8 (August 2020): S177–83, https://doi.org/10.37765\n/ajmc.2020.88482.\n5. Alzheimer’s Association, “2023 Alzheimer’s Disease Facts and \nFigures,” March 14, 2023, https://doi.org/10.1002/alz.13016; \nA. W. Willis, E. Roberts, J. C. Beck, et al., “Incidence of Parkinson’s \nDisease in North America,” npj Parkinson’s Disease 8, no. 170 \n(December 2022), https://doi.org/10.1038/s41531-022-00410-y.\n6. Vassilis E. Koliatsos and Vani Rao, “The Behavioral Neuro\u0002science of Traumatic Brain Injury,” Psychiatric Clinics of North \nAmerica 43, no. 2 (June 2020): 305–30, https://doi.org/10.1016\n/j.psc.2020.02.009.\n7. Wu Tsai Neurosciences Institute, “NeuroChoice Initiative \n(Phase  2),” Stanford University, accessed August 30, 2023, \nhttps://neuroscience.stanford.edu/research/funded-research\n/neurochoice.\n8. Low end: Pew Charitable Trust, “The High Price of the Opioid \nCrisis, 2021,” August 27, 2021, https://www.pewtrusts.org/en\n/research-and-analysis/data-visualizations/2021/the-high-price\n-of-the-opioid-crisis-2021; high end: Feijin Luo, Mengyao Li, \nand Curtis Florence, “State-Level Economic Costs of Opioid Use \nDisorder and Fatal Opioid Overdose—United States, 2017,” Mor\u0002bidity and Mortality Weekly Report 70, no. 15 (April 16, 2021): \n541–46, http://dx.doi.org/10.15585/mmwr.mm7015a1.\n9. Office of Justice Programs, “National Drug Threat Assess\u0002ment, 2011,” US Department of Justice, August 2011, https://\nwww.ojp.gov/ncjrs/virtual-library/abstracts/national-drug-threat\n-assessment-2011.\n10. National Institute on Drug Abuse, “Drug Overdose Deaths: \nFacts and Figures,” National Institutes of Health, August 2024, \nhttps://nida.nih.gov/research-topics/trends-statistics/overdose\n-death-rates.\n11. Overdose included in the accidental death statistic in Centers \nfor Disease Control and Prevention, “Leading Causes of Death,” \nlast modified May 2, 2024, https://www.cdc.gov/nchs/fastats\n/leading-causes-of-death.htm.\n12. National Center for Health Statistics, “U.S. Overdose Deaths \nDecrease in 2023, First Time Since 2018,” US Centers for Disease \nControl and Prevention, May 15, 2024, https://www.cdc.gov/nchs\n/pressroom/nchs_press_releases/2024/20240515.htm.\n13. US Food and Drug Administration, “FDA Takes Steps Aimed \nat Fostering Development of Non-Addictive Alternatives to Opi\u0002oids for Acute Pain Management,” February 9, 2022, https://\nwww.fda.gov/news-events/press-announcements/fda-takes\n-steps-aimed-fostering-development-non-addictive-alternatives\u0002opioids-acute-pain-management.\n14. Gordy Slack, “Social Aversion During Opioid Withdrawal \nReflects Blocked Serotonin Cues, Mouse Study Finds,” Wu Tsai \nNeurosciences Institute, Stanford University, November 2, 2022, \nhttps://neuroscience.stanford.edu/news/social-aversion-during\n-opioid-withdrawal-reflects-blocked-serotonin-cues-mouse-study\n-finds. \n15 For more on this subject, see Nina Bai, “Researchers Treat \nDepression by Reversing Brain Signals Traveling the Wrong Way,” \nNews Center, Stanford Medicine, May 15, 2023, https://med\n.stanford.edu/news/all-news/2023/05/depression-reverse-brain\n-signals.html.\n16. Wayne C. Drevets, Jonathan Savitz, and Michael Trimble, \n“The Subgenual Anterior Cingulate Cortex in Mood Disorders,” \nCNS Spectrums 13, no. 8 (August 2008): 663–81, https://doi\n.org/10.1017/s1092852900013754. \n17. Aditya Somani and Sujita Kumar Kar, “Efficacy of Repetitive \nTranscranial Magentic Stimulation in Treatment-Resistant Depression: \nThe Evidence Thus Far,” General Psychiatry 32, no. 4 (August 2019): \ne100074, https://doi.org/10.1136/gpsych-2019-100074.\n18. Catherine J. Mummery, Anne Börjesson-Hanson, Daniel \nJ. Blackburn, et al., “Tau-Targeting Antisense Oligonucleotide \nMAPTRX in Mild Alzheimer’s Disease: A Phase 1b, Randomized, \nPlacebo-Controlled Trial,” Nature Medicine 29 (April 2023): 1437–47, \nhttps://doi.org/10.1038/s41591-023-02326-3.\n19. Isabella Ciccone, “Biogen’s BIIB080 Exhibits Favorable Out\u0002comes in Phase 1b Trial for Early Alzheimer’s Disease,” Neurology Live, \nOctober 26, 2023, https://www.neurologylive.com/view/biogen\n-biib080-exhibits-favorable-outcomes-phase-1b-trial-early-ad.\n20. Alzheimers.gov, “BIIB080 for Mild Cognitive Impairment Due \nto Ahlzeimer’s Disease or Mild Alzheimer’s,” National Institute on \nAging, National Institutes of Health, accessed March 14, 2023, \nhttps://www.alzheimers.gov/clinical-trials/biib080-mild-cognitive\n-impairment-due-alzheimers-disease-or-mild-alzheimers.\n21. Wu Tsai Neurosciences Institute, “Center for Mind, Brain, Compu\u0002tation, and Technology,” Stanford University, accessed October 14, \n06 Neuro\nSTANFORD EXPERT CONTRIBUTORS\nDr. Kang Shen\nSETR Faculty Council, Frank Lee and Carol Hall \nProfessor, and Professor of Biology and of Pathology\nDr. Keith Humphreys\nEsther Ting Memorial Professor in the Department \nof Psychiatry and Behavioral Sciences \nDr. Paul Nuyujukian\nAssistant Professor of Bioengineering, of Neurosurgery, \nand, by courtesy, of Electrical Engineering\nDr. Michael Greicius\nIqbal Farrukh and Asad Jamal Professor and \nProfessor, by courtesy, of Psychiatry and Behavioral \nSciences\nDr. Alec Condon\nSETR Fellow and Postdoctoral Scholar in Biology \nand at the Howard Hughes Medical Institute\n2024, https://neuroscience.stanford.edu/initiatives-centers/center\n-mind-brain-computation-and-technology.\n22. Society for Neuroscience, “Attendance Statistics: Meeting \nAttendance,” accessed August 30, 2023, https://www.sfn.org\n/meetings/attendance-statistics.\n23. Federal Register, “Schedules of Controlled Substances: \nRescheduling of Marijuana,” May 21, 2024, https://www.federal\nregister.gov/documents/2024/05/21/2024-11137/schedules-of\n-controlled-substances-rescheduling-of-marijuana.\n24. Roper v. Simmons, 543 U.S. 551 (2005).\n25. Jocelyn Kaiser, “Major Budget Cuts to Two High-Profile NIH \nEfforts Leave Researchers Reeling,” ScienceInsider, Science, April \n30, 2024, https://www.science.org/content/article/major-budget\n-cuts-two-high-profile-nih-programs-leave-researchers-reeling.\n26. Jeffrey Mervis, “Pall of Suspicion: The National Institutes of \nHealth’s ‘China Initiative’ Has Upended Hundreds of Lives and \nDestroyed Scores of Academic Careers,” Science, March 23, \n2023, https://www.science.org/content/article/pall-suspicion-nihs\n-secretive-china-initiative-destroyed-scores-academic-careers.\n\n103\nOverview\nResearchers do not universally agree on the defi\u0002nition of a robot, but a consensus seems to have \nemerged that, at the very least, a robot is a human\u0002made physical entity with ways of sensing itself or \nthe world around it and ways of creating physical \neffects on that world.1\nRobots, which include both static entities such as \nfixed robotic arms on production lines and mobile \nentities such as drones, must integrate many differ\u0002ent component technologies to combine percep\u0002tion of the environment with action in it. Perception \nrequires generating a representation of the robot’s \nenvironment and its interaction with its surround\u0002ings. Action requires the robot to make physical \nchanges to itself or the environment based on those \nperceptions. \nThe key engineering challenges in robotics involve \nthe design of components and integration of them \nKEY TAKEAWAYS\n° Future robots may be useful for improving the \nUS manufacturing base, reducing supply chain \nvulnerabilities, delivering eldercare, enhancing \nfood production, tackling the housing shortage, \nimproving energy sustainability, and performing \nalmost any task involving physical presence.\n° Progress in artificial intelligence holds the \npotential to advance robotics significantly but \nalso raises ethical concerns that are essential to \naddress, including the privacy of data used to \ntrain robots, data bias that could lead to physical \nharm by robots, and other safety issues.\n° Achieving the full potential of robots will require \na major push from the federal government and \nthe private sector to improve robotics adoption \nand research across the nation.\nROBOTICS\n07\n104 STANFORD EMERGING TECHNOLOGY REVIEW\nstructured ways. “Soft” robots that are flexible \nand conform to the environment can offer better \nperformance in more unstructured and chaotic \nenvironments.2\n° Power sources that can be tethered to a robot or \nthat are untethered. A robot tethered to a “mother \nship” can be energized from a power source on \nthat ship indefinitely, while untethered robots \nneed self-contained power sources or sources \nthat harvest energy from the environment. \n° Real-time computing that determines the spe\u0002cific timeframes in which operations of robots \ntake place. This ensures, for example, that a \nrobotic arm in a workplace will stop very fast \nif the robot detects a human in its immediate \nproximity.\nFinally, some robots use computer vision and other \ntypes of artificial intelligence (AI) for understanding \ntheir environments and decision-making, but robot\u0002ics and AI do not always go together (see figure 7.1). \nRobots with varying degrees of autonomy have been \nwithin a robot’s body so it can perform intended \ntasks in different settings in a given environment. \nDifferent types of robots operate in different \nenvironments—including factories, homes, and even \nspace—and each environment poses distinct com\u0002plexities beyond just a robot’s technical perfor\u0002mance. For example, working alongside humans \nraises critical issues of safety and liability.\nImportant component technologies include:\n° Actuators that enable movement, such as motors \nand grasping appendages \n° Sensors that receive real-time input about the \nimmediate physical environment of the robot and \nthe robot’s own configuration \n° Control systems that decide what the robot \nshould do based on sensor readings \n° Materials that robots are made of. Those built \nfrom rigid materials typically interact with their \noperating environments in highly prescribed and \nRobotics Arti\u0016cial\nTele-operated intelligence surgical robots\nEarly robotic\nvacuum \ncleaners\nAI robotic\nvacuum\ncleaners\nZoom AI\nsummaries\nChatGPT\nAutocorrect\nSiri\nAlexa\nFacial\nrecognition\nHumanoid\nrobots\nArti\u0016cially\nintelligent\nrobots\nFIGURE 7.1  Not all robots use artificial intelligence\n07 Robot\nsudden shortage of motor vehicles and consumer \nelectronics, to name just two industries. These and \nother disruptions underline the importance of bring\u0002ing more manufacturing back to the United States \nand making manufacturing supply chains more resil\u0002ient across the world.\nRobots have the potential to reduce vulnerabilities \nin these supply chains in many ways. New innova\u0002tions such as robotic graspers that can handle even \nvery fragile goods can make current manufacturing \nlines more adaptable and reconfigurable. The devel\u0002opment and increased deployment of collaborative \nrobots, or cobots, that can interact with human work\u0002ers could help alleviate labor shortages—though \nhuman workers’ reactions to these robots and safety \nissues associated with them are significant concerns \nthat need to be addressed. Robots also hold prom\u0002ise for manufacturing in extreme environments, such \nas in space or underwater, and researchers are work\u0002ing on issues like precision control and visual per\u0002ception to help them operate in these domains.\nThe “Now” Economy\nNear-real-time delivery of goods and services, also \nknown as the “now” economy, refers to the deploy\u0002ment of goods and services as close to custom\u0002ers as possible so that products are available very \nrapidly upon demand. Another dimension involves \nthe remote delivery of services, including medical \ntreatments.\nThese areas pose multiple challenges, including \nfinding better ways to deliver goods to customers \nquickly and cost-effectively; applying expertise at \nthe point of need through teleoperation, even if the \nexperts are geographically distant; and finding ways \nto automate services to offset labor shortages.\nRobotics is already providing solutions to address \nthese needs. Various kinds of robots, including \ndrones and multiwheeled delivery vehicles that can \nnavigate sidewalks, are being trialed to conduct last\u0002mile deliveries (the final step in the shipping process \nused in everything from delicate surgical procedures \nto space exploration.\nKey Developments\nAcademic and commercial robotics activities are \ninfluenced by global socioeconomic trends as well \nas by technology developments in other domains. \nHere we briefly review some of the most important \ncurrent influences on the robotics field.\nManufacturing\nThe manufacturing sector is a major contributor to \nthe US economy. In 2023, it accounted for just over \na tenth of GDP, adding $2.87 trillion.3 It is also a \nmajor contributor to employment, with an estimated \nthirteen million people working in manufacturing.4\nHowever, the sector is facing several challenges that \nrobotics can help overcome.\nOne of the most important of these challenges is a \nshortage of skilled labor. Approximately two-thirds \nof the respondents in the National Association of \nManufacturers’ outlook survey, conducted in the \nfirst quarter of 2024, highlighted attracting and \nretaining skilled workers as their primary challenge.5\nSeveral reasons have led to this situation, including \nmore people retiring and a declining population \ngrowth rate. Unless solutions are found, millions \nof manufacturing jobs could remain unfilled in the \nfuture, impacting America’s prosperity and national \nsecurity.\nAnother major challenge is the vulnerability of the \nUS supply chain, a result of the trend toward out\u0002sourcing manufacturing to countries with lower \nlabor costs. The globalization of manufacturing has \nhelped reduce the price and increase the variety of \ngoods available to US consumers, but it has left US \nsupply chains more vulnerable to disruptions—as \nwe saw during the COVID-19 pandemic with the \n106 STANFORD EMERGING TECHNOLOGY REVIEW\nthe types of crops they use—and their seeding and \nharvesting practices—to keep up with demand. \nRobotics can support these efforts and help stream\u0002line the production and processing of food. Currently, \nrobots are deployed mainly to reduce the cost of \nspecific processes such as milking, seeding fields, \nand picking fruit. The main hurdle to expanding \nthe use of robotics in agriculture and food produc\u0002tion is the dexterity needed to accomplish certain \ncomplex tasks. Meat production, for example, is a \nheavily subsidized market with very little automa\u0002tion.8\n Although some companies such as Cargill \nand Tyson Foods have recently invested heavily in \ndeveloping automated solutions,9 much can still \nbe done to improve the quality and efficiency of the \nmeat-carving process. \nRobotics alone cannot address the goal of signifi\u0002cantly increasing food production. Integrating it \nwith other technologies, such as AI and computer \nvision, is critical for agriculture and food processing. \nCombining these technologies can help increase \nfood production yields in two main ways:\nDirect interface with robots With the use of \nAI—and, specifically, the implementation of re -\ninforcement learning, an approach that mimics the \ntrial-and-error learning process of humans—robots \ncan be trained to accomplish complex tasks in sim\u0002ulated environments before they are deployed and \nto keep learning from their mistakes once they are \nin place. \nData capture For example, by using computer \nvision, a seeding robot can also keep track of the \nhealth of a crop, check the level of ripeness of fruit, \nand generally provide farmers with a wealth of infor\u0002mation they would otherwise not have access to or \nthat would require a significant effort to collect.\nAdvancing automation in agriculture can increase \nthe efficiency and productivity of the industry, help\u0002ing it meet growing demand. The additional data \nacquired via robots will also help drive and improve \nwhen goods are transported from the distribution \nhub to the customer). Remote robot-assisted sur\u0002gery for gallbladder procedures and hernia repair is \nincreasingly available, a trend covered in more detail \nlater in this chapter. Robots can automate basic ser\u0002vices around the home, such as floor cleaning. They \ncan also serve as seasonal, on-demand labor in \nsome industries such as agriculture, where autono\u0002mous robots can be used to pick fruit and perform \nother tasks in the harvesting season.\nWhile the potential for further innovation is clear, \nthere are also many challenges, both technical \nand nontechnical, to address. These include the \nfollowing:\n° Ensuring that the use of robots in last-mile deliv\u0002ery strategies does not lead to safety and privacy \nviolations. For instance, increasing use of delivery \ndrones has led to a number of reported shootings \nof drones, an activity with obvious safety implica\u0002tions.6\n A delivery drone could also inadvertently \ncapture images of people in their homes and \nbackyards. \n° Developing adaptable and reliable manipulation \ncapabilities that enable autonomous robots to \nhandle a wide variety of goods and other objects \nfor delivery to customers\n° Creating more networking infrastructure that \ncan support wider adoption of reliable remote \nrobotic applications in healthcare and other areas \nFood Production\nThe world’s population increased from seven billion \nto eight billion between 2010 and 2022, and the \nUnited Nations estimates it will rise by a further \ntwo billion people by 2030.7 To keep up with \ndemand, food production is predicted to increase \nby 50 percent by 2050. With the increased fre\u0002quency of dramatic weather events such as floods \nand droughts, agriculture is becoming increasingly \nchallenging. Farmers will need to more rapidly adapt \n07 Robotics 107\ncompanions that help people with basic tasks asso\u0002ciated with the activities of daily living both inside \nand outside their homes. Assistive robots can also \ntake the form of exoskeletons. These are wearable \nrobotic devices that provide support with movement \nby, for instance, working with calf muscles to give \npeople extra propulsion with each step taken (see \nfigure  7.2).13 Smaller devices like trackers can also \nhelp monitor symptoms, recognize falls and alert \nhealth professionals, and detect early signs of car\u0002diac issues.\nAnother application of robotics in healthcare is in \nsurgery. Around 30 percent of the surgeries neces\u0002sary worldwide every year go unperformed.14 Using \nrobots, like the one shown in figure 7.3, to automate \nall or parts of routine procedures can streamline sur\u0002gical interventions, making them safer and more effi\u0002cient. Developments in force sensor feedback and \nhaptics—the science of simulating pressure, vibra\u0002tions, and other sensations related to touch—are \nland-management practices. Strategies such as \nwell-timed crop rotation can improve soil fertility, \nand changing seed types can boost yields dramati\u0002cally. Implementing precision agriculture, which uses \nsensors to collect data and algorithms to analyze \nthe information, can help develop automated sys\u0002tems that track and report key metrics for farmers. \nThis strategy can also improve the environment by \ndecreasing the use of fertilizers and irrigation in \nagriculture.\nOver the Horizon\nImpact of Robotic Technologies\nSUPPORTING AN AGING POPULATION\nRobotic technologies can assist in the support and \ncare of the elderly. Although the US population is \nnot aging as fast as those of some other countries \nsuch as Japan and Italy, more than a fifth of people in \nAmerica will be over sixty-five by 2030.10 Americans \nare also living longer: Average life expectancy in the \nUnited States has risen to over seventy-seven years, \naccording to the Centers for Disease Control and \nPrevention, up from less than seventy-four years in \n1980.11 Sadly, the prevalence of cognitive impair\u0002ment increases with longer lifespans, and medical \ninterventions for this issue are the focus of much neu\u0002roscience research (see chapter 6 on neuroscience).\nThere is a huge dearth of qualified personnel for \neldercare. Long hours, low wages, and the inten\u0002sity of the tasks involved have made the industry \nunattractive for many prospective employees. More \nrestrictive immigration policies have exacerbated \nthe issue, as a large fraction of eldercare workers are \nimmigrants, and the demand for eldercare workers \nis growing.12\nAgainst this backdrop, assistive and rehabilitative \nrobots are being developed and deployed to support \nhuman caregivers. These robots can be electronic \nSource: Shutterstock / Unai Huizi Photography\nFIGURE 7.2 A wearable exoskeleton \n108 STANFORD EMERGING TECHNOLOGY REVIEW\nan immense amount of training data is required to \nensure it will function safely.\nAs a parallel, before self-driving cars are allowed \non the road, they must complete millions of miles \nof journeys to test their capabilities.16 How can we \nachieve a comparable level of experience in health\u0002care? One solution is to use simulation so that \nhuman participants are not endangered in testing. \nHowever, there is still concern that even state-of\u0002the-art simulation may not fully capture enough sce\u0002narios and behaviors to ensure a robot can interact \nsafely with patients.\nTACKLING THE HOUSING SHORTAGE\nHigh prices and low supply have exacerbated a crisis \nin housing in the United States. Moreover, while \ndemand for housing-construction workers contin\u0002ues to grow, the number of workers available is not \ndriving the development of surgical robots that \ncan be controlled from remote locations, making \nit unnecessary for a doctor to be physically present \nto either diagnose or treat a patient. Telerobotics is \nparticularly promising for rural areas, where it can be \nharder to get physical access to specialists. Robotic \nsurgery focused on low- and middle-income coun\u0002tries may help to improve access to surgical care in \nthose countries.15\nThe main challenge here is the complexity of the tasks \ninvolved. In a routine surgery, organ location, shape, \nand size can vary dramatically among patients, and \nrobots must adjust for this. Even a seemingly easy \ntask like feeding a patient can be difficult for a robot \nbecause small movements of the individual can be \nhard to adjust for. AI and machine learning (ML) are \nbeing talked about as potential solutions to such \nissues, but for every new task a robot has to learn, \nSource: © 2024 Intuitive Surgical Operations, Inc. \nFIGURE 7.3 Robotic surgical systems can make surgical interventions safer and \nmore efficient\n07 Robot\nusing AI and other methods will depend on suffi\u0002cient availability of high-quality data with which to \ntrain them. Human workers also need to be trained \nto interact safely and efficiently with robotic systems \non construction sites and to maintain these systems.\nADVANCING SUSTAINABILITY\nRobotics can contribute significantly to sustainable \npractices, addressing pressing global challenges \nwhile opening new avenues for industrial applica\u0002tions. These applications span renewable energy \ndevelopment and harvesting, sustainable agricul\u0002tural practices, and infrastructure maintenance. \nIn the renewable energy sector, robots are indispens\u0002able in the construction, maintenance, and manage\u0002ment of solar and wind farms. They assist in tasks \nranging from the design and installation of infra\u0002structure to ongoing upkeep, such as cleaning solar \npanels and maintaining wind turbines. For instance, \nBladeBUG has developed six-legged robots that \nscale turbines, conduct detailed inspections, and \nmake minor repairs, reducing downtime and main\u0002tenance costs for wind farms.19 Robots can facilitate \nthe development of other renewable energy sources \nlike geothermal and wave energy by automating \nlabor-intensive and hazardous tasks.\nThey are also helping to gather resources in a more \nsustainable way. Robots can harvest materials, such \nas lumber, with minimal ecological impact and col\u0002lect metallic nodules from the ocean floor without \ndamaging marine ecosystems. In recycling and \nwaste management, advanced robotic systems can \nhandle hazardous materials and complex products, \nprotecting workers and streamlining processing. \nFor example, robotics company AMP has created \nAI-powered robots that pick out recyclable materials \nfrom complex waste streams with high precision.20\nIn agriculture, robots help to optimize water usage, \nreduce chemical inputs, and minimize runoff. John \nDeere’s See & Spray uses advanced camera detec\u0002tion and ML to apply herbicide to crops only where \nneeded with high precision, thereby minimizing \nrising enough to match it. For example, one study \nnotes that even before the pandemic, construction \nfirms ranked labor shortages as the biggest hurdle \nfor their businesses, and 78 percent of them were \nexperiencing difficulty in filling their job positions.17\nRobots could potentially address this shortage, \nincrease overall construction productivity, and also \nreduce worker injuries and deaths over time.\nCurrently, there are commercial robots that are \ncapable of bricklaying, house framing, wall finish\u0002ing, and moving heavy items on construction sites. \nThe SAM100 robot (SAM stands for semi-automated \nmason), developed by Construction Robotics in \nthe United States, can assist with brick retrieval, \nmortar placement, and brick placement in a wall. It \ncan help complete this process three to five times \nfaster than a bricklayer working alone,18 but it still \nneeds human involvement for certain tasks such as \nsmoothing out excess mortar. Another construction \nrobot, the Hadrian X, developed by FBR in Australia, \nstill requires manual loading of the bricks it handles. \nFuture innovation will drive toward increased auto\u0002mation of such processes.\nIn addition to construction, robotics could contrib\u0002ute to other aspects of building and maintaining \ninfrastructure. Paving, grading, striping and marking, \nand repairing roads could be either fully or partially \nautomated. Robots equipped with cameras and sen\u0002sors can conduct these tasks with greater accuracy \nand more precise control than humans, and they can \nbe fully automated or operated via remote control. \nCurrently, road inspection relies on surveyors to be \non-site, but robots could be used to inspect and \nrepair roads and other transportation infrastructure, \nespecially in areas not easily accessible to humans.\nThere are still various challenges to address when \nintegrating robotics with housing construction. \nUnpredictable changes, such as moving machinery \nor heavy equipment on work sites, can pose safety \nhazards for both personnel and robots that the latter \nmust be better equipped to deal with. Training \nrobots to respond appropriately to such scenarios \n110 STANFORD EMERGING TECHNOLOGY REVIEW\nChallenges of Innovation and \nImplementation \nADOPTION AND FUNDING\nAchieving the full potential of robots to help drive \neconomic growth and address pressing supply chain \nand labor constraints will require a major push from \nthe federal government and the private sector to \nimprove robotics adoption and research across the \nnation.\nIn terms of adoption, manufacturing-robot den\u0002sity in the United States in 2022 was 285 robots per \n10,000 employees, ranking the country tenth in the \nworld behind nations such as Singapore, Germany, \nSouth Korea, Japan, and China (see figure 7.4).21 This \nreflects the fact that US businesses—especially small \nand midsize ones—are still adopting robots more \nslowly than those in some other large economies. \nAddressing regulatory and standard-setting issues \nmore comprehensively could help. For instance, \ndeveloping comprehensive guidelines and policies \nchemical usage. Such technologies can enhance \nharvesting efficiency, detect the spread of diseases \nin crops, and make it easier to adapt agricultural \npractices quickly to the changing climate. \nIn infrastructure maintenance, robots can monitor \nand repair aging facilities, preventing failures that \ncould harm people and business operations. For \nexample, Boston Dynamics’ Spot robot has been \nused to inspect industrial sites, monitor construction \nprogress, and detect gas leaks. \nThis and the other examples cited above highlight \nthe potential for robots to play an important role \nin advancing sustainability goals, but significant \ninvestment in technology development and infra\u0002structure is still needed. For instance, ensuring the \nreliability and efficiency of robotic systems in diverse \nand often harsh and unpredictable environments is \ncrucial. Creating safe and efficient human–machine \ninteractions in work such as waste management and \ninfrastructure maintenance will also be vital.\n1012\n730\n415 397 392\n343 333\n296 292 285 284 274 248\n100\n300\n500\n700\n900\n1100\nRep. of Korea\nSingapore\nGermany\nJapan\nChina\nSweden\nHong Kong\nSwitzerland\nChinese Taipei\nUnited States\nSlovenia\nDenmark\nNetherlands\nRobots installed per 10,000 employees\nFIGURE 7.4 The United States lags behind many other countries in manufacturing-robot adoption \nSource: Adapted from International Federation of Robotics, “Global Robotics Race: Korea, Singapore and \nGermany in the Lead,” January 10, 2024\n07 Robot\nPolicy, Legal, and Regulatory Issues\nRobots can improve manufacturing efficiency, bolster \nsustainability, and provide treatment to the elderly, \nbut their adoption raises legal and ethical questions \nthat are essential to address. There is much talk \nabout the benefits of AI, for instance, but the impli\u0002cations of the ongoing data collection required to \ntrain robots for complex tasks and ensure their safe \ninteraction with the world are significant. This sec\u0002tion categorizes the legal and ethical considerations \nassociated with robotics that warrant further policy \ndevelopment into three broad areas: privacy and \nconsent, inclusion and integrity, and safety. \nPRIVACY AND CONSENT\nAnyone using social media or browsing the internet \nhas encountered warnings and consent requests \nfor things like advertisements and tracking cookies. \nNews reports are filled with cybersecurity breaches \nthat involve unauthorized access to huge amounts \nof personal data. How, then, should we be thinking \nabout handling the vast amount of personal data \naccumulated by robots in our hospitals, homes, and \nelsewhere? \nAccess to health-related information is heavily mon\u0002itored and regulated to protect patient privacy. It is \nimportant that policies for safeguarding personal \ndata be developed in tandem with current efforts \nto ensure the safe and effective use of robots in con\u0002struction is critical.\nFurthermore, the adoption of robotics in this and \nother areas has implications for workforce dynam\u0002ics. As robots become more capable of taking on \ndull, dirty, and dangerous tasks, enabling human \nworkers to focus on more complex and rewarding \nactivities, there is a clear need to manage the tran\u0002sition carefully to prevent significant job displace\u0002ment. Investing in education and training programs \nis essential to prepare the workforce for new roles \nand ensure that the benefits of robotics are widely \nshared.\nCreating clear standards for—or at least achieving \na rough consensus on—use of robotic technologies \nacross industries and regions could help accelerate \nmore widespread use of robots. This is important in \nall of the areas touched on in this chapter but espe\u0002cially in relatively new domains such as the use of \nrobots for advancing sustainable practices.\nFinally, funding of robotics research and develop\u0002ment remains an issue. Despite efforts such as the \nNational Robotics Initiative, which was launched \nduring the Obama administration, more support will \nbe needed if the United States is to make the most \nof the exciting and transformative opportunities that \nrobotics offers.\nAchieving the full potential of robots to help drive \neconomic growth and address pressing supply chain and \nlabor constraints will require a major push from the federal \ngovernment and the private sector to improve robotics \nadoption and research across the nation.\n112 STANFORD EMERGING TECHNOLOGY REVIEW\nan average human or whether it should be judged \nagainst a higher standard, perhaps one of near per\u0002fection. From a practical standpoint, the former is \na reasonable standard, and widespread deployment \nof robots that met such a standard would improve \nsafety on average. But whether the public is willing \nto accept this standard is unproven.\nSetting standards for the performance of robotic \ncontrol systems is crucial for ensuring their success\u0002ful and continued adoption. Similarly, cybersecu\u0002rity standards for robotics need to be on a par with \nthose in the domains that robots are used in, includ\u0002ing healthcare and national security.\nto collect very large amounts of data for training \nrobotic systems, as well as during the deployment \nof these systems. With the exponential increase of \ndata production, harvesting, and use, it is also cru\u0002cial to ensure data is held securely. In the wrong \nhands, health and other personal information can be \nused to coerce and control individuals, disrupt the \nsmooth and secure conduct of daily activities, and \nundermine trust in essential systems.\nINCLUSION AND INTEGRITY\nIn addition to concerns about consent and privacy \nin data acquisition, inclusion and integrity are also \ncrucial issues—and ones that concern AI more \nbroadly. The launch of products that make use of \nML, like facial recognition systems, has revealed that \nbias in datasets can lead to unintended and some\u0002times harmful outcomes. This issue is going to be \nvitally important for robotics. For example, what if \nbias embedded in algorithms leads a surgical robot \nto be less well trained to operate on women than \nmen? Or what if a robotic safety system scanning \npeople infers that someone might be carrying a \ngun because of their ethnicity? The consequences \ncould be grave. Promoting and enforcing norms and \nstandards when it comes to robot-training datasets \nis essential to ensure that the diversity of America’s \npopulation is adequately reflected and that robots \nare used safely.\nSAFETY\nThe deployment of robots in society raises ethical \nquestions regarding both physical safety and cyber\u0002security. As noted earlier, physical safety pertains \nnot only to how well a robot is trained to perform a \nspecific task, but also to how it manages unforeseen \ndisruptions. \nFrom the standpoint of public acceptability, one crit\u0002ical question is the nature of the standards that are \nappropriate for society to use for judging the safety \nof a robot in any given application. A threshold ques\u0002tion is whether its safety performance must simply \nbe comparable to—or slightly better than—that of \nNOTES\n1. Ralph Lässig, Markus Lorenz, Emmanuel Sissimatos, et al., “Robot\u0002ics Outlook 2030: How Intelligence and Mobility Will Shape \nthe Future,” Boston Consulting Group, June 28, 2021, https://\nwww.bcg.com/publications/2021/how-intelligence-and-mobility\n-will-shape-the-future-of-the-robotics-industry.\n2. Some examples of soft robots can be found at Alberto Paitoni \nFaustinoni, “Soft Robotics: Examples, Research and Applica\u0002tions,” Robotics24, February 3, 2023, https://robotics24.net/blog\n/soft-robotics-examples-research-and-applications/.\n3. National Association of Manufacturers, “Manufacturing in the \nUnited States,” accessed September 15, 2024, https://nam.org\n/manufacturing-in-the-united-states/.\n4. National Association of Manufacturers, “Manufacturing in the \nUnited States.”\n5. National Association of Manufacturers, “2024 First Quarter \nManufacturers’ Outlook Survey,” March 5, 2024, https://nam\n.org/2024-first-quarter-manufacturers-outlook-survey.\n6. Sasha Rogelberg, “Walmart’s Latest Challenge with Its Drone \nDelivery System Is Gun Owners Shooting Packages Out of the \nAir,” Fortune, July 3, 2024, https://fortune.com/2024/07/03\n/walmart-drones-gun-owners-delivery-florida-droneup/.\n7. United Nations, “Global Issues: Population,” accessed Septem\u0002ber 16, 2024, https://www.un.org/en/global-issues/population#.\n8. David Gillette and Warren Barge, “The True Cost of a Ham\u0002burger,” American Institute for Economic Research, April 20, 2022, \nhttps://www.aier.org/article/the-true-cost-of-a-hamburger/.\n9. Simon Creasey, “How the Meat Industry Is Embracing Automa\u0002tion,” Just Food, October 26, 2022, https://www.just-food.com\n/features/how-the-meat-industry-is-embracing-automation/.\n10. America Counts Staff, “By 2030, All Baby Boomers Will Be Age \n65 or Older,” US Census Bureau, December 10, 2019, https://www\n.census.gov/library/stories/2019/12/by-2030-all-baby-boomers\n-will-be-age-65-or-older.html.\n11. Shameek Rakshit, Matthew McGough, and Krutika Amin, “How \nDoes U.S. Life Expectancy Compare to Other Countries?,” Health \n07 Robot\nSystem Tracker, January 30, 2024, https://www.healthsystemtracker\n.org/chart-collection/u-s-life-expectancy-compare-countries/.\n12. Robert Espinoza, Lina Stepick, Jessica King, et al., “Bridging the \nGap: Enhancing Support for Immigrant Direct Care Workers and \nMeeting Long-Term Care Needs,” PHI, September 26, 2023, https://\nwww.phinational.org/resource/bridging-the-gap-enhancing\n-support-for-immigrant-direct-care-workers/.\n13. Patrick Slade, Mykel J. Kochenderfer, Scott L. Delp, et al., \n“Personalizing Exoskeleton Assistance While Walking in the \nReal World,” Nature 610 (October 2022): 277–82, https://doi\n.org/10.1038/s41586-022-05191-1. \n14. One study estimates that about 143 million additional surgical \nprocedures are needed in low- and middle-income countries each \nyear to save lives and prevent disability, while 313 million such \nprocedures are performed each year. See John G. Meara, Andrew \nJ. M. Leather, Lars Hagander, et al., “Global Surgery 2030: Evi\u0002dence and Solutions for Achieving Health, Welfare, and Economic \nDevelopment,” The Lancet 386, no. 9993 (August 2015): 569–624, \nhttps://doi.org/10.1016/S0140-6736(15)60160-X. \n15. Aashna Mehta, Jyi Cheng Ng, Wireko Andrew Awuah, et al., \n“Embracing Robot Surgery in Low- and Middle-Income Countries: \nPotential Benefits, Challenges, and Scope in the Future,” Annals \nof Medicine and Surgery 84 (December 2022): 104803, https://\ndoi.org/10.1016/j.amsu.2022.104803.\n16. Waymo was authorized to begin offering fully driverless rides \nin San Francisco in June 2024. Waymo had logged over 7.14 mil\u0002lion fully autonomous miles driven 24/7 through the end of \nOctober 2023. See Waymo, “Waymo Significantly Outperforms \nComparable Human Benchmarks Over 7+ Million Miles of Rider\u0002Only Driving,” Waypoint (blog), December 23, 2023, https://\nwaymo.com/blog/2023/12/waymo-significantly-outperforms\n-comparable-human-benchmarks-over-7-million/.\n17. Troup Howard, Mengqi Wang, and Dayin Zhang, “How Do \nLabor Shortages Affect Residential Construction and Housing \nAffordability?,” Haas School of Business, University of California \nBerkeley, April 2023, https://www.haas.berkeley.edu/wp-content\n/uploads/Howard_Wang_Zhang_Housing-Supply-and-Construction\n-Labor-Dayin-Zhang.pdf.\n18. Construction Robotics, “SAM,” accessed September 16, 2024, \nhttps://www.construction-robotics.com/sam-2/.\n19. Bladebug, “Home,” accessed September 16, 2024, https://\nbladebug.co.uk/.\n20. Contrary Research, “AMP Robotics,” last modified October 5, \n2023, https://research.contrary.com/company/amp-robotics.\n21. Assembly, “US Ranks 10th in Robot Density,” January 17, 2024, \nhttps://www.assemblymag.com/articles/98270-us-ranks-10th-in\n-robot-density.\nSTANFORD EXPERT CONTRIBUTORS\nDr. Allison Okamura\nSETR Faculty Council and Richard W. Weiland \nProfessor in the School of Engineering\nDr. Steve Collins\nAssociate Professor of Mechanical Engineering and, \nby courtesy, of Bioengineering\nDr. Mykel Kochenderfer\nAssociate Professor of Aeronautics and Astronautics \nand, by courtesy, of Computer Science\nDr. Jiajun Wu\nAssistant Professor of Computer Science and, by \ncourtesy, of Psychology\nDr. Karen Liu\nProfessor of Computer Science\nDr. Cosima du Pasquier\nSETR Fellow and Postdoctoral Scholar in Mechanical \nEngineering\nDr. Alaa Eldin Abdelaal\nSETR Fellow and Postdoctoral Scholar in Mechanical \nEngineering\nBrian Vuong\nSETR Fellow and PhD Student in Mechanical \nEngineering\n\n115\nOverview\nSemiconductors, often in the form of microchips, \nare crucial components used in everyday physical \ndevices, from smartphones and toasters to cars and \nlawn mowers. Chips also control heating and cool\u0002ing systems, elevators, and fire alarms in modern \nbuildings. Traffic lights are controlled by chips. On \nfarms, tractors and irrigation systems are controlled \nby chips. Modern militaries could not function with\u0002out chips in their weapons, navigation devices, and \ncockpit life-support systems in fighter jets. The list \ngoes on and on—in every aspect of modern life, \nchips are essential.\nAll chips are involved in the handling of information. \nDifferent types of them are specialized for different \ntasks. Some are processor chips that ingest data, \nperform computations on the data, and output the \nresults of those computations. Memory chips store \ninformation and are used with processors. Still other \nchips act as interfaces between digital computations \nKEY TAKEAWAYS\n° The growing demand for artificial intelligence and \nmachine learning is driving innovations in chip \nfabrication that are essential for enhancing com\u0002putational power and managing energy efficiency.\n° Advances in memory technologies and high\u0002bandwidth interconnects, including photonic \nlinks, are critical for meeting the increasing data \nneeds of modern applications.\n° Even if quantum computing advancements are \nrealized, the United States will still need compre\u0002hensive innovation across the technology stack \nto continue to scale the power of information \ntechnology.\nSEMICONDUCTORS\n08\n116 STANFORD EMERGING TECHNOLOGY REVIEW\ndesigners to create chips that do more complex pro\u0002cessing (see figure 8.1), although the cost of design\u0002ing them also increases with their complexity. \nRecently, however, the energy costs associated with \nthe hardware that holds information on a chip have \nbeen falling more slowly, and the cost of manufac\u0002turing per unit area has increased. This means that \nthe cost and energy advantages of scaling have \nnearly stopped. As a result, researchers have been \ninvestigating other ways to improve computer \ntechnology and to deal with the problem of high \ndesign costs. \nSince the best technologies for performing differ\u0002ent chip functions are themselves different, systems \nstill need to use different chips for those functions. \nFinding new ways to manage the inefficiency of \nand the physical world. In all these cases, some \namount of energy is needed to represent each bit of \ninformation inside a chip. The magic of chips is that \nit takes several orders of magnitude less energy to \nrepresent information inside one than it takes to do \nso outside it (e.g., in wires leading to and from the \nchip). This means that, in a multi-chip system, much \nmore energy and chip space are required for data \nmoving between chips than for data that remains on \na chip; this is one of the driving forces to integrate \nmore functions on single chips. \nAs chip fabrication technologies improve, it takes \nless energy and chip space to represent a given bit of \ninformation; hence, processing those bits becomes \nmore energy efficient. This phenomenon is what has \nenabled the semiconductor industry to pack more \nprocessing power on chips over time—it enables \nFIGURE 8.1 Increased energy efficiency has allowed designers to create more complex chips\nSource: Wikimedia Commons. Used under CC BY-SA 4\n08 Semic\nFabrication also entails a significant degree of pro\u0002cess engineering to continue to improve process \ntechnology and to achieve stringent manufacturing \nstandards. For example, the “clean rooms” in which \nchips are made require air that is one thousand times \nmore particle-free than the air in a hospital operat\u0002ing room.1\nBecause chip design and chip fabrication are so dif\u0002ferent in character, only a few companies, such as \nIntel, do both. Many businesses specialize in design, \nincluding Qualcomm, Broadcom, Apple, and Nvidia. \nSuch companies are called “fabless” in recognition \nof the fact that they do the design work and out\u0002source fabrication to others—a strategy based on \nthe theory that the former activity has higher profit \nmargins compared to the latter.\nToday, “others” usually means one company: Taiwan \nSemiconductor Manufacturing Company (TSMC), \nwhich is by far the world’s largest contract chip\u0002manufacturing company. In 2024, TSMC controlled \nover 60 percent of the world’s contract semicon\u0002ductor manufacturing and 90 percent of the world’s \nadvanced chip manufacturing.2 Samsung, in South \ninformation movement in and among chips, along \nwith the issue of high design costs, is a central focus \nof research on semiconductors. Further improve\u0002ment will take the form of innovation in design, \nmaterials, and integration methods.\nTwo aspects of chips are important for the purposes \nof this report. They must be designed and then fab\u0002ricated (i.e., manufactured), and each function calls \nfor different skill sets. Chip design is primarily an \nintellectual task that requires tools and teams able \nto create and test systems containing billions of \ncomponents. Fabrication is primarily a physical effort \nthat requires large factories, or fab facilities, that can \nproduce chips by the millions and billions—and can \ncost billions of dollars and take several years to build \nfrom the ground up (see figure 8.2).\nFabrication integrates many complex processes to \nproduce chips. Each one requires substantial exper\u0002tise to master and operate, and the integration of \nall of them requires still further expertise. For these \nreasons, modern fabrication plants are operated \nby workforces with a substantial number of people \ntrained in engineering.\nSource: SkyWater Technology\nFIGURE 8.2 Chip fabrication requires large factories that can produce chips at scale\n118 STANFORD EMERGING TECHNOLOGY REVIEW\nmoney than before or build a more powerful system \nfor the same cost. This scaling has been so consistent \nthat it is widely believed that the cost of computing \nwill always decrease with time. This expectation is \nso pervasive that in almost all fields of work, people \nare developing more complex algorithms to achieve \nbetter results while relying on Moore’s law to rescue \nthem from the consequences of that additional \ncomplexity.\nBut the future will not look like the past. As the com\u0002plexity of chips increases, the traditional cost-benefit \nrelationships associated with Moore’s law scaling are \ndiminishing, leading to rising costs in chip manu\u0002facturing. As figure 8.3 shows, the actual cost of a \nchip per transistor (represented by the solid red line) \nwas tracking the cost predicted by Moore’s law (the \ndashed blue line) relatively well from 2004 to 2012.6\nHowever, the actual cost per transistor started to \nlevel off around 2012—and it has not kept up with \nMoore’s law predictions since then.7\nAgainst this backdrop, the past year has witnessed \nsignificant advancements and challenges in the semi\u0002conductor industry. For example, increasing demand \nfor computing power driven by artificial intelligence \n(AI) and machine learning (ML) applications (as dis\u0002cussed in chapter 1 on artificial intelligence) has \nled to a surge in the development of, and demand \nfor, advanced graphical processing units (GPUs), \ncreating a strain on both production and energy \nresources. Advanced GPU systems, such as Nvidia’s \nGB200 NVL72 system, are pushing the boundaries \nof what is possible in terms of performance and \npower consumption. This surge in demand under\u0002scores the importance of finding innovative solutions \nto meet computational needs without compromising \non energy efficiency or physical space.\nThe rapid expansion of AI applications is also driving \ndemand for more specialized hardware. Traditional \nprocessors are no longer sufficient for the intensive \ncomputational tasks required by modern AI algo\u0002rithms. As a result, there has been a significant invest\u0002ment in developing GPUs and other specialized \nKorea, is a distant second, with around 13 per\u0002cent of the world’s chip manufacturing.3 United \nMicroelectronics Corporation, also based in Taiwan, \nranks third at about 6 percent.4\nBy contrast, US chip-manufacturing capacity has lost \nsignificant ground. Fabrication plants in America \naccounted for 37  percent of global production in \n1990, but their share dropped to just 12 percent by \n2021.5 Industry concentration, low US production \ncapacity, and geopolitical concerns about China’s \nintentions toward Taiwan mean the global supply \nchain for chips will remain fragile for the foreseeable \nfuture, despite the passage of the Creating Helpful \nIncentives to Produce Semiconductors (CHIPS) and \nScience Act of 2022 (discussed in more detail later \nin this chapter).\nKey Developments\nMoore’s Law, Past and Future\nFor over half a century, information technology has \nbeen driven by improvements in the chip fabrication \nprocess. In 1965, Intel cofounder Gordon Moore \nobserved that the cost of fabricating a transistor was \ndropping exponentially with time—an observation \nthat has come to be known as Moore’s law. It’s not a \nlaw of physics but rather a statement about the opti\u0002mal rate at which economic value can be extracted \nfrom improvements in the chip fabrication process. \nAlthough Moore’s law is often stated as the number \nof transistors on a chip doubling every few years, his\u0002torically the cost of making a chip was mostly inde\u0002pendent of the components on it. This has meant \nthat every few years, a chip whose size and cost \nremains approximately the same will have twice the \nnumber of transistors on it. \nMoore’s law scaling (i.e., the exponential increasing \nof the number of transistors on a chip) meant that \neach year one could build last year’s devices for less \n08 Semic\nperformance while reducing the need for full inte\u0002gration on a single chip.\nFrom an economic perspective, the chiplet approach \noffers significant advantages. Semiconductor com\u0002panies can create a set of building-block chiplets \nthat can be combined in various ways, allowing for \na wide range of products with different performance \ncharacteristics. This strategy allows companies to \nbetter tailor their products to specific application \ndomains and more effectively monetize their silicon \ninvestments, ultimately leading to increased product \ndiversity and market responsiveness.\nGiven the changing economics of scaling, the use \nof chiplets reduces costs overall and allows for more \ncustomized solutions. Semiconductor company \nAMD’s approach of keeping components that trans\u0002fer data to and from devices in older technology \nnodes while advancing core computing resources \nwith the latest processes exemplifies this strategy. \nMoreover, this modular strategy facilitates the inte\u0002gration of emerging technologies such as photon\u0002ics (discussed in greater detail later in this chapter), \nwhich can significantly enhance communication \nspeeds and bandwidth within and between chips. \nprocessors designed specifically for AI workloads. \nThis shift is reshaping the semiconductor industry, \nemphasizing the need for high-performance, energy\u0002efficient computing solutions.\nChiplets and 2.5-D Integration \nChiplets and 2.5-D (2.5-dimensional) integration rep\u0002resent a significant shift from traditional monolithic \nchip design. Chiplets are smaller functional blocks of \nsilicon that can be combined to create complete sys\u0002tems, offering greater flexibility and customization. \nBy enabling the use of different manufacturing tech\u0002nologies for various components, they facilitate \nthe integration of high-density memory with high\u0002performance processing units, resulting in improved \nsystem bandwidth and performance.\nCentral to 2.5-D integration, which puts chiplets \nnext to one another in an integrated circuit, is the \ninterposer—a specialized substrate facilitating com\u0002munication between the chiplets. This technology \nenables more energy-efficient data transfer com\u0002pared to traditional circuit board wiring. By allowing \noptimized memory, compute, and communication \nchips to reside side by side, it enhances system \nSource: Adapted from Steve Mollenkopf, “Our Future Is Mobile: Accelerating Innovation After Moore’s Law,” presentation at \nthe Electronics Resurgence Initiative Summit, Detroit, MI, July 15–17, 2019\nActual cost per transistor (normalized) Cost predicted by Moore‘s law\nCost per transistor\n(normalized)\n2004 2006 2008 2010 2012 2014 2016 2018 2020 2022\n1\n0.8\n1.2\n0.6\n0.4\n0.2\n0\nFIGURE 8.3 Cost per transistor over time\n120 STANFORD EMERGING TECHNOLOGY REVIEW\nten times more powerful than a consumer one, \ndissipating a kilowatt of power each. This results in \na pod that dissipates 10 kilowatts (kW), and a full \nstructure of 256 GPUs dissipates over a third of a \nmegawatt. (For comparison, a 2,500-square-foot \nhouse might require a furnace that generates about \n20 kW of heat.) Thus, thermal management stands \nout as a critical issue. Effective thermal-management \nsolutions, such as advanced cooling techniques and \nmaterials with high thermal conductivity, will be \nessential to maintaining performance and reliability \nin high-performance computing systems.\nPhotonic Links and Components\nIt is becoming difficult to scale electrical data\u0002transmission links and their associated bandwidth. \nInnovations such as silicon photonics are emerging \nas potential solutions, offering the promise of higher \nbandwidth and lower-power consumption by using \nlight to transmit data. Photonics are the optical ana\u0002logue of electronics—the latter use electrons for sig\u0002naling and carrying information while photonics use \nphotons (light) for the same purposes.\nCompared to traditional electrical interconnects, \nphotonic links have the potential to reduce energy \nconsumption and increase bandwidth in data centers \nand long-distance data transmissions.8 Furthermore, \nthey can handle different wavelengths on a single \noptical fiber simultaneously, thereby enhancing \ndata-carrying capacity and making photonics an \nattractive solution for high-performance computing \nand data center applications. By replacing electrical \ninterconnects with optical ones, data centers can \nreduce the amount of energy required for transmit\u0002ting data, leading to lower operational costs and a \nsmaller environmental footprint. These advantages \nof photonics have always been drivers of research in \nthis area, but the recent rise in the demand for power\u0002hungry AI-enabled applications has created even \nmore impetus for such research.\nIntegrating photonic components with silicon-based \ntechnologies is challenging as a result of material \nThree-Dimensional Heterogeneous \nIntegration\nThe research community is working on an even \nmore advanced technique: three-dimensional (3-D) \nheterogeneous integration. Unlike 2.5-D inte\u0002gration, where different chiplets are placed on a \ncommon substrate, 3-D heterogeneous integration \nis a semiconductor-manufacturing technique that \ninvolves the vertical stacking of different electronic \ncomponents, such as processors and memory. The \nheterogeneous aspect means that these stacked \ncomponents can be made from different materials \nand technologies optimized for their specific func\u0002tions. For example, a processor made from one type \nof material can be stacked with memory made from \nanother, each using the most suitable technology \nfor its purpose. This approach has the potential to \nimprove performance and efficiency by reducing the \ndistance data travels between components, making \ndevices faster and more compact—albeit at the cost \nof a more complex fabrication process and a harder \nheat-dissipation task for the resulting chips. \nNeed for High Bandwidth\nA second approach to enhancing performance \nfocuses on improving bandwidth and interconnect \ntechnologies. AI-training models must handle vast \namounts of data, and high-speed interconnects such \nas those employed in Nvidia’s H100 systems play a \ncritical role in facilitating the rapid transfer of data \nbetween compute units and memory. \nThe physical limitations of traditional electrical inter\u0002connects are one of the primary barriers to improv\u0002ing bandwidth. As data rates increase, so do power \nconsumption and heat generation, which can limit \nthe performance and reliability of semiconductor \ndevices. For example, Nvidia’s GPU modules achieve \nbandwidths on the order of several terabytes per \nsecond. The company’s NVLink enables the connec\u0002tion of up to 256 GPUs, forming a supercomputer \npod with immense computational power and band\u0002width. Each GPU in these setups is approximately \n08 Semic\nSimilarly, NAND flash memory—the most common \ntype of such memory—has faced scaling challenges \ndue to the limitations of traditional planar cell archi\u0002tectures. The development of 3-D NAND, which \ninvolves stacking multiple layers of memory cells \nvertically, has enabled continued increases in stor\u0002age density. However, this approach requires sophis\u0002ticated manufacturing processes to ensure the \nreliability and performance of the resulting memory \ndevices.\nEmerging memory technologies, such as magneto\u0002resistive random-access memory9\n and phase-change \nmemory,10 are also gaining traction in some appli\u0002cations. These technologies offer unique advan\u0002tages in terms of speed, endurance, and energy \nefficiency, making them attractive alternatives to tra\u0002ditional embedded nonvolatile memory solutions. \n(Nonvolatile memory retains its contents even when \npower is turned off.)\nFurther innovations in memory technology are \ncritical for enabling the continued growth of data\u0002intensive applications. From AI-training models to \ncloud computing and big data analytics, modern \napplications require vast amounts of memory to \nstore and process data efficiently. \nQuantum Computing Advancements\nQuantum computing remains a field of intense \nresearch and development, with significant progress \nmade in both the number and quality of quantum \nbits, or qubits. Recent innovations in error correction \nand the potential for practical quantum computing \ncould revolutionize specific applications,11 although \ncommercial viability remains years away. The prom\u0002ise of quantum computing lies in its potential to per\u0002form certain complex calculations at unprecedented \nspeeds, with possible relevance for applications \nsuch as cryptography, materials science, and com\u0002plex system simulations.\nQuantum computing offers a fundamentally differ\u0002ent approach to computation compared to classical \nincompatibilities; for example, efficient light-emitting \nmaterials like III–V semiconductors do not integrate \nwell with silicon. (A III–V semiconductor is made by \ncombining boron, aluminum, gallium, or indium with \nnitrogen, phosphorus, arsenic, or antimony.) While \nuseful for light detectors, silicon is ineffective for \nlight emission, complicating the scalable integration \nof these technologies at the chip or circuit board \nlevel. Overcoming these challenges is crucial for \nrealizing the full potential of photonic links in large\u0002scale, low-energy applications. \nMemory Technology Developments\nMemory technology continues to evolve, with \ninnovations in both stacking and new materials. \nTechniques such as stacking multiple layers of flash \nmemory are pushing the boundaries of what is possi\u0002ble, enabling higher density and better performance. \nThese advancements are crucial for supporting the \ngrowing data needs of modern applications, from AI \nto big data analytics.\nDynamic random-access memory (DRAM) and \nflash memory technologies have both seen signif\u0002icant advancements in recent years, but the asso\u0002ciated increase in manufacturing cost means there \nhas been only modest improvement in cost per bit. \nThe development of 3-D structures, such as vertical \nDRAM transistors, has allowed for continued scal\u0002ing of memory density by overcoming the physical \nlimitations of traditional planar structures and has \nenabled the production of memory devices with \nhigher capacity and improved performance.\nAs memory technologies scale, maintaining per\u0002formance and reliability becomes increasingly \nchallenging. For DRAM, issues such as leakage \ncurrents and quantum effects limit the scalability \nof capacitors and transistors. To address these \nchallenges, researchers have developed advanced \nmanufacturing techniques for the creation of com\u0002plex 3-D structures that enable increased storage \ndensity while maintaining the required electrical \ncharacteristics.\n122 STANFORD EMERGING TECHNOLOGY REVIEW\nRecent work has improved the fidelity of a modest \nnumber (around thirty) of qubits and has reduced the \noverhead of quantum error correction. But far larger \nnumbers of high-quality qubits (two or three orders \nof magnitude more) will be needed for quantum \ncomputers to become more broadly useful. A large \nnumber of companies and research labs have been \npushing forward with work on this area. Progress \non the quantum algorithm front is harder to track. \nWhile many groups are working on finding prac\u0002tical  applications for early quantum computers, no \nsuch applications have yet been publicized.\nOver the Horizon\nThe Impact of Technology\nThe semiconductor industry is poised for significant \nadvancements in coming years, driven by the growing \ndemands of AI, especially ML, and high-performance \ncomputing. The introduction of new technologies, \nsuch as 2.5-D integration, chiplets, and photonic \ninterconnects, is expected to play a crucial role in \nmeeting these demands. These innovations will \nenhance performance, increase bandwidth, and \ncomputing. Classical computers use individual bits \nas the smallest unit of data, with each being 0 or 1. \nIn contrast, the qubits in quantum computers, like \nthe one shown in figure 8.4, can be in multiple states \nsimultaneously due to the principles of quantum \nmechanics. This property, known as superposition, \nallows quantum computers to process a vast number \nof possibilities at once, a phenomenon called quan\u0002tum parallelism. \nHowever, quantum computing also presents signif\u0002icant challenges. Using it to implement an applica\u0002tion is hard because reading the result of a quantum \ncomputation generally yields a result correspond\u0002ing to only one of the many results possible when \nsuch a computation is performed on a superposition \nstate. Realizing the advantage of a quantum com\u0002putation means reading the correct result, which in \nturn requires designing computational algorithms so \nthat the correct result is the one most likely to show \nup. Moreover, quantum computing operations are \nanalog rather than digital in nature and thus can be \ndisrupted by “noise” in the environment, such as \nvibrations and fluctuations in temperature. \nRecent advancements in algorithms and error\u0002correcting codes that can compensate for such \nerrors have reduced the overhead associated with \nerror correction. Still, many further advancements \nin error correction will be necessary before quantum \ncomputing can be more widely applied. Creating a \nuseful quantum computer necessitates innovation \nacross the entire quantum software and hardware \nstack: algorithms, compilers, control electronics, error \ncorrection, and quantum hardware.\nVarious technologies are under consideration for the \nphysical construction of qubits, including trapped \nion, superconducting, cold atom, photonic, crys\u0002tal defect, quantum dot, and topological technol\u0002ogies.12 The most advanced quantum computing \nmachines currently use trapped ion or supercon\u0002ducting qubits. However, neither technology has a \nclear path to scaling up to larger machines, prompt\u0002ing ongoing research into other approaches.\nSource: IBM. Used under CC BY-ND 2.0\nFIGURE 8.4  A close-up view of a quantum computer\n08 Semic\nfor radical innovation conflicts with the high costs \nand long timelines of chip development, which can \nexceed $100 million and take over two years. \nTo address this, the industry must make system\u0002design exploration easier, cheaper, and faster. \nResearchers are working to ensure that specific \ndesign changes to a chip do not require redesign \nof the entire chip. Solutions include enabling soft\u0002ware designers to test custom accelerators without \ndeep hardware knowledge and developing tools \nfor application developers to make small hard\u0002ware extensions to base platforms. This approach, \ndescribed in more detail in the inaugural Stanford \nEmerging Technology Review (2023), depends on \nthe involvement of major technology firms, who \nwould need to participate in an app store–like model \nfor hardware customization, balancing open innova\u0002tion with profit motives.\nCHALLENGES OF INNOVATION \nAND IMPLEMENTATION\nA critical challenge facing the US semiconductor \nindustry is its significant talent shortage, particularly \nin hardware design and manufacturing. For example, \nthe Semiconductor Industry Association projects the \nnumber of jobs in the sector in the United States \nwill grow by nearly 115,000 by 2030, to a total of \napproximately 460,000.13 Moreover, it estimates that \nroughly 67,000, or 58 percent, of these new jobs \nrisk going unfilled at current degree-completion \nrates. Looking at just the new jobs that are technical \nimprove energy efficiency, addressing the limitations \nof traditional semiconductor designs.\nQuantum computing remains an important area of \ndevelopment, with progress in error correction and \nqubit quality being made. However, its timeline \nremains uncertain. Even if it becomes successful, it \nwill likely be useful for only a limited class of appli\u0002cations and won’t replace today’s semiconductor \ntechnology. Quantum computers will complement, \nrather than supplant, classical semiconductors, \naddressing specific complex problems in fields such \nas cryptography, materials science, and complex \nsimulations.\nEmerging memory technologies and advanced \nmanufacturing techniques are also critical for the \nindustry’s growth. Innovations in 3-D memory stack\u0002ing and integration with processors will improve \ndata-transfer speeds and reduce latency, meeting \nthe increasing data requirements of modern applica\u0002tions. The development of advanced materials and \ntransistor architectures will further push the bound\u0002aries of semiconductor capabilities, enabling contin\u0002ued miniaturization and enhanced performance.\nFinally, as Moore’s law reaches its limits, future \nimprovements in computing will rely more on opti\u0002mizing algorithms, hardware, and technologies for \nspecific applications rather than on general technol\u0002ogy scaling. This requires innovation across the entire \ntechnology stack, from materials to design methods. \nHowever, the industry faces a paradox: The need \nA critical challenge facing the US semiconductor \nindustry is its significant talent shortage, particularly \nin hardware design and manufacturing. \n124 STANFORD EMERGING TECHNOLOGY REVIEW\nplus significant tax credits for private investment \nin the field. Full implementation of the act has not \nyet occurred, partly because not enough time has \nelapsed and partly because the appropriations it \ncalled for have not been fully funded.\nin nature, the percentage at risk of going unfilled \nis higher, at 80 percent. Almost two-thirds of the \nunfilled jobs would require at least a bachelor’s \ndegree in engineering.14\nThe pipeline of college graduates interested in \nsemiconductors is also troubling. Student inter\u0002est in hardware has diminished as graduating stu\u0002dents flock to software companies.15 Several factors \nappear to play a role, including the perception of \nhigher salaries in software development and a lack \nof awareness about the diverse and exciting oppor\u0002tunities in hardware. \nSince appropriately trained people are the only real \nsource of new ideas, this trend does not bode well \nfor the industry. Addressing this issue requires more \nand even closer collaboration among educational \ninstitutions, industry, and government to develop \nprograms that attract and train the next generation \nof semiconductor engineers and researchers. \nPolicy, Legal, and Regulatory Issues\nSupply chain resilience Building a resilient and \ndiversified supply chain is critical for mitigating geo\u0002political risks. Investing in domestic manufacturing \ncapabilities and promoting regional cooperation will \nenhance supply chain security and ensure a steady \nsupply of essential components. \nGeopolitical risks The extreme concentration of \nsemiconductor manufacturing in Taiwan poses a \nsignificant risk to the global supply chain. Political \ntensions, trade disputes, and potential conflict in \nthe region can disrupt the supply of critical compo\u0002nents, impacting industries and economies world\u0002wide. Diversifying supply chains and investing in \ndomestic manufacturing capabilities are essential \nstrategies for building resilience against geopolitical \nrisks. Initial steps toward this were taken in the pas\u0002sage of the CHIPS and Science Act of 2022, which \nearmarked $52.7 billion for semiconductor manu\u0002facturing, research, and workforce development, \nNOTES\n1. Intel Corporation, “Inside an Intel Chip Fab: One of the \nCleanest Conference Rooms on Earth,” March 28, 2018, \nhttps://www.intc.com/news-events/press-releases/detail/165\n/inside-an-intel-chip-fab-one-of-the-cleanest-conference.\n2. Jeremy Bowman, “This 1 Number May Ensure TSMC’s Market \nDominance,” The Motley Fool, August 17, 2024, https://www.nasdaq\n.com/articles/1-number-may-ensure-tsmcs-market-dominance.\n3. Counterpoint, “Global Semiconductor Foundry Revenue Share: \nQ1 2024,” June 12, 2024, https://www.counterpointresearch.com\n/insights/global-semiconductor-foundry-market-share/.\n4. Counterpoint, “Global Semiconductor Foundry Revenue Share.”\n5. Semiconductor Industry Association, 2021 State of the U.S. \nSemiconductor Industry, 2021, https://www.semiconductors.org\n/wp-content/uploads/2021/09/2021-SIA-State-of-the-Industry\n-Report.pdf. \n6. Electronics Resurgence Initiative 2.0, “2019 Summit Agenda, \nVideos, and Slides,” Defense Advanced Research Projects \nAgency, accessed August 30, 2023, https://eri-summit.darpa.mil\n/2019-archive-keynote-slides.\n7. In fact, transistor costs are usually plotted in a semi-log graph, \nwhere the log of the cost is plotted against time. In these plots the \nexponential decline in transistor costs becomes a straight line. The \nfact that the scale of the y-axis is linear is a clear indication that \nMoore’s law is over.\n8. David A. B. Miller, “Attojoule Optoelectronics for Low-Energy \nInformation Processing and Communications,” Journal of Light\u0002wave Technology 35, no. 4 (February 2017): 34696, https://doi\n.org/10.1109/JLT.2017.2647779.\n9. Paolo Cappaletti and Jon Slaughter, “Embedded Memory Solu\u0002tions: Charge Storage Based, Resistive and Magnetic,” in Andrea \nRedaelli and Fabio Pellizzer, eds., Semiconductor Memories and \nSystems (Cambridge, MA: Woodhead Publishing, 2022): 159–215, \nhttps://doi.org/10.1016/B978-0-12-820758-1.00007-8. \n10. Cappaletti and Slaughter, “Embedded Memory Solutions.” \n11. Ziqian Li, Tanay Roy, David Rodríguez Pérez, et al., “Autono\u0002mous Error Correction of a Single Logical Qubit Using Two Trans\u0002mons,” Nature Communications 15, no. 1681 (February 2024), \nhttps://doi.org/10.1038/s41467-024-45858-z.\n12. An overview of these technologies can be found in Eunmi \nChae, Joonhee Choi, and Junki Kim, “An Elementary Review on \nBasic Principles and Developments of Qubits for Quantum Com\u0002puting,” Nano Convergence 11, no. 11 (March 2024), https://doi\n.org/10.1186/s40580-024-00418-5.\n13. Dan Martin and Dan Rosso, “Chipping Away: Assessing and \nAddressing the Labor Market Gap Facing the U.S. Semiconduc\u0002tor Industry,” Semiconductor Industry Association and Oxford \n08 Semic\nSTANFORD EXPERT CONTRIBUTORS\nDr. Mark A. Horowitz\nSETR Faculty Council, Fortinet Founders Chair of \nthe Department of Electrical Engineering, Yahoo! \nFounders Professor in the School of Engineering, \nand Professor of Computer Science\nDr. David Miller\nW.  M. Keck Foundation Professor of Electrical \nEngineering and Professor, by courtesy, of Applied \nPhysics\nDr. David Schuster\nProfessor of Applied Physics\nDr. Asir Intisar Khan\nSETR Fellow and Visiting Postdoctoral Scholar in \nElectrical Engineering\nEconomics, July 25, 2023, https://www.semiconductors.org\n/chipping-away-assessing-and-addressing-the-labor-market-gap\n-facing-the-u-s-semiconductor-industry/.\n14. Martin and Rosso, “Chipping Away.” \n15. Tom Dillinger, “A Crisis in Engineering Education—Where Are \nthe Microelectronic Engineers?,” SemiWiki, July 3, 2022, https://\nsemiwiki.com/events/314964-a-crisis-in-engineering-education\n-where-are-the-microelectronics-engineers.\n",
    "length": 373754,
    "domain_restricted": true,
    "university": "Stanford University",
    "domain": "stanford.edu"
  }
]