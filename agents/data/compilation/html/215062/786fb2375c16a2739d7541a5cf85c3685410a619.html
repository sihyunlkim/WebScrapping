<!DOCTYPE html>
<!--[if IE 8 ]>
	<html class="no-js ie8" lang="en-US">
<![endif]-->
<!--[if IE 9 ]>
	<html class="no-js ie9" lang="en-US">
<![endif]-->
<!--[if gt IE 9]><!-->
<html lang="en-US"><!--<![endif]-->
	<head>
		    <!-- Google Tag Manager - The Wharton School-->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-MZQZFC4');</script>
    <!-- End Google Tag Manager -->
    <!-- Google Tag Manager - Executive Education-->
            		<meta charset="UTF-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<link rel="profile" href="http://gmpg.org/xfn/11">
		<link rel="pingback" href="https://ai.wharton.upenn.edu/xmlrpc.php">
		<link rel="icon" type="image/x-icon" href="https://ai.wharton.upenn.edu/wp-content/plugins/martech-chupacabra/includes/images/favicon.ico" /><link rel="apple-touch-icon" sizes="180x180" href="https://ai.wharton.upenn.edu/wp-content/plugins/martech-chupacabra/includes/images/apple-touch-icon.png" /><link rel="mask-icon" color="#004785" href="https://ai.wharton.upenn.edu/wp-content/plugins/martech-chupacabra/includes/images/penn-logo-mask.svg" /><script type="text/javascript">window.ajaxurl = "https://ai.wharton.upenn.edu/wp-admin/admin-ajax.php"</script>
		<meta name='robots' content='index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1' />
	<style>img:is([sizes="auto" i], [sizes^="auto," i]) { contain-intrinsic-size: 3000px 1500px }</style>
	
		
	<!-- This site is optimized with the Yoast SEO Premium plugin v26.0 (Yoast SEO v26.0) - https://yoast.com/wordpress/plugins/seo/ -->
	<title>Artificial Intelligence Risk &amp; Governance - Wharton Human-AI Research</title>
	<link rel="canonical" href="https://ai.wharton.upenn.edu/white-paper/artificial-intelligence-risk-governance/" />
	<meta property="og:locale" content="en_US" />
	<meta property="og:type" content="article" />
	<meta property="og:title" content="Artificial Intelligence Risk &amp; Governance" />
	<meta property="og:url" content="https://ai.wharton.upenn.edu/white-paper/artificial-intelligence-risk-governance/" />
	<meta property="og:site_name" content="Wharton Human-AI Research" />
	<meta property="article:published_time" content="2023-01-11T21:48:16+00:00" />
	<meta property="article:modified_time" content="2024-03-18T14:14:18+00:00" />
	<meta property="og:image" content="https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/ai-governance.jpeg" />
	<meta name="author" content="Jillian Rogers" />
	<meta name="twitter:card" content="summary_large_image" />
	<meta name="twitter:image" content="https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/ai-governance.jpeg" />
	<meta name="twitter:label1" content="Written by" />
	<meta name="twitter:data1" content="Jillian Rogers" />
	<meta name="twitter:label2" content="Est. reading time" />
	<meta name="twitter:data2" content="36 minutes" />
	<script type="application/ld+json" class="yoast-schema-graph">{"@context":"https://schema.org","@graph":[{"@type":"WebPage","@id":"https://ai.wharton.upenn.edu/white-paper/artificial-intelligence-risk-governance/","url":"https://ai.wharton.upenn.edu/white-paper/artificial-intelligence-risk-governance/","name":"Artificial Intelligence Risk & Governance - Wharton Human-AI Research","isPartOf":{"@id":"https://ai.wharton.upenn.edu/#website"},"datePublished":"2023-01-11T21:48:16+00:00","dateModified":"2024-03-18T14:14:18+00:00","author":{"@id":"https://ai.wharton.upenn.edu/#/schema/person/b26964e8c9ace7b095ecb769c2c85685"},"breadcrumb":{"@id":"https://ai.wharton.upenn.edu/white-paper/artificial-intelligence-risk-governance/#breadcrumb"},"inLanguage":"en-US","potentialAction":[{"@type":"ReadAction","target":["https://ai.wharton.upenn.edu/white-paper/artificial-intelligence-risk-governance/"]}]},{"@type":"BreadcrumbList","@id":"https://ai.wharton.upenn.edu/white-paper/artificial-intelligence-risk-governance/#breadcrumb","itemListElement":[]},{"@type":"WebSite","@id":"https://ai.wharton.upenn.edu/#website","url":"https://ai.wharton.upenn.edu/","name":"Wharton Human-AI Research","description":"","potentialAction":[{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https://ai.wharton.upenn.edu/?s={search_term_string}"},"query-input":{"@type":"PropertyValueSpecification","valueRequired":true,"valueName":"search_term_string"}}],"inLanguage":"en-US"},{"@type":"Person","@id":"https://ai.wharton.upenn.edu/#/schema/person/b26964e8c9ace7b095ecb769c2c85685","name":"Jillian Rogers"}]}</script>
	<!-- / Yoast SEO Premium plugin. -->


<link rel='dns-prefetch' href='//ai.wharton.upenn.edu' />
<link rel="alternate" type="application/rss+xml" title="Wharton Human-AI Research &raquo; Feed" href="https://ai.wharton.upenn.edu/feed/" />
<link rel="alternate" type="application/rss+xml" title="Wharton Human-AI Research &raquo; Comments Feed" href="https://ai.wharton.upenn.edu/comments/feed/" />
<script type="text/javascript">
/* <![CDATA[ */
window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/16.0.1\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/16.0.1\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/ai.wharton.upenn.edu\/wp-includes\/js\/wp-emoji-release.min.js?ver=6.8.3"}};
/*! This file is auto-generated */
!function(s,n){var o,i,e;function c(e){try{var t={supportTests:e,timestamp:(new Date).valueOf()};sessionStorage.setItem(o,JSON.stringify(t))}catch(e){}}function p(e,t,n){e.clearRect(0,0,e.canvas.width,e.canvas.height),e.fillText(t,0,0);var t=new Uint32Array(e.getImageData(0,0,e.canvas.width,e.canvas.height).data),a=(e.clearRect(0,0,e.canvas.width,e.canvas.height),e.fillText(n,0,0),new Uint32Array(e.getImageData(0,0,e.canvas.width,e.canvas.height).data));return t.every(function(e,t){return e===a[t]})}function u(e,t){e.clearRect(0,0,e.canvas.width,e.canvas.height),e.fillText(t,0,0);for(var n=e.getImageData(16,16,1,1),a=0;a<n.data.length;a++)if(0!==n.data[a])return!1;return!0}function f(e,t,n,a){switch(t){case"flag":return n(e,"\ud83c\udff3\ufe0f\u200d\u26a7\ufe0f","\ud83c\udff3\ufe0f\u200b\u26a7\ufe0f")?!1:!n(e,"\ud83c\udde8\ud83c\uddf6","\ud83c\udde8\u200b\ud83c\uddf6")&&!n(e,"\ud83c\udff4\udb40\udc67\udb40\udc62\udb40\udc65\udb40\udc6e\udb40\udc67\udb40\udc7f","\ud83c\udff4\u200b\udb40\udc67\u200b\udb40\udc62\u200b\udb40\udc65\u200b\udb40\udc6e\u200b\udb40\udc67\u200b\udb40\udc7f");case"emoji":return!a(e,"\ud83e\udedf")}return!1}function g(e,t,n,a){var r="undefined"!=typeof WorkerGlobalScope&&self instanceof WorkerGlobalScope?new OffscreenCanvas(300,150):s.createElement("canvas"),o=r.getContext("2d",{willReadFrequently:!0}),i=(o.textBaseline="top",o.font="600 32px Arial",{});return e.forEach(function(e){i[e]=t(o,e,n,a)}),i}function t(e){var t=s.createElement("script");t.src=e,t.defer=!0,s.head.appendChild(t)}"undefined"!=typeof Promise&&(o="wpEmojiSettingsSupports",i=["flag","emoji"],n.supports={everything:!0,everythingExceptFlag:!0},e=new Promise(function(e){s.addEventListener("DOMContentLoaded",e,{once:!0})}),new Promise(function(t){var n=function(){try{var e=JSON.parse(sessionStorage.getItem(o));if("object"==typeof e&&"number"==typeof e.timestamp&&(new Date).valueOf()<e.timestamp+604800&&"object"==typeof e.supportTests)return e.supportTests}catch(e){}return null}();if(!n){if("undefined"!=typeof Worker&&"undefined"!=typeof OffscreenCanvas&&"undefined"!=typeof URL&&URL.createObjectURL&&"undefined"!=typeof Blob)try{var e="postMessage("+g.toString()+"("+[JSON.stringify(i),f.toString(),p.toString(),u.toString()].join(",")+"));",a=new Blob([e],{type:"text/javascript"}),r=new Worker(URL.createObjectURL(a),{name:"wpTestEmojiSupports"});return void(r.onmessage=function(e){c(n=e.data),r.terminate(),t(n)})}catch(e){}c(n=g(i,f,p,u))}t(n)}).then(function(e){for(var t in e)n.supports[t]=e[t],n.supports.everything=n.supports.everything&&n.supports[t],"flag"!==t&&(n.supports.everythingExceptFlag=n.supports.everythingExceptFlag&&n.supports[t]);n.supports.everythingExceptFlag=n.supports.everythingExceptFlag&&!n.supports.flag,n.DOMReady=!1,n.readyCallback=function(){n.DOMReady=!0}}).then(function(){return e}).then(function(){var e;n.supports.everything||(n.readyCallback(),(e=n.source||{}).concatemoji?t(e.concatemoji):e.wpemoji&&e.twemoji&&(t(e.twemoji),t(e.wpemoji)))}))}((window,document),window._wpemojiSettings);
/* ]]> */
</script>
<style id='wp-emoji-styles-inline-css' type='text/css'>

	img.wp-smiley, img.emoji {
		display: inline !important;
		border: none !important;
		box-shadow: none !important;
		height: 1em !important;
		width: 1em !important;
		margin: 0 0.07em !important;
		vertical-align: -0.1em !important;
		background: none !important;
		padding: 0 !important;
	}
</style>
<link rel='stylesheet' id='wp-block-library-css' href='https://ai.wharton.upenn.edu/wp-includes/css/dist/block-library/style.min.css?ver=6.8.3' type='text/css' media='all' />
<style id='classic-theme-styles-inline-css' type='text/css'>
/*! This file is auto-generated */
.wp-block-button__link{color:#fff;background-color:#32373c;border-radius:9999px;box-shadow:none;text-decoration:none;padding:calc(.667em + 2px) calc(1.333em + 2px);font-size:1.125em}.wp-block-file__button{background:#32373c;color:#fff;text-decoration:none}
</style>
<style id='global-styles-inline-css' type='text/css'>
:root{--wp--preset--aspect-ratio--square: 1;--wp--preset--aspect-ratio--4-3: 4/3;--wp--preset--aspect-ratio--3-4: 3/4;--wp--preset--aspect-ratio--3-2: 3/2;--wp--preset--aspect-ratio--2-3: 2/3;--wp--preset--aspect-ratio--16-9: 16/9;--wp--preset--aspect-ratio--9-16: 9/16;--wp--preset--color--black: #000000;--wp--preset--color--cyan-bluish-gray: #abb8c3;--wp--preset--color--white: #ffffff;--wp--preset--color--pale-pink: #f78da7;--wp--preset--color--vivid-red: #cf2e2e;--wp--preset--color--luminous-vivid-orange: #ff6900;--wp--preset--color--luminous-vivid-amber: #fcb900;--wp--preset--color--light-green-cyan: #7bdcb5;--wp--preset--color--vivid-green-cyan: #00d084;--wp--preset--color--pale-cyan-blue: #8ed1fc;--wp--preset--color--vivid-cyan-blue: #0693e3;--wp--preset--color--vivid-purple: #9b51e0;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple: linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan: linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange: linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red: linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray: linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum: linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple: linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux: linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk: linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean: linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass: linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight: linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--font-size--small: 13px;--wp--preset--font-size--medium: 20px;--wp--preset--font-size--large: 36px;--wp--preset--font-size--x-large: 42px;--wp--preset--spacing--20: 0.44rem;--wp--preset--spacing--30: 0.67rem;--wp--preset--spacing--40: 1rem;--wp--preset--spacing--50: 1.5rem;--wp--preset--spacing--60: 2.25rem;--wp--preset--spacing--70: 3.38rem;--wp--preset--spacing--80: 5.06rem;--wp--preset--shadow--natural: 6px 6px 9px rgba(0, 0, 0, 0.2);--wp--preset--shadow--deep: 12px 12px 50px rgba(0, 0, 0, 0.4);--wp--preset--shadow--sharp: 6px 6px 0px rgba(0, 0, 0, 0.2);--wp--preset--shadow--outlined: 6px 6px 0px -3px rgba(255, 255, 255, 1), 6px 6px rgba(0, 0, 0, 1);--wp--preset--shadow--crisp: 6px 6px 0px rgba(0, 0, 0, 1);}:where(.is-layout-flex){gap: 0.5em;}:where(.is-layout-grid){gap: 0.5em;}body .is-layout-flex{display: flex;}.is-layout-flex{flex-wrap: wrap;align-items: center;}.is-layout-flex > :is(*, div){margin: 0;}body .is-layout-grid{display: grid;}.is-layout-grid > :is(*, div){margin: 0;}:where(.wp-block-columns.is-layout-flex){gap: 2em;}:where(.wp-block-columns.is-layout-grid){gap: 2em;}:where(.wp-block-post-template.is-layout-flex){gap: 1.25em;}:where(.wp-block-post-template.is-layout-grid){gap: 1.25em;}.has-black-color{color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-color{color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-color{color: var(--wp--preset--color--white) !important;}.has-pale-pink-color{color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-color{color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-color{color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-color{color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-color{color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-color{color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-color{color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-color{color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-color{color: var(--wp--preset--color--vivid-purple) !important;}.has-black-background-color{background-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-background-color{background-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-background-color{background-color: var(--wp--preset--color--white) !important;}.has-pale-pink-background-color{background-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-background-color{background-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-background-color{background-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-background-color{background-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-background-color{background-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-background-color{background-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-background-color{background-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-background-color{background-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-background-color{background-color: var(--wp--preset--color--vivid-purple) !important;}.has-black-border-color{border-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-border-color{border-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-border-color{border-color: var(--wp--preset--color--white) !important;}.has-pale-pink-border-color{border-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-border-color{border-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-border-color{border-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-border-color{border-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-border-color{border-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-border-color{border-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-border-color{border-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-border-color{border-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-border-color{border-color: var(--wp--preset--color--vivid-purple) !important;}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background: var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important;}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background: var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important;}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important;}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important;}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background: var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important;}.has-cool-to-warm-spectrum-gradient-background{background: var(--wp--preset--gradient--cool-to-warm-spectrum) !important;}.has-blush-light-purple-gradient-background{background: var(--wp--preset--gradient--blush-light-purple) !important;}.has-blush-bordeaux-gradient-background{background: var(--wp--preset--gradient--blush-bordeaux) !important;}.has-luminous-dusk-gradient-background{background: var(--wp--preset--gradient--luminous-dusk) !important;}.has-pale-ocean-gradient-background{background: var(--wp--preset--gradient--pale-ocean) !important;}.has-electric-grass-gradient-background{background: var(--wp--preset--gradient--electric-grass) !important;}.has-midnight-gradient-background{background: var(--wp--preset--gradient--midnight) !important;}.has-small-font-size{font-size: var(--wp--preset--font-size--small) !important;}.has-medium-font-size{font-size: var(--wp--preset--font-size--medium) !important;}.has-large-font-size{font-size: var(--wp--preset--font-size--large) !important;}.has-x-large-font-size{font-size: var(--wp--preset--font-size--x-large) !important;}
:where(.wp-block-post-template.is-layout-flex){gap: 1.25em;}:where(.wp-block-post-template.is-layout-grid){gap: 1.25em;}
:where(.wp-block-columns.is-layout-flex){gap: 2em;}:where(.wp-block-columns.is-layout-grid){gap: 2em;}
:root :where(.wp-block-pullquote){font-size: 1.5em;line-height: 1.6;}
</style>
<link rel='stylesheet' id='wpa-css-css' href='https://ai.wharton.upenn.edu/wp-content/plugins/honeypot/includes/css/wpa.css?ver=2.2.10' type='text/css' media='all' />
<link rel='stylesheet' id='responsive-mobile-style-css' href='https://ai.wharton.upenn.edu/wp-content/themes/responsive-mobile/css/style.css?ver=0.0.10' type='text/css' media='all' />
<link rel='stylesheet' id='vc_plugin_table_style_css-css' href='https://ai.wharton.upenn.edu/wp-content/plugins/easy-tables-vc/assets/css/style.min.css?ver=2.2.0' type='text/css' media='all' />
<link rel='stylesheet' id='vc_plugin_themes_css-css' href='https://ai.wharton.upenn.edu/wp-content/plugins/easy-tables-vc/assets/css/themes.min.css?ver=2.2.0' type='text/css' media='all' />
<link rel='stylesheet' id='js_composer_front-css' href='https://ai.wharton.upenn.edu/wp-content/plugins/js_composer/assets/css/js_composer.min.css?ver=8.7.1' type='text/css' media='all' />
<link rel='stylesheet' id='js_composer_custom_css-css' href='//ai.wharton.upenn.edu/wp-content/uploads/js_composer/custom.css?ver=8.7.1' type='text/css' media='all' />
<link rel='stylesheet' id='martech-chupacabra-style-css' href='https://ai.wharton.upenn.edu/wp-content/plugins/martech-chupacabra/includes/css/style-v2.css?ver=3.49.4' type='text/css' media='all' />
<link rel='stylesheet' id='martech-chupacabra-style-v3-css' href='https://ai.wharton.upenn.edu/wp-content/plugins/martech-chupacabra/includes/css/style.css?ver=3.49.4' type='text/css' media='all' />
<link rel='stylesheet' id='martech-chupacabra-style-print-css' href='https://ai.wharton.upenn.edu/wp-content/plugins/martech-chupacabra/includes/css/print.css?ver=3.49.4' type='text/css' media='print' />
<link rel='stylesheet' id='responsive-mobile-child-style-css' href='https://ai.wharton.upenn.edu/wp-content/themes/Chupe2-0-child/style.css?ver=1.0' type='text/css' media='all' />
<link rel='stylesheet' id='bsf-Defaults-css' href='https://ai.wharton.upenn.edu/wp-content/uploads/smile_fonts/Defaults/Defaults.css?ver=3.21.1' type='text/css' media='all' />
<script type="text/javascript" src="https://ai.wharton.upenn.edu/wp-includes/js/jquery/jquery.min.js?ver=3.7.1" id="jquery-core-js"></script>
<script type="text/javascript" src="https://ai.wharton.upenn.edu/wp-includes/js/jquery/jquery-migrate.min.js?ver=3.4.1" id="jquery-migrate-js"></script>
<script type="text/javascript" src="https://ai.wharton.upenn.edu/wp-content/plugins/martech-chupacabra/includes/lib/fast-average-color/browser.min.js?ver=3.49.4" id="fac-js-js"></script>
<script></script><link rel="https://api.w.org/" href="https://ai.wharton.upenn.edu/wp-json/" /><link rel="alternate" title="JSON" type="application/json" href="https://ai.wharton.upenn.edu/wp-json/wp/v2/posts/8443" /><link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://ai.wharton.upenn.edu/xmlrpc.php?rsd" />
<meta name="generator" content="WordPress 6.8.3" />
<link rel='shortlink' href='https://ai.wharton.upenn.edu/?p=8443' />
<link rel="alternate" title="oEmbed (JSON)" type="application/json+oembed" href="https://ai.wharton.upenn.edu/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fai.wharton.upenn.edu%2Fwhite-paper%2Fartificial-intelligence-risk-governance%2F" />
<link rel="alternate" title="oEmbed (XML)" type="text/xml+oembed" href="https://ai.wharton.upenn.edu/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fai.wharton.upenn.edu%2Fwhite-paper%2Fartificial-intelligence-risk-governance%2F&#038;format=xml" />
<meta name="martech:page-title" content="Artificial Intelligence Risk & Governance"><meta name="martech:site-title" content="Wharton Human-AI Research"><meta name="martech:site-pname" content="wharton-ai"><meta name="martech:site-penv" content="live"><meta name="martech:site-hostname" content="ai.wharton.upenn.edu"><meta name="martech:archive" content="false"><meta name="martech:home" content="false"><meta name="martech:content-type" content="post"><meta name="martech:tag" content=""><meta name="martech:category" content="Policy Ethics Governance;White Paper"><meta name="martech:thumbnail" content="https://ai.wharton.upenn.edu/wp-content/plugins/martech-chupacabra/includes/images/social.png">
        <script type="text/javascript">
            var jQueryMigrateHelperHasSentDowngrade = false;

			window.onerror = function( msg, url, line, col, error ) {
				// Break out early, do not processing if a downgrade reqeust was already sent.
				if ( jQueryMigrateHelperHasSentDowngrade ) {
					return true;
                }

				var xhr = new XMLHttpRequest();
				var nonce = 'c8b0af833e';
				var jQueryFunctions = [
					'andSelf',
					'browser',
					'live',
					'boxModel',
					'support.boxModel',
					'size',
					'swap',
					'clean',
					'sub',
                ];
				var match_pattern = /\)\.(.+?) is not a function/;
                var erroredFunction = msg.match( match_pattern );

                // If there was no matching functions, do not try to downgrade.
                if ( typeof erroredFunction !== 'object' || typeof erroredFunction[1] === "undefined" || -1 === jQueryFunctions.indexOf( erroredFunction[1] ) ) {
                    return true;
                }

                // Set that we've now attempted a downgrade request.
                jQueryMigrateHelperHasSentDowngrade = true;

				xhr.open( 'POST', 'https://ai.wharton.upenn.edu/wp-admin/admin-ajax.php' );
				xhr.setRequestHeader( 'Content-Type', 'application/x-www-form-urlencoded' );
				xhr.onload = function () {
					var response,
                        reload = false;

					if ( 200 === xhr.status ) {
                        try {
                        	response = JSON.parse( xhr.response );

                        	reload = response.data.reload;
                        } catch ( e ) {
                        	reload = false;
                        }
                    }

					// Automatically reload the page if a deprecation caused an automatic downgrade, ensure visitors get the best possible experience.
					if ( reload ) {
						location.reload();
                    }
				};

				xhr.send( encodeURI( 'action=jquery-migrate-downgrade-version&_wpnonce=' + nonce ) );

				// Suppress error alerts in older browsers
				return true;
			}
        </script>

		<meta name="martech:intranet" content="false"><meta name="generator" content="Powered by WPBakery Page Builder - drag and drop page builder for WordPress."/>
<style type="text/css" data-type="vc_shortcodes-custom-css">.vc_custom_1673473682159{margin-top: 0px !important;margin-bottom: 0px !important;padding-top: 50px !important;padding-bottom: 50px !important;}.vc_custom_1601920849905{margin-bottom: 10px !important;}.vc_custom_1601920802767{margin-bottom: 10px !important;}.vc_custom_1601921149641{margin-bottom: 10px !important;}.vc_custom_1601921106628{margin-bottom: 10px !important;}.vc_custom_1601921268840{margin-bottom: 10px !important;}.vc_custom_1601922405918{margin-bottom: 10px !important;}.vc_custom_1601923161761{margin-bottom: 10px !important;}.vc_custom_1601923380392{margin-bottom: 10px !important;}.vc_custom_1601923393965{margin-bottom: 10px !important;}.vc_custom_1673473625538{margin-bottom: 0px !important;}.vc_custom_1601929935333{margin-bottom: 10px !important;}.vc_custom_1607629398530{margin-bottom: 10px !important;}.vc_custom_1607629566809{margin-bottom: 10px !important;}.vc_custom_1607629844271{margin-bottom: 10px !important;}.vc_custom_1601930459898{margin-bottom: 10px !important;}.vc_custom_1601930483152{margin-bottom: 10px !important;}.vc_custom_1673473648422{margin-bottom: 0px !important;}.vc_custom_1601930748003{margin-bottom: 10px !important;}.vc_custom_1601930805664{margin-bottom: 10px !important;}.vc_custom_1601930885760{margin-bottom: 10px !important;}.vc_custom_1601930960865{margin-bottom: 10px !important;}.vc_custom_1601930996052{margin-bottom: 10px !important;}.vc_custom_1601931242972{margin-bottom: 10px !important;}.vc_custom_1601931279698{margin-bottom: 10px !important;}.vc_custom_1601931663523{margin-bottom: 10px !important;}.vc_custom_1601931883188{margin-bottom: 10px !important;}.vc_custom_1607631298646{margin-bottom: 10px !important;}.vc_custom_1601932043880{margin-bottom: 10px !important;}.vc_custom_1607631569353{margin-bottom: 10px !important;}.vc_custom_1607631760499{margin-bottom: 10px !important;}.vc_custom_1601932512394{margin-bottom: 10px !important;}</style><noscript><style> .wpb_animate_when_almost_visible { opacity: 1; }</style></noscript>	</head>

<body class="wp-singular post-template-default single single-post postid-8443 single-format-standard wp-theme-responsive-mobile wp-child-theme-Chupe2-0-child default-site-header wharton-ai martech-menu--dropdown-click group-blog full-window-home-page-header wpb-js-composer js-comp-ver-8.7.1 vc_responsive wharton-cms" itemscope="itemscope" itemtype="http://schema.org/WebPage">
    <!-- Google Tag Manager (noscript) The Wharton School-->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MZQZFC4" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->
                <div id="container" class="site">
	<nav id="jump-links" aria-label="Jump links">
		<a class="skip-link screen-reader-text" href="#main">Skip to content</a>
		<a class="skip-link screen-reader-text" href="#main-navigation">Skip to main menu</a>
	</nav>
	<header id="header" class="container-full-width site-header" role="banner" itemscope="itemscope" itemtype="http://schema.org/WPHeader">
		<div id="top-menu-container" class="group header--container">
			<a href="https://www.wharton.upenn.edu" class="global-brand" title="Wharton Home"><img src="https://ai.wharton.upenn.edu/wp-content/plugins/martech-chupacabra/includes/images/wharton-logo.svg" alt="Wharton" class="screen"/><img class="print"  style="display:none;" alt="Logo for The Wharton School" src="https://ai.wharton.upenn.edu/wp-content/plugins/martech-chupacabra/includes/images/Wharton-Logo-RGB.png" /></a>	<ul class="global-nav">
				<li class="martech-nav-faculty-and-research"><a href="https://www.wharton.upenn.edu/faculty-research-publications/">Faculty</a></li>
				<li class="martech-nav-youth-program"><a href="https://globalyouth.wharton.upenn.edu">Youth Program</a></li>
				<li class="martech-nav-undergrad"><a href="https://undergrad.wharton.upenn.edu">Undergrad</a></li>
				<li class="martech-nav-msqf"><a href="https://jacobs-msqf.wharton.upenn.edu">Jacobs MSQF</a></li>
				<li class="martech-nav-mba"><a href="https://mba.wharton.upenn.edu">MBA</a></li>
				<li class="martech-nav-emba"><a href="https://executivemba.wharton.upenn.edu">EMBA</a></li>
				<li class="martech-nav-phd"><a href="https://doctoral.wharton.upenn.edu">PhD</a></li>
				<li class="martech-nav-executive-education"><a href="https://executiveeducation.wharton.upenn.edu">Exec Ed</a></li>
				<li class="martech-nav-wharton-online"><a href="https://online.wharton.upenn.edu">Wharton Online</a></li>
				<li class="martech-nav-kw"><a href="https://knowledge.wharton.upenn.edu"><img src="https://ai.wharton.upenn.edu/wp-content/plugins/martech-chupacabra/includes/images/kw-logo.svg" alt="Knowledge at Wharton" class="kw-header-logo"/></a></li>
			</ul>        <a href="/search/#t=ThisSite" class="search-button search--closed">
        <span class="accessible-label">Search Wharton</span>
    </a>
    <button id="mobile-nav-button" aria-label="Mobile menu toggle">
        <span class="accessible-label">Mobile menu toggle</span>
    </button>
		</div>
		<div class="main-navigation header--container main-navigation--dropdown-click">
			<div class="header-row">
				<div id="martech-site-branding">
					<div class="martech-site-branding-container group">
						    <div id="department" class="department department--text">
    	<div class="department-inner">
	        <a class="dept-container" title="Wharton Human-AI Research" href="https://ai.wharton.upenn.edu/">
					            <div class="brand-text brand-text--desktop"  height="53">Wharton Human-AI Research</div>
	        </a>
	    </div>
    </div>
    											</div>
				</div>
				<!-- main menu -->
				<div id="main-menu-container" class="container-full-width">
					<nav id="main-navigation" class="site-navigation" itemscope="itemscope" itemtype="http://schema.org/SiteNavigationElement" aria-label="Primary navigation links">
						<div id="mobile-current-item">Menu</div>
												<div class="main-nav group"><ul id="menu-header-navigation" class="menu-header-navigation"><li id="menu-item-9673" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-9673"><a href="https://ai.wharton.upenn.edu/">Home</a></li>
<li id="menu-item-8832" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-8832"><a href="https://ai.wharton.upenn.edu/about/people/">People</a></li>
<li id="menu-item-114" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-114"><a href="https://ai.wharton.upenn.edu/research/">Research</a></li>
<li id="menu-item-8838" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-8838"><a href="https://ai.wharton.upenn.edu/education/">Education</a></li>
<li id="menu-item-8899" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-8899"><a href="https://ai.wharton.upenn.edu/get-involved/">Get Involved</a>
<ul class="sub-menu">
	<li id="menu-item-8937" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-8937"><a href="https://ai.wharton.upenn.edu/get-involved/academics/">Academics</a></li>
	<li id="menu-item-8931" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-8931"><a href="https://ai.wharton.upenn.edu/get-involved/industry/">Industry</a></li>
	<li id="menu-item-95" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-95"><a href="https://ai.wharton.upenn.edu/about/contact-us/">Contact Us</a></li>
</ul>
</li>
<li id="menu-item-10143" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-10143"><a href="#">Content</a>
<ul class="sub-menu">
	<li id="menu-item-668" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-668"><a href="https://ai.wharton.upenn.edu/news/">News &#038; Stories</a></li>
	<li id="menu-item-9564" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-9564"><a href="https://ai.wharton.upenn.edu/ai-horizons/">AI Horizons Webinar Series</a></li>
	<li id="menu-item-9376" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-9376"><a target="_blank" href="https://knowledge.wharton.upenn.edu/category/ai/">AI | Knowledge at Wharton</a></li>
	<li id="menu-item-12572" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-12572"><a href="https://ai.wharton.upenn.edu/creative-intelligence/">Creative Intelligence</a></li>
	<li id="menu-item-10773" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-10773"><a target="_blank" href="https://accountableai.net/">The Road to Accountable AI</a></li>
	<li id="menu-item-12126" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-12126"><a href="https://knowledge.wharton.upenn.edu/where-ai-works-show/">Where AI Works Podcast</a></li>
</ul>
</li>
<li id="menu-item-3387" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-3387"><a href="https://ai.wharton.upenn.edu/events/">Events</a>
<ul class="sub-menu">
	<li id="menu-item-12748" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-12748"><a href="https://ai.wharton.upenn.edu/events/">Upcoming Events</a></li>
	<li id="menu-item-12788" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-12788"><a href="#">AI and the Future of Work Conference</a>
	<ul class="sub-menu">
		<li id="menu-item-13407" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-13407"><a href="https://ai.wharton.upenn.edu/ai-and-the-future-of-work-conference-2026/">AI and the Future of Work Conference 2026</a></li>
		<li id="menu-item-12789" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-12789"><a href="https://ai.wharton.upenn.edu/ai-and-the-future-of-work-conference-2025/">AI and the Future of Work Conference 2025</a></li>
		<li id="menu-item-12810" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-12810"><a href="https://ai.wharton.upenn.edu/ai-and-the-future-of-work-conference-2024/">AI and the Future of Work Conference 2024</a></li>
	</ul>
</li>
	<li id="menu-item-12774" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-12774"><a href="#">Business &#038; Generative AI Conference</a>
	<ul class="sub-menu">
		<li id="menu-item-12775" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-12775"><a href="https://ai.wharton.upenn.edu/business-generative-ai-conference-2025/">Business &#038; Generative AI Conference 2025</a></li>
		<li id="menu-item-12776" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-12776"><a href="https://ai.wharton.upenn.edu/business-generative-ai-workshop-2024/">Business &#038; Generative AI Workshop 2024</a></li>
		<li id="menu-item-12777" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-12777"><a href="https://ai.wharton.upenn.edu/business-generative-ai-workshop-2023/">Business &#038; Generative AI Workshop 2023</a></li>
	</ul>
</li>
</ul>
</li>
</ul></div>					</nav><!-- #site-navigation -->
									</div><!-- #main-menu-container -->
				<div id="sub-menu-container" class="container-full-width">
					<div id="sub-menu" class="container">
						<nav id="sub-navigation" class="site-navigation" itemscope="itemscope" itemtype="http://schema.org/SiteNavigationElement" aria-label="Secondary navigation links">
												</nav><!-- #site-navigation -->
					</div><!-- #sub-menu -->
				</div><!-- #sub-menu-container -->
			</div>
		</div>

			</header><!-- #header -->


	<div id="wrapper" class="site-content container-full-width">

	<div id="content-full" class="content-area">
			<main id="main" class="site-main" role="main">

				
				
					
<article id="post-8443" class="post-8443 post type-post status-publish format-standard hentry category-policy-ethics-governance category-white-paper">
		
	    	
	<div class="post-entry">
		<div class="wpb-content-wrapper"><div class="vc_row wpb_row vc_row-fluid vc_custom_1673473682159 None" style="None"><div class="wpb_column vc_column_container vc_col-sm-1"><div class="vc_column-inner"><div class="wpb_wrapper"></div></div></div><div class="wpb_column vc_column_container vc_col-sm-10"><div class="vc_column-inner"><div class="wpb_wrapper">
	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<h1 style="text-align: center;"><strong>Artificial Intelligence Risk &amp; Governance</strong></h1>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p style="text-align: center;">By Artificial Intelligence/Machine Learning Risk &amp; Security Working Group (AIRS)</p>

		</div>
	</div>

	<div  class="wpb_single_image wpb_content_element vc_align_center wpb_content_element">
		
		<figure class="wpb_wrapper vc_figure">
			<div class="vc_single_image-wrapper   vc_box_border_grey"><img loading="lazy" decoding="async" width="1200" height="572" src="https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/ai-governance.jpeg" class="vc_single_image-img attachment-full" alt="ai-governance" title="ai-governance" srcset="https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/ai-governance.jpeg 1200w, https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/ai-governance-299x143.jpeg 299w, https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/ai-governance-399x190.jpeg 399w, https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/ai-governance-768x366.jpeg 768w, https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/ai-governance-100x48.jpeg 100w, https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/ai-governance-150x72.jpeg 150w, https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/ai-governance-200x95.jpeg 200w, https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/ai-governance-300x143.jpeg 300w, https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/ai-governance-450x215.jpeg 450w, https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/ai-governance-600x286.jpeg 600w, https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/ai-governance-900x429.jpeg 900w, https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/ai-governance-929x443.jpeg 929w" sizes="auto, (max-width: 1200px) 100vw, 1200px" /></div>
		</figure>
	</div>

	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<h2 style="text-align: center;"><strong>Executive Summary</strong></h2>

		</div>
	</div>
<div class="vc_separator wpb_content_element vc_separator_align_center vc_sep_width_60 vc_sep_pos_align_center vc_separator_no_text vc_sep_color_purple wpb_content_element  wpb_content_element" ><span class="vc_sep_holder vc_sep_holder_l"><span class="vc_sep_line"></span></span><span class="vc_sep_holder vc_sep_holder_r"><span class="vc_sep_line"></span></span>
</div>
	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p>As financial services firms evaluate the potential applications of artificial intelligence (AI), for example: to enhance the customer experience and garner operational efficiencies, <em>Artificial Intelligence/Machine Learning (AI/ML) Risk and Security</em> (&#8220;AIRS&#8221;)<a href="#_ftn1" name="_ftnref1">[1]</a> is committed to furthering this dialogue and has drafted the following overview discussing AI implementation and the corresponding potential risks firms may wish to consider in formulating their AI strategy. This white paper provides AIRS’s views on potential approaches to AI governance for financial services including potential risks, risk categorization, interpretability, discrimination, and risk mitigation, in particular, as applied to the financial industry.</p>
<p>This paper is intended for discussion purposes only and is not intended to serve as a prescriptive roadmap for implementing AI/ML tools or as a comprehensive inventory of risk associated with the use of AI. Readers are encouraged to consider the information provided in this paper for reference and discussion purposes. They should assess, implement, and tailor their firms’ AI/ML programs and respective controls as appropriate for their business model, product and service mix, and applicable legal and regulatory requirements.</p>
<p>The views expressed in this paper are those of the individual contributors and do not constitute the views of any of the firms with which the contributors are associated or by which they are employed.</p>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element vc_custom_1601920849905" >
		<div class="wpb_wrapper">
			<h4>Key Takeaways</h4>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<ul class="bullet-list">
<li>AIRS believes there are significant potential benefits of AI and that its adoption within financial services presents opportunities to improve both business and societal outcomes when risks are managed responsibly.</li>
<li>This paper explores the potential risks of AI and provides a standardized practical categorization of these risks: Data Related Risks, AI/ML Attacks, Testing and Trust, and Compliance</li>
<li>AI governance frameworks could help organizations learn, govern, monitor, and mature AI adoption. Four core components of AI governance are: definitions, inventory, policy/standards, and a governance framework, including controls.</li>
<li>AI, in certain use cases, could lead to privacy issues, and/or potentially discriminatory or unfair outcomes, if not implemented with appropriate care. We explore, in detail, the subject of interpretability and discrimination in using AI for certain use cases.</li>
<li>While there is no one-size-fits-all approach, practices institutions might consider adopting to mitigate AI risk include oversight and monitoring, enhancing explainability and interpretability, as well as exploring the use of evolving risk-mitigating techniques like differential privacy, and watermarking, among others.</li>
</ul>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element vc_custom_1601920802767" >
		<div class="wpb_wrapper">
			<h4>AIRS Next Steps</h4>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p>This document is meant to be the first of several iterations and further contributions from the AIRS group. These insights are based on collective experience of AIRS, and the suggestions we outline are, as a result, not meant to be comprehensive. AIRS plans to continue to build and engage an active community on these issues. Contact information is provided in Section 6 (Acknowledgments) if there is any feedback or if readers wish to comment on this paper or AIRS.</p>

		</div>
	</div>
</div></div></div><div class="wpb_column vc_column_container vc_col-sm-1"><div class="vc_column-inner"><div class="wpb_wrapper"></div></div></div></div><div class="vc_row wpb_row vc_row-fluid None" style="None"><div class="wpb_column vc_column_container vc_col-sm-1"><div class="vc_column-inner"><div class="wpb_wrapper"></div></div></div>    <div class="wpb_column vc_column_container vc_col-sm-10"><div class="vc_column-inner"><div class="wpb_wrapper">
	<div class="wpb_text_column wpb_content_element vc_custom_1601921149641" >
		<div class="wpb_wrapper">
			<h2 style="text-align: left;"><strong>1. Overview</strong></h2>

		</div>
	</div>
<div class="vc_separator wpb_content_element vc_separator_align_center vc_sep_width_10 vc_sep_border_width_4 vc_sep_pos_align_left vc_separator_no_text vc_sep_color_purple wpb_content_element  wpb_content_element" ><span class="vc_sep_holder vc_sep_holder_l"><span class="vc_sep_line"></span></span><span class="vc_sep_holder vc_sep_holder_r"><span class="vc_sep_line"></span></span>
</div>
	<div class="wpb_text_column wpb_content_element vc_custom_1601921106628" >
		<div class="wpb_wrapper">
			<h4>1.1 Document Purpose &amp; Scope</h4>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p>The business uses, regulatory interest and research in artificial intelligence and machine learning (AI/ML) have seen an exponential increase over the last few years. Discussions regarding the use of Artificial Intelligence (AI) and Machine Learning (ML) have gained momentum as financial services firms evaluate the potential applications of AI, including for example: to enhance the customer experience and garner operational efficiencies. AIRS is committed to furthering this dialogue and has drafted the following overview addressing the use of AI within financial services, discussing AI implementation and the corresponding potential risks firms may wish to consider in formulating their AI strategy. Our hope is to contribute to and establish an industry-wide view of the potential risks and mitigants in this rapidly evolving domain of AI/ML, as they may apply to individual firms depending on their use of AI systems.</p>
<p>In this document, AIRS members present their views, guidance, and a structure for conceiving AI/ML risks and governance, drawing upon our combined experience implementing and managing technology risks in the financial sector. The views expressed in this paper are meant to assist individuals and organizations facing risks and governance challenges presented by AI/ML. However, it is critical that each institution assess its own AI uses, risk profile and risk tolerance and design governance frameworks that fit their unique circumstances. As such, this paper is not meant to be comprehensive or prescriptive.</p>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element vc_custom_1601921268840" >
		<div class="wpb_wrapper">
			<h4>1.2 AIRS</h4>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p>AIRS is an informal group of practitioners and academics from varied backgrounds, including technology risk, information security, legal, privacy, architects, model risk management, and others working for financial, technology organizations, and academic institutions. The AIRS working group, based in New York, was initiated in early 2019 and has grown to nearly 40 members from dozens of institutions (and continues to grow).</p>

		</div>
	</div>
<div class=" vc_call_to_action wpb_content_element vc_cta_btn_pos_right vc_cta_rounded vc_txt_align_left  "><p>The AIRS Working Group seeks to promote, educate, and advance AI/ML governance for the financial services industry by focusing on risk identification, categorization, and mitigation.</p>
</div>
	<div class="wpb_text_column wpb_content_element vc_custom_1601922405918" >
		<div class="wpb_wrapper">
			<h4>1.3 Definitions &amp; Assumptions</h4>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p>We use several terms throughout this document specific to AI/ML, some of which are subject to vigorous discussions and debates in the research community. Because this document attempts to form a starting point for broader AI/ML governance and risk management efforts, we purposely leverage and encourage readers to refer to various papers for the definition of AI. As such, we note that specific definitions should be (and generally are) tailored to each organization depending on the scope, risk appetite, internal structure, culture, and implementation details of AI/ML efforts.</p>
<p>While there is no universally accepted definition of AI, it is generally understood to refer to “a branch of computer science dealing with the simulation of intelligent behavior in computers, or the capability of a machine to imitate intelligent human behavior.” Generally, machine learning is referred to as, “a field of computer science that uses algorithms to process large amounts of data and learn from it.”<a href="#_ftn2" name="_ftnref2">[2]</a> The term AI is broadly used and typically includes aspects of machine learning and natural language processing. For purposes of this paper, the focus is largely on the use of and potential risks related to machine learning, though the overarching discussion applies more broadly to the abovementioned areas.</p>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element vc_custom_1601923161761" >
		<div class="wpb_wrapper">
			<h4>1.4 A Brief Note on AI/ML Uses and Benefits</h4>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p>The use of AI in financial institutions is increasing as technological barriers have fallen and its benefits and potential risks have become clearer. The Financial Stability Board recently highlighted four areas where AI could impact banking specifically.<a href="#_ftn3" name="_ftnref3">[3]</a> First, customer-facing uses could expand access to credit and other financial services. For example, combining expanded consumer data sets with new ML algorithms to assess credit quality or price insurance policies, and using AI to offer new and innovative channels to deliver financial services could be a potent way to advance financial inclusion. Also, the use of AI chatbots could provide help and even financial advice to consumers, saving them time they might otherwise waste while waiting to speak with a live operator. Second, there is the potential for strengthening back-office operations, such as advanced models for capital optimization, model risk management, stress testing, and market impact analysis. Third, AI approaches could be applied to trading and investment strategies, from identifying new signals on price movements to using past trading behavior to anticipate a client’s next order. Finally, there are likely to be AI advancements in compliance and risk mitigation by banks. AI solutions are already being used by some firms in areas like fraud detection, capital optimization, and portfolio management.</p>
<p>Several papers and articles describe the potential uses and benefits of AI adoption and innovation in financial services. Widespread AI/ML adoption within the financial services industry could provide a unique opportunity to significantly improve financial outcomes for consumers and businesses, if this is done responsibly. That is why AIRS has chosen to focus this paper on understanding and managing the potential risks of AI/ML so that those benefits may be realized.</p>

		</div>
	</div>
</div></div></div><div class="wpb_column vc_column_container vc_col-sm-1"><div class="vc_column-inner"><div class="wpb_wrapper"></div></div></div></div><div class="vc_row wpb_row vc_row-fluid None" style="None"><div class="wpb_column vc_column_container vc_col-sm-1"><div class="vc_column-inner"><div class="wpb_wrapper"></div></div></div>    <div class="wpb_column vc_column_container vc_col-sm-10"><div class="vc_column-inner"><div class="wpb_wrapper">
	<div class="wpb_text_column wpb_content_element vc_custom_1601923380392" >
		<div class="wpb_wrapper">
			<h2 style="text-align: left;"><strong>2. AI Risks and Risk Categorization</strong></h2>

		</div>
	</div>
<div class="vc_separator wpb_content_element vc_separator_align_center vc_sep_width_10 vc_sep_border_width_4 vc_sep_pos_align_left vc_separator_no_text vc_sep_color_purple wpb_content_element  wpb_content_element" ><span class="vc_sep_holder vc_sep_holder_l"><span class="vc_sep_line"></span></span><span class="vc_sep_holder vc_sep_holder_r"><span class="vc_sep_line"></span></span>
</div>
	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p>Key potential risks of AI relate to potential harms that may affect organizations, consumers, or create broader detrimental effects on society. Such potential risks may arise in whole or in part from sources including the data used to train the AI system; potential risks arising from the AI system itself; potential risks arising from the usage of the AI system; and potential risks arising from poor overall governance of the AI system.</p>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element vc_custom_1601923393965" >
		<div class="wpb_wrapper">
			<h4>2.1 Risk Categorization</h4>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p>Various research papers, articles, and discussions have covered the topics of risks associated with AI. It is up to individual financial services firms to categorize potential risks of using AI: however, we have included a suggested approach to AI risk categorization in the financial services industry below.</p>
<p>The areas of data related risks, AI/ML attacks, testing and trust, as well as people risk constitute potential areas of risk, which could be subcategorized as illustrated in <strong>Figure 1. </strong>We address these sub-categories in further detail below.</p>

		</div>
	</div>
<div class=" vc_call_to_action wpb_content_element vc_cta_btn_pos_right vc_cta_rounded vc_txt_align_left  "><p>It’s important to note that the applicability and relevance of risks illustrated in Figure 1 are dependent on an individual organization’s risk profile, appetite, and existing controls, and it is up to each firm to determine whether their existing controls are sufficient.</p>
</div>
	<div  class="wpb_single_image wpb_content_element vc_align_center wpb_content_element vc_custom_1673473625538">
		
		<figure class="wpb_wrapper vc_figure">
			<div class="vc_single_image-wrapper   vc_box_border_grey"><img loading="lazy" decoding="async" width="782" height="549" src="https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/AIRS-AI-Risk-Categories_Figure-1_Dec2020.png" class="vc_single_image-img attachment-full" alt="AIRS-AI-Risk-Categories_Figure-1_Dec2020" title="AIRS-AI-Risk-Categories_Figure-1_Dec2020" srcset="https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/AIRS-AI-Risk-Categories_Figure-1_Dec2020.png 782w, https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/AIRS-AI-Risk-Categories_Figure-1_Dec2020-299x210.png 299w, https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/AIRS-AI-Risk-Categories_Figure-1_Dec2020-399x280.png 399w, https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/AIRS-AI-Risk-Categories_Figure-1_Dec2020-768x539.png 768w, https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/AIRS-AI-Risk-Categories_Figure-1_Dec2020-100x70.png 100w, https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/AIRS-AI-Risk-Categories_Figure-1_Dec2020-150x105.png 150w, https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/AIRS-AI-Risk-Categories_Figure-1_Dec2020-200x140.png 200w, https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/AIRS-AI-Risk-Categories_Figure-1_Dec2020-300x211.png 300w, https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/AIRS-AI-Risk-Categories_Figure-1_Dec2020-450x316.png 450w, https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/AIRS-AI-Risk-Categories_Figure-1_Dec2020-600x421.png 600w" sizes="auto, (max-width: 782px) 100vw, 782px" /></div>
		</figure>
	</div>

	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p style="text-align: center;"><strong>Figure </strong><strong>1</strong><strong>: </strong>AIRS AI Risk Categorization</p>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element vc_custom_1601929935333" >
		<div class="wpb_wrapper">
			<h4>2.1.1 Inadequate Governance</h4>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p><strong>Learning Limitations<br />
</strong>Unlike humans, AI systems lack the judgment and context for many of the environments in which they are deployed. An AI/ML system is generally as effective as the data used to train it and the various scenarios considered while training the system. In most cases, it is not possible to train the AI system on all possible scenarios and data. Lack of context, judgment, and overall learning limitations may play a key role in informing risk-based reviews, and strategic deployment discussions.</p>
<p><strong>Data Quality<br />
</strong>The risk of poor data quality is not unique to AI, but for AI/ML systems, poor data quality could not only limit the learning capability of the system, but could also potentially negatively impact how it makes inferences and decisions in the future. Poor data quality could include incomplete data, erroneous or unsuitable data, stale data, or data used in the wrong context. Such deficiencies may give rise to potentially erroneous or poor predictions, or potentially result in a failure to achieve the intended objectives.</p>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element vc_custom_1607629398530" >
		<div class="wpb_wrapper">
			<h4>2.1.2 Potential AI/ML Attacks</h4>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p>The proliferation of research papers on AI/ML has increased significantly over the last decade, with many of these devoted to potential security weaknesses in AI/ML systems. Most of the known potential attacks against AI/ML systems could be grouped into one of the following categories: data privacy, data poisoning, and model extraction.</p>
<p>The likelihood and impact of various potential attacks are specific to each organization’s risk posture and controls. It is possible that some potential attacks illustrated below may not be relevant to a particular organization and may be mitigated by customary security controls.</p>
<p><strong>Data Privacy Attacks<br />
</strong>In data privacy attacks, an attacker is potentially able to infer the data set used to train the model, thereby potentially compromising the privacy of the data. An adversary could potentially infer sensitive information from the training data set by analyzing the parameters or querying the model. Two major attack types in data privacy include membership inference and model inversion attacks.</p>
<p>In a membership inference attack, an attacker could potentially determine if a particular record (or set of records) exists in a training data set. Generally, if the attack is successful, an attacker could determine, to a certain degree of probability, whether a particular record was part of the training data set used to train the AI system. In model inversion attacks, an attacker could potentially extract the training data used to train the model directly.</p>
<p><strong>Training Data Poisoning<br />
</strong>Data poisoning is the contamination of data used to train the AI/ML system, negatively affecting its learning process or output. Data poisoning could potentially be used to increase the error rate of the AI/ML system or to potentially influence the retraining process. Some of the attacks in this category are known as “label-flipping” and “frog-boil” attacks.</p>
<p><strong>Adversarial Inputs<br />
</strong>AI systems that use input from external system(s)/ user(s), interpret the input and perform an action, like classifying the input data. An adversary could potentially use a malicious input or a payload explicitly designed to bypass AI systems classifier. Such malicious inputs are known as adversarial inputs.</p>
<p><strong>Model Extraction<br />
</strong>In this attack, an adversary tries to steal the model itself. Model extraction attacks are potentially the most impactful types of AI/ML attacks, as the stolen model could be used as a ‘tool’ to create additional risks. Research into such attacks indicates that, given unlimited ability to query the model, extraction could occur without requiring high levels of technical sophistication and could be accomplished at high speeds.</p>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element vc_custom_1607629566809" >
		<div class="wpb_wrapper">
			<h4>2.1.3 Testing and Trust</h4>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p>Depending on the implementation and use case, the AI system could potentially evolve over time at varying degrees. Some forms of AI could generate complexities that may accrue, evolve or worsen over time.<a href="#_ftn4" name="_ftnref4">[4]</a> ML models may be sensitive to environmental developments, for example, that could potentially alter their performance, Some AI systems may not have exposures to the below potential risks, either due to the nature of implementation or controls in place. Potential concerns related to testing and trust risk are discussed in detail below:<a href="#_ftn6" name="_ftnref6"></a></p>
<p><strong>Incorrect Output<br />
</strong>Testing and validation of AI/ML systems may pose challenges relative to traditional systems as certain AI/ML systems are inherently dynamic, apt to change over time, and by extension, may result in changes to their outputs. Testing for all scenarios, permutations and combinations of available data may not be possible, thus leading to potential gaps in coverage. The severity of these gaps may vary with each system and its applications.</p>
<p><strong>Lack of Transparency<br />
</strong>As an emerging technology, the awareness of (and hype related to) AI and the lack of adequate understanding of the technology could potentially give rise to trust issues with AI systems. There is a perception, for example, that AI systems are a “black box” and therefore cannot be explained. (We address this belief further in Section 4.) Generally, it is difficult to thoroughly assess systems that cannot easily be understood.</p>
<p><strong>Bias<br />
</strong>AI systems could potentially amplify risks relating to unfairly biased outcomes or discrimination. For example, the subjects of data ethics, fairness and the possibility of unfairly biased outcomes from the use of AI are still evolving. It is evident, however, that, depending on the use case, there is a risk that AI systems could potentially lead to unfairly biased outcomes for individuals and/or organizations. Furthermore, AI-driven unfairly biased outcomes could have privacy compliance implications, constitute regulatory, litigation and reputational risk, impact operations and result in customer dissatisfaction and attrition. Section 4 of this paper (focused on transparency, explainability and bias) discusses unfairly biased outcomes and discrimination in AI in greater detail.</p>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element vc_custom_1607629844271" >
		<div class="wpb_wrapper">
			<h4>2.1.4 Compliance</h4>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p><strong>Policy Non-Compliance<br />
</strong>As AI implementations mature in organizations, their impact on existing internal policies should be considered. Regulatory bodies have expressed growing interest in AI deployments in the financial industry. Regulators have formed working groups representing various authorities across the globe to discuss supervisory challenges posed by emerging technologies, which have led to the publication of guidelines, white papers, and surveys. This interest is driven by the understanding that AI/ML poses new challenges, and readers should evaluate how regulations may impact the use and governance of AI/ML. AIRS is not advocating for new regulation(s), but merely would encourage readers to monitor existing regulations and their potential applicability to AI.</p>

		</div>
	</div>
</div></div></div><div class="wpb_column vc_column_container vc_col-sm-1"><div class="vc_column-inner"><div class="wpb_wrapper"></div></div></div></div><div class="vc_row wpb_row vc_row-fluid None" style="None"><div class="wpb_column vc_column_container vc_col-sm-1"><div class="vc_column-inner"><div class="wpb_wrapper"></div></div></div><div class="wpb_column vc_column_container vc_col-sm-10"><div class="vc_column-inner"><div class="wpb_wrapper">
	<div class="wpb_text_column wpb_content_element vc_custom_1601930459898" >
		<div class="wpb_wrapper">
			<h2 style="text-align: left;"><strong>3. AI Governance</strong></h2>

		</div>
	</div>
<div class="vc_separator wpb_content_element vc_separator_align_center vc_sep_width_10 vc_sep_border_width_4 vc_sep_pos_align_left vc_separator_no_text vc_sep_color_purple wpb_content_element  wpb_content_element" ><span class="vc_sep_holder vc_sep_holder_l"><span class="vc_sep_line"></span></span><span class="vc_sep_holder vc_sep_holder_r"><span class="vc_sep_line"></span></span>
</div>
	<div class="wpb_text_column wpb_content_element vc_custom_1601930483152" >
		<div class="wpb_wrapper">
			<h4>3.1 The AI Governance Survey</h4>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p>To better understand current enterprise-wide governance practices within the industry, AIRS members were surveyed about their approach to managing AI/ML risk<strong>.</strong> <strong>Figure 2</strong> depicts the results of the survey.</p>

		</div>
	</div>

	<div  class="wpb_single_image wpb_content_element vc_align_center wpb_content_element vc_custom_1673473648422">
		
		<figure class="wpb_wrapper vc_figure">
			<div class="vc_single_image-wrapper   vc_box_border_grey"><img loading="lazy" decoding="async" width="659" height="557" src="https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/AIRS-AI-Governance-Survey_Figure-2.png" class="vc_single_image-img attachment-full" alt="AIRS-AI-Governance-Survey_Figure-2" title="AIRS-AI-Governance-Survey_Figure-2" srcset="https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/AIRS-AI-Governance-Survey_Figure-2.png 659w, https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/AIRS-AI-Governance-Survey_Figure-2-299x253.png 299w, https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/AIRS-AI-Governance-Survey_Figure-2-399x337.png 399w, https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/AIRS-AI-Governance-Survey_Figure-2-100x85.png 100w, https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/AIRS-AI-Governance-Survey_Figure-2-150x127.png 150w, https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/AIRS-AI-Governance-Survey_Figure-2-200x169.png 200w, https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/AIRS-AI-Governance-Survey_Figure-2-300x254.png 300w, https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/AIRS-AI-Governance-Survey_Figure-2-450x380.png 450w, https://ai.wharton.upenn.edu/wp-content/uploads/2023/01/AIRS-AI-Governance-Survey_Figure-2-600x507.png 600w" sizes="auto, (max-width: 659px) 100vw, 659px" /></div>
		</figure>
	</div>

	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p style="text-align: center;"><strong>Figure </strong><strong>2</strong><strong>: </strong>AIRS AI Governance Survey</p>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p>Given the results of this survey, which shows that the financial services industry is focused on AI, and potentially in early stages of adoption, the industry would benefit from a common set of definitions and more collaboration in developing risk categorization and taxonomies.<a href="#_ftn5" name="_ftnref7">[5]</a> The response to the questions do not reflect future plans of the institutions. For example, “updated existing policies/standards for AI/ML?” merely confirms if a policy was updated and does not necessarily explain if the respondents will or plan to update their policies; one potential reason could be that the firms believe that existing controls address the potential AI risks.</p>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element vc_custom_1601930748003" >
		<div class="wpb_wrapper">
			<h4>3.2 AI Governance</h4>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p>As AI use advances and becomes more widespread within the industry, AIRS acknowledges that there are multiple ways to govern potential risks and any risk management framework should be tailored to each individual firm’s unique circumstances. The following are just a few examples of what some firms may find useful as they think about managing risks related to their own adoption of AI systems. For instance, some firms may find it more effective to adapt their existing risk infrastructure to manage AI risk rather than adding entirely new structures.</p>
<p>Four core components of AI governance include: definitions, inventory, policy/standards, and framework, including controls.</p>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element vc_custom_1601930805664" >
		<div class="wpb_wrapper">
			<h4>3.2.1 Definitions</h4>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p>Depending on the adoption, environment, and culture of an organization, there may be a long series of nuanced definitions for AI/ML. As the first step to achieving AI governance, a clear definition of what constitutes AI (and what does not) is critical for any organization. This definition provides the foundation and establishes a clear understanding of the other components of the governance structure, informing the remaining building blocks that comprise the overall AI program (e.g., inventory).</p>
<p>Any definition of AI should consider, among other factors, the variety of techniques used by the organization in training and developing the AI, what distinguishes AI from other traditional rule-based systems, and the implications of the definition, enabling the kind of AI inventory efforts we set forth below. Definitions and supporting documentation should provide clarity related to how various stakeholders &#8211; including Senior Management, Legal, System Developers, Compliance, and Information Security Officers – identify with the AI definition relative to other well-established definitions.<a href="#_ftn6" name="_ftnref8">[6]</a></p>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element vc_custom_1601930885760" >
		<div class="wpb_wrapper">
			<h4>3.2.2 Inventory</h4>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p>The purpose of an inventory is to allow the organization to identify and track the AI/ML systems it has deployed and monitor associated risk(s) (if any). Such an inventory might describe the purpose for which the system is designed, its intended use, and any restrictions on such use. Inventories might also list key data elements for each AI/ML system, including any feeder systems/models, the owners, developers, validators, and key dates associated with the AI/ML lifecycle.</p>
<p>Organizations may benefit from the implementation of protocols, structures, frameworks, and tools to assist in maintaining an accurate and comprehensive systems inventory.</p>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element vc_custom_1601930960865" >
		<div class="wpb_wrapper">
			<h4>3.2.3 Policies</h4>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p>Existing policies and standards might already apply to many use cases for AI. In such circumstances, however, additional or revisions to policies and standards may be necessary to ensure that AI is deployed appropriately. Potential enhancement of existing processes and the creation of new documentation should, as a result, be considered.</p>
<p>We note that ethical principles for AI have been in discussion for some time in the industry, with a handful of institutions circulating these AI ethical principles publicly. Members of the AIRS group have seen firsthand the positive impact these principles could have, and actively encourage their further development, including as appropriate in conjunction with any data governance efforts regarding ethical use of data.</p>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element vc_custom_1601930996052" >
		<div class="wpb_wrapper">
			<h4>3.2.4 Framework</h4>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p>AI governance frameworks could help organizations learn, govern, monitor, and mature AI adoption. An AI governance framework might begin with an organization identifying key stakeholders representing various groups and departments. Such a ‘coalition’ may be formalized in a Center of Excellence (CoE), working group, or council, among other examples. Such groups might develop best practices for their organization, share knowledge, and build guard rails for the use of AI systems. These efforts are generally most successful when they establish close links with technology, data engineers and line of business stakeholders to complement existing frameworks, and to support existing workflows in monitoring and oversight of the activities of AI systems and AI-enabled products.</p>
<p>In reviewing an AI-enabled initiative, the ‘coalition’ should take various considerations into account, (as applicable) including data ethics, privacy rights, applicable regulatory considerations, whether the data on which the AI system is being trained is suitable (i.e., was it provided for this purpose or is it being leveraged in a manner unrelated to that for which it was provided), whether notice of such use may be required to third parties, whether the data set is appropriately safeguarded (via access right controls and encryption protocols, for instance), and the manner of supervisory oversight that is appropriate to evidence control over the AI system, whether developed internally or by a third party.</p>
<p>Depending on the scale of adoption, a formal approval process might be put in place, governed by a central body having subject matter expertise from various fields. When sufficient comfort with AI governance is achieved, this central structure may also be disbanded and replaced with a federated structure that could cater to business-specific needs and risks.</p>
<p>Note that identification of potential AI/ML risks (as set forth in Section 2) is critical to formulating an operational risk and control framework. Upon identification of potential risks, a gap analysis might then be instituted against existing controls. Depending on the control library of an institution, this may require participation from multiple control owners and requires a structured approach and thorough planning. Results of the gap analysis should then lead to the creation of potential new or enhanced controls to mitigate against the identified potential AI/ML risks.</p>
<p>AI governance frameworks should also consider a host of other factors, some of which we outline below.</p>
<p><strong>Monitoring and Oversight<br />
</strong>A central monitoring and escalation process is, in many cases, essential­: providing sufficient exposure within an organization to the decisions being made and an opportunity to raise concerns or challenges when appropriate. This structure should enable the monitoring system to adapt to the changing needs of the organization, as AI adoption matures or substantial changes in the industry occur. It’s noted that some firms may believe that existing monitoring and oversight procedures sufficiently address potential AI risks.</p>
<p>Existing governance systems in most organizations are designed for processes where there is a high degree of human involvement. Business experts, for example, are oftentimes on-hand to override erroneous results. Reducing or removing interventions may, however, improve the accuracy, consistency, and efficiency of existing processes. This is especially true where each new data iteration dynamically optimizes the AI system and improves upon it – a chatbot, for example, learning and tuning its response with each customer interaction. Governance around these dynamically calibrating processes typically require additional safety protocols, including, for example, more robust and continuous monitoring, pre-defined performance thresholds, and “kill-switches” that could remove the system from deployment entirely, if necessary, depending on the use case.</p>
<p><strong>Third-Party Risk Management<br />
</strong>The use of AI/ML deployment may involve third party applications and/or data, as discussed in Section 2, which could enable scalability, increased compute power and access to vendors that are part of the larger fintech ecosystem. As a result, firms may need to strengthen their third-party risk management (TPRM) capabilities. These developments may test certain aspects of current practices, such as TPRM transparency around model interpretability, information security issues for cloud-based service providers, and broader concerns around technology dependencies for the third parties themselves. Depending on the use case, Firms may consider including contractual clauses for third parties regarding the AI system’s testing methodology, explainability of the results generated by the system, and/or intellectual property rights which may be derived from use of the system.</p>
<p><strong>Three Lines of Defense<br />
</strong>Most financial institutions follow a three-lines-of-defense model, which separates front line groups, which are generally accountable for business risks (the First Line), from other risk oversight and independent challenge groups (the Second Line) and assurance (the Third Line). AI governance frameworks should ensure that sufficient oversight, challenge, and assurance requirements are met in AI system development and utilization. Furthermore, as both the potential risks and regulations related to AI are evolving, the second and third lines of defense should, likewise, ensure they have adequate subject matter expertise to effectively challenge the first line in evaluating the proposed use and implementation of the AI systems, as outlined earlier in Section 2.</p>
<p><strong>Roles and Responsibilities<br />
</strong>Every organization is different with respect to their internal organizational structure and general roles and responsibilities. The roles/activities below provide some examples for organizations that are discussing roles and responsibilities with respect to AI implementations to consider. It is not intended to be an exhaustive or prescriptive list.</p>
<p><em>Ethics Review Board<br />
</em>An ethics review board may review AI projects in accordance with an organization’s ethical principles, e.g.  AI deemed to be high risk.</p>
<p><em>Center of Excellence<br />
</em>A Center of Excellence (CoE) may provide a knowledge-sharing platform in an organization. Depending on the organization, a CoE could create a collective view and create and share best practices. Furthermore, the CoE could maintain engagement with the industry to share and learn best practices.</p>
<p><em>Data Science<br />
</em>Some organizations have mature Data Science practices. In addition to their assigned responsibilities, the Data Science team could manage AI system inventory and version control.</p>
<p><em>ML Operations<br />
</em>A ML operations team provisions data for analysis by the data science team. They may also create and maintain data sets for the purpose of training AI systems.</p>

		</div>
	</div>
</div></div></div><div class="wpb_column vc_column_container vc_col-sm-1"><div class="vc_column-inner"><div class="wpb_wrapper"></div></div></div></div><div class="vc_row wpb_row vc_row-fluid None" style="None"><div class="wpb_column vc_column_container vc_col-sm-1"><div class="vc_column-inner"><div class="wpb_wrapper"></div></div></div><div class="wpb_column vc_column_container vc_col-sm-10"><div class="vc_column-inner"><div class="wpb_wrapper">
	<div class="wpb_text_column wpb_content_element vc_custom_1601931242972" >
		<div class="wpb_wrapper">
			<h2 style="text-align: left;"><strong>4. Interpretability and Discrimination</strong></h2>

		</div>
	</div>
<div class="vc_separator wpb_content_element vc_separator_align_center vc_sep_width_10 vc_sep_border_width_4 vc_sep_pos_align_left vc_separator_no_text vc_sep_color_purple wpb_content_element  wpb_content_element" ><span class="vc_sep_holder vc_sep_holder_l"><span class="vc_sep_line"></span></span><span class="vc_sep_holder vc_sep_holder_r"><span class="vc_sep_line"></span></span>
</div>
	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p>Interpretability (presenting the AI system’s results in human understandable format), and discrimination (unfairly biased outcomes) are crucial concepts that factor into the risks associated with AI/ML systems used for certain use cases. In this section, we explore potential risks associated with discrimination and interpretability as they relate to certain applications of AI, e.g., loan approvals.</p>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element vc_custom_1601931279698" >
		<div class="wpb_wrapper">
			<h4>4.1 Discrimination in AI</h4>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p>Depending on the use case, AI may potentially lead to discriminatory and/or unfairly biased outcomes if not implemented appropriately. Poor implementation may arise from biased data, the AI system itself not being properly trained or when there are alternate systems and data sources that could potentially be used to generate better outcomes for disadvantaged groups. Ultimately, the use of an AI system which may cause potentially unfair biased outcomes may lead to regulatory non-compliance issues, potential lawsuits and reputational risk. That said, these risks could be managed. There is even growing evidence that AI/ML systems could be harnessed to more effectively control for discriminatory outcomes.</p>
<p><strong>Existing Legal and Regulatory Frameworks<br />
</strong>Federal and state statutes prohibit discrimination in areas that impact our daily lives, including employment, housing, and lending, to name a few. By way of example, a potential impact in the use of AI for lending is described in greater detail below.</p>
<p>The primary U.S. federal statutes that define illegal discrimination in lending are the Equal Credit Opportunity Act (ECOA) and the Fair Housing Act (FHA); however, lenders are subject to many other federal regulations and state laws addressing fairness. Each statute defines types of “protected classes,” such as gender, race, or ethnicity, that a lender cannot legally disfavor.</p>
<p>Generally speaking, three types of discrimination are recognized by federal banking regulators: overt discrimination, disparate treatment, and disparate impact when not supported by a legitimate business justification. Disparate treatment discrimination could occur when similarly situated individuals are treated differently based on a prohibited basis, but the treatment does not have to be motivated by prejudice or an intent to discriminate. In an AI context, this may potentially occur, for example, when a firm explicitly uses protected class status in an AI system used to underwrite creditworthiness.</p>
<p>Disparate impact, on the other hand, occurs when a system includes features that lead to disproportionately unfavorable outcomes for a protected class. Importantly, evidence of disparate impact is almost always assessed independently of the accuracy and validity of the system. In other words, just because a given system is statistically sound does not mean that it is legally non-discriminatory. Such systems are generally not considered legally discriminatory if they and their constituent features could be demonstrated to meet a legitimate business need and where no less discriminatory alternative system or process could be identified that also meets those needs.</p>
<p>Concerns over using and potentially amplifying implicitly biased data also arise in other contexts. For instance, the New York Department of Financial Security (NY DFS) discussed <a href="#_ftn7" name="_ftnref9">[7]</a> the use of external consumer data and information sources in insurance underwriting, noting the potential of leveraging these sources to help establish lifestyle indicators that may inform the review of an application for insurance coverage. In doing so, however, NY DFS observed that such data may be inaccurate or unreliable, and its use may result in a significant detrimental impact to the insured.</p>
<p>Similarly, in a speech <a href="#_ftn8" name="_ftnref10">[8]</a> by Charles Randell, Chair of the UK Financial Conduct Authority, concerns over misuse of big data to inform potentially detrimental outcomes were raised, with a real-world example in the use of data mining credit card charges for services such as marriage counseling, and reducing cardholders’ credit limits on the basis of the correlation between marriage breakdown and debt default. The use and potential for misuse of big data is no longer a theoretical concern and should be considered in determining the types of data that may be used in developing AI/ML systems.</p>
<p>We reference these legal and regulatory considerations to illustrate existing standards that already apply to many algorithmic activities of financial institutions, especially as they relate to unfairly biased outcomes.</p>
<p><strong>Data as a Cause of Discriminatory AI<br />
</strong>A host of factors may result in AI-related illegal discrimination. Input data may cause illegal discrimination if it identifies or closely proxies class membership, if it causes protected class members to experience less favorable outcomes, or if it is differentially predictive of the outcome for the protected class.</p>
<p>Traditional data inputs, such as many credit bureau attributes, tend to be less likely to raise disparate impact concerns because they are generally thoroughly vetted and accepted for credit worthiness. They may also be differentially predictive if the system’s weights or coefficients do not properly account for class-specific idiosyncrasies.</p>
<p>Non-traditional data, such as utility payment history, rental payments, or a person’s digital footprint (including social media posting), may generate heightened concerns relative to traditional data. From a fairness perspective, such data may have substantial merit, as its use has been shown to expand access to the financial system for unbanked or underserved populations that are often more likely to be members of some protected groups. However, such data use often raises coverage and accuracy concerns.</p>
<p><strong>Algorithms as a Cause of Discriminatory AI<br />
</strong>Algorithms themselves may result in discriminatory outcomes exacerbated by their complexity and opacity. Some of this concern arises from the fact that some machine learning algorithms create variable interactions and non-linear relationships that are too complex for humans to identify and review. These relationships have the potential to cause disparate treatment discrimination by creating proxies for protected class status. To some degree, these concerns have been lessened by advances in explainable AI techniques that allow additional insight into these complex relationships, which we address in Subsection 4.2 below.</p>
<p>System misspecification may also cause discriminatory outcomes. Here, features may be independently predictive of both the outcome and protected class status, but the class effect is incorporated into the prediction. For example, suppose a credit system included whether a person tended to shop at a discount store. It is likely that such a variable would capture a measure of wealth, which may be a reasonable predictor of repayment, but may also unintentionally capture a race effect. In addition, if the store is more likely to be located in minority neighborhoods, then the system may further exacerbate this effect. That is, the variable may act as a proxy for the neighborhood, which in turn acts as a proxy for race. Importantly, this is not a problem that is unique to AI. In fact, to the extent that machine learning is more accurate than traditional methods, it may be more likely to identify such a relationship and remove the non-predictive race effect.</p>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element vc_custom_1601931663523" >
		<div class="wpb_wrapper">
			<h4>4.2 Interpretability/Explainability</h4>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p>Interpretability relates to the ability of humans to gain insight into the inner workings of AI systems, which may be complex and opaque. In a practical sense, the two primary aspects of AI/ML interpretability are directly interpretable system mechanisms and posthoc explanations (explainability) of system mechanisms and predictions.</p>
<p>Well-known interpretable systems include linear systems, decision trees, and rule-based systems, where internal system mechanisms are composed of a relatively small number of learned coefficients or Boolean rules. Examples of newer and perhaps relatively more accurate and sophisticated types of interpretable AI/ML systems include scalable Bayesian rule lists, Explainable Boosting Machines (EBMs), monotonic Gradient Boosting Machines (GBMs), various Bayesian or constrained variants of traditional AI/ML systems or other novel interpretable-by-design systems.<a href="#_ftn9" name="_ftnref12">[9]</a></p>
<p><strong>Inconsistent Explanations<br />
</strong>Unlike more traditional linear systems, the same training data set may be used to train many possible accurate AI/ML systems, such that any AI/ML system a practitioner trains is just one of many potentially good systems. As a result, while the outcome of the AI/ML systems may be similar, there may be many different logical explanations for how the AI generated the output. Therefore, two systems giving different explanations for the same result or decision may create unwanted outcomes. Explanation inconsistency could also rear its head when systems are refreshed. When using low quality or inconsistent explanation techniques, simply retraining a system on newer data could also result in different explanations for the same customer and decision.</p>
<p>Posthoc explanation methods, such as feature importance and partial dependence, give approximate summaries of AI/ML system mechanisms or predictions across an entire dataset. Many newer explanation approaches tend to focus on high-fidelity summaries of local system behavior, essentially attempting to describe why an AI/ML system made a decision about a single customer, transaction, or other entity. These newer techniques include local interpretable system-agnostic explanations (LIME), Shapley additive explanations (SHAP), or saliency maps. Importantly, novel interpretable systems and posthoc explanations are already in use today.<a href="#_ftn10" name="_ftnref13">[10]</a></p>
<p>Methods for interpretability facilitate the human understanding of AI/ML systems, which could help to mitigate many of the risks elaborated throughout this paper. Such interpretability could help mitigate the risks from incorrect AI/ML system decisions, enable security audits of AI/ML systems, and align with regulatory compliance efforts.</p>
<p><strong>Detection and Appeal of Incorrect Decisions<br />
</strong>Because AI/ML systems are probabilistic, they may make incorrect decisions. In extremely opaque systems, however, neither the developer nor the user may have enough insight to understand how, or even if, the decision is wrong. This fact makes interpretability of high-impact AI/ML decisions a significant imperative and a source of potential risk. If such effects are adverse or otherwise perceived as incorrect, both organizations and impacted individuals alike may seek to detect and mitigate the harms created by the AI/ML-based decisions.</p>
<p><strong>Security Audit<br />
</strong>Malicious actors could potentially misuse or abuse traditional IT systems in multiple ways, and AI/ML is no exception. Indeed, security is evolving in the world of AI/ML, and interpretability plays a major role in ensuring that such systems are protected. Red team or white-hat hacking audits or exercises to test AI/ML systems may, for example, use variants of posthoc explanation techniques in system stealing, system inversion, and membership inference attacks against AI/ML systems.</p>
<p><strong>Regulatory Compliance<br />
</strong>Interpretable systems, posthoc explanations, and the documentation they facilitate may also be required under several applicable regulations and legal frameworks, such as the Equal Credit Opportunity Act, the Fair Credit Reporting Act, and the E.U. General Data Privacy Regulation (GDPR), among others. This both increases the importance of interpretability in AI/ML systems generally and highlights the compliance-related risks associated with their use.</p>

		</div>
	</div>
</div></div></div><div class="wpb_column vc_column_container vc_col-sm-1"><div class="vc_column-inner"><div class="wpb_wrapper"></div></div></div></div><div class="vc_row wpb_row vc_row-fluid None" style="None"><div class="wpb_column vc_column_container vc_col-sm-1"><div class="vc_column-inner"><div class="wpb_wrapper"></div></div></div><div class="wpb_column vc_column_container vc_col-sm-10"><div class="vc_column-inner"><div class="wpb_wrapper">
	<div class="wpb_text_column wpb_content_element vc_custom_1601931883188" >
		<div class="wpb_wrapper">
			<h2 style="text-align: left;"><strong>5. Common Practices to Mitigate AI Risk</strong></h2>

		</div>
	</div>
<div class="vc_separator wpb_content_element vc_separator_align_center vc_sep_width_10 vc_sep_border_width_4 vc_sep_pos_align_left vc_separator_no_text vc_sep_color_purple wpb_content_element  wpb_content_element" ><span class="vc_sep_holder vc_sep_holder_l"><span class="vc_sep_line"></span></span><span class="vc_sep_holder vc_sep_holder_r"><span class="vc_sep_line"></span></span>
</div>
	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p>In this section, we outline potential mitigants and emerging best practices that could guide firms in their internal discussions regarding potential AI risks. These insights are based on our collective experience, and the suggestions we outline are, as a result, not meant to be comprehensive or prescriptive.</p>
<p>In general, machine learning pipelines contain three possible points of intervention: the training data, the learning procedure, and the output predictions, with three corresponding classes of mitigation algorithms &#8211; pre-processing, in-processing, and post-processing.<a href="#_ftn11" name="_ftnref14">[11]</a> The advantages of post-processing approaches are that they may not require access to the training process and are thus suitable for run-time environments. Moreover, post-processing algorithms operate in a black-box approach, meaning that they do not necessarily need access to the internals of models, their derivatives, and may therefore be applicable to any machine learning model (or amalgamation of models).</p>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element vc_custom_1607631298646" >
		<div class="wpb_wrapper">
			<h4>5.1 Oversight and Monitoring</h4>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p><strong>Oversight Processes<br />
</strong>An oversight process based on thorough monitoring to validate the outputs, thresholds, and other aspects of the system could help maintain the overall accuracy and efficiency of AI systems. Oversight processes might begin with the creation of an inventory of all AI systems employed at the organization, the specific uses of such systems, techniques used, names of the developers/teams and business owners, and risk ratings – measuring, for example, the potential social or financial risks that may come into play should such a system fail. Another process might also evaluate the inputs, and the outputs of the AI system, as well as the AI system itself. Even though data quality requirements are not specific to AI/ML, data quality has significant impact on AI systems, which learn using data and provide output based on that learning. Training data could be assessed for data quality as well as for potential biases the data set may contain. AI system evaluation could involve benchmarking against alternative models and applying known techniques to enable model interpretability, where applicable and feasible. Understanding the factors driving AI systems recommendations could improve trust in the AI systems.</p>
<p><strong>Monitoring for Drift<br />
</strong>Drift may lead to multiple types of errors and risks in AI systems. Poor model accuracy, for example, could sometimes be attributed to the relationship between target variables and independent variables changing with time. As such, drift detection could play an important capability in mitigating some types of AI-related risks, including characteristics that contribute to a model’s security, privacy, and fairness. Monitoring could account for the data received by the model in production and estimate the accuracy of the model, which is one of the ways to provide insight into the “accuracy drift” of the model. Monitoring could also assess if input data significantly deviates from the model’s training data, which could help inform the identification of “data drift.&#8221;</p>
<p>Detecting accuracy drift may be helpful to enterprise applications in that it may identify a decrease in model accuracy before the change results in a significant impact to the business. Accuracy drift can make your model worse. Data drift, on the other hand, helps enterprises understand the change in data characteristics at runtime.</p>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element vc_custom_1601932043880" >
		<div class="wpb_wrapper">
			<h4>5.2 Addressing Discrimination in AI</h4>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p>Most lending institutions employ compliance, fair lending, and system governance teams that review input variables and systems for evidence of discrimination. Eventually, some or even most of this work may be automated and streamlined through technological advances and the use of de-biasing AI (discussed below). Fair AI, nevertheless, may require a human-centric approach. It is unlikely that an automated process could fully replace the generalized knowledge and experience of a well-trained and diverse group reviewing AI systems for potential discrimination bias. Thus, the first line of defense against discriminatory AI typically could include some degree of manual review.</p>
<p>Some recently researched algorithms that diminish discrimination have also been shown to minimize class-control disparities while maintaining the system&#8217;s predictive quality. Mitigation algorithms find the &#8220;optimal&#8221; system for a given level of quality and discrimination measure in order to minimize these disparities. In this, the algorithms attempt to find alternative systems where, for any given level of discrimination, no system can be found with a higher level of quality. Conversely, for any given level of quality, no system can be found that decreases discrimination. Further testing and research need to happen before leveraging such algorithms in a production environment.</p>
<p>Broadly, these methods could be separated into two groups: more traditional methods that search across possible algorithmic and feature specifications in order to find less discriminatory but valid systems, and more recently developed approaches that change the input data or the optimization functions of the algorithms themselves.<a href="#_ftn12" name="_ftnref15">[12]</a></p>
<p>Minimizing disparate impact may focus on feature selection whereby typically one or two variables that drive disparate impact are excluded from the system, while a few other variables are tested as replacements. These methods have been shown to have limited success in complex AI/ML systems.</p>
<p>More recently developed approaches minimize discrimination by focusing on data pre-processing, within-algorithm decision making, and output post-processing.<a href="#_ftn13" name="_ftnref16">[13]</a> Whether these methods are suitable for use in a particular case depends on the legal environment in which the system is used and the system’s usage itself.</p>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element vc_custom_1607631569353" >
		<div class="wpb_wrapper">
			<h4>5.3 Enhancing Interpretability and Explainability</h4>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p>This section focuses on AI use cases where interpretability/explainability is required by law or is otherwise appropriate. AIRS acknowledges that it may not always be necessary or appropriate. As of this writing, there is no commonly agreed upon standard definition of AI explainability.</p>
<p><strong>Ensuring Quality Explanations<br />
</strong>Ensuring that AI/ML explanations (explainability) are both reliable and useful could be a challenge for many organizations. Like the underlying AI/ML systems, for example, AI/ML explanations could be rough approximations, inaccurate, or inconsistent. Inconsistency bears special consideration in financial services, especially in the context of adverse action notices for credit lending decisions. (Posthoc explanation techniques are receiving considerable attention for the generation of adverse action notices. However, a thorough discussion of explanation in the adverse action notice context is outside the scope of this broad report.<a href="#_ftn14" name="_ftnref17">[14]</a>)</p>
<p>Depending on specific implementations, organizations may test explanatory techniques in human evaluation studies or, for accuracy and stability, on simulated data, to potentially reduce risks associated with explainability.</p>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element vc_custom_1607631760499" >
		<div class="wpb_wrapper">
			<h4>5.4 Potential Risk Mitigation</h4>

		</div>
	</div>

	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p>Recent research indicates that providing explanations of how AI systems work, along with predictions, could help malicious actors. To mitigate the potential risks, organizations should only share the minimal information required by respective consumers or as applicable by law. Depending on the implementation and control environment, AI/ML systems trained on sensitive data with predictions accessible to end-users could also be protected using existing security measures, such as real-time anomaly detection, user authentication, and API throttling.<a href="#_ftn15" name="_ftnref20">[15]</a></p>
<p>Traditional strong technology and cyber controls could act as effective risk mitigants for AI implementations. The evolving field of adversarial learning may help with building secure machine learning systems as it matures. Although this is still a field of evolving research, some theoretical mitigation techniques are being further researched in the technology industry. For example, one suggested method for maintaining the privacy of the training data is differential privacy. Differential privacy makes data anonymous by introducing random noise to a dataset, which allows for statistical analysis without any personal information being identifiable. Therefore, the results of the system are similar even if a particular user/data element record is omitted. Although mitigation techniques are still being researched for AI/ML attacks discussed in Section 2, depending on implementations and environment, having strong technology and cyber controls could act as effective mitigation. Prevention of model extraction attacks could potentially be achieved using strong information security practices; however, the identification of an extracted model is possible with a method known as watermarking. In watermarking, the AI/ML system is trained to produce unique outputs for certain inputs. If another system produces the same unique output for the same inputs, it may point to Intellectual Property theft.</p>

		</div>
	</div>
</div></div></div><div class="wpb_column vc_column_container vc_col-sm-1"><div class="vc_column-inner"><div class="wpb_wrapper"></div></div></div></div><div class="vc_row wpb_row vc_row-fluid None" style="None"><div class="wpb_column vc_column_container vc_col-sm-1"><div class="vc_column-inner"><div class="wpb_wrapper"></div></div></div>    <div class="wpb_column vc_column_container vc_col-sm-10"><div class="vc_column-inner"><div class="wpb_wrapper">
	<div class="wpb_text_column wpb_content_element vc_custom_1601932512394" >
		<div class="wpb_wrapper">
			<h2 style="text-align: left;"><strong>6. Acknowledgments</strong></h2>

		</div>
	</div>
<div class="vc_separator wpb_content_element vc_separator_align_center vc_sep_width_10 vc_sep_border_width_4 vc_sep_pos_align_left vc_separator_no_text vc_sep_color_purple wpb_content_element  wpb_content_element" ><span class="vc_sep_holder vc_sep_holder_l"><span class="vc_sep_line"></span></span><span class="vc_sep_holder vc_sep_holder_r"><span class="vc_sep_line"></span></span>
</div>
	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p><strong><em>For any inquiries or questions regarding AIRS or the white paper, please contact Yogesh Mudgal (</em></strong><a href="mailto:yogesh@airsgroup.ai"><strong><em>yogesh@airsgroup.ai</em></strong></a><strong><em>).</em></strong></p>
<p><strong>AIRS would like to thank our authors for their key role in the creation and development of the AI/ML Risk and Governance white paper. </strong></p>
<p>Yogesh Mudgal, AIRS Founder/Curator; Operational Risk Management, Citi<br />
Andrew Burt, Managing Partner at bnh.ai, Chief Legal Officer at Immuta<br />
Ruchir Puri, Chief Scientist, IBM Research, IBM Fellow at IBM<br />
Patrick Hall, Advisor at H2O.ai, Principal Scientist at bnh.ai, Visiting Professor at GWU<br />
Nicholas Schmidt, Director and AI Practice Leader at BLDS, LLC<br />
Anusha Dandapani, United Nations International Computing Centre, Adjunct Faculty at NYU<br />
Anuj Prakash, Model Risk Management, HSBC<br />
Nick Lewins, Financial Services Lead at Microsoft Research<br />
Armando Lemos, Technology Risk at Wells Fargo<br />
Brennan Lodge, Data Science Lead at Goldman Sachs<br />
Kartik Hosanagar, John C. Hower Professor at The Wharton School, Director of Wharton AI for Business<br />
Marina Kaganovich, Director, U.S. CIB Digital Advisory Compliance, BNP Paribas</p>
<p><strong>AIRS would like to thank the following contributors for their valuable insights and contribution to the AI/ML Risk and Governance white paper (in alphabetic order)</strong></p>
<p>Akash Verma, Discover<br />
Andres Fortino, PhD, NYU<br />
Bruno Domingues, Intel<br />
Daragh Morrissey, Microsoft<br />
Kawbena Poku, Discover<br />
Kevin Fitzpatrick, Wells Fargo<br />
Keyvan Kasiri, Bank of Montreal<br />
Kjersten Moody, State Farm<br />
Manan N. Rawal, MUFG<br />
Meeta Dash, Appen<br />
Narahara (Chari) Dingari, PhD, DeutscheBank<br />
Parviz Peiravi, Intel<br />
Patrick Dutton, HSBC<br />
Priti Ved, Bloomberg</p>

		</div>
	</div>
<div class=" vc_call_to_action wpb_content_element vc_cta_btn_pos_right vc_cta_rounded vc_txt_align_left  "><p><strong><em>Contributions are made in an individual capacity and do not represent the views of the institution who the authors and contributors work for or are associated with</em></strong><strong><em>.</em></strong></p>
</div></div></div></div><div class="wpb_column vc_column_container vc_col-sm-1"><div class="vc_column-inner"><div class="wpb_wrapper"></div></div></div></div><div class="vc_row wpb_row vc_row-fluid None" style="None"><div class="wpb_column vc_column_container vc_col-sm-1"><div class="vc_column-inner"><div class="wpb_wrapper"></div></div></div><div class="wpb_column vc_column_container vc_col-sm-10"><div class="vc_column-inner"><div class="wpb_wrapper"><div class="vc_separator wpb_content_element vc_separator_align_center vc_sep_width_100 vc_sep_pos_align_center vc_separator_no_text vc_sep_color_grey wpb_content_element  wpb_content_element" ><span class="vc_sep_holder vc_sep_holder_l"><span class="vc_sep_line"></span></span><span class="vc_sep_holder vc_sep_holder_r"><span class="vc_sep_line"></span></span>
</div>
	<div class="wpb_text_column wpb_content_element" >
		<div class="wpb_wrapper">
			<p><a href="#_ftnref1" name="_ftn1">[1]</a> AIRS is an informal group of practitioners and academics from varied backgrounds, including technology risk, information security, legal, privacy, architects, model risk management, and others, working for financial and technology organizations and academic institutions. The AIRS working group, based in New York, was initiated in early 2019. It has grown beyond the original members to around 40 members from dozens of institutions.</p>
<p><a href="#_ftnref2" name="_ftn2">[2]</a> FINRA’s Report on the Use of AI in the Securities Industry: <a href="https://www.finra.org/sites/default/files/2020-06/ai-report-061020.pdf" target="_blank" rel="noopener noreferrer">https://www.finra.org/sites/default/files/2020-06/ai-report-061020.pdf</a></p>
<p><a href="#_ftnref3" name="_ftn3">[3]</a> Financial Stability Board, Artificial Intelligence and Machine Learning: <a href="https://www.fsb.org/wp-content/uploads/P011117.pdf" target="_blank" rel="noopener noreferrer">https://www.fsb.org/wp-content/uploads/P011117.pdf</a>. <u> See also, Brainard, “What Are We Learning about Artificial Intelligence in Financial Services? </u><a href="https://www.federalreserve.gov/newsevents/speech/files/brainard20181113a.pdf" target="_blank" rel="noopener noreferrer">https://www.federalreserve.gov/newsevents/speech/files/brainard20181113a.pdf</a></p>
<p><a href="#_ftnref4" name="_ftn4">[4]</a> See D. Sculley et al., “Machine Learning: The High Interest Credit Card of Technical Debt,” SE4ML: Software Engineering for Machine Learning (NIPS 2014 Workshop), available at <a href="https://research.google/pubs/pub43146/" target="_blank" rel="noopener noreferrer">https://research.google/pubs/pub43146/</a>.</p>
<p><a href="#_ftnref5" name="_ftn5">[5]</a> Our results appear to align with broader public industry surveys and trends as well. See, for example, “Transforming Paradigms: A Global AI in Financial Services Survey,” World Economic Forum, January 2020, available at <a href="http://www3.weforum.org/docs/WEF_AI_in_Financial_Services_Survey.pdf" target="_blank" rel="noopener noreferrer">http://www3.weforum.org/docs/WEF_AI_in_Financial_Services_Survey.pdf</a>.</p>
<p><a href="#_ftnref6" name="_ftn6">[6]</a> See, for example, SR 11-7 Guidance on Model Risk Management SR 11-7, April 2011, available at <a href="https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm" target="_blank" rel="noopener noreferrer">https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm</a>.</p>
<p><a href="#_ftnref7" name="_ftn7">[7]</a> See NY DFS Insurance Circular No. 1, January 18, 2019, <a href="https://www.dfs.ny.gov/industry_guidance/circular_letters/cl2019_01" target="_blank" rel="noopener noreferrer">https://www.dfs.ny.gov/industry_guidance/circular_letters/cl2019_01</a></p>
<p><a href="#_ftnref8" name="_ftn8">[8]</a> See “How can we ensure that Big Data does not make us prisoners of technology?”, July 11, 2018, <a href="https://www.fca.org.uk/news/speeches/how-can-we-ensure-big-data-does-not-make-us-prisoners-technology" target="_blank" rel="noopener noreferrer">https://www.fca.org.uk/news/speeches/how-can-we-ensure-big-data-does-not-make-us-prisoners-technology</a></p>
<p><a href="#_ftnref9" name="_ftn9">[9]</a> See, for example, XGBoost, h2o’s GBM or Microsoft’s InterpretML toolkit, available at <a href="https://xgboost.readthedocs.io/en/latest/tutorials/monotonic.html" target="_blank" rel="noopener noreferrer">https://xgboost.readthedocs.io/en/latest/tutorials/monotonic.html</a>, <a href="https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/H2O_tutorial_gbm_monotonicity.ipynb" target="_blank" rel="noopener noreferrer">https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/H2O_tutorial_gbm_monotonicity.ipynb</a> and <a href="https://interpret.ml/" target="_blank" rel="noopener noreferrer">https://interpret.ml/</a> respectively.</p>
<p><a href="#_ftnref10" name="_ftn10">[10]</a> Jie Chen, “Deep Insights into Explainability and Interpretability of Machine Learning Algorithms and Applications to Risk Management,” JSM 2019 Online, available at <a href="https://ww2.amstat.org/meetings/jsm/2019/onlineprogram/AbstractDetails.cfm?abstractid=303053" target="_blank" rel="noopener noreferrer">https://ww2.amstat.org/meetings/jsm/2019/onlineprogram/AbstractDetails.cfm?abstractid=303053</a>.</p>
<p><a href="#_ftnref11" name="_ftn11">[11]</a> This three-part approach aligns, broadly, with the key sources of risk set forth in Subsection 2.1.</p>
<p><a href="#_ftnref12" name="_ftn12">[12]</a> For a further discussion of potential and utilized techniques that can mitigate the disparate impact in financial services, see  Nicholas Schmidt and Bryce Stephens, &#8220;An Introduction to Artificial Intelligence and Solutions to the Problem of Algorithmic Discrimination,&#8221; Conference on Consumer Finance Law (CCFL) Quarterly Report, Volume 73, Number 2 (October 2019) available at <a href="https://arxiv.org/abs/1911.05755" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1911.05755</a>.</p>
<p><a href="#_ftnref13" name="_ftn13">[13]</a> IBM&#8217;s AI Fairness 360 toolkit and Microsoft’s FairLearn toolkit, for example, provide open-source compilations of many of the metrics and techniques that have been developed recently, available at <a href="https://github.com/IBM/AIF360" target="_blank" rel="noopener noreferrer">https://github.com/IBM/AIF360</a> and https://fairlearn.ai respectively.</p>
<p><a href="#_ftnref14" name="_ftn14">[14]</a> For a more detailed discussion of adverse action notices and posthoc explanation see Navdeep Gill et al., “A Responsible Machine Learning Workflow with Focus on Interpretable Models, Posthoc Explanation, and Discrimination Testing,” <em>Information</em> 11(3), March 2020, available at <a href="https://www.mdpi.com/2078-2489/11/3/137" target="_blank" rel="noopener noreferrer">https://www.mdpi.com/2078-2489/11/3/137</a>.</p>
<p><a href="#_ftnref15" name="_ftn15">[15]</a> For a high-level discussion of attacks on AI systems and proposed mitigation tactics, see Sophie Stalla-Bourdillon et al., “Warning Signs: Identifying Privacy and Security Risks to Machine Learning Systems,” Future of Privacy Forum, September 2019,  available at <a href="https://fpf.org/wp-content/uploads/2019/09/FPF_WarningSigns_Report.pdf" target="_blank" rel="noopener noreferrer">https://fpf.org/wp-content/uploads/2019/09/FPF_WarningSigns_Report.pdf</a>.</p>

		</div>
	</div>
</div></div></div><div class="wpb_column vc_column_container vc_col-sm-1"><div class="vc_column-inner"><div class="wpb_wrapper"></div></div></div></div>
</div>					</div><!-- .post-entry -->
	
<footer class="post-data">
		 	<span class="posted-on">
			<i class="icon-calendar mini-before text-top"></i><a href="https://ai.wharton.upenn.edu/white-paper/artificial-intelligence-risk-governance/" title="4:48 pm" rel="bookmark">
			<time class="entry-date" datetime="2023-01-11T16:48:16-05:00">January 11, 2023</time>
		</a>		</span>
			<span class="cat-links">
					<span class="cat-links"> &nbsp; | &nbsp; <i class="icon-archive mini-before"></i>
			 <a href="https://ai.wharton.upenn.edu/category/focus-areas/policy-ethics-governance/">Policy Ethics Governance</a>, <a href="https://ai.wharton.upenn.edu/category/white-paper/">White Paper</a>		</span>
			</span>
			<span class="tags-links">
					</span>
	
	<div class="entry-meta">
			</div><!-- .entry-meta -->
</footer><!-- .post-data -->
	</article><!-- #post-## -->

					
															
				
			</main><!-- #main -->

			<div class="sidebar sidebar--expanded sidebar--flyout">
	<div class="sidebar-inner sidebar-inner--menu">
		<aside>
			<style>.sidebar{display:none!important}</style>		</aside>
	</div>
	<div id="widgets" class="widget-area sidebar-inner sidebar-inner--widgets"  role="complementary" itemscope="itemscope" itemtype="http://schema.org/WPSideBar">

					</div><!-- end of #widgets -->
</div>
	</div><!-- #content -->


		<a data-scroll href="#header" title="Scroll to top link." id="scrollTop" class="martech-scroll-top">
			<i class="icon-arrow-up2"></i>
			<span>Back To Top</span>
		</a>
		</div><!-- end of #wrapper -->
</div><!-- end of #container -->

<footer id="footer" class="site-footer" role="contentinfo" itemscope="itemscope" itemtype="https://schema.org/WPFooter">
	<div id="martech-footer-wrapper" class="wrapper footer-row">
		<div class="footer-primary"> 		<a href="#" title="Additional Links" class="footer-primary-toggle">Additional Links <i class="icon-arrow-down3"></i></a>
				<nav class="masonry additional-links" aria-label="Footer navigation links">
						<ul class="footer-menu masonry-brick">
				<div class="list-title">Wharton Human-AI Research</div>				<div class="menu-header-navigation-container"><ul id="menu-header-navigation" class="masonry-menu"><li id="menu-item-9673" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-9673"><a href="https://ai.wharton.upenn.edu/">Home</a></li>
<li id="menu-item-8832" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-8832"><a href="https://ai.wharton.upenn.edu/about/people/">People</a></li>
<li id="menu-item-114" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-114"><a href="https://ai.wharton.upenn.edu/research/">Research</a></li>
<li id="menu-item-8838" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-8838"><a href="https://ai.wharton.upenn.edu/education/">Education</a></li>
<li id="menu-item-8899" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-8899"><a href="https://ai.wharton.upenn.edu/get-involved/">Get Involved</a></li>
<li id="menu-item-10143" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-10143"><a href="#">Content</a></li>
<li id="menu-item-3387" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-3387"><a href="https://ai.wharton.upenn.edu/events/">Events</a></li>
</ul></div>
			</ul>
									<ul class="footer-menu masonry-brick grid-sizer">
				<li class="list-title">Programs</li>
				<li><a href="https://undergrad.wharton.upenn.edu/">Undergraduate</a></li>
				<li><a href="https://jacobs-msqf.wharton.upenn.edu/">Jacobs MSQF</a></li>
				<li><a href="https://mba.wharton.upenn.edu">MBA</a></li>
				<li><a href="https://executivemba.wharton.upenn.edu">EMBA</a></li>
				<li><a href="https://doctoral.wharton.upenn.edu/">Doctorate</a></li>
				<li><a href="https://executiveeducation.wharton.upenn.edu/">Executive Education</a></li>
				<li><a href="https://executiveeducation.wharton.upenn.edu/online-learning/">Wharton Online</a></li>
			</ul>
			<ul class="footer-menu masonry-brick grid-sizer">
				<li class="list-title">Locations</li>
				<li><a href="https://www.wharton.upenn.edu/philadelphia-campus/">Philadelphia</a></li>
				<li><a href="https://sf.wharton.upenn.edu/">San Francisco</a></li>
				<li><a href="https://global.upenn.edu/pwcc/">Beijing</a></li>
			</ul>
			<ul class="footer-menu masonry-brick">
				<li class="list-title">The Power of Wharton</li>
				<li><a href="https://global.wharton.upenn.edu/">Global Influence</a></li>
				<li><a href="https://ai-analytics.wharton.upenn.edu/">AI & Analytics</a></li>
				<li><a href="https://venturelab.upenn.edu/">Entrepreneurship & Innovation</a></li>
			</ul>
			<ul class="footer-menu masonry-brick">
				<li class="list-title">Featured</li>
				<li><a href="https://giving.aws.cloud.upenn.edu/fund?program=WHA&fund=342105&appeal=WHAWEB">Give to Wharton</a></li>
				<li><a href="https://alumni.wharton.upenn.edu/">Alumni</a></li>
				<li><a href="https://knowledge.wharton.upenn.edu/">Knowledge at Wharton</a></li>
				<li><a href="https://recruiters-corp.wharton.upenn.edu/">Recruiters & Corporations</a></li>
			</ul>
			<ul class="footer-menu masonry-brick">
				<li class="list-title">Wharton</li>
				<li><a href="https://www.wharton.upenn.edu/faculty-directory/">Faculty</a></li>
				<li><a href="https://www.wharton.upenn.edu/about-wharton/">About Us</a></li>
				<li><a href="https://research.wharton.upenn.edu/research-centers/">Research Centers</a></li>
				<li><a href="https://www.wharton.upenn.edu/departments/">Departments</a></li>
			</ul>
			<ul class="footer-menu masonry-brick">
				<li class="list-title">Resources</li>
				<li><a href="https://www.wharton.upenn.edu/contact-wharton">Contact Us</a></li>
				<li><a href="https://news.wharton.upenn.edu/">News</a></li>
				<li><a href="https://inside.wharton.upenn.edu/">Faculty &amp; Staff</a></li>
			</ul>
		</nav>
	</div>
	<div class="footer-secondary">

		<div class="copyright">
			<a class="footer-logo hide-650" href="https://www.upenn.edu"><img src="https://ai.wharton.upenn.edu/wp-content/plugins/martech-chupacabra/includes/images/penn-logo-white.svg" class="martech-footer-logo" alt="The University of Pennsylvania" ></a><div class="btn-wrap"><ul class="social-icons"><li class="twitter-icon"><a href="https://twitter.com/AIatWharton" target="_blank"><span class="accessible-label">Twitter</span></a></li><li class="linkedin-icon"><a href="https://www.linkedin.com/groups/3740981/" target="_blank"><span class="accessible-label">LinkedIn</span></a></li><li class="youtube-icon"><a href="https://www.youtube.com/channel/UCyW3M3ghGY9SjfsYxHu1jDA" target="_blank"><span class="accessible-label">YouTube</span></a></li></ul><!-- .social-icons --><span class="give"><a href="https://giving.aws.cloud.upenn.edu/fund?program=WHA&fund=342105&appeal=WHAWEB" class="btn--secondary vc_btn vc_btn_red vc_btn-sm" target="_blank" title="Give to Wharton">Support Wharton</a></span></div><a href="https://apps.wharton.upenn.edu/authentication?service=wordpress&returnURL=https://ai.wharton.upenn.edu/cms-login">&copy;</a>2026&nbsp;<a href="https://www.wharton.upenn.edu/">The Wharton School,</a> &nbsp;<a href="https://www.upenn.edu/">The University of Pennsylvania</a> &nbsp;<span>| &nbsp;<a href="https://ai.wharton.upenn.edu" title="Wharton Human-AI Research">Wharton Human-AI Research</a>&nbsp; </span>| &nbsp;<a href="https://www.wharton.upenn.edu/privacy-policy/">Privacy&nbsp;Policy</a>&nbsp; | &nbsp;<a href="https://accessibility.web-resources.upenn.edu/get-help" target="_blank">Report&nbsp;Accessibility&nbsp;Issues&nbsp;and&nbsp;Get&nbsp;Help</a>		</div>

	</div>
</div><!--/#martech-footer-wrapper-->
	</footer><!-- #footer -->
<script type="speculationrules">
{"prefetch":[{"source":"document","where":{"and":[{"href_matches":"\/*"},{"not":{"href_matches":["\/wp-*.php","\/wp-admin\/*","\/wp-content\/uploads\/*","\/wp-content\/*","\/wp-content\/plugins\/*","\/wp-content\/themes\/Chupe2-0-child\/*","\/wp-content\/themes\/responsive-mobile\/*","\/*\\?(.+)"]}},{"not":{"selector_matches":"a[rel~=\"nofollow\"]"}},{"not":{"selector_matches":".no-prefetch, .no-prefetch a"}}]},"eagerness":"conservative"}]}
</script>
<script type="text/html" id="wpb-modifications"> window.wpbCustomElement = 1; </script>      <script type="text/javascript">
      (function ($) {
        $(window).load(function(){
          $('#footer-2-col').masonry({
            columnWidth:  '.grid-sizer',
            itemSelector: '.menu-item'
          });
        });
        })(jQuery);
      </script>
    <script type="text/javascript" src="https://ai.wharton.upenn.edu/wp-content/plugins/honeypot/includes/js/wpa.js?ver=2.2.10" id="wpascript-js"></script>
<script type="text/javascript" id="wpascript-js-after">
/* <![CDATA[ */
wpa_field_info = {"wpa_field_name":"wnueph242","wpa_field_value":411285,"wpa_add_test":"no"}
/* ]]> */
</script>
<script type="text/javascript" src="https://ai.wharton.upenn.edu/wp-includes/js/imagesloaded.min.js?ver=5.0.0" id="imagesloaded-js"></script>
<script type="text/javascript" src="https://ai.wharton.upenn.edu/wp-includes/js/masonry.min.js?ver=4.2.2" id="masonry-js"></script>
<script type="text/javascript" src="https://ai.wharton.upenn.edu/wp-includes/js/jquery/jquery.masonry.min.js?ver=3.1.2b" id="jquery-masonry-js"></script>
<script type="text/javascript" src="https://ai.wharton.upenn.edu/wp-content/plugins/wp-armour-extended/includes/js/wpae.js?ver=1.38" id="wpaescript-js"></script>
<script type="text/javascript" src="https://ai.wharton.upenn.edu/wp-content/themes/responsive-mobile/js/responsive-scripts.min.js?ver=1.2.5" id="responsive-scripts-js"></script>
<script type="text/javascript" src="https://ai.wharton.upenn.edu/wp-includes/js/hoverIntent.min.js?ver=1.10.2" id="hoverIntent-js"></script>
<script type="text/javascript" id="martech-v3-script-js-extra">
/* <![CDATA[ */
var ajax_object = {"ajaxurl":"https:\/\/ai.wharton.upenn.edu\/wp-admin\/admin-ajax.php","ajaxnonce":"1d78e4dee0"};
/* ]]> */
</script>
<script type="text/javascript" src="https://ai.wharton.upenn.edu/wp-content/plugins/martech-chupacabra/includes/js/scripts.min.js?ver=3.49.4" id="martech-v3-script-js"></script>
<script type="text/javascript" src="https://ai.wharton.upenn.edu/wp-content/plugins/js_composer/assets/js/dist/js_composer_front.min.js?ver=8.7.1" id="wpb_composer_front_js-js"></script>
<script></script></body>
</html>